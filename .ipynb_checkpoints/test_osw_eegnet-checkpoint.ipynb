{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7da84a4a-fdad-4369-b05d-76e9b1bd85f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for EEGNet:\n\tMissing key(s) in state_dict: \"separableConv1.weight\", \"separableConv2.weight\", \"batchnorm4.weight\", \"batchnorm4.bias\", \"batchnorm4.running_mean\", \"batchnorm4.running_var\". \n\tUnexpected key(s) in state_dict: \"separableConv.weight\". \n\tsize mismatch for dense.weight: copying a param with shape torch.Size([2, 240]) from checkpoint, the shape in current model is torch.Size([2, 16]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 148\u001b[0m\n\u001b[0;32m    146\u001b[0m model \u001b[38;5;241m=\u001b[39m EEGNet(nb_classes\u001b[38;5;241m=\u001b[39mnb_classes)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    147\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m--> 148\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m model\u001b[38;5;241m.\u001b[39meval\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Move the model to the appropriate device\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\detectlie\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EEGNet:\n\tMissing key(s) in state_dict: \"separableConv1.weight\", \"separableConv2.weight\", \"batchnorm4.weight\", \"batchnorm4.bias\", \"batchnorm4.running_mean\", \"batchnorm4.running_var\". \n\tUnexpected key(s) in state_dict: \"separableConv.weight\". \n\tsize mismatch for dense.weight: copying a param with shape torch.Size([2, 240]) from checkpoint, the shape in current model is torch.Size([2, 16])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, accuracy_score\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Constants\n",
    "EEG_DATA_DIR = r'C:\\Users\\User\\Documents\\Lie detect data\\TestData-1'\n",
    "max_length = 500  # Define maximum length for padding\n",
    "\n",
    "# Define EEGNet model\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, nb_classes, Chans=65, Samples=500, dropoutRate=0.3, \n",
    "                 kernLength=250, F1=8, D=2, F2=None, norm_rate=0.25, dropoutType='Dropout'):\n",
    "        super(EEGNet, self).__init__()\n",
    "        if F2 is None:\n",
    "            F2 = F1 * D\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, F1, (1, kernLength), padding='same', bias=False)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(F1)\n",
    "        \n",
    "        self.depthwiseConv = nn.Conv2d(F1, F1 * D, (Chans, 1), groups=F1, bias=False)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(F1 * D)\n",
    "        \n",
    "        self.averagePool1 = nn.AvgPool2d((1, 4))\n",
    "        \n",
    "        if dropoutType == 'SpatialDropout2D':\n",
    "            self.dropout1 = nn.Dropout2d(dropoutRate)\n",
    "        elif dropoutType == 'Dropout':\n",
    "            self.dropout1 = nn.Dropout(dropoutRate)\n",
    "        else:\n",
    "            raise ValueError('dropoutType must be one of SpatialDropout2D or Dropout')\n",
    "        \n",
    "        self.separableConv = nn.Conv2d(F1 * D, F2, (1, 16), padding='same', bias=False)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(F2)\n",
    "        \n",
    "        self.averagePool2 = nn.AvgPool2d((1, 8))\n",
    "        \n",
    "        if dropoutType == 'SpatialDropout2D':\n",
    "            self.dropout2 = nn.Dropout2d(dropoutRate)\n",
    "        elif dropoutType == 'Dropout':\n",
    "            self.dropout2 = nn.Dropout(dropoutRate)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Linear(F2 * (Samples // 32), nb_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        \n",
    "        x = self.depthwiseConv(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        x = self.averagePool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.separableConv(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        x = self.averagePool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "# Function to load and label data (same as in your training script)\n",
    "def load_data(data_dir, max_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    file_list = os.listdir(data_dir)\n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        label = 1 if 'truth' in file else 0\n",
    "        if data.shape[1] > max_length:\n",
    "            processed_data = data[:, :max_length]  # Cut data if it exceeds max_length\n",
    "        else:\n",
    "            processed_data = np.zeros((data.shape[0], max_length))\n",
    "            processed_data[:, :data.shape[1]] = data  # Pad data if it is shorter than max_length\n",
    "        X.append(processed_data)\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure the data is reshaped to [1, Chans, Samples]\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32).unsqueeze(0), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load the saved model\n",
    "    model_path = r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\fold3_model_fold_0.pth'\n",
    "    nb_classes = 2\n",
    "    model = EEGNet(nb_classes=nb_classes).to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval\n",
    "\n",
    "    # Move the model to the appropriate device\n",
    "    model.to(device)\n",
    "\n",
    "    # Load and preprocess the training data to create the scaler\n",
    "    X, y = load_data(EEG_DATA_DIR, max_length)\n",
    "\n",
    "    # Load the scaler\n",
    "    with open(r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\simpleEEGNet_scaler.pkl', 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "    X = scaler.transform(X.reshape(X.shape[0], -1))\n",
    "    X = X.reshape(-1, 65, max_length)\n",
    "    \n",
    "\n",
    "    test_dataset = EEGDataset(X, y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        print(f\"batch shape: {X_batch.shape}\")\n",
    "        \n",
    "        X_batch = X_batch.to(device)\n",
    "        labels = y_batch.to(device)\n",
    "        \n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays for metric calculations\n",
    "all_labels = np.array(all_labels)\n",
    "all_predictions = np.array(all_predictions)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "auc = roc_auc_score(all_labels, all_predictions)\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f'Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf969e6f-a1aa-460c-95df-dddb9b77f486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea5dbca-54c6-4a17-aa0b-ebe1f71e4aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
