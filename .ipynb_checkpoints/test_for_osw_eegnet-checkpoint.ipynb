{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7da84a4a-fdad-4369-b05d-76e9b1bd85f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: The model predicts: Truth\n",
      "Sample 2: The model predicts: Truth\n",
      "Sample 3: The model predicts: Truth\n",
      "Sample 4: The model predicts: Truth\n",
      "Sample 5: The model predicts: Truth\n",
      "Sample 6: The model predicts: Truth\n",
      "Sample 7: The model predicts: Truth\n",
      "Sample 8: The model predicts: Truth\n",
      "\n",
      "Total samples: 8\n",
      "Predicted as Lie: 0 (0.00%)\n",
      "Predicted as Truth: 8 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define EEGNet model (same as in your training script)\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, 63), padding='same')\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "        self.depthwiseConv2d = nn.Conv2d(16, 32, (65, 1), groups=16, padding='same')\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
    "        self.activation = nn.ELU()\n",
    "        self.pooling = nn.AvgPool2d((1, 4))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(32 * 65 * 31, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.depthwiseConv2d(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Function to load and label data (same as in your training script)\n",
    "def load_data(data_dir):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for file_name in os.listdir(data_dir):\n",
    "        if file_name.endswith('.pkl'):\n",
    "            file_path = os.path.join(data_dir, file_name)\n",
    "            data = pd.read_pickle(file_path)\n",
    "            label = 0 if 'lie' in file_name else 1\n",
    "            X.append(data)\n",
    "            y.extend([label] * data.shape[0])\n",
    "    \n",
    "    X = np.vstack(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "# Function to create and fit the scaler\n",
    "def create_scaler(X):\n",
    "    scaler = StandardScaler()\n",
    "    X_flat = X.reshape(X.shape[0], -1)  # Reshape to (n_samples, n_features)\n",
    "    scaler.fit(X_flat)\n",
    "    return scaler\n",
    "\n",
    "# Function to load and preprocess new data\n",
    "def load_and_preprocess_data(file_path, scaler):\n",
    "    # Load your data\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = np.load(f, allow_pickle=True)\n",
    "    \n",
    "    # Ensure the data is 2D (samples, features)\n",
    "    if data.ndim == 3:\n",
    "        data = data.reshape(data.shape[0], -1)\n",
    "    elif data.ndim == 2:\n",
    "        data = data.reshape(1, -1)\n",
    "    \n",
    "    # Scale the data\n",
    "    data_scaled = scaler.transform(data)\n",
    "    \n",
    "    # Reshape back to (samples, channels, time points)\n",
    "    data_reshaped = data_scaled.reshape(-1, 65, 125)\n",
    "    \n",
    "    return torch.tensor(data_reshaped, dtype=torch.float32)\n",
    "\n",
    "# Function to make predictions\n",
    "def predict(model, data, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data.to(device))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    return predicted.cpu().numpy()\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load the saved model\n",
    "    model_path = r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\BEST_OSW_EEGNet.pth'\n",
    "    model = EEGNet(num_classes=2).to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Load and preprocess the training data to create the scaler\n",
    "    data_dir = r'C:\\Users\\User\\Documents\\Lie detect data\\AugmentedEEGData'\n",
    "    X, _ = load_data(data_dir)\n",
    "    scaler = create_scaler(X)\n",
    "\n",
    "    # Path to your new, unseen data\n",
    "    new_data_path = r'C:\\Users\\User\\Documents\\Lie detect data\\AugmentedEEGData\\augmented_truth_6.pkl'\n",
    "\n",
    "    # Load and preprocess the new data\n",
    "    new_data = load_and_preprocess_data(new_data_path, scaler)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = predict(model, new_data, device)\n",
    "\n",
    "    # Print results\n",
    "    for i, pred in enumerate(predictions):\n",
    "        result = \"Lie\" if pred == 0 else \"Truth\"\n",
    "        print(f\"Sample {i+1}: The model predicts: {result}\")\n",
    "\n",
    "    # Print overall statistics\n",
    "    lie_count = np.sum(predictions == 0)\n",
    "    truth_count = np.sum(predictions == 1)\n",
    "    total_samples = len(predictions)\n",
    "\n",
    "    print(f\"\\nTotal samples: {total_samples}\")\n",
    "    print(f\"Predicted as Lie: {lie_count} ({lie_count/total_samples*100:.2f}%)\")\n",
    "    print(f\"Predicted as Truth: {truth_count} ({truth_count/total_samples*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf969e6f-a1aa-460c-95df-dddb9b77f486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
