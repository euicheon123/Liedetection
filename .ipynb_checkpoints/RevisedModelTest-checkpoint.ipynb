{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b170e6c-139c-436a-bf23-da8f138cb825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape: torch.Size([32, 1, 65, 500])\n",
      "batch shape: torch.Size([32, 1, 65, 500])\n",
      "batch shape: torch.Size([20, 1, 65, 500])\n",
      "Accuracy: 0.4762\n",
      "Precision: 0.5833333333333334, Recall: 0.6481481481481481, F1-score: 0.6140350877192983, AUC: 0.40740740740740744\n",
      "Confusion Matrix:\n",
      "[[ 5 25]\n",
      " [19 35]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, accuracy_score\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Constants\n",
    "EEG_DATA_DIR = r'C:\\Users\\User\\Documents\\Lie detect data\\TestData-1'\n",
    "max_length = 500  # Define maximum length for padding\n",
    "\n",
    "# Define EEGNet model\n",
    "class Conv2dWithConstraint(nn.Conv2d):\n",
    "    def __init__(self, *args, max_norm=1, **kwargs):\n",
    "        self.max_norm = max_norm\n",
    "        super(Conv2dWithConstraint, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data = torch.renorm(\n",
    "            self.weight.data, p=2, dim=0, maxnorm=self.max_norm\n",
    "        )\n",
    "        return super(Conv2dWithConstraint, self).forward(x)\n",
    "\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def InitialBlocks(self, dropoutRate, *args, **kwargs):\n",
    "        block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, self.F1, (1, self.kernelLength), stride=1, padding=(0, self.kernelLength // 2), bias=False),\n",
    "            nn.BatchNorm2d(self.F1, momentum=0.01, affine=True, eps=1e-3),\n",
    "\n",
    "            # DepthwiseConv2D =======================\n",
    "            Conv2dWithConstraint(self.F1, self.F1 * self.D, (self.channels, 1), max_norm=1, stride=1, padding=(0, 0),\n",
    "                                 groups=self.F1, bias=False),\n",
    "            # ========================================\n",
    "\n",
    "            nn.BatchNorm2d(self.F1 * self.D, momentum=0.01, affine=True, eps=1e-3),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 4), stride=4),\n",
    "            nn.Dropout(p=dropoutRate))\n",
    "        block2 = nn.Sequential(\n",
    "            # SeparableConv2D =======================\n",
    "            nn.Conv2d(self.F1 * self.D, self.F1 * self.D, (1, self.kernelLength2), stride=1,\n",
    "                      padding=(0, self.kernelLength2 // 2), bias=False, groups=self.F1 * self.D),\n",
    "            nn.Conv2d(self.F1 * self.D, self.F2, 1, padding=(0, 0), groups=1, bias=False, stride=1),\n",
    "            # ========================================\n",
    "\n",
    "            nn.BatchNorm2d(self.F2, momentum=0.01, affine=True, eps=1e-3),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 8), stride=8),\n",
    "            nn.Dropout(p=dropoutRate))\n",
    "        return nn.Sequential(block1, block2)\n",
    "\n",
    "\n",
    "    def ClassifierBlock(self, inputSize, n_classes):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(inputSize, n_classes, bias=False),\n",
    "            nn.Softmax(dim=1))\n",
    "\n",
    "    def CalculateOutSize(self, model, channels, samples):\n",
    "        '''\n",
    "        Calculate the output based on input size.\n",
    "        model is from nn.Module and inputSize is a array.\n",
    "        '''\n",
    "        data = torch.rand(1, 1, channels, samples)\n",
    "        model.eval()\n",
    "        out = model(data).shape\n",
    "        return out[2:]\n",
    "\n",
    "    def __init__(self, n_classes=2, channels=65, samples=500,\n",
    "                 dropoutRate=0.5, kernelLength=128, kernelLength2=32, F1=8,\n",
    "                 D=2, F2=16):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.F1 = F1\n",
    "        self.F2 = F2\n",
    "        self.D = D\n",
    "        self.samples = samples\n",
    "        self.n_classes = n_classes\n",
    "        self.channels = channels\n",
    "        self.kernelLength = kernelLength\n",
    "        self.kernelLength2 = kernelLength2\n",
    "        self.dropoutRate = dropoutRate\n",
    "\n",
    "        self.blocks = self.InitialBlocks(dropoutRate)\n",
    "        self.blockOutputSize = self.CalculateOutSize(self.blocks, channels, samples)\n",
    "        self.classifierBlock = self.ClassifierBlock(self.F2 * self.blockOutputSize[1], n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        x = x.view(x.size()[0], -1)  # Flatten\n",
    "        x = self.classifierBlock(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def categorical_cross_entropy(y_pred, y_true):\n",
    "    # y_pred = y_pred.cuda()\n",
    "    # y_true = y_true.cuda()\n",
    "    y_pred = torch.clamp(y_pred, 1e-9, 1 - 1e-9)\n",
    "    return -(y_true * torch.log(y_pred)).sum(dim=1).mean()\n",
    "\n",
    "def torch_summarize(model, show_weights=True, show_parameters=True):\n",
    "    \"\"\"Summarizes torch model by showing trainable parameters and weights.\"\"\"\n",
    "    tmpstr = model.__class__.__name__ + ' (\\n'\n",
    "    for key, module in model._modules.items():\n",
    "        # if it contains layers let call it recursively to get params and weights\n",
    "        if type(module) in [\n",
    "            torch.nn.modules.container.Container,\n",
    "            torch.nn.modules.container.Sequential\n",
    "        ]:\n",
    "            modstr = torch_summarize(module)\n",
    "        else:\n",
    "            modstr = module.__repr__()\n",
    "        modstr = _addindent(modstr, 2)\n",
    "\n",
    "        params = sum([np.prod(p.size()) for p in module.parameters()])\n",
    "        weights = tuple([tuple(p.size()) for p in module.parameters()])\n",
    "\n",
    "        tmpstr += '  (' + key + '): ' + modstr\n",
    "        if show_weights:\n",
    "            tmpstr += ', weights={}'.format(weights)\n",
    "        if show_parameters:\n",
    "            tmpstr +=  ', parameters={}'.format(params)\n",
    "        tmpstr += '\\n'\n",
    "\n",
    "    tmpstr = tmpstr + ')'\n",
    "    return tmpstr\n",
    "        \n",
    "# Function to load and label data (same as in your training script)\n",
    "def load_data(data_dir, max_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    file_list = os.listdir(data_dir)\n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        label = 1 if 'truth' in file else 0\n",
    "        if data.shape[1] > max_length:\n",
    "            processed_data = data[:, :max_length]  # Cut data if it exceeds max_length\n",
    "        else:\n",
    "            processed_data = np.zeros((data.shape[0], max_length))\n",
    "            processed_data[:, :data.shape[1]] = data  # Pad data if it is shorter than max_length\n",
    "        X.append(processed_data)\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure the data is reshaped to [1, Chans, Samples]\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32).unsqueeze(0), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load the saved model\n",
    "    model_path = r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\revise_model_fold_4.pth'\n",
    "    model = EEGNet().to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval\n",
    "\n",
    "    # Move the model to the appropriate device\n",
    "    model.to(device)\n",
    "\n",
    "    # Load and preprocess the training data to create the scaler\n",
    "    X, y = load_data(EEG_DATA_DIR, max_length)\n",
    "\n",
    "    # Load the scaler\n",
    "    with open(r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\RevisedEEGNet_scaler.pkl', 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "    X = scaler.transform(X.reshape(X.shape[0], -1))\n",
    "    X = X.reshape(-1, 65, max_length)\n",
    "    \n",
    "\n",
    "    test_dataset = EEGDataset(X, y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        print(f\"batch shape: {X_batch.shape}\")\n",
    "        \n",
    "        X_batch = X_batch.to(device)\n",
    "        labels = y_batch.to(device)\n",
    "        \n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays for metric calculations\n",
    "all_labels = np.array(all_labels)\n",
    "all_predictions = np.array(all_predictions)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "auc = roc_auc_score(all_labels, all_predictions)\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f'Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee996a90-92a7-469b-9987-4017a71a518d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
