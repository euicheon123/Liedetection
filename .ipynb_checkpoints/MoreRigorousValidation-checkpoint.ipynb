{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7bb66d-7268-4596-9d35-bb1e3ddcfd74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\detectlie\\Lib\\site-packages\\torch\\nn\\init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/100, Train Loss: 0.5606, Val Loss: 0.6740\n",
      "Epoch 2/100, Train Loss: 0.5851, Val Loss: 0.6609\n",
      "Epoch 3/100, Train Loss: 0.5299, Val Loss: 0.5815\n",
      "Epoch 4/100, Train Loss: 0.3915, Val Loss: 0.5838\n",
      "Epoch 5/100, Train Loss: 0.3885, Val Loss: 0.5520\n",
      "Epoch 6/100, Train Loss: 0.3276, Val Loss: 0.5116\n",
      "Epoch 7/100, Train Loss: 0.2776, Val Loss: 0.5153\n",
      "Epoch 8/100, Train Loss: 0.2354, Val Loss: 0.5340\n",
      "Epoch 9/100, Train Loss: 0.1888, Val Loss: 0.5611\n",
      "Epoch 10/100, Train Loss: 0.2999, Val Loss: 0.5503\n",
      "Epoch 11/100, Train Loss: 0.1625, Val Loss: 0.5024\n",
      "Epoch 12/100, Train Loss: 0.2183, Val Loss: 0.5188\n",
      "Epoch 13/100, Train Loss: 0.1976, Val Loss: 0.5110\n",
      "Epoch 14/100, Train Loss: 0.1747, Val Loss: 0.5162\n",
      "Epoch 15/100, Train Loss: 0.1714, Val Loss: 0.5373\n",
      "Epoch 16/100, Train Loss: 0.1682, Val Loss: 0.4942\n",
      "Epoch 17/100, Train Loss: 0.2229, Val Loss: 0.5259\n",
      "Epoch 18/100, Train Loss: 0.1985, Val Loss: 0.5097\n",
      "Epoch 19/100, Train Loss: 0.1499, Val Loss: 0.5115\n",
      "Epoch 20/100, Train Loss: 0.2244, Val Loss: 0.4874\n",
      "Epoch 21/100, Train Loss: 0.1910, Val Loss: 0.4913\n",
      "Epoch 22/100, Train Loss: 0.1731, Val Loss: 0.4812\n",
      "Epoch 23/100, Train Loss: 0.1508, Val Loss: 0.4980\n",
      "Epoch 24/100, Train Loss: 0.1786, Val Loss: 0.4948\n",
      "Epoch 25/100, Train Loss: 0.1943, Val Loss: 0.5144\n",
      "Epoch 26/100, Train Loss: 0.2478, Val Loss: 0.4964\n",
      "Epoch 27/100, Train Loss: 0.1762, Val Loss: 0.5140\n",
      "Epoch 28/100, Train Loss: 0.1714, Val Loss: 0.4910\n",
      "Epoch 29/100, Train Loss: 0.2244, Val Loss: 0.4862\n",
      "Epoch 30/100, Train Loss: 0.1369, Val Loss: 0.4627\n",
      "Epoch 31/100, Train Loss: 0.1437, Val Loss: 0.5074\n",
      "Epoch 32/100, Train Loss: 0.1810, Val Loss: 0.5255\n",
      "Epoch 33/100, Train Loss: 0.1434, Val Loss: 0.5004\n",
      "Epoch 34/100, Train Loss: 0.2818, Val Loss: 0.4786\n",
      "Epoch 35/100, Train Loss: 0.2989, Val Loss: 0.4731\n",
      "Epoch 36/100, Train Loss: 0.1996, Val Loss: 0.4737\n",
      "Epoch 37/100, Train Loss: 0.1850, Val Loss: 0.4965\n",
      "Epoch 38/100, Train Loss: 0.1448, Val Loss: 0.4822\n",
      "Epoch 39/100, Train Loss: 0.2435, Val Loss: 0.4679\n",
      "Epoch 40/100, Train Loss: 0.1496, Val Loss: 0.4801\n",
      "Epoch 41/100, Train Loss: 0.1555, Val Loss: 0.5096\n",
      "Epoch 42/100, Train Loss: 0.2028, Val Loss: 0.4970\n",
      "Epoch 43/100, Train Loss: 0.1742, Val Loss: 0.4783\n",
      "Epoch 44/100, Train Loss: 0.1511, Val Loss: 0.5194\n",
      "Epoch 45/100, Train Loss: 0.1704, Val Loss: 0.5146\n",
      "Epoch 46/100, Train Loss: 0.2786, Val Loss: 0.4883\n",
      "Epoch 47/100, Train Loss: 0.1965, Val Loss: 0.4895\n",
      "Epoch 48/100, Train Loss: 0.1428, Val Loss: 0.5032\n",
      "Epoch 49/100, Train Loss: 0.1532, Val Loss: 0.4895\n",
      "Epoch 50/100, Train Loss: 0.1784, Val Loss: 0.5034\n",
      "Early stopping at epoch 50\n",
      "Final Accuracy for fold 1: 77.78%\n",
      "Final Validation Loss for fold 1: 0.4871\n",
      "Final Precision for fold 1: 0.79\n",
      "Final Recall for fold 1: 0.78\n",
      "Final F1-Score for fold 1: 0.78\n",
      "Final AUC for fold 1: 0.83\n",
      "\n",
      "Fold 2\n",
      "Epoch 1/100, Train Loss: 0.8841, Val Loss: 0.7040\n",
      "Epoch 2/100, Train Loss: 0.5370, Val Loss: 0.6532\n",
      "Epoch 3/100, Train Loss: 0.4887, Val Loss: 0.6133\n",
      "Epoch 4/100, Train Loss: 0.4353, Val Loss: 0.5857\n",
      "Epoch 5/100, Train Loss: 0.3670, Val Loss: 0.5719\n",
      "Epoch 6/100, Train Loss: 0.2927, Val Loss: 0.5782\n",
      "Epoch 7/100, Train Loss: 0.2855, Val Loss: 0.6047\n",
      "Epoch 8/100, Train Loss: 0.2462, Val Loss: 0.6521\n",
      "Epoch 9/100, Train Loss: 0.2368, Val Loss: 0.6451\n",
      "Epoch 10/100, Train Loss: 0.2443, Val Loss: 0.6137\n",
      "Epoch 11/100, Train Loss: 0.1766, Val Loss: 0.6157\n",
      "Epoch 12/100, Train Loss: 0.1961, Val Loss: 0.6249\n",
      "Epoch 13/100, Train Loss: 0.2642, Val Loss: 0.6097\n",
      "Epoch 14/100, Train Loss: 0.1899, Val Loss: 0.6178\n",
      "Epoch 15/100, Train Loss: 0.2312, Val Loss: 0.6054\n",
      "Epoch 16/100, Train Loss: 0.3150, Val Loss: 0.6036\n",
      "Epoch 17/100, Train Loss: 0.1719, Val Loss: 0.6296\n",
      "Epoch 18/100, Train Loss: 0.2011, Val Loss: 0.5972\n",
      "Epoch 19/100, Train Loss: 0.1836, Val Loss: 0.5789\n",
      "Epoch 20/100, Train Loss: 0.2238, Val Loss: 0.6063\n",
      "Epoch 21/100, Train Loss: 0.2803, Val Loss: 0.6000\n",
      "Epoch 22/100, Train Loss: 0.1823, Val Loss: 0.6141\n",
      "Epoch 23/100, Train Loss: 0.1845, Val Loss: 0.5963\n",
      "Epoch 24/100, Train Loss: 0.2614, Val Loss: 0.6164\n",
      "Epoch 25/100, Train Loss: 0.2782, Val Loss: 0.6159\n",
      "Early stopping at epoch 25\n",
      "Final Accuracy for fold 2: 72.22%\n",
      "Final Validation Loss for fold 2: 0.6174\n",
      "Final Precision for fold 2: 0.75\n",
      "Final Recall for fold 2: 0.70\n",
      "Final F1-Score for fold 2: 0.70\n",
      "Final AUC for fold 2: 0.76\n",
      "\n",
      "Fold 3\n",
      "Epoch 1/100, Train Loss: 0.7914, Val Loss: 0.5820\n",
      "Epoch 2/100, Train Loss: 0.6121, Val Loss: 0.5836\n",
      "Epoch 3/100, Train Loss: 0.4824, Val Loss: 0.5901\n",
      "Epoch 4/100, Train Loss: 0.5356, Val Loss: 0.5885\n",
      "Epoch 5/100, Train Loss: 0.5252, Val Loss: 0.6124\n",
      "Epoch 6/100, Train Loss: 0.4682, Val Loss: 0.5660\n",
      "Epoch 7/100, Train Loss: 0.3651, Val Loss: 0.4987\n",
      "Epoch 8/100, Train Loss: 0.3092, Val Loss: 0.4669\n",
      "Epoch 9/100, Train Loss: 0.3597, Val Loss: 0.4610\n",
      "Epoch 10/100, Train Loss: 0.2793, Val Loss: 0.4738\n",
      "Epoch 11/100, Train Loss: 0.3231, Val Loss: 0.4643\n",
      "Epoch 12/100, Train Loss: 0.2690, Val Loss: 0.4720\n",
      "Epoch 13/100, Train Loss: 0.2776, Val Loss: 0.4848\n",
      "Epoch 14/100, Train Loss: 0.2502, Val Loss: 0.4687\n",
      "Epoch 15/100, Train Loss: 0.2625, Val Loss: 0.4631\n",
      "Epoch 16/100, Train Loss: 0.2367, Val Loss: 0.4587\n",
      "Epoch 17/100, Train Loss: 0.2001, Val Loss: 0.4448\n",
      "Epoch 18/100, Train Loss: 0.2307, Val Loss: 0.4283\n",
      "Epoch 19/100, Train Loss: 0.2333, Val Loss: 0.4288\n",
      "Epoch 20/100, Train Loss: 0.2621, Val Loss: 0.4384\n",
      "Epoch 21/100, Train Loss: 0.3174, Val Loss: 0.4201\n",
      "Epoch 22/100, Train Loss: 0.2656, Val Loss: 0.4179\n",
      "Epoch 23/100, Train Loss: 0.2121, Val Loss: 0.4019\n",
      "Epoch 24/100, Train Loss: 0.2363, Val Loss: 0.4264\n",
      "Epoch 25/100, Train Loss: 0.1991, Val Loss: 0.4317\n",
      "Epoch 26/100, Train Loss: 0.2271, Val Loss: 0.4391\n",
      "Epoch 27/100, Train Loss: 0.2246, Val Loss: 0.4336\n",
      "Epoch 28/100, Train Loss: 0.1865, Val Loss: 0.4326\n",
      "Epoch 29/100, Train Loss: 0.1908, Val Loss: 0.4355\n",
      "Epoch 30/100, Train Loss: 0.1630, Val Loss: 0.4365\n",
      "Epoch 31/100, Train Loss: 0.3449, Val Loss: 0.4099\n",
      "Epoch 32/100, Train Loss: 0.1758, Val Loss: 0.4194\n",
      "Epoch 33/100, Train Loss: 0.2522, Val Loss: 0.4258\n",
      "Epoch 34/100, Train Loss: 0.2150, Val Loss: 0.4537\n",
      "Epoch 35/100, Train Loss: 0.2010, Val Loss: 0.4659\n",
      "Epoch 36/100, Train Loss: 0.2133, Val Loss: 0.4494\n",
      "Epoch 37/100, Train Loss: 0.2298, Val Loss: 0.4634\n",
      "Epoch 38/100, Train Loss: 0.2803, Val Loss: 0.4504\n",
      "Epoch 39/100, Train Loss: 0.2670, Val Loss: 0.4337\n",
      "Epoch 40/100, Train Loss: 0.2245, Val Loss: 0.4371\n",
      "Epoch 41/100, Train Loss: 0.2568, Val Loss: 0.4387\n",
      "Epoch 42/100, Train Loss: 0.2432, Val Loss: 0.4232\n",
      "Epoch 43/100, Train Loss: 0.2809, Val Loss: 0.4186\n",
      "Early stopping at epoch 43\n",
      "Final Accuracy for fold 3: 77.78%\n",
      "Final Validation Loss for fold 3: 0.4186\n",
      "Final Precision for fold 3: 0.75\n",
      "Final Recall for fold 3: 0.86\n",
      "Final F1-Score for fold 3: 0.75\n",
      "Final AUC for fold 3: 0.93\n",
      "\n",
      "Fold 4\n",
      "Epoch 1/100, Train Loss: 0.7099, Val Loss: 0.6859\n",
      "Epoch 2/100, Train Loss: 0.5632, Val Loss: 0.6623\n",
      "Epoch 3/100, Train Loss: 0.5191, Val Loss: 0.6442\n",
      "Epoch 4/100, Train Loss: 0.5627, Val Loss: 0.5991\n",
      "Epoch 5/100, Train Loss: 0.4941, Val Loss: 0.5394\n",
      "Epoch 6/100, Train Loss: 0.4364, Val Loss: 0.3693\n",
      "Epoch 7/100, Train Loss: 0.3814, Val Loss: 0.3407\n",
      "Epoch 8/100, Train Loss: 0.4016, Val Loss: 0.3172\n",
      "Epoch 9/100, Train Loss: 0.3649, Val Loss: 0.2858\n",
      "Epoch 10/100, Train Loss: 0.3319, Val Loss: 0.2556\n",
      "Epoch 11/100, Train Loss: 0.2775, Val Loss: 0.2600\n",
      "Epoch 12/100, Train Loss: 0.2980, Val Loss: 0.2568\n",
      "Epoch 13/100, Train Loss: 0.3003, Val Loss: 0.2429\n",
      "Epoch 14/100, Train Loss: 0.3156, Val Loss: 0.2578\n",
      "Epoch 15/100, Train Loss: 0.3311, Val Loss: 0.2530\n",
      "Epoch 16/100, Train Loss: 0.2862, Val Loss: 0.2411\n",
      "Epoch 17/100, Train Loss: 0.2635, Val Loss: 0.2400\n",
      "Epoch 18/100, Train Loss: 0.3002, Val Loss: 0.2466\n",
      "Epoch 19/100, Train Loss: 0.2878, Val Loss: 0.2144\n",
      "Epoch 20/100, Train Loss: 0.3125, Val Loss: 0.2092\n",
      "Epoch 21/100, Train Loss: 0.2428, Val Loss: 0.2423\n",
      "Epoch 22/100, Train Loss: 0.3211, Val Loss: 0.2348\n",
      "Epoch 23/100, Train Loss: 0.3807, Val Loss: 0.2334\n",
      "Epoch 24/100, Train Loss: 0.2752, Val Loss: 0.2235\n",
      "Epoch 25/100, Train Loss: 0.2834, Val Loss: 0.2217\n",
      "Epoch 26/100, Train Loss: 0.2392, Val Loss: 0.2267\n",
      "Epoch 27/100, Train Loss: 0.2701, Val Loss: 0.2321\n",
      "Epoch 28/100, Train Loss: 0.2693, Val Loss: 0.2320\n",
      "Epoch 29/100, Train Loss: 0.2667, Val Loss: 0.2587\n",
      "Epoch 30/100, Train Loss: 0.3114, Val Loss: 0.2361\n",
      "Epoch 31/100, Train Loss: 0.2517, Val Loss: 0.2461\n",
      "Epoch 32/100, Train Loss: 0.3052, Val Loss: 0.2480\n",
      "Epoch 33/100, Train Loss: 0.3156, Val Loss: 0.2246\n",
      "Epoch 34/100, Train Loss: 0.3574, Val Loss: 0.2341\n",
      "Epoch 35/100, Train Loss: 0.2568, Val Loss: 0.2325\n",
      "Epoch 36/100, Train Loss: 0.2508, Val Loss: 0.2403\n",
      "Epoch 37/100, Train Loss: 0.3087, Val Loss: 0.2458\n",
      "Epoch 38/100, Train Loss: 0.2846, Val Loss: 0.2216\n",
      "Epoch 39/100, Train Loss: 0.2300, Val Loss: 0.2343\n",
      "Epoch 40/100, Train Loss: 0.2999, Val Loss: 0.2240\n",
      "Early stopping at epoch 40\n",
      "Final Accuracy for fold 4: 94.44%\n",
      "Final Validation Loss for fold 4: 0.2313\n",
      "Final Precision for fold 4: 0.93\n",
      "Final Recall for fold 4: 0.96\n",
      "Final F1-Score for fold 4: 0.94\n",
      "Final AUC for fold 4: 0.96\n",
      "\n",
      "Fold 5\n",
      "Epoch 1/100, Train Loss: 0.6125, Val Loss: 0.6323\n",
      "Epoch 2/100, Train Loss: 0.5413, Val Loss: 0.6175\n",
      "Epoch 3/100, Train Loss: 0.4903, Val Loss: 0.6172\n",
      "Epoch 4/100, Train Loss: 0.4091, Val Loss: 0.6332\n",
      "Epoch 5/100, Train Loss: 0.4220, Val Loss: 0.5712\n",
      "Epoch 6/100, Train Loss: 0.3215, Val Loss: 0.4535\n",
      "Epoch 7/100, Train Loss: 0.3070, Val Loss: 0.3617\n",
      "Epoch 8/100, Train Loss: 0.2473, Val Loss: 0.3872\n",
      "Epoch 9/100, Train Loss: 0.2553, Val Loss: 0.4474\n",
      "Epoch 10/100, Train Loss: 0.1775, Val Loss: 0.5076\n",
      "Epoch 11/100, Train Loss: 0.1726, Val Loss: 0.5006\n",
      "Epoch 12/100, Train Loss: 0.2637, Val Loss: 0.4946\n",
      "Epoch 13/100, Train Loss: 0.1823, Val Loss: 0.4789\n",
      "Epoch 14/100, Train Loss: 0.2510, Val Loss: 0.4925\n",
      "Epoch 15/100, Train Loss: 0.1611, Val Loss: 0.4777\n",
      "Epoch 16/100, Train Loss: 0.1530, Val Loss: 0.4571\n",
      "Epoch 17/100, Train Loss: 0.2407, Val Loss: 0.4634\n",
      "Epoch 18/100, Train Loss: 0.2229, Val Loss: 0.4456\n",
      "Epoch 19/100, Train Loss: 0.2997, Val Loss: 0.4502\n",
      "Epoch 20/100, Train Loss: 0.2527, Val Loss: 0.4396\n",
      "Epoch 21/100, Train Loss: 0.1746, Val Loss: 0.4659\n",
      "Epoch 22/100, Train Loss: 0.2179, Val Loss: 0.4444\n",
      "Epoch 23/100, Train Loss: 0.2368, Val Loss: 0.4591\n",
      "Epoch 24/100, Train Loss: 0.1581, Val Loss: 0.4542\n",
      "Epoch 25/100, Train Loss: 0.1845, Val Loss: 0.4503\n",
      "Epoch 26/100, Train Loss: 0.1750, Val Loss: 0.4503\n",
      "Epoch 27/100, Train Loss: 0.1790, Val Loss: 0.4818\n",
      "Early stopping at epoch 27\n",
      "Final Accuracy for fold 5: 83.33%\n",
      "Final Validation Loss for fold 5: 0.5000\n",
      "Final Precision for fold 5: 0.83\n",
      "Final Recall for fold 5: 0.88\n",
      "Final F1-Score for fold 5: 0.83\n",
      "Final AUC for fold 5: 0.92\n",
      "\n",
      "Average Accuracy: 81.11%\n",
      "Average Validation Loss: 0.4509\n",
      "Average Precision: 0.81\n",
      "Average Recall: 0.83\n",
      "Average F1-Score: 0.80\n",
      "Average AUC: 0.88\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Define constants\n",
    "DATA_DIR = 'C:/Users/User/Documents/Lie detect data/EEGData'\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100  # Increased to allow early stopping\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_FOLDS = 5\n",
    "PATIENCE = 20  # Early stopping patience\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Custom Dataset class for EEG data\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.load_data(data_dir)\n",
    "        self.normalize_data()\n",
    "\n",
    "    def load_data(self, data_dir):\n",
    "        max_length = 0\n",
    "        temp_data = []\n",
    "        \n",
    "        for file_name in os.listdir(data_dir):\n",
    "            file_path = os.path.join(data_dir, file_name)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                eeg_data = pickle.load(f)\n",
    "                label = 1 if 'lie' in file_name else 0  # Assuming file names contain 'lie' or 'truth'\n",
    "                temp_data.append((eeg_data, label))\n",
    "                max_length = max(max_length, eeg_data.shape[1])\n",
    "\n",
    "        for eeg_data, label in temp_data:\n",
    "            padded_data = np.pad(eeg_data, ((0, 0), (0, max_length - eeg_data.shape[1])), mode='constant')\n",
    "            self.data.append(padded_data)\n",
    "            self.labels.append(label)\n",
    "        \n",
    "        self.data = [torch.tensor(d, dtype=torch.float32, device=device) for d in self.data]\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long, device=device)\n",
    "    \n",
    "    def normalize_data(self):\n",
    "        all_data = torch.cat([d.unsqueeze(0) for d in self.data], dim=0)\n",
    "        mean = all_data.mean()\n",
    "        std = all_data.std()\n",
    "        self.data = [(d - mean) / std for d in self.data]\n",
    "\n",
    "    def augment_data(self, data):\n",
    "        # Advanced augmentations: Gaussian noise, time shift, scaling\n",
    "        noise = torch.randn_like(data, device=device) * 0.01\n",
    "        shift = torch.roll(data, shifts=int(data.shape[1] * 0.1), dims=1)\n",
    "        scale = data * (1 + 0.1 * torch.randn(1, device=device))\n",
    "        return noise + shift + scale\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.data[idx], self.labels[idx]\n",
    "        data = self.augment_data(data)  # Apply augmentation\n",
    "        return data, label\n",
    "\n",
    "# Define the EEGNet model\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.firstconv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, (1, 51), padding=(0, 25)),\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "        self.depthwiseConv = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, (65, 1), groups=16),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d((1, 4)),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.separableConv = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, (1, 15), padding=(0, 7)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d((1, 8)),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.classify = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(output_size, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.firstconv(x)\n",
    "        x = self.depthwiseConv(x)\n",
    "        x = self.separableConv(x)\n",
    "        return self.classify(x)\n",
    "\n",
    "# Function to determine the output size of the EEGNet model before the linear layer\n",
    "def get_output_size(model, shape):\n",
    "    with torch.no_grad():\n",
    "        x = torch.zeros(shape, device=device)\n",
    "        x = model.firstconv(x)\n",
    "        x = model.depthwiseConv(x)\n",
    "        x = model.separableConv(x)\n",
    "        return x.view(x.size(0), -1).size(1)\n",
    "\n",
    "# Load data\n",
    "dataset = EEGDataset(DATA_DIR)\n",
    "\n",
    "# Determine the correct input size for the linear layer\n",
    "dummy_input_shape = (1, 1, 65, max([d.shape[1] for d in dataset.data]))  # (batch_size, channels, height, width)\n",
    "output_size = get_output_size(EEGNet(output_size=0).to(device), dummy_input_shape)\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "kf = KFold(n_splits=NUM_FOLDS, shuffle=True)\n",
    "\n",
    "final_accuracies = []\n",
    "final_precisions = []\n",
    "final_recalls = []\n",
    "final_f1s = []\n",
    "final_aucs = []\n",
    "final_val_losses = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(dataset)):\n",
    "    print(f'Fold {fold+1}')\n",
    "    \n",
    "    train_subset = Subset(dataset, train_index)\n",
    "    val_subset = Subset(dataset, val_index)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = EEGNet(output_size=output_size).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.unsqueeze(1))\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs.unsqueeze(1))\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "\n",
    "        val_loss = val_running_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probs = []\n",
    "    final_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs.unsqueeze(1))\n",
    "            loss = criterion(outputs, labels)\n",
    "            final_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_probs.extend(torch.softmax(outputs, dim=1)[:, 1].cpu().numpy())\n",
    "\n",
    "    final_val_loss /= len(val_loader)\n",
    "    final_accuracy = 100 * correct / total\n",
    "    final_precision = precision_score(all_labels, all_predictions, average='macro')\n",
    "    final_recall = recall_score(all_labels, all_predictions, average='macro')\n",
    "    final_f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    final_auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    # Store final metrics\n",
    "    final_accuracies.append(final_accuracy)\n",
    "    final_precisions.append(final_precision)\n",
    "    final_recalls.append(final_recall)\n",
    "    final_f1s.append(final_f1)\n",
    "    final_aucs.append(final_auc)\n",
    "    final_val_losses.append(final_val_loss)\n",
    "\n",
    "    print(f'Final Accuracy for fold {fold+1}: {final_accuracy:.2f}%')\n",
    "    print(f'Final Validation Loss for fold {fold+1}: {final_val_loss:.4f}')\n",
    "    print(f'Final Precision for fold {fold+1}: {final_precision:.2f}')\n",
    "    print(f'Final Recall for fold {fold+1}: {final_recall:.2f}')\n",
    "    print(f'Final F1-Score for fold {fold+1}: {final_f1:.2f}')\n",
    "    print(f'Final AUC for fold {fold+1}: {final_auc:.2f}\\n')\n",
    "\n",
    "# Report average performance across all folds\n",
    "print(f'Average Accuracy: {np.mean(final_accuracies):.2f}%')\n",
    "print(f'Average Validation Loss: {np.mean(final_val_losses):.4f}')\n",
    "print(f'Average Precision: {np.mean(final_precisions):.2f}')\n",
    "print(f'Average Recall: {np.mean(final_recalls):.2f}')\n",
    "print(f'Average F1-Score: {np.mean(final_f1s):.2f}')\n",
    "print(f'Average AUC: {np.mean(final_aucs):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faffd59-d784-4037-a0f7-d591f8cb26d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
