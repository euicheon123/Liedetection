{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "656cdbc2-da54-45ba-97fc-195b59bd7820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 0: Train Loss: 0.6664607127507528, Validation Loss: 0.6818138957023621\n",
      "Epoch 1: Train Loss: 0.594059407711029, Validation Loss: 0.6677617430686951\n",
      "Epoch 2: Train Loss: 0.5546716054280599, Validation Loss: 0.6531331539154053\n",
      "Epoch 3: Train Loss: 0.5347662369410197, Validation Loss: 0.6373401284217834\n",
      "Epoch 4: Train Loss: 0.5083467861016592, Validation Loss: 0.618558406829834\n",
      "Epoch 5: Train Loss: 0.4967416723569234, Validation Loss: 0.5998541116714478\n",
      "Epoch 6: Train Loss: 0.466926505168279, Validation Loss: 0.5814759731292725\n",
      "Epoch 7: Train Loss: 0.47401655713717145, Validation Loss: 0.5644693970680237\n",
      "Epoch 8: Train Loss: 0.45488593975702923, Validation Loss: 0.5511776804924011\n",
      "Epoch 9: Train Loss: 0.4492443601290385, Validation Loss: 0.5413287878036499\n",
      "Epoch 10: Train Loss: 0.4538353681564331, Validation Loss: 0.5115668773651123\n",
      "Epoch 11: Train Loss: 0.42954176664352417, Validation Loss: 0.49470311403274536\n",
      "Epoch 12: Train Loss: 0.3915867308775584, Validation Loss: 0.4840153157711029\n",
      "Epoch 13: Train Loss: 0.39184712370236713, Validation Loss: 0.4783053398132324\n",
      "Epoch 14: Train Loss: 0.3693026304244995, Validation Loss: 0.47681954503059387\n",
      "Epoch 15: Train Loss: 0.34597186247507733, Validation Loss: 0.4761417508125305\n",
      "Epoch 16: Train Loss: 0.34699520468711853, Validation Loss: 0.4769246578216553\n",
      "Epoch 17: Train Loss: 0.34326985478401184, Validation Loss: 0.47478944063186646\n",
      "Epoch 18: Train Loss: 0.3398910462856293, Validation Loss: 0.47205910086631775\n",
      "Epoch 19: Train Loss: 0.3411112030347188, Validation Loss: 0.4693151116371155\n",
      "Epoch 20: Train Loss: 0.3298811415831248, Validation Loss: 0.4718552529811859\n",
      "Epoch 21: Train Loss: 0.3215324878692627, Validation Loss: 0.4744096100330353\n",
      "Epoch 22: Train Loss: 0.2983407179514567, Validation Loss: 0.4784674346446991\n",
      "Epoch 23: Train Loss: 0.28666602571805316, Validation Loss: 0.48170918226242065\n",
      "Epoch 24: Train Loss: 0.2984439829985301, Validation Loss: 0.4939201772212982\n",
      "Epoch 25: Train Loss: 0.33187254269917804, Validation Loss: 0.49477460980415344\n",
      "Epoch 26: Train Loss: 0.2757665415604909, Validation Loss: 0.4941695034503937\n",
      "Epoch 27: Train Loss: 0.2553139279286067, Validation Loss: 0.49328914284706116\n",
      "Epoch 28: Train Loss: 0.2534825454155604, Validation Loss: 0.49202972650527954\n",
      "Epoch 29: Train Loss: 0.24791490038235983, Validation Loss: 0.4921533167362213\n",
      "Epoch 30: Train Loss: 0.25421148041884106, Validation Loss: 0.4926185607910156\n",
      "Epoch 31: Train Loss: 0.2498452216386795, Validation Loss: 0.4976690709590912\n",
      "Epoch 32: Train Loss: 0.24477719267209372, Validation Loss: 0.5054268836975098\n",
      "Epoch 33: Train Loss: 0.2353015790383021, Validation Loss: 0.5146322846412659\n",
      "Epoch 34: Train Loss: 0.2148419072230657, Validation Loss: 0.522671103477478\n",
      "Epoch 35: Train Loss: 0.23759887119134268, Validation Loss: 0.5433814525604248\n",
      "Epoch 36: Train Loss: 0.22077758113543192, Validation Loss: 0.5515936017036438\n",
      "Epoch 37: Train Loss: 0.20895742873350778, Validation Loss: 0.5505108833312988\n",
      "Epoch 38: Train Loss: 0.2179900904496511, Validation Loss: 0.5468891263008118\n",
      "Epoch 39: Train Loss: 0.20331106086572012, Validation Loss: 0.5455023050308228\n",
      "Epoch 40: Train Loss: 0.2245843062798182, Validation Loss: 0.5204229950904846\n",
      "Epoch 41: Train Loss: 0.2087883601586024, Validation Loss: 0.5295008420944214\n",
      "Epoch 42: Train Loss: 0.20554236074288687, Validation Loss: 0.5284583568572998\n",
      "Epoch 43: Train Loss: 0.17779570321242014, Validation Loss: 0.5283784866333008\n",
      "Epoch 44: Train Loss: 0.17796667913595834, Validation Loss: 0.5311193466186523\n",
      "Epoch 45: Train Loss: 0.16442734003067017, Validation Loss: 0.5367348194122314\n",
      "Epoch 46: Train Loss: 0.19030918180942535, Validation Loss: 0.5491432547569275\n",
      "Epoch 47: Train Loss: 0.17180184523264566, Validation Loss: 0.5515448451042175\n",
      "Epoch 48: Train Loss: 0.17054561773935953, Validation Loss: 0.5505087375640869\n",
      "Epoch 49: Train Loss: 0.1673168589671453, Validation Loss: 0.5479779839515686\n",
      "Epoch 50: Train Loss: 0.1758939872185389, Validation Loss: 0.5453368425369263\n",
      "Epoch 51: Train Loss: 0.17851530015468597, Validation Loss: 0.5634461641311646\n",
      "Epoch 52: Train Loss: 0.1475002964337667, Validation Loss: 0.578548789024353\n",
      "Epoch 53: Train Loss: 0.15212044616540274, Validation Loss: 0.5910997986793518\n",
      "Epoch 54: Train Loss: 0.14667383084694544, Validation Loss: 0.6032082438468933\n",
      "Epoch 55: Train Loss: 0.1645889381567637, Validation Loss: 0.597917377948761\n",
      "Epoch 56: Train Loss: 0.13402860114971796, Validation Loss: 0.5890172123908997\n",
      "Epoch 57: Train Loss: 0.13900352517763773, Validation Loss: 0.5819214582443237\n",
      "Epoch 58: Train Loss: 0.13622955481211343, Validation Loss: 0.5788319110870361\n",
      "Epoch 59: Train Loss: 0.1766406993071238, Validation Loss: 0.5813624262809753\n",
      "Epoch 60: Train Loss: 0.1380727762977282, Validation Loss: 0.5892851948738098\n",
      "Epoch 61: Train Loss: 0.15688790380954742, Validation Loss: 0.57454514503479\n",
      "Epoch 62: Train Loss: 0.141935666402181, Validation Loss: 0.5714097023010254\n",
      "Epoch 63: Train Loss: 0.1680606628457705, Validation Loss: 0.5772690176963806\n",
      "Epoch 64: Train Loss: 0.12496602535247803, Validation Loss: 0.6019800901412964\n",
      "Epoch 65: Train Loss: 0.11824722588062286, Validation Loss: 0.625281810760498\n",
      "Epoch 66: Train Loss: 0.12741480767726898, Validation Loss: 0.634149432182312\n",
      "Epoch 67: Train Loss: 0.11036886026461919, Validation Loss: 0.6340346336364746\n",
      "Epoch 68: Train Loss: 0.14150218665599823, Validation Loss: 0.6313934922218323\n",
      "Epoch 69: Train Loss: 0.11305743455886841, Validation Loss: 0.6281729936599731\n",
      "Epoch 70: Train Loss: 0.12841695795456567, Validation Loss: 0.5807009339332581\n",
      "Epoch 71: Train Loss: 0.1268263707558314, Validation Loss: 0.576629638671875\n",
      "Epoch 72: Train Loss: 0.124961423377196, Validation Loss: 0.6127389669418335\n",
      "Epoch 73: Train Loss: 0.1205242599050204, Validation Loss: 0.6486644744873047\n",
      "Epoch 74: Train Loss: 0.12040781726439793, Validation Loss: 0.6692832708358765\n",
      "Epoch 75: Train Loss: 0.10509224981069565, Validation Loss: 0.6815346479415894\n",
      "Epoch 76: Train Loss: 0.11401919523874919, Validation Loss: 0.6817916631698608\n",
      "Epoch 77: Train Loss: 0.09706473847230275, Validation Loss: 0.6708778738975525\n",
      "Epoch 78: Train Loss: 0.15199681371450424, Validation Loss: 0.6635555028915405\n",
      "Epoch 79: Train Loss: 0.10464527954657872, Validation Loss: 0.6621854305267334\n",
      "Epoch 80: Train Loss: 0.099599522848924, Validation Loss: 0.6492682099342346\n",
      "Epoch 81: Train Loss: 0.1140394036968549, Validation Loss: 0.6453615427017212\n",
      "Epoch 82: Train Loss: 0.10739131271839142, Validation Loss: 0.6548457145690918\n",
      "Epoch 83: Train Loss: 0.1257423311471939, Validation Loss: 0.6531955599784851\n",
      "Epoch 84: Train Loss: 0.11724899709224701, Validation Loss: 0.6755934953689575\n",
      "Epoch 85: Train Loss: 0.10643726338942845, Validation Loss: 0.7134904861450195\n",
      "Epoch 86: Train Loss: 0.13216961671908697, Validation Loss: 0.7086175680160522\n",
      "Epoch 87: Train Loss: 0.1289238234361013, Validation Loss: 0.6958714723587036\n",
      "Epoch 88: Train Loss: 0.10007383177677791, Validation Loss: 0.6987454891204834\n",
      "Epoch 89: Train Loss: 0.09235130747159322, Validation Loss: 0.6966758966445923\n",
      "Epoch 90: Train Loss: 0.09803966184457143, Validation Loss: 0.6956919431686401\n",
      "Epoch 91: Train Loss: 0.10169228663047154, Validation Loss: 0.6794054508209229\n",
      "Epoch 92: Train Loss: 0.1015678346157074, Validation Loss: 0.6604390740394592\n",
      "Epoch 93: Train Loss: 0.08169475694497426, Validation Loss: 0.6736066341400146\n",
      "Epoch 94: Train Loss: 0.10482875754435857, Validation Loss: 0.6946490406990051\n",
      "Epoch 95: Train Loss: 0.07326346387465794, Validation Loss: 0.6897183656692505\n",
      "Epoch 96: Train Loss: 0.07244963323076566, Validation Loss: 0.6876189112663269\n",
      "Epoch 97: Train Loss: 0.07506494224071503, Validation Loss: 0.6882330775260925\n",
      "Epoch 98: Train Loss: 0.11131862799326579, Validation Loss: 0.6849707961082458\n",
      "Epoch 99: Train Loss: 0.08970315257708232, Validation Loss: 0.6830710768699646\n",
      "Fold 2\n",
      "Epoch 0: Train Loss: 0.7320617039998373, Validation Loss: 0.6692193746566772\n",
      "Epoch 1: Train Loss: 0.6075407663981119, Validation Loss: 0.6606598496437073\n",
      "Epoch 2: Train Loss: 0.5850779016812643, Validation Loss: 0.6510790586471558\n",
      "Epoch 3: Train Loss: 0.5447102387746176, Validation Loss: 0.6393247842788696\n",
      "Epoch 4: Train Loss: 0.5187702973683676, Validation Loss: 0.6303961277008057\n",
      "Epoch 5: Train Loss: 0.5136538743972778, Validation Loss: 0.616544783115387\n",
      "Epoch 6: Train Loss: 0.4980413814385732, Validation Loss: 0.6054677367210388\n",
      "Epoch 7: Train Loss: 0.49582386016845703, Validation Loss: 0.5966805815696716\n",
      "Epoch 8: Train Loss: 0.4728708565235138, Validation Loss: 0.5887907147407532\n",
      "Epoch 9: Train Loss: 0.47885002692540485, Validation Loss: 0.5829278230667114\n",
      "Epoch 10: Train Loss: 0.46779556075731915, Validation Loss: 0.5670159459114075\n",
      "Epoch 11: Train Loss: 0.42992159724235535, Validation Loss: 0.5400899648666382\n",
      "Epoch 12: Train Loss: 0.41612136363983154, Validation Loss: 0.5173467397689819\n",
      "Epoch 13: Train Loss: 0.39765432476997375, Validation Loss: 0.5035778880119324\n",
      "Epoch 14: Train Loss: 0.3987913529078166, Validation Loss: 0.5111400485038757\n",
      "Epoch 15: Train Loss: 0.3646361529827118, Validation Loss: 0.5086228251457214\n",
      "Epoch 16: Train Loss: 0.36593424280484516, Validation Loss: 0.5014262199401855\n",
      "Epoch 17: Train Loss: 0.35259363055229187, Validation Loss: 0.49026626348495483\n",
      "Epoch 18: Train Loss: 0.3537510534127553, Validation Loss: 0.48386281728744507\n",
      "Epoch 19: Train Loss: 0.35236677527427673, Validation Loss: 0.48166465759277344\n",
      "Epoch 20: Train Loss: 0.3337984581788381, Validation Loss: 0.45461609959602356\n",
      "Epoch 21: Train Loss: 0.34608497222264606, Validation Loss: 0.45027807354927063\n",
      "Epoch 22: Train Loss: 0.30937602122624713, Validation Loss: 0.46582651138305664\n",
      "Epoch 23: Train Loss: 0.2873166501522064, Validation Loss: 0.4458085000514984\n",
      "Epoch 24: Train Loss: 0.2829046646753947, Validation Loss: 0.43641114234924316\n",
      "Epoch 25: Train Loss: 0.2910538117090861, Validation Loss: 0.44763025641441345\n",
      "Epoch 26: Train Loss: 0.2615175743897756, Validation Loss: 0.44572195410728455\n",
      "Epoch 27: Train Loss: 0.2572671224673589, Validation Loss: 0.446615606546402\n",
      "Epoch 28: Train Loss: 0.2503114293018977, Validation Loss: 0.44601139426231384\n",
      "Epoch 29: Train Loss: 0.26255367199579877, Validation Loss: 0.44795629382133484\n",
      "Epoch 30: Train Loss: 0.24593891700108847, Validation Loss: 0.4361498951911926\n",
      "Epoch 31: Train Loss: 0.2512309004863103, Validation Loss: 0.4168725311756134\n",
      "Epoch 32: Train Loss: 0.228133295973142, Validation Loss: 0.41209957003593445\n",
      "Epoch 33: Train Loss: 0.2283362497886022, Validation Loss: 0.434662789106369\n",
      "Epoch 34: Train Loss: 0.2092363784710566, Validation Loss: 0.45772141218185425\n",
      "Epoch 35: Train Loss: 0.20230340460936228, Validation Loss: 0.4317120611667633\n",
      "Epoch 36: Train Loss: 0.23462372521559396, Validation Loss: 0.42389345169067383\n",
      "Epoch 37: Train Loss: 0.20091968278090158, Validation Loss: 0.42295318841934204\n",
      "Epoch 38: Train Loss: 0.2017236202955246, Validation Loss: 0.4232475459575653\n",
      "Epoch 39: Train Loss: 0.1908950755993525, Validation Loss: 0.4242069721221924\n",
      "Epoch 40: Train Loss: 0.20915492872397104, Validation Loss: 0.46245598793029785\n",
      "Epoch 41: Train Loss: 0.18860396246115366, Validation Loss: 0.40073397755622864\n",
      "Epoch 42: Train Loss: 0.17403583228588104, Validation Loss: 0.39890772104263306\n",
      "Epoch 43: Train Loss: 0.18379975855350494, Validation Loss: 0.4236118793487549\n",
      "Epoch 44: Train Loss: 0.168182964126269, Validation Loss: 0.44713282585144043\n",
      "Epoch 45: Train Loss: 0.16219665110111237, Validation Loss: 0.43608197569847107\n",
      "Epoch 46: Train Loss: 0.15680334468682608, Validation Loss: 0.4352034330368042\n",
      "Epoch 47: Train Loss: 0.18939527372519174, Validation Loss: 0.437874436378479\n",
      "Epoch 48: Train Loss: 0.16583308080832163, Validation Loss: 0.43628427386283875\n",
      "Epoch 49: Train Loss: 0.14665361742178598, Validation Loss: 0.4362308979034424\n",
      "Epoch 50: Train Loss: 0.16230556865533194, Validation Loss: 0.4436857998371124\n",
      "Epoch 51: Train Loss: 0.1596297820409139, Validation Loss: 0.425342857837677\n",
      "Epoch 52: Train Loss: 0.15000658730665842, Validation Loss: 0.44084370136260986\n",
      "Epoch 53: Train Loss: 0.1782972663640976, Validation Loss: 0.4611422121524811\n",
      "Epoch 54: Train Loss: 0.17687437434991202, Validation Loss: 0.3991980254650116\n",
      "Epoch 55: Train Loss: 0.1469131807486216, Validation Loss: 0.41899675130844116\n",
      "Epoch 56: Train Loss: 0.12482854972283046, Validation Loss: 0.45289868116378784\n",
      "Epoch 57: Train Loss: 0.18404203156630197, Validation Loss: 0.4612584412097931\n",
      "Epoch 58: Train Loss: 0.14467145254214606, Validation Loss: 0.4615108370780945\n",
      "Epoch 59: Train Loss: 0.13737444082895914, Validation Loss: 0.4620931148529053\n",
      "Epoch 60: Train Loss: 0.12979628145694733, Validation Loss: 0.4411104917526245\n",
      "Epoch 61: Train Loss: 0.1423376500606537, Validation Loss: 0.4173753559589386\n",
      "Epoch 62: Train Loss: 0.15353392561276755, Validation Loss: 0.3802211582660675\n",
      "Epoch 63: Train Loss: 0.11881920198599498, Validation Loss: 0.4168022572994232\n",
      "Epoch 64: Train Loss: 0.11268338561058044, Validation Loss: 0.47159287333488464\n",
      "Epoch 65: Train Loss: 0.16749396175146103, Validation Loss: 0.4335041046142578\n",
      "Epoch 66: Train Loss: 0.11988793810208638, Validation Loss: 0.377395361661911\n",
      "Epoch 67: Train Loss: 0.11789542188247044, Validation Loss: 0.3738222122192383\n",
      "Epoch 68: Train Loss: 0.11621187627315521, Validation Loss: 0.37959998846054077\n",
      "Epoch 69: Train Loss: 0.13779213527838388, Validation Loss: 0.3837006688117981\n",
      "Epoch 70: Train Loss: 0.1248102808992068, Validation Loss: 0.5510129332542419\n",
      "Epoch 71: Train Loss: 0.11462946732838948, Validation Loss: 0.4308854937553406\n",
      "Epoch 72: Train Loss: 0.1022501140832901, Validation Loss: 0.4001384973526001\n",
      "Epoch 73: Train Loss: 0.10551657030979793, Validation Loss: 0.40371185541152954\n",
      "Epoch 74: Train Loss: 0.10148994376262029, Validation Loss: 0.42555975914001465\n",
      "Epoch 75: Train Loss: 0.09376684079567592, Validation Loss: 0.423939049243927\n",
      "Epoch 76: Train Loss: 0.10882481684287389, Validation Loss: 0.41871657967567444\n",
      "Epoch 77: Train Loss: 0.09250524888436, Validation Loss: 0.4174768030643463\n",
      "Epoch 78: Train Loss: 0.11412779738505681, Validation Loss: 0.4206838309764862\n",
      "Epoch 79: Train Loss: 0.09667740762233734, Validation Loss: 0.4213908314704895\n",
      "Epoch 80: Train Loss: 0.09264144549767177, Validation Loss: 0.42859917879104614\n",
      "Epoch 81: Train Loss: 0.13965978970130286, Validation Loss: 0.3894343674182892\n",
      "Epoch 82: Train Loss: 0.10951052109400432, Validation Loss: 0.39722490310668945\n",
      "Epoch 83: Train Loss: 0.09169945120811462, Validation Loss: 0.4507521092891693\n",
      "Epoch 84: Train Loss: 0.0978234236439069, Validation Loss: 0.487196147441864\n",
      "Epoch 85: Train Loss: 0.09902515759070714, Validation Loss: 0.48929810523986816\n",
      "Epoch 86: Train Loss: 0.07939740518728892, Validation Loss: 0.47219547629356384\n",
      "Epoch 87: Train Loss: 0.08116586009661357, Validation Loss: 0.4547504186630249\n",
      "Epoch 88: Train Loss: 0.07301551972826321, Validation Loss: 0.4509827196598053\n",
      "Epoch 89: Train Loss: 0.07762759551405907, Validation Loss: 0.4481346011161804\n",
      "Epoch 90: Train Loss: 0.08265723784764607, Validation Loss: 0.45240259170532227\n",
      "Epoch 91: Train Loss: 0.1036301205555598, Validation Loss: 0.4445915222167969\n",
      "Epoch 92: Train Loss: 0.10891894996166229, Validation Loss: 0.4894745349884033\n",
      "Epoch 93: Train Loss: 0.071477010846138, Validation Loss: 0.4653337895870209\n",
      "Epoch 94: Train Loss: 0.09369778881470363, Validation Loss: 0.44762423634529114\n",
      "Epoch 95: Train Loss: 0.0810245896379153, Validation Loss: 0.45068275928497314\n",
      "Epoch 96: Train Loss: 0.08534657210111618, Validation Loss: 0.4596281349658966\n",
      "Epoch 97: Train Loss: 0.08604283382495244, Validation Loss: 0.4612318277359009\n",
      "Epoch 98: Train Loss: 0.06652891511718433, Validation Loss: 0.4653077721595764\n",
      "Epoch 99: Train Loss: 0.1129138246178627, Validation Loss: 0.46643921732902527\n",
      "Fold 3\n",
      "Epoch 0: Train Loss: 0.6886764764785767, Validation Loss: 0.68285071849823\n",
      "Epoch 1: Train Loss: 0.6107204159100851, Validation Loss: 0.6731801629066467\n",
      "Epoch 2: Train Loss: 0.5821497440338135, Validation Loss: 0.6628751158714294\n",
      "Epoch 3: Train Loss: 0.5600547393163046, Validation Loss: 0.6510895490646362\n",
      "Epoch 4: Train Loss: 0.5320315361022949, Validation Loss: 0.6382333040237427\n",
      "Epoch 5: Train Loss: 0.5130967895189921, Validation Loss: 0.6263800263404846\n",
      "Epoch 6: Train Loss: 0.4967509110768636, Validation Loss: 0.6151357293128967\n",
      "Epoch 7: Train Loss: 0.49164730310440063, Validation Loss: 0.6057239770889282\n",
      "Epoch 8: Train Loss: 0.4833636184533437, Validation Loss: 0.5982452034950256\n",
      "Epoch 9: Train Loss: 0.48504769802093506, Validation Loss: 0.5921280980110168\n",
      "Epoch 10: Train Loss: 0.4721229573090871, Validation Loss: 0.5756450295448303\n",
      "Epoch 11: Train Loss: 0.43953941265741986, Validation Loss: 0.5617363452911377\n",
      "Epoch 12: Train Loss: 0.4467102785905202, Validation Loss: 0.5516667366027832\n",
      "Epoch 13: Train Loss: 0.3977595070997874, Validation Loss: 0.5480510592460632\n",
      "Epoch 14: Train Loss: 0.395650049050649, Validation Loss: 0.5448657274246216\n",
      "Epoch 15: Train Loss: 0.3820078472296397, Validation Loss: 0.5427278876304626\n",
      "Epoch 16: Train Loss: 0.3660114606221517, Validation Loss: 0.5418722033500671\n",
      "Epoch 17: Train Loss: 0.35246684153874713, Validation Loss: 0.5412164926528931\n",
      "Epoch 18: Train Loss: 0.34237539768218994, Validation Loss: 0.5409812927246094\n",
      "Epoch 19: Train Loss: 0.3441331684589386, Validation Loss: 0.5408667325973511\n",
      "Epoch 20: Train Loss: 0.33691280086835224, Validation Loss: 0.5401120781898499\n",
      "Epoch 21: Train Loss: 0.32474644978841144, Validation Loss: 0.5374134182929993\n",
      "Epoch 22: Train Loss: 0.2993358572324117, Validation Loss: 0.536474347114563\n",
      "Epoch 23: Train Loss: 0.2987801829973857, Validation Loss: 0.5384189486503601\n",
      "Epoch 24: Train Loss: 0.2760728398958842, Validation Loss: 0.5411071181297302\n",
      "Epoch 25: Train Loss: 0.2978932062784831, Validation Loss: 0.5466159582138062\n",
      "Epoch 26: Train Loss: 0.30311261614163715, Validation Loss: 0.5467111468315125\n",
      "Epoch 27: Train Loss: 0.2775689313809077, Validation Loss: 0.5480661988258362\n",
      "Epoch 28: Train Loss: 0.2578916748364766, Validation Loss: 0.5471872687339783\n",
      "Epoch 29: Train Loss: 0.2476534346739451, Validation Loss: 0.5462623238563538\n",
      "Epoch 30: Train Loss: 0.24737518529097238, Validation Loss: 0.5437933206558228\n",
      "Epoch 31: Train Loss: 0.2707171042760213, Validation Loss: 0.5503643155097961\n",
      "Epoch 32: Train Loss: 0.24353582163651785, Validation Loss: 0.5588274002075195\n",
      "Epoch 33: Train Loss: 0.23377273480097452, Validation Loss: 0.5603114366531372\n",
      "Epoch 34: Train Loss: 0.20672686398029327, Validation Loss: 0.560107946395874\n",
      "Epoch 35: Train Loss: 0.23790401220321655, Validation Loss: 0.5561677813529968\n",
      "Epoch 36: Train Loss: 0.22306511302789053, Validation Loss: 0.557809591293335\n",
      "Epoch 37: Train Loss: 0.2379262795050939, Validation Loss: 0.5587841868400574\n",
      "Epoch 38: Train Loss: 0.1875905841588974, Validation Loss: 0.560312807559967\n",
      "Epoch 39: Train Loss: 0.1868151972691218, Validation Loss: 0.5575981736183167\n",
      "Epoch 40: Train Loss: 0.1873616725206375, Validation Loss: 0.5675315856933594\n",
      "Epoch 41: Train Loss: 0.18350920577843985, Validation Loss: 0.5650321245193481\n",
      "Epoch 42: Train Loss: 0.19671301047007242, Validation Loss: 0.5734214782714844\n",
      "Epoch 43: Train Loss: 0.1839658866326014, Validation Loss: 0.5788872241973877\n",
      "Epoch 44: Train Loss: 0.16548225780328116, Validation Loss: 0.5809274911880493\n",
      "Epoch 45: Train Loss: 0.1592698966463407, Validation Loss: 0.5797777771949768\n",
      "Epoch 46: Train Loss: 0.1607417712608973, Validation Loss: 0.5776829123497009\n",
      "Epoch 47: Train Loss: 0.17325113713741302, Validation Loss: 0.5754051208496094\n",
      "Epoch 48: Train Loss: 0.1659471740325292, Validation Loss: 0.5738816261291504\n",
      "Epoch 49: Train Loss: 0.1996699720621109, Validation Loss: 0.5682857632637024\n",
      "Epoch 50: Train Loss: 0.1667123188575109, Validation Loss: 0.5786570906639099\n",
      "Epoch 51: Train Loss: 0.1915619820356369, Validation Loss: 0.5741305351257324\n",
      "Epoch 52: Train Loss: 0.15027121702829996, Validation Loss: 0.5729492902755737\n",
      "Epoch 53: Train Loss: 0.14559580385684967, Validation Loss: 0.5588764548301697\n",
      "Epoch 54: Train Loss: 0.155247171719869, Validation Loss: 0.5615759491920471\n",
      "Epoch 55: Train Loss: 0.17529596388339996, Validation Loss: 0.5702927708625793\n",
      "Epoch 56: Train Loss: 0.14918937037388483, Validation Loss: 0.5789568424224854\n",
      "Epoch 57: Train Loss: 0.13443725307782492, Validation Loss: 0.5850112438201904\n",
      "Epoch 58: Train Loss: 0.126794862250487, Validation Loss: 0.5883763432502747\n",
      "Epoch 59: Train Loss: 0.13322697083155313, Validation Loss: 0.5886302590370178\n",
      "Epoch 60: Train Loss: 0.12765382478634515, Validation Loss: 0.6316594481468201\n",
      "Epoch 61: Train Loss: 0.12948834151029587, Validation Loss: 0.645673930644989\n",
      "Epoch 62: Train Loss: 0.13602404296398163, Validation Loss: 0.627086341381073\n",
      "Epoch 63: Train Loss: 0.13016929974158606, Validation Loss: 0.6026042103767395\n",
      "Epoch 64: Train Loss: 0.11296291400988896, Validation Loss: 0.5911461114883423\n",
      "Epoch 65: Train Loss: 0.11339329431454341, Validation Loss: 0.5987470746040344\n",
      "Epoch 66: Train Loss: 0.17915349950393042, Validation Loss: 0.6093153953552246\n",
      "Epoch 67: Train Loss: 0.11711959044138591, Validation Loss: 0.6185792088508606\n",
      "Epoch 68: Train Loss: 0.1051601693034172, Validation Loss: 0.6197193264961243\n",
      "Epoch 69: Train Loss: 0.12229676793018977, Validation Loss: 0.6210566163063049\n",
      "Epoch 70: Train Loss: 0.11028422911961873, Validation Loss: 0.6125615239143372\n",
      "Epoch 71: Train Loss: 0.11936527242263158, Validation Loss: 0.600387692451477\n",
      "Epoch 72: Train Loss: 0.11307444175084432, Validation Loss: 0.5995627641677856\n",
      "Epoch 73: Train Loss: 0.1027647132674853, Validation Loss: 0.6054890155792236\n",
      "Epoch 74: Train Loss: 0.14433750758568445, Validation Loss: 0.6126051545143127\n",
      "Epoch 75: Train Loss: 0.09176069746414821, Validation Loss: 0.6251434683799744\n",
      "Epoch 76: Train Loss: 0.1106035312016805, Validation Loss: 0.6357158422470093\n",
      "Epoch 77: Train Loss: 0.09310251226027806, Validation Loss: 0.639570415019989\n",
      "Epoch 78: Train Loss: 0.09591861814260483, Validation Loss: 0.6406586766242981\n",
      "Epoch 79: Train Loss: 0.09248265872399013, Validation Loss: 0.6395553350448608\n",
      "Epoch 80: Train Loss: 0.10865561912457149, Validation Loss: 0.6431426405906677\n",
      "Epoch 81: Train Loss: 0.1079128086566925, Validation Loss: 0.6348091959953308\n",
      "Epoch 82: Train Loss: 0.12844849626223245, Validation Loss: 0.6520267724990845\n",
      "Epoch 83: Train Loss: 0.09844085574150085, Validation Loss: 0.7189384698867798\n",
      "Epoch 84: Train Loss: 0.11602795869112015, Validation Loss: 0.736325204372406\n",
      "Epoch 85: Train Loss: 0.08451368659734726, Validation Loss: 0.7100541591644287\n",
      "Epoch 86: Train Loss: 0.08070041239261627, Validation Loss: 0.6912631988525391\n",
      "Epoch 87: Train Loss: 0.08274094015359879, Validation Loss: 0.6810707449913025\n",
      "Epoch 88: Train Loss: 0.07482294365763664, Validation Loss: 0.6785351037979126\n",
      "Epoch 89: Train Loss: 0.07874046762784322, Validation Loss: 0.6773932576179504\n",
      "Epoch 90: Train Loss: 0.12438647697369258, Validation Loss: 0.6651609539985657\n",
      "Epoch 91: Train Loss: 0.08702882627646129, Validation Loss: 0.6449266076087952\n",
      "Epoch 92: Train Loss: 0.07917895168066025, Validation Loss: 0.6227198243141174\n",
      "Epoch 93: Train Loss: 0.07408592353264491, Validation Loss: 0.6254280209541321\n",
      "Epoch 94: Train Loss: 0.07071722795565923, Validation Loss: 0.6352748870849609\n",
      "Epoch 95: Train Loss: 0.0726408138871193, Validation Loss: 0.6352313756942749\n",
      "Epoch 96: Train Loss: 0.06949043149749438, Validation Loss: 0.6464414596557617\n",
      "Epoch 97: Train Loss: 0.070959967871507, Validation Loss: 0.6499375700950623\n",
      "Epoch 98: Train Loss: 0.06516112635533015, Validation Loss: 0.6535688638687134\n",
      "Epoch 99: Train Loss: 0.06700888772805531, Validation Loss: 0.6511953473091125\n",
      "Fold 4\n",
      "Epoch 0: Train Loss: 0.693620761235555, Validation Loss: 0.6984713077545166\n",
      "Epoch 1: Train Loss: 0.604946513970693, Validation Loss: 0.693665087223053\n",
      "Epoch 2: Train Loss: 0.563931941986084, Validation Loss: 0.6879966259002686\n",
      "Epoch 3: Train Loss: 0.5214561720689138, Validation Loss: 0.6819618940353394\n",
      "Epoch 4: Train Loss: 0.5127544899781545, Validation Loss: 0.6749860644340515\n",
      "Epoch 5: Train Loss: 0.4953772226969401, Validation Loss: 0.6681466102600098\n",
      "Epoch 6: Train Loss: 0.458857258160909, Validation Loss: 0.661385178565979\n",
      "Epoch 7: Train Loss: 0.4589198629061381, Validation Loss: 0.6554768085479736\n",
      "Epoch 8: Train Loss: 0.4572192132472992, Validation Loss: 0.6497820615768433\n",
      "Epoch 9: Train Loss: 0.4507724543412526, Validation Loss: 0.644480288028717\n",
      "Epoch 10: Train Loss: 0.45536741614341736, Validation Loss: 0.6287415623664856\n",
      "Epoch 11: Train Loss: 0.4255494872728984, Validation Loss: 0.6152907013893127\n",
      "Epoch 12: Train Loss: 0.411580224831899, Validation Loss: 0.6016706824302673\n",
      "Epoch 13: Train Loss: 0.3954093356927236, Validation Loss: 0.5882003307342529\n",
      "Epoch 14: Train Loss: 0.3691568076610565, Validation Loss: 0.575701117515564\n",
      "Epoch 15: Train Loss: 0.3746141195297241, Validation Loss: 0.5692569613456726\n",
      "Epoch 16: Train Loss: 0.35339803496996564, Validation Loss: 0.5627699494361877\n",
      "Epoch 17: Train Loss: 0.34823761383692425, Validation Loss: 0.560080885887146\n",
      "Epoch 18: Train Loss: 0.376137634118398, Validation Loss: 0.5577231049537659\n",
      "Epoch 19: Train Loss: 0.34528347849845886, Validation Loss: 0.5573802590370178\n",
      "Epoch 20: Train Loss: 0.3393612901369731, Validation Loss: 0.5565289855003357\n",
      "Epoch 21: Train Loss: 0.34254563848177594, Validation Loss: 0.5509337782859802\n",
      "Epoch 22: Train Loss: 0.3482683797677358, Validation Loss: 0.5401426553726196\n",
      "Epoch 23: Train Loss: 0.3154961168766022, Validation Loss: 0.5265339612960815\n",
      "Epoch 24: Train Loss: 0.29573219021161395, Validation Loss: 0.5138194561004639\n",
      "Epoch 25: Train Loss: 0.29234620928764343, Validation Loss: 0.5065018534660339\n",
      "Epoch 26: Train Loss: 0.27940577268600464, Validation Loss: 0.5013267993927002\n",
      "Epoch 27: Train Loss: 0.2799912591775258, Validation Loss: 0.49829041957855225\n",
      "Epoch 28: Train Loss: 0.2762841482957204, Validation Loss: 0.49820655584335327\n",
      "Epoch 29: Train Loss: 0.2747197151184082, Validation Loss: 0.49746495485305786\n",
      "Epoch 30: Train Loss: 0.2938484599192937, Validation Loss: 0.48554375767707825\n",
      "Epoch 31: Train Loss: 0.26380207141240436, Validation Loss: 0.47953665256500244\n",
      "Epoch 32: Train Loss: 0.25046318769454956, Validation Loss: 0.46971395611763\n",
      "Epoch 33: Train Loss: 0.2545756349960963, Validation Loss: 0.4658782482147217\n",
      "Epoch 34: Train Loss: 0.24683178464571634, Validation Loss: 0.4623567759990692\n",
      "Epoch 35: Train Loss: 0.23196702202161154, Validation Loss: 0.45747464895248413\n",
      "Epoch 36: Train Loss: 0.23744449019432068, Validation Loss: 0.4498266875743866\n",
      "Epoch 37: Train Loss: 0.22681648035844168, Validation Loss: 0.44869014620780945\n",
      "Epoch 38: Train Loss: 0.21032066643238068, Validation Loss: 0.44929227232933044\n",
      "Epoch 39: Train Loss: 0.24071812629699707, Validation Loss: 0.44988274574279785\n",
      "Epoch 40: Train Loss: 0.2114713490009308, Validation Loss: 0.4658706486225128\n",
      "Epoch 41: Train Loss: 0.24857680002848306, Validation Loss: 0.4516623318195343\n",
      "Epoch 42: Train Loss: 0.20294202864170074, Validation Loss: 0.44101282954216003\n",
      "Epoch 43: Train Loss: 0.1949773927529653, Validation Loss: 0.43634194135665894\n",
      "Epoch 44: Train Loss: 0.18708705405394235, Validation Loss: 0.4280373454093933\n",
      "Epoch 45: Train Loss: 0.1936954657236735, Validation Loss: 0.4188857674598694\n",
      "Epoch 46: Train Loss: 0.19785338640213013, Validation Loss: 0.42131784558296204\n",
      "Epoch 47: Train Loss: 0.20428670942783356, Validation Loss: 0.41804561018943787\n",
      "Epoch 48: Train Loss: 0.18564231197039285, Validation Loss: 0.4182775914669037\n",
      "Epoch 49: Train Loss: 0.20265674591064453, Validation Loss: 0.41880136728286743\n",
      "Epoch 50: Train Loss: 0.17746013899644217, Validation Loss: 0.4288903772830963\n",
      "Epoch 51: Train Loss: 0.17516202727953592, Validation Loss: 0.42195066809654236\n",
      "Epoch 52: Train Loss: 0.1684762438138326, Validation Loss: 0.40133342146873474\n",
      "Epoch 53: Train Loss: 0.18374134600162506, Validation Loss: 0.41369280219078064\n",
      "Epoch 54: Train Loss: 0.16545895238717398, Validation Loss: 0.41653355956077576\n",
      "Epoch 55: Train Loss: 0.1674651155869166, Validation Loss: 0.40508392453193665\n",
      "Epoch 56: Train Loss: 0.14494628210862479, Validation Loss: 0.39663925766944885\n",
      "Epoch 57: Train Loss: 0.1600565661986669, Validation Loss: 0.3912852704524994\n",
      "Epoch 58: Train Loss: 0.15144485731919607, Validation Loss: 0.39236515760421753\n",
      "Epoch 59: Train Loss: 0.15696906050046286, Validation Loss: 0.39570388197898865\n",
      "Epoch 60: Train Loss: 0.14483455320199332, Validation Loss: 0.42776164412498474\n",
      "Epoch 61: Train Loss: 0.16065559287865958, Validation Loss: 0.4064933955669403\n",
      "Epoch 62: Train Loss: 0.136202206214269, Validation Loss: 0.37922510504722595\n",
      "Epoch 63: Train Loss: 0.1368104045589765, Validation Loss: 0.3508549630641937\n",
      "Epoch 64: Train Loss: 0.18965541323026022, Validation Loss: 0.3522641956806183\n",
      "Epoch 65: Train Loss: 0.13625837365786234, Validation Loss: 0.37123385071754456\n",
      "Epoch 66: Train Loss: 0.14850925157467523, Validation Loss: 0.38863593339920044\n",
      "Epoch 67: Train Loss: 0.12874223540226618, Validation Loss: 0.38590365648269653\n",
      "Epoch 68: Train Loss: 0.13288163393735886, Validation Loss: 0.379106342792511\n",
      "Epoch 69: Train Loss: 0.16108172635237375, Validation Loss: 0.38076063990592957\n",
      "Epoch 70: Train Loss: 0.16160397976636887, Validation Loss: 0.3520473539829254\n",
      "Epoch 71: Train Loss: 0.1779207835594813, Validation Loss: 0.3983197510242462\n",
      "Epoch 72: Train Loss: 0.1718833049138387, Validation Loss: 0.46422961354255676\n",
      "Epoch 73: Train Loss: 0.14568307747443518, Validation Loss: 0.3939273953437805\n",
      "Epoch 74: Train Loss: 0.14515583217144012, Validation Loss: 0.36929720640182495\n",
      "Epoch 75: Train Loss: 0.12752210100491843, Validation Loss: 0.373371422290802\n",
      "Epoch 76: Train Loss: 0.11392597109079361, Validation Loss: 0.385943740606308\n",
      "Epoch 77: Train Loss: 0.12363985429207484, Validation Loss: 0.39820098876953125\n",
      "Epoch 78: Train Loss: 0.1227614978949229, Validation Loss: 0.4061661958694458\n",
      "Epoch 79: Train Loss: 0.14053438107172647, Validation Loss: 0.40255284309387207\n",
      "Epoch 80: Train Loss: 0.11905424793561299, Validation Loss: 0.45197123289108276\n",
      "Epoch 81: Train Loss: 0.1107228547334671, Validation Loss: 0.39290910959243774\n",
      "Epoch 82: Train Loss: 0.14025789747635523, Validation Loss: 0.35146406292915344\n",
      "Epoch 83: Train Loss: 0.11204916735490163, Validation Loss: 0.3363070487976074\n",
      "Epoch 84: Train Loss: 0.10724776983261108, Validation Loss: 0.34216558933258057\n",
      "Epoch 85: Train Loss: 0.11451972275972366, Validation Loss: 0.3421984016895294\n",
      "Epoch 86: Train Loss: 0.08987699449062347, Validation Loss: 0.3589923679828644\n",
      "Epoch 87: Train Loss: 0.08843495448430379, Validation Loss: 0.3656388521194458\n",
      "Epoch 88: Train Loss: 0.10504338641961415, Validation Loss: 0.3716800808906555\n",
      "Epoch 89: Train Loss: 0.09124944855769475, Validation Loss: 0.3722386360168457\n",
      "Epoch 90: Train Loss: 0.10745810965696971, Validation Loss: 0.39911535382270813\n",
      "Epoch 91: Train Loss: 0.09609269102414449, Validation Loss: 0.41424450278282166\n",
      "Epoch 92: Train Loss: 0.11169460912545522, Validation Loss: 0.40842562913894653\n",
      "Epoch 93: Train Loss: 0.10597781588633855, Validation Loss: 0.39952489733695984\n",
      "Epoch 94: Train Loss: 0.08346701661745708, Validation Loss: 0.38075751066207886\n",
      "Epoch 95: Train Loss: 0.08884964138269424, Validation Loss: 0.3505686819553375\n",
      "Epoch 96: Train Loss: 0.08982716252406438, Validation Loss: 0.33741268515586853\n",
      "Epoch 97: Train Loss: 0.1083596001068751, Validation Loss: 0.3419265151023865\n",
      "Epoch 98: Train Loss: 0.0854024092356364, Validation Loss: 0.34043821692466736\n",
      "Epoch 99: Train Loss: 0.10211626440286636, Validation Loss: 0.3384825587272644\n",
      "Fold 5\n",
      "Epoch 0: Train Loss: 0.6913621226946512, Validation Loss: 0.7008848786354065\n",
      "Epoch 1: Train Loss: 0.5930623610814413, Validation Loss: 0.702698826789856\n",
      "Epoch 2: Train Loss: 0.5571256081263224, Validation Loss: 0.699758768081665\n",
      "Epoch 3: Train Loss: 0.53005983432134, Validation Loss: 0.698027491569519\n",
      "Epoch 4: Train Loss: 0.5161705215771993, Validation Loss: 0.6991411447525024\n",
      "Epoch 5: Train Loss: 0.4937868018945058, Validation Loss: 0.7017773985862732\n",
      "Epoch 6: Train Loss: 0.4905223449071248, Validation Loss: 0.70594722032547\n",
      "Epoch 7: Train Loss: 0.4636017481486003, Validation Loss: 0.7125025391578674\n",
      "Epoch 8: Train Loss: 0.4656568268934886, Validation Loss: 0.7228474617004395\n",
      "Epoch 9: Train Loss: 0.46130041281382245, Validation Loss: 0.7381576299667358\n",
      "Epoch 10: Train Loss: 0.4660826027393341, Validation Loss: 0.7285401821136475\n",
      "Epoch 11: Train Loss: 0.42488303780555725, Validation Loss: 0.7152644991874695\n",
      "Epoch 12: Train Loss: 0.4193274974822998, Validation Loss: 0.7022045850753784\n",
      "Epoch 13: Train Loss: 0.40092362960179645, Validation Loss: 0.6956815123558044\n",
      "Epoch 14: Train Loss: 0.36650875210762024, Validation Loss: 0.6910133957862854\n",
      "Epoch 15: Train Loss: 0.35767698287963867, Validation Loss: 0.6896288394927979\n",
      "Epoch 16: Train Loss: 0.3364710013071696, Validation Loss: 0.6836497187614441\n",
      "Epoch 17: Train Loss: 0.33948907256126404, Validation Loss: 0.6750044226646423\n",
      "Epoch 18: Train Loss: 0.339581698179245, Validation Loss: 0.6762354373931885\n",
      "Epoch 19: Train Loss: 0.3299882610638936, Validation Loss: 0.675315797328949\n",
      "Epoch 20: Train Loss: 0.3238558868567149, Validation Loss: 0.6665951013565063\n",
      "Epoch 21: Train Loss: 0.29658061265945435, Validation Loss: 0.6481999754905701\n",
      "Epoch 22: Train Loss: 0.2842677930990855, Validation Loss: 0.6334507465362549\n",
      "Epoch 23: Train Loss: 0.27250147859255475, Validation Loss: 0.61891770362854\n",
      "Epoch 24: Train Loss: 0.2746969262758891, Validation Loss: 0.5885926485061646\n",
      "Epoch 25: Train Loss: 0.25347696244716644, Validation Loss: 0.5855808258056641\n",
      "Epoch 26: Train Loss: 0.24478348096211752, Validation Loss: 0.5922537446022034\n",
      "Epoch 27: Train Loss: 0.24897895256678262, Validation Loss: 0.6001677513122559\n",
      "Epoch 28: Train Loss: 0.2226144274075826, Validation Loss: 0.6060923337936401\n",
      "Epoch 29: Train Loss: 0.24903202056884766, Validation Loss: 0.6084379553794861\n",
      "Epoch 30: Train Loss: 0.23758549491564432, Validation Loss: 0.5879030227661133\n",
      "Epoch 31: Train Loss: 0.24920455614725748, Validation Loss: 0.5626420378684998\n",
      "Epoch 32: Train Loss: 0.2125951647758484, Validation Loss: 0.550581157207489\n",
      "Epoch 33: Train Loss: 0.2004136194785436, Validation Loss: 0.5430043935775757\n",
      "Epoch 34: Train Loss: 0.20399662355581918, Validation Loss: 0.5351858735084534\n",
      "Epoch 35: Train Loss: 0.19697884221871695, Validation Loss: 0.5311734080314636\n",
      "Epoch 36: Train Loss: 0.18237192928791046, Validation Loss: 0.5338441729545593\n",
      "Epoch 37: Train Loss: 0.18460369110107422, Validation Loss: 0.5370239019393921\n",
      "Epoch 38: Train Loss: 0.17730328937371573, Validation Loss: 0.5398087501525879\n",
      "Epoch 39: Train Loss: 0.17235090335210165, Validation Loss: 0.5405940413475037\n",
      "Epoch 40: Train Loss: 0.17975562810897827, Validation Loss: 0.5566805601119995\n",
      "Epoch 41: Train Loss: 0.1738345076640447, Validation Loss: 0.5523517727851868\n",
      "Epoch 42: Train Loss: 0.16902455190817514, Validation Loss: 0.5350674390792847\n",
      "Epoch 43: Train Loss: 0.1811530664563179, Validation Loss: 0.5328555703163147\n",
      "Epoch 44: Train Loss: 0.1568784068028132, Validation Loss: 0.5352259278297424\n",
      "Epoch 45: Train Loss: 0.15799374878406525, Validation Loss: 0.54051673412323\n",
      "Epoch 46: Train Loss: 0.14502046257257462, Validation Loss: 0.544585108757019\n",
      "Epoch 47: Train Loss: 0.14464335640271506, Validation Loss: 0.5463229417800903\n",
      "Epoch 48: Train Loss: 0.1565007915099462, Validation Loss: 0.5506598353385925\n",
      "Epoch 49: Train Loss: 0.15331191817919412, Validation Loss: 0.5533174276351929\n",
      "Epoch 50: Train Loss: 0.13696260998646417, Validation Loss: 0.541333794593811\n",
      "Epoch 51: Train Loss: 0.13460336128870645, Validation Loss: 0.5322205424308777\n",
      "Epoch 52: Train Loss: 0.13068735599517822, Validation Loss: 0.5352441668510437\n",
      "Epoch 53: Train Loss: 0.1288767953713735, Validation Loss: 0.5472736954689026\n",
      "Epoch 54: Train Loss: 0.14661772052447, Validation Loss: 0.5332359671592712\n",
      "Epoch 55: Train Loss: 0.11525347332159679, Validation Loss: 0.5361847877502441\n",
      "Epoch 56: Train Loss: 0.13344349215428034, Validation Loss: 0.5313608646392822\n",
      "Epoch 57: Train Loss: 0.14178830633560816, Validation Loss: 0.5323955416679382\n",
      "Epoch 58: Train Loss: 0.11062458902597427, Validation Loss: 0.5332267880439758\n",
      "Epoch 59: Train Loss: 0.11926533033450444, Validation Loss: 0.5353012681007385\n",
      "Epoch 60: Train Loss: 0.11510057747364044, Validation Loss: 0.5343782305717468\n",
      "Epoch 61: Train Loss: 0.10669301946957906, Validation Loss: 0.5389773845672607\n",
      "Epoch 62: Train Loss: 0.11349370082219441, Validation Loss: 0.5438912510871887\n",
      "Epoch 63: Train Loss: 0.10271754860877991, Validation Loss: 0.565491259098053\n",
      "Epoch 64: Train Loss: 0.10632896671692531, Validation Loss: 0.5687884092330933\n",
      "Epoch 65: Train Loss: 0.09951747208833694, Validation Loss: 0.5602905750274658\n",
      "Epoch 66: Train Loss: 0.11007752517859141, Validation Loss: 0.5401613116264343\n",
      "Epoch 67: Train Loss: 0.11012827605009079, Validation Loss: 0.5312801003456116\n",
      "Epoch 68: Train Loss: 0.09194945295651753, Validation Loss: 0.5296914577484131\n",
      "Epoch 69: Train Loss: 0.11656597008307774, Validation Loss: 0.5321909189224243\n",
      "Epoch 70: Train Loss: 0.09889338910579681, Validation Loss: 0.5181012749671936\n",
      "Epoch 71: Train Loss: 0.10938101013501485, Validation Loss: 0.5337186455726624\n",
      "Epoch 72: Train Loss: 0.10326167941093445, Validation Loss: 0.5415380597114563\n",
      "Epoch 73: Train Loss: 0.10526016354560852, Validation Loss: 0.5774579048156738\n",
      "Epoch 74: Train Loss: 0.08952705313762029, Validation Loss: 0.596829354763031\n",
      "Epoch 75: Train Loss: 0.08650460590918858, Validation Loss: 0.6025628447532654\n",
      "Epoch 76: Train Loss: 0.08100371311108272, Validation Loss: 0.5916718244552612\n",
      "Epoch 77: Train Loss: 0.07947610939542453, Validation Loss: 0.5810437202453613\n",
      "Epoch 78: Train Loss: 0.07843471318483353, Validation Loss: 0.5773452520370483\n",
      "Epoch 79: Train Loss: 0.07843782628575961, Validation Loss: 0.5771133899688721\n",
      "Epoch 80: Train Loss: 0.09021304547786713, Validation Loss: 0.5726673603057861\n",
      "Epoch 81: Train Loss: 0.10912500321865082, Validation Loss: 0.5544536709785461\n",
      "Epoch 82: Train Loss: 0.07866453876097997, Validation Loss: 0.5454865097999573\n",
      "Epoch 83: Train Loss: 0.09125139067570369, Validation Loss: 0.5789921283721924\n",
      "Epoch 84: Train Loss: 0.10286955162882805, Validation Loss: 0.6268096566200256\n",
      "Epoch 85: Train Loss: 0.12853635102510452, Validation Loss: 0.6111868619918823\n",
      "Epoch 86: Train Loss: 0.08155382176240285, Validation Loss: 0.5834953188896179\n",
      "Epoch 87: Train Loss: 0.10206471880276997, Validation Loss: 0.5621335506439209\n",
      "Epoch 88: Train Loss: 0.08739109585682552, Validation Loss: 0.5525087118148804\n",
      "Epoch 89: Train Loss: 0.0851585070292155, Validation Loss: 0.546623706817627\n",
      "Epoch 90: Train Loss: 0.09149570018053055, Validation Loss: 0.5435434579849243\n",
      "Epoch 91: Train Loss: 0.09575252731641133, Validation Loss: 0.5948402285575867\n",
      "Epoch 92: Train Loss: 0.08083771914243698, Validation Loss: 0.6508591175079346\n",
      "Epoch 93: Train Loss: 0.0944574624300003, Validation Loss: 0.6583802700042725\n",
      "Epoch 94: Train Loss: 0.07813813164830208, Validation Loss: 0.6083053946495056\n",
      "Epoch 95: Train Loss: 0.0668513389925162, Validation Loss: 0.5841588973999023\n",
      "Epoch 96: Train Loss: 0.0784213791290919, Validation Loss: 0.5754578113555908\n",
      "Epoch 97: Train Loss: 0.06834689776102702, Validation Loss: 0.5705095529556274\n",
      "Epoch 98: Train Loss: 0.06852925692995389, Validation Loss: 0.5657100677490234\n",
      "Epoch 99: Train Loss: 0.07335414985815684, Validation Loss: 0.5659692883491516\n",
      "Accuracy: 0.8083333333333333,Precision: 0.8363636363636363, Recall: 0.7666666666666667, F1-score: 0.8, AUC: 0.8083333333333332\n",
      "Confusion Matrix:\n",
      "[[51  9]\n",
      " [14 46]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nweight decay = 1e-7\\nlearning rate = 0.0001\\nepoch = 100\\nbatch size = 32\\nearly stopping patience = 10\\nstandard scaler\\nReLU\\ncross entropy loss\\ndrop out = 0.8\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "model_save_dir = r'C:\\Users\\User\\Documents\\Lie detect data\\Model'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Define a function to load and pad data\n",
    "def load_data(data_dir, max_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    file_list = os.listdir(data_dir)\n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        label = 1 if 'truth' in file else 0\n",
    "        padded_data = np.zeros((65, max_length))\n",
    "        length = min(data.shape[1], max_length)\n",
    "        padded_data[:, :length] = data[:, :length]\n",
    "        X.append(padded_data)\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load dataset and pad the data\n",
    "data_dir = \"C:\\\\Users\\\\User\\\\Documents\\\\Lie detect data\\\\56M_DWTEEGData\"\n",
    "max_length = 500  # Define maximum length for padding\n",
    "X, y = load_data(data_dir, max_length)\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# Define EEGNet model\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(65, 32, kernel_size=63, padding=31)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "        self.depthwiseConv1d = nn.Conv1d(32, 64, kernel_size=65, groups=32, padding=32)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)  # Additional convolutional layer\n",
    "        self.batchnorm3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.pooling = nn.AvgPool1d(kernel_size=4)\n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        \n",
    "        self._calculate_num_features()\n",
    "        self.fc = nn.Linear(self.num_features, num_classes)\n",
    "\n",
    "    def _calculate_num_features(self):\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.zeros(1, 65, 1400)\n",
    "            sample_output = self._forward_features(sample_input)\n",
    "            self.num_features = sample_output.shape[1]\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.depthwiseConv1d(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.conv2(x)  # Additional convolutional layer\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.pooling(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.global_pool(x)  # Global average pooling layer\n",
    "        x = torch.flatten(x, 1)  # Flatten the output for the fully connected layer\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_and_evaluate(train_loader, val_loader, y_train):\n",
    "    model = EEGNet(num_classes=2).to(device)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "    num_epochs = 100\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 100\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "            fold_model_path = os.path.join(model_save_dir, f'fold3_model_fold_{fold_idx}.pth')\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': best_val_loss,\n",
    "            }, fold_model_path)\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_idx = 0\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "for train_index, val_index in kf.split(X, y):\n",
    "    print(f'Fold {fold_idx + 1}')\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Normalize data using scaler fitted on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_val = scaler.transform(X_val.reshape(X_val.shape[0], -1))\n",
    "    X_train = X_train.reshape(-1, 65, max_length)\n",
    "    X_val = X_val.reshape(-1, 65, max_length)\n",
    "\n",
    "    # Save the scaler to a file\n",
    "    with open(r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\simpleEEGNet_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = EEGDataset(X_train, y_train)\n",
    "    val_dataset = EEGDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = train_and_evaluate(train_loader, val_loader, y_train)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    fold_idx += 1\n",
    "\n",
    "\n",
    "# Calculate additional metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "auc = roc_auc_score(all_labels, all_predictions)\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy},Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "weight decay = 1e-7\n",
    "learning rate = 0.0001\n",
    "epoch = 100\n",
    "batch size = 32\n",
    "early stopping patience = 10\n",
    "standard scaler\n",
    "ReLU\n",
    "cross entropy loss\n",
    "drop out = 0.8\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66654d63-c450-4d9f-a3e3-63701fd1d41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934cb89-5ea1-4eb0-a152-00c71e49e06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
