{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "656cdbc2-da54-45ba-97fc-195b59bd7820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 0: Train Loss: 0.6822081009546915, Validation Loss: 0.6616445779800415\n",
      "Epoch 1: Train Loss: 0.6350602904955546, Validation Loss: 0.6529412269592285\n",
      "Epoch 2: Train Loss: 0.604487140973409, Validation Loss: 0.6395267248153687\n",
      "Epoch 3: Train Loss: 0.5838523109753927, Validation Loss: 0.6243512034416199\n",
      "Epoch 4: Train Loss: 0.5828545689582825, Validation Loss: 0.6090649962425232\n",
      "Epoch 5: Train Loss: 0.559188723564148, Validation Loss: 0.5932019948959351\n",
      "Epoch 6: Train Loss: 0.555841306845347, Validation Loss: 0.5803635120391846\n",
      "Epoch 7: Train Loss: 0.5466872851053873, Validation Loss: 0.5687816739082336\n",
      "Epoch 8: Train Loss: 0.536859134833018, Validation Loss: 0.5595747232437134\n",
      "Epoch 9: Train Loss: 0.537756343682607, Validation Loss: 0.5518912672996521\n",
      "Epoch 10: Train Loss: 0.5451904137929281, Validation Loss: 0.53648442029953\n",
      "Epoch 11: Train Loss: 0.5124578376611074, Validation Loss: 0.5226565003395081\n",
      "Epoch 12: Train Loss: 0.5183619856834412, Validation Loss: 0.5085954070091248\n",
      "Epoch 13: Train Loss: 0.4945578376452128, Validation Loss: 0.49509644508361816\n",
      "Epoch 14: Train Loss: 0.4793398479620616, Validation Loss: 0.4834643304347992\n",
      "Epoch 15: Train Loss: 0.4670265118281047, Validation Loss: 0.47280353307724\n",
      "Epoch 16: Train Loss: 0.47685953974723816, Validation Loss: 0.46489301323890686\n",
      "Epoch 17: Train Loss: 0.46677517890930176, Validation Loss: 0.4595681428909302\n",
      "Epoch 18: Train Loss: 0.44789045055707294, Validation Loss: 0.455666184425354\n",
      "Epoch 19: Train Loss: 0.4571912884712219, Validation Loss: 0.4531363844871521\n",
      "Epoch 20: Train Loss: 0.4538674255212148, Validation Loss: 0.4576745629310608\n",
      "Epoch 21: Train Loss: 0.43940385182698566, Validation Loss: 0.45170968770980835\n",
      "Epoch 22: Train Loss: 0.4406224588553111, Validation Loss: 0.44528305530548096\n",
      "Epoch 23: Train Loss: 0.40359823902448017, Validation Loss: 0.4432433843612671\n",
      "Epoch 24: Train Loss: 0.41296835740407306, Validation Loss: 0.44098395109176636\n",
      "Epoch 25: Train Loss: 0.39604563514391583, Validation Loss: 0.4410754442214966\n",
      "Epoch 26: Train Loss: 0.3839563528696696, Validation Loss: 0.4391985237598419\n",
      "Epoch 27: Train Loss: 0.397571196158727, Validation Loss: 0.43606293201446533\n",
      "Epoch 28: Train Loss: 0.37321436405181885, Validation Loss: 0.4335939884185791\n",
      "Epoch 29: Train Loss: 0.3775760928789775, Validation Loss: 0.43238183856010437\n",
      "Epoch 30: Train Loss: 0.37728777527809143, Validation Loss: 0.42538267374038696\n",
      "Epoch 31: Train Loss: 0.3616800308227539, Validation Loss: 0.42883169651031494\n",
      "Epoch 32: Train Loss: 0.3606601059436798, Validation Loss: 0.41967830061912537\n",
      "Epoch 33: Train Loss: 0.35014665126800537, Validation Loss: 0.410333514213562\n",
      "Epoch 34: Train Loss: 0.3371327718098958, Validation Loss: 0.41113534569740295\n",
      "Epoch 35: Train Loss: 0.3260755141576131, Validation Loss: 0.41782495379447937\n",
      "Epoch 36: Train Loss: 0.3255436917146047, Validation Loss: 0.4202478229999542\n",
      "Epoch 37: Train Loss: 0.3254108627637227, Validation Loss: 0.4196579158306122\n",
      "Epoch 38: Train Loss: 0.3216446042060852, Validation Loss: 0.4194779098033905\n",
      "Epoch 39: Train Loss: 0.31769593556722003, Validation Loss: 0.4194921851158142\n",
      "Epoch 40: Train Loss: 0.3128754099210103, Validation Loss: 0.41394248604774475\n",
      "Epoch 41: Train Loss: 0.322227418422699, Validation Loss: 0.4100564122200012\n",
      "Epoch 42: Train Loss: 0.31432100137074787, Validation Loss: 0.42238709330558777\n",
      "Epoch 43: Train Loss: 0.3025520245234172, Validation Loss: 0.4481455981731415\n",
      "Epoch 44: Train Loss: 0.2925252616405487, Validation Loss: 0.4467766582965851\n",
      "Epoch 45: Train Loss: 0.2902453790108363, Validation Loss: 0.4321348965167999\n",
      "Epoch 46: Train Loss: 0.2792007625102997, Validation Loss: 0.42339715361595154\n",
      "Epoch 47: Train Loss: 0.27939306696256, Validation Loss: 0.4224936366081238\n",
      "Epoch 48: Train Loss: 0.27253654102484387, Validation Loss: 0.42080679535865784\n",
      "Epoch 49: Train Loss: 0.2684619128704071, Validation Loss: 0.42044851183891296\n",
      "Epoch 50: Train Loss: 0.27407066027323407, Validation Loss: 0.4319465458393097\n",
      "Early stopping at epoch 51\n",
      "Fold 2\n",
      "Epoch 0: Train Loss: 0.7016117771466573, Validation Loss: 0.693436861038208\n",
      "Epoch 1: Train Loss: 0.6333331068356832, Validation Loss: 0.6860105991363525\n",
      "Epoch 2: Train Loss: 0.5935069719950358, Validation Loss: 0.6809541583061218\n",
      "Epoch 3: Train Loss: 0.5734188954035441, Validation Loss: 0.6749410033226013\n",
      "Epoch 4: Train Loss: 0.5558407107988993, Validation Loss: 0.6658276319503784\n",
      "Epoch 5: Train Loss: 0.5468138257662455, Validation Loss: 0.6569236516952515\n",
      "Epoch 6: Train Loss: 0.5363262295722961, Validation Loss: 0.6482061743736267\n",
      "Epoch 7: Train Loss: 0.5290490388870239, Validation Loss: 0.6391136050224304\n",
      "Epoch 8: Train Loss: 0.52154740691185, Validation Loss: 0.6313260197639465\n",
      "Epoch 9: Train Loss: 0.5146397749582926, Validation Loss: 0.6249352693557739\n",
      "Epoch 10: Train Loss: 0.5074792702992758, Validation Loss: 0.6182946562767029\n",
      "Epoch 11: Train Loss: 0.4938221573829651, Validation Loss: 0.6168862581253052\n",
      "Epoch 12: Train Loss: 0.4794410765171051, Validation Loss: 0.6093383431434631\n",
      "Epoch 13: Train Loss: 0.4593835671742757, Validation Loss: 0.5947356224060059\n",
      "Epoch 14: Train Loss: 0.4459221561749776, Validation Loss: 0.5751889944076538\n",
      "Epoch 15: Train Loss: 0.43695234258969623, Validation Loss: 0.5693656206130981\n",
      "Epoch 16: Train Loss: 0.42092881600062054, Validation Loss: 0.5625377893447876\n",
      "Epoch 17: Train Loss: 0.4207564095656077, Validation Loss: 0.5572137832641602\n",
      "Epoch 18: Train Loss: 0.4247834583123525, Validation Loss: 0.5555055737495422\n",
      "Epoch 19: Train Loss: 0.41565313935279846, Validation Loss: 0.5543592572212219\n",
      "Epoch 20: Train Loss: 0.42103686928749084, Validation Loss: 0.5509706735610962\n",
      "Epoch 21: Train Loss: 0.4033552308877309, Validation Loss: 0.5565125942230225\n",
      "Epoch 22: Train Loss: 0.38865522543589276, Validation Loss: 0.5424739718437195\n",
      "Epoch 23: Train Loss: 0.3850589493910472, Validation Loss: 0.5355083346366882\n",
      "Epoch 24: Train Loss: 0.3655640383561452, Validation Loss: 0.5334845781326294\n",
      "Epoch 25: Train Loss: 0.36495927969614667, Validation Loss: 0.5344207286834717\n",
      "Epoch 26: Train Loss: 0.3521938920021057, Validation Loss: 0.5328582525253296\n",
      "Epoch 27: Train Loss: 0.34626397490501404, Validation Loss: 0.5296583771705627\n",
      "Epoch 28: Train Loss: 0.34928667545318604, Validation Loss: 0.5288727283477783\n",
      "Epoch 29: Train Loss: 0.358140766620636, Validation Loss: 0.5297319889068604\n",
      "Epoch 30: Train Loss: 0.3426230748494466, Validation Loss: 0.5119444727897644\n",
      "Epoch 31: Train Loss: 0.32992105682690936, Validation Loss: 0.510707676410675\n",
      "Epoch 32: Train Loss: 0.3203057547410329, Validation Loss: 0.5017960667610168\n",
      "Epoch 33: Train Loss: 0.32535170515378314, Validation Loss: 0.49153685569763184\n",
      "Epoch 34: Train Loss: 0.30486488342285156, Validation Loss: 0.4903472661972046\n",
      "Epoch 35: Train Loss: 0.30296717087427777, Validation Loss: 0.49182644486427307\n",
      "Epoch 36: Train Loss: 0.2905401786168416, Validation Loss: 0.48482272028923035\n",
      "Epoch 37: Train Loss: 0.2894214987754822, Validation Loss: 0.48215562105178833\n",
      "Epoch 38: Train Loss: 0.2909836173057556, Validation Loss: 0.48209938406944275\n",
      "Epoch 39: Train Loss: 0.2889142731825511, Validation Loss: 0.4822715222835541\n",
      "Epoch 40: Train Loss: 0.28394115964571637, Validation Loss: 0.49502333998680115\n",
      "Epoch 41: Train Loss: 0.33611834049224854, Validation Loss: 0.5008940100669861\n",
      "Epoch 42: Train Loss: 0.279801607131958, Validation Loss: 0.48419612646102905\n",
      "Epoch 43: Train Loss: 0.2837161173423131, Validation Loss: 0.4683525860309601\n",
      "Epoch 44: Train Loss: 0.2636113266150157, Validation Loss: 0.4559367895126343\n",
      "Epoch 45: Train Loss: 0.26198258499304455, Validation Loss: 0.45181384682655334\n",
      "Epoch 46: Train Loss: 0.24554066359996796, Validation Loss: 0.4526880085468292\n",
      "Epoch 47: Train Loss: 0.2745749056339264, Validation Loss: 0.4620956778526306\n",
      "Epoch 48: Train Loss: 0.24308430155118307, Validation Loss: 0.466467022895813\n",
      "Epoch 49: Train Loss: 0.25799427429835003, Validation Loss: 0.4676397144794464\n",
      "Epoch 50: Train Loss: 0.2542739262183507, Validation Loss: 0.4584188759326935\n",
      "Epoch 51: Train Loss: 0.2465030699968338, Validation Loss: 0.5002223253250122\n",
      "Epoch 52: Train Loss: 0.23945651451746622, Validation Loss: 0.4513855278491974\n",
      "Epoch 53: Train Loss: 0.23281538486480713, Validation Loss: 0.4336634576320648\n",
      "Epoch 54: Train Loss: 0.234510600566864, Validation Loss: 0.42669227719306946\n",
      "Epoch 55: Train Loss: 0.21179876724878946, Validation Loss: 0.43160706758499146\n",
      "Epoch 56: Train Loss: 0.22465474406878153, Validation Loss: 0.4354654550552368\n",
      "Epoch 57: Train Loss: 0.2243408759435018, Validation Loss: 0.44131049513816833\n",
      "Epoch 58: Train Loss: 0.20803899566332498, Validation Loss: 0.43969205021858215\n",
      "Epoch 59: Train Loss: 0.2150276998678843, Validation Loss: 0.4459807574748993\n",
      "Epoch 60: Train Loss: 0.2165584365526835, Validation Loss: 0.45846301317214966\n",
      "Epoch 61: Train Loss: 0.2110730012257894, Validation Loss: 0.4698214828968048\n",
      "Epoch 62: Train Loss: 0.22280468543370566, Validation Loss: 0.425969660282135\n",
      "Epoch 63: Train Loss: 0.1941422571738561, Validation Loss: 0.4315551519393921\n",
      "Epoch 64: Train Loss: 0.20271695156892142, Validation Loss: 0.4591449499130249\n",
      "Epoch 65: Train Loss: 0.19081281622250876, Validation Loss: 0.4437147378921509\n",
      "Epoch 66: Train Loss: 0.18143483996391296, Validation Loss: 0.4231194257736206\n",
      "Epoch 67: Train Loss: 0.20396818220615387, Validation Loss: 0.41447362303733826\n",
      "Epoch 68: Train Loss: 0.17968743046124777, Validation Loss: 0.41095641255378723\n",
      "Epoch 69: Train Loss: 0.17275348802407584, Validation Loss: 0.4097690284252167\n",
      "Epoch 70: Train Loss: 0.1930911143620809, Validation Loss: 0.41813376545906067\n",
      "Epoch 71: Train Loss: 0.21291757126649222, Validation Loss: 0.40029260516166687\n",
      "Epoch 72: Train Loss: 0.18708636363347372, Validation Loss: 0.41178077459335327\n",
      "Epoch 73: Train Loss: 0.18076862394809723, Validation Loss: 0.5065019726753235\n",
      "Epoch 74: Train Loss: 0.1809436877568563, Validation Loss: 0.46914273500442505\n",
      "Epoch 75: Train Loss: 0.16833524902661642, Validation Loss: 0.42882758378982544\n",
      "Epoch 76: Train Loss: 0.18117342392603555, Validation Loss: 0.41328164935112\n",
      "Epoch 77: Train Loss: 0.16942399243513742, Validation Loss: 0.41097912192344666\n",
      "Epoch 78: Train Loss: 0.16256668666998544, Validation Loss: 0.4112897515296936\n",
      "Epoch 79: Train Loss: 0.17197943230470022, Validation Loss: 0.4101742208003998\n",
      "Epoch 80: Train Loss: 0.16608801235755286, Validation Loss: 0.4101630449295044\n",
      "Epoch 81: Train Loss: 0.15422086914380392, Validation Loss: 0.38170918822288513\n",
      "Epoch 82: Train Loss: 0.17379282911618552, Validation Loss: 0.431135892868042\n",
      "Epoch 83: Train Loss: 0.15437484284241995, Validation Loss: 0.4391483962535858\n",
      "Epoch 84: Train Loss: 0.150635356704394, Validation Loss: 0.4308824837207794\n",
      "Epoch 85: Train Loss: 0.1458670695622762, Validation Loss: 0.4265877306461334\n",
      "Epoch 86: Train Loss: 0.1404797782500585, Validation Loss: 0.42812663316726685\n",
      "Epoch 87: Train Loss: 0.13438095400730768, Validation Loss: 0.431803822517395\n",
      "Epoch 88: Train Loss: 0.1477635701497396, Validation Loss: 0.43154045939445496\n",
      "Epoch 89: Train Loss: 0.18951654185851416, Validation Loss: 0.4285300374031067\n",
      "Epoch 90: Train Loss: 0.14234709242979685, Validation Loss: 0.4016067385673523\n",
      "Early stopping at epoch 91\n",
      "Fold 3\n",
      "Epoch 0: Train Loss: 0.7399109800656637, Validation Loss: 0.7106335163116455\n",
      "Epoch 1: Train Loss: 0.6614195505777994, Validation Loss: 0.6963370442390442\n",
      "Epoch 2: Train Loss: 0.6258007685343424, Validation Loss: 0.6820792555809021\n",
      "Epoch 3: Train Loss: 0.6131983399391174, Validation Loss: 0.6663427352905273\n",
      "Epoch 4: Train Loss: 0.5904185970624288, Validation Loss: 0.6599350571632385\n",
      "Epoch 5: Train Loss: 0.5785929958025614, Validation Loss: 0.6571199893951416\n",
      "Epoch 6: Train Loss: 0.5689522822697958, Validation Loss: 0.6543679237365723\n",
      "Epoch 7: Train Loss: 0.5749962131182352, Validation Loss: 0.6523113250732422\n",
      "Epoch 8: Train Loss: 0.5849410692850748, Validation Loss: 0.6500992178916931\n",
      "Epoch 9: Train Loss: 0.5623518427213033, Validation Loss: 0.6483108401298523\n",
      "Epoch 10: Train Loss: 0.5614434679349264, Validation Loss: 0.6321768164634705\n",
      "Epoch 11: Train Loss: 0.5420829653739929, Validation Loss: 0.6129932999610901\n",
      "Epoch 12: Train Loss: 0.5160794059435526, Validation Loss: 0.590657651424408\n",
      "Epoch 13: Train Loss: 0.5013289153575897, Validation Loss: 0.5701289176940918\n",
      "Epoch 14: Train Loss: 0.49486151337623596, Validation Loss: 0.5583433508872986\n",
      "Epoch 15: Train Loss: 0.48955939213434857, Validation Loss: 0.5500816702842712\n",
      "Epoch 16: Train Loss: 0.47876065969467163, Validation Loss: 0.5478572249412537\n",
      "Epoch 17: Train Loss: 0.4728982448577881, Validation Loss: 0.5467236638069153\n",
      "Epoch 18: Train Loss: 0.4634395241737366, Validation Loss: 0.5476747751235962\n",
      "Epoch 19: Train Loss: 0.46623213092486065, Validation Loss: 0.5473745465278625\n",
      "Epoch 20: Train Loss: 0.45626171429951984, Validation Loss: 0.5456555485725403\n",
      "Epoch 21: Train Loss: 0.4486718376477559, Validation Loss: 0.5531554222106934\n",
      "Epoch 22: Train Loss: 0.4278334379196167, Validation Loss: 0.5658264756202698\n",
      "Epoch 23: Train Loss: 0.4111816684405009, Validation Loss: 0.5573536157608032\n",
      "Epoch 24: Train Loss: 0.403253177801768, Validation Loss: 0.542500376701355\n",
      "Epoch 25: Train Loss: 0.38617733120918274, Validation Loss: 0.5420477986335754\n",
      "Epoch 26: Train Loss: 0.3810742298762004, Validation Loss: 0.5248643755912781\n",
      "Epoch 27: Train Loss: 0.3941628336906433, Validation Loss: 0.5294188261032104\n",
      "Epoch 28: Train Loss: 0.37551156679789227, Validation Loss: 0.5306609272956848\n",
      "Epoch 29: Train Loss: 0.38486560185750324, Validation Loss: 0.5294008255004883\n",
      "Epoch 30: Train Loss: 0.37466328342755634, Validation Loss: 0.4988346993923187\n",
      "Epoch 31: Train Loss: 0.3587653636932373, Validation Loss: 0.4787481129169464\n",
      "Epoch 32: Train Loss: 0.3485061526298523, Validation Loss: 0.48871171474456787\n",
      "Epoch 33: Train Loss: 0.3419922689596812, Validation Loss: 0.5140085220336914\n",
      "Epoch 34: Train Loss: 0.3358393708864848, Validation Loss: 0.5032651424407959\n",
      "Epoch 35: Train Loss: 0.32665014266967773, Validation Loss: 0.48177310824394226\n",
      "Epoch 36: Train Loss: 0.3187638719876607, Validation Loss: 0.47350654006004333\n",
      "Epoch 37: Train Loss: 0.3403062621752421, Validation Loss: 0.4714110195636749\n",
      "Epoch 38: Train Loss: 0.30924737453460693, Validation Loss: 0.4658520221710205\n",
      "Epoch 39: Train Loss: 0.3264591693878174, Validation Loss: 0.4665285348892212\n",
      "Epoch 40: Train Loss: 0.3030267655849457, Validation Loss: 0.4717760384082794\n",
      "Epoch 41: Train Loss: 0.30683653553326923, Validation Loss: 0.4734798073768616\n",
      "Epoch 42: Train Loss: 0.28812835613886517, Validation Loss: 0.4439217448234558\n",
      "Epoch 43: Train Loss: 0.29574040571848553, Validation Loss: 0.42890864610671997\n",
      "Epoch 44: Train Loss: 0.2864815294742584, Validation Loss: 0.4263601303100586\n",
      "Epoch 45: Train Loss: 0.2665517081816991, Validation Loss: 0.42746350169181824\n",
      "Epoch 46: Train Loss: 0.2709743032852809, Validation Loss: 0.4287813603878021\n",
      "Epoch 47: Train Loss: 0.26637545228004456, Validation Loss: 0.429589182138443\n",
      "Epoch 48: Train Loss: 0.26117845873037976, Validation Loss: 0.4265116751194\n",
      "Epoch 49: Train Loss: 0.2656579464673996, Validation Loss: 0.42605623602867126\n",
      "Epoch 50: Train Loss: 0.2607612609863281, Validation Loss: 0.4171474874019623\n",
      "Epoch 51: Train Loss: 0.26642603675524396, Validation Loss: 0.41753384470939636\n",
      "Epoch 52: Train Loss: 0.25283183157444, Validation Loss: 0.4147258996963501\n",
      "Epoch 53: Train Loss: 0.26245637734731037, Validation Loss: 0.41746076941490173\n",
      "Epoch 54: Train Loss: 0.2385545919338862, Validation Loss: 0.41977354884147644\n",
      "Epoch 55: Train Loss: 0.24156303207079569, Validation Loss: 0.41103512048721313\n",
      "Epoch 56: Train Loss: 0.23276658356189728, Validation Loss: 0.4066936671733856\n",
      "Epoch 57: Train Loss: 0.24378735323747, Validation Loss: 0.40596428513526917\n",
      "Epoch 58: Train Loss: 0.2193811982870102, Validation Loss: 0.40549972653388977\n",
      "Epoch 59: Train Loss: 0.22410751382509866, Validation Loss: 0.4061836302280426\n",
      "Epoch 60: Train Loss: 0.22827889025211334, Validation Loss: 0.4105989933013916\n",
      "Epoch 61: Train Loss: 0.22599156697591147, Validation Loss: 0.4061923623085022\n",
      "Epoch 62: Train Loss: 0.22122380137443542, Validation Loss: 0.4052245318889618\n",
      "Epoch 63: Train Loss: 0.2096431404352188, Validation Loss: 0.4049568474292755\n",
      "Epoch 64: Train Loss: 0.21573776503403982, Validation Loss: 0.40739327669143677\n",
      "Epoch 65: Train Loss: 0.2010414352019628, Validation Loss: 0.41225117444992065\n",
      "Epoch 66: Train Loss: 0.21672247350215912, Validation Loss: 0.414474219083786\n",
      "Epoch 67: Train Loss: 0.20340855419635773, Validation Loss: 0.41004425287246704\n",
      "Epoch 68: Train Loss: 0.1893432786067327, Validation Loss: 0.40875035524368286\n",
      "Epoch 69: Train Loss: 0.19010594487190247, Validation Loss: 0.4115354120731354\n",
      "Epoch 70: Train Loss: 0.19707746803760529, Validation Loss: 0.406070739030838\n",
      "Epoch 71: Train Loss: 0.19999796152114868, Validation Loss: 0.40946874022483826\n",
      "Epoch 72: Train Loss: 0.19325905044873556, Validation Loss: 0.4216223359107971\n",
      "Early stopping at epoch 73\n",
      "Fold 4\n",
      "Epoch 0: Train Loss: 0.7145061294237772, Validation Loss: 0.7068905234336853\n",
      "Epoch 1: Train Loss: 0.6583062012990316, Validation Loss: 0.7075005769729614\n",
      "Epoch 2: Train Loss: 0.6297882994016012, Validation Loss: 0.7070667743682861\n",
      "Epoch 3: Train Loss: 0.6106021801630656, Validation Loss: 0.7052636742591858\n",
      "Epoch 4: Train Loss: 0.5905110836029053, Validation Loss: 0.7030765414237976\n",
      "Epoch 5: Train Loss: 0.5786501169204712, Validation Loss: 0.7017040848731995\n",
      "Epoch 6: Train Loss: 0.5760350426038107, Validation Loss: 0.7009180188179016\n",
      "Epoch 7: Train Loss: 0.5626166264216105, Validation Loss: 0.6996580958366394\n",
      "Epoch 8: Train Loss: 0.5752948721249899, Validation Loss: 0.6981555223464966\n",
      "Epoch 9: Train Loss: 0.5570379694302877, Validation Loss: 0.6967325806617737\n",
      "Epoch 10: Train Loss: 0.5523695945739746, Validation Loss: 0.684506893157959\n",
      "Epoch 11: Train Loss: 0.5393426716327667, Validation Loss: 0.6716427206993103\n",
      "Epoch 12: Train Loss: 0.5243814786275228, Validation Loss: 0.6604005098342896\n",
      "Epoch 13: Train Loss: 0.519214540719986, Validation Loss: 0.6537529230117798\n",
      "Epoch 14: Train Loss: 0.5094038049379984, Validation Loss: 0.6492133140563965\n",
      "Epoch 15: Train Loss: 0.4940909842650096, Validation Loss: 0.6441860795021057\n",
      "Epoch 16: Train Loss: 0.4996892213821411, Validation Loss: 0.6402838826179504\n",
      "Epoch 17: Train Loss: 0.47990143299102783, Validation Loss: 0.6386275887489319\n",
      "Epoch 18: Train Loss: 0.4756174484888713, Validation Loss: 0.6387406587600708\n",
      "Epoch 19: Train Loss: 0.47325417399406433, Validation Loss: 0.638630211353302\n",
      "Epoch 20: Train Loss: 0.46999319394429523, Validation Loss: 0.6269060373306274\n",
      "Epoch 21: Train Loss: 0.4643339316050212, Validation Loss: 0.6169148683547974\n",
      "Epoch 22: Train Loss: 0.44579607248306274, Validation Loss: 0.6115666627883911\n",
      "Epoch 23: Train Loss: 0.4340713421503703, Validation Loss: 0.6073066592216492\n",
      "Epoch 24: Train Loss: 0.42433557907740277, Validation Loss: 0.6029990911483765\n",
      "Epoch 25: Train Loss: 0.41267837087313336, Validation Loss: 0.5987715721130371\n",
      "Epoch 26: Train Loss: 0.4043901065985362, Validation Loss: 0.5962870717048645\n",
      "Epoch 27: Train Loss: 0.4047526816527049, Validation Loss: 0.5961124897003174\n",
      "Epoch 28: Train Loss: 0.4192582170168559, Validation Loss: 0.5959034562110901\n",
      "Epoch 29: Train Loss: 0.40151570240656537, Validation Loss: 0.5957391858100891\n",
      "Epoch 30: Train Loss: 0.4010711908340454, Validation Loss: 0.5928966999053955\n",
      "Epoch 31: Train Loss: 0.3838430643081665, Validation Loss: 0.5859827995300293\n",
      "Epoch 32: Train Loss: 0.3866265018781026, Validation Loss: 0.5802162289619446\n",
      "Epoch 33: Train Loss: 0.3643098970254262, Validation Loss: 0.5720856189727783\n",
      "Epoch 34: Train Loss: 0.3915596604347229, Validation Loss: 0.5724191665649414\n",
      "Epoch 35: Train Loss: 0.3491408129533132, Validation Loss: 0.5648397207260132\n",
      "Epoch 36: Train Loss: 0.346749484539032, Validation Loss: 0.5581710338592529\n",
      "Epoch 37: Train Loss: 0.3395206034183502, Validation Loss: 0.5576220154762268\n",
      "Epoch 38: Train Loss: 0.35268791516621906, Validation Loss: 0.5569791793823242\n",
      "Epoch 39: Train Loss: 0.33896468579769135, Validation Loss: 0.5576038360595703\n",
      "Epoch 40: Train Loss: 0.33061684171358746, Validation Loss: 0.5572742819786072\n",
      "Epoch 41: Train Loss: 0.33172930280367535, Validation Loss: 0.5564636588096619\n",
      "Epoch 42: Train Loss: 0.3113949994246165, Validation Loss: 0.5495280027389526\n",
      "Epoch 43: Train Loss: 0.3080953061580658, Validation Loss: 0.5427707433700562\n",
      "Epoch 44: Train Loss: 0.31509049733479816, Validation Loss: 0.5452204942703247\n",
      "Epoch 45: Train Loss: 0.29830408096313477, Validation Loss: 0.5432257056236267\n",
      "Epoch 46: Train Loss: 0.2916150639454524, Validation Loss: 0.5447821617126465\n",
      "Epoch 47: Train Loss: 0.29195350408554077, Validation Loss: 0.5404050350189209\n",
      "Epoch 48: Train Loss: 0.2837710678577423, Validation Loss: 0.5379999876022339\n",
      "Epoch 49: Train Loss: 0.30426353216171265, Validation Loss: 0.5380257964134216\n",
      "Epoch 50: Train Loss: 0.2896064966917038, Validation Loss: 0.5318175554275513\n",
      "Epoch 51: Train Loss: 0.29056721925735474, Validation Loss: 0.5444133281707764\n",
      "Epoch 52: Train Loss: 0.2775529623031616, Validation Loss: 0.5454777479171753\n",
      "Epoch 53: Train Loss: 0.2740776389837265, Validation Loss: 0.5211187601089478\n",
      "Epoch 54: Train Loss: 0.2666371911764145, Validation Loss: 0.5213011503219604\n",
      "Epoch 55: Train Loss: 0.2827333410580953, Validation Loss: 0.5267317295074463\n",
      "Epoch 56: Train Loss: 0.25616521636645, Validation Loss: 0.533291757106781\n",
      "Epoch 57: Train Loss: 0.26271000504493713, Validation Loss: 0.5370441675186157\n",
      "Epoch 58: Train Loss: 0.25090233484903973, Validation Loss: 0.5351316332817078\n",
      "Epoch 59: Train Loss: 0.2485367258389791, Validation Loss: 0.533115565776825\n",
      "Epoch 60: Train Loss: 0.2501779943704605, Validation Loss: 0.5058232545852661\n",
      "Epoch 61: Train Loss: 0.26355381309986115, Validation Loss: 0.49984994530677795\n",
      "Epoch 62: Train Loss: 0.24624680479367575, Validation Loss: 0.5257989764213562\n",
      "Epoch 63: Train Loss: 0.2468026081720988, Validation Loss: 0.5240352153778076\n",
      "Epoch 64: Train Loss: 0.2312010476986567, Validation Loss: 0.5025010704994202\n",
      "Epoch 65: Train Loss: 0.23739143709341684, Validation Loss: 0.49693116545677185\n",
      "Epoch 66: Train Loss: 0.22824234267075857, Validation Loss: 0.5004109740257263\n",
      "Epoch 67: Train Loss: 0.22609477738539377, Validation Loss: 0.5025867819786072\n",
      "Epoch 68: Train Loss: 0.2444740285476049, Validation Loss: 0.5030056238174438\n",
      "Epoch 69: Train Loss: 0.2291286438703537, Validation Loss: 0.500728964805603\n",
      "Epoch 70: Train Loss: 0.2339709053436915, Validation Loss: 0.5172504186630249\n",
      "Epoch 71: Train Loss: 0.22940820455551147, Validation Loss: 0.501304566860199\n",
      "Epoch 72: Train Loss: 0.2094535529613495, Validation Loss: 0.4890720844268799\n",
      "Epoch 73: Train Loss: 0.23058120906352997, Validation Loss: 0.49911415576934814\n",
      "Epoch 74: Train Loss: 0.20466431975364685, Validation Loss: 0.510370135307312\n",
      "Epoch 75: Train Loss: 0.20551355679829916, Validation Loss: 0.514892041683197\n",
      "Epoch 76: Train Loss: 0.20174631973107657, Validation Loss: 0.5050789713859558\n",
      "Epoch 77: Train Loss: 0.2055256317059199, Validation Loss: 0.5023192167282104\n",
      "Epoch 78: Train Loss: 0.1913297474384308, Validation Loss: 0.4999086558818817\n",
      "Epoch 79: Train Loss: 0.20659791926542917, Validation Loss: 0.49753403663635254\n",
      "Epoch 80: Train Loss: 0.20880801479021707, Validation Loss: 0.5018338561058044\n",
      "Epoch 81: Train Loss: 0.19618356227874756, Validation Loss: 0.48921966552734375\n",
      "Epoch 82: Train Loss: 0.1961189309755961, Validation Loss: 0.47200414538383484\n",
      "Epoch 83: Train Loss: 0.1858877738316854, Validation Loss: 0.47318753600120544\n",
      "Epoch 84: Train Loss: 0.18223834037780762, Validation Loss: 0.47803932428359985\n",
      "Epoch 85: Train Loss: 0.17894570529460907, Validation Loss: 0.4824974238872528\n",
      "Epoch 86: Train Loss: 0.18211139738559723, Validation Loss: 0.4885757267475128\n",
      "Epoch 87: Train Loss: 0.18068773547808328, Validation Loss: 0.4887007176876068\n",
      "Epoch 88: Train Loss: 0.17787393430868784, Validation Loss: 0.4889674782752991\n",
      "Epoch 89: Train Loss: 0.1693937530120214, Validation Loss: 0.48662543296813965\n",
      "Epoch 90: Train Loss: 0.1760356624921163, Validation Loss: 0.47571954131126404\n",
      "Epoch 91: Train Loss: 0.1793663501739502, Validation Loss: 0.4893728196620941\n",
      "Early stopping at epoch 92\n",
      "Fold 5\n",
      "Epoch 0: Train Loss: 0.6785494685173035, Validation Loss: 0.6978905200958252\n",
      "Epoch 1: Train Loss: 0.6241418719291687, Validation Loss: 0.6978801488876343\n",
      "Epoch 2: Train Loss: 0.5975319743156433, Validation Loss: 0.6970561742782593\n",
      "Epoch 3: Train Loss: 0.5798473954200745, Validation Loss: 0.6934259533882141\n",
      "Epoch 4: Train Loss: 0.5600059628486633, Validation Loss: 0.6896740794181824\n",
      "Epoch 5: Train Loss: 0.5502215226491293, Validation Loss: 0.6844313144683838\n",
      "Epoch 6: Train Loss: 0.5408928791681925, Validation Loss: 0.6809600591659546\n",
      "Epoch 7: Train Loss: 0.5304612318674723, Validation Loss: 0.6796368956565857\n",
      "Epoch 8: Train Loss: 0.5343892772992452, Validation Loss: 0.6788480877876282\n",
      "Epoch 9: Train Loss: 0.528007964293162, Validation Loss: 0.678328275680542\n",
      "Epoch 10: Train Loss: 0.5249085227648417, Validation Loss: 0.6717899441719055\n",
      "Epoch 11: Train Loss: 0.5176882942517599, Validation Loss: 0.6647189259529114\n",
      "Epoch 12: Train Loss: 0.48568788170814514, Validation Loss: 0.6545903086662292\n",
      "Epoch 13: Train Loss: 0.4955912133057912, Validation Loss: 0.6473445892333984\n",
      "Epoch 14: Train Loss: 0.4637772540251414, Validation Loss: 0.634606122970581\n",
      "Epoch 15: Train Loss: 0.4569843113422394, Validation Loss: 0.6289353966712952\n",
      "Epoch 16: Train Loss: 0.4465299646059672, Validation Loss: 0.6233950853347778\n",
      "Epoch 17: Train Loss: 0.4430399735768636, Validation Loss: 0.6221702694892883\n",
      "Epoch 18: Train Loss: 0.4361151456832886, Validation Loss: 0.620656430721283\n",
      "Epoch 19: Train Loss: 0.44770553708076477, Validation Loss: 0.6182675361633301\n",
      "Epoch 20: Train Loss: 0.4308740794658661, Validation Loss: 0.604655385017395\n",
      "Epoch 21: Train Loss: 0.42048725485801697, Validation Loss: 0.6135965585708618\n",
      "Epoch 22: Train Loss: 0.4227789839108785, Validation Loss: 0.626776933670044\n",
      "Epoch 23: Train Loss: 0.3831577201684316, Validation Loss: 0.626695454120636\n",
      "Epoch 24: Train Loss: 0.37949909766515094, Validation Loss: 0.6385040879249573\n",
      "Epoch 25: Train Loss: 0.3650388816992442, Validation Loss: 0.6373670101165771\n",
      "Epoch 26: Train Loss: 0.3634500602881114, Validation Loss: 0.6361281275749207\n",
      "Epoch 27: Train Loss: 0.35266706347465515, Validation Loss: 0.6324707269668579\n",
      "Epoch 28: Train Loss: 0.35209272305170697, Validation Loss: 0.6294779777526855\n",
      "Epoch 29: Train Loss: 0.35362542668978375, Validation Loss: 0.6254127025604248\n",
      "Early stopping at epoch 30\n",
      "Accuracy: 0.75,Precision: 0.7884615384615384, Recall: 0.6833333333333333, F1-score: 0.7321428571428571, AUC: 0.75\n",
      "Confusion Matrix:\n",
      "[[49 11]\n",
      " [19 41]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nweight decay = 1e-7\\nlearning rate = 0.0001\\nepoch = 100\\nbatch size = 32\\nearly stopping patience = 10\\nstandard scaler\\nReLU\\ncross entropy loss\\ndrop out = 0.8\\n\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "model_save_dir = r'C:\\Users\\User\\Documents\\Lie detect data\\Model'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Define a function to load and pad data\n",
    "def load_data(data_dir, max_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    file_list = os.listdir(data_dir)\n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        label = 1 if 'truth' in file else 0\n",
    "        padded_data = np.zeros((65, max_length))\n",
    "        length = min(data.shape[1], max_length)\n",
    "        padded_data[:, :length] = data[:, :length]\n",
    "        X.append(padded_data)\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load dataset and pad the data\n",
    "data_dir = \"C:\\\\Users\\\\User\\\\Documents\\\\Lie detect data\\\\56M_DWTEEGData\"\n",
    "max_length = 1400  # Define maximum length for padding\n",
    "X, y = load_data(data_dir, max_length)\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# Define EEGNet model\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(65, 32, kernel_size=63, padding=31)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "        self.depthwiseConv1d = nn.Conv1d(32, 64, kernel_size=65, groups=32, padding=32)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)  # Additional convolutional layer\n",
    "        self.batchnorm3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.pooling = nn.AvgPool1d(kernel_size=4)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        \n",
    "        self._calculate_num_features()\n",
    "        self.fc = nn.Linear(self.num_features, num_classes)\n",
    "\n",
    "    def _calculate_num_features(self):\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.zeros(1, 65, 1400)\n",
    "            sample_output = self._forward_features(sample_input)\n",
    "            self.num_features = sample_output.shape[1]\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.depthwiseConv1d(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.conv2(x)  # Additional convolutional layer\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.pooling(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.global_pool(x)  # Global average pooling layer\n",
    "        x = torch.flatten(x, 1)  # Flatten the output for the fully connected layer\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_and_evaluate(train_loader, val_loader, y_train):\n",
    "    model = EEGNet(num_classes=2).to(device)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-2)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "    num_epochs = 100\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "            fold_model_path = os.path.join(model_save_dir, f'fold3_model_fold_{fold_idx}.pth')\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': best_val_loss,\n",
    "            }, fold_model_path)\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_idx = 0\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "for train_index, val_index in kf.split(X, y):\n",
    "    print(f'Fold {fold_idx + 1}')\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Normalize data using scaler fitted on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_val = scaler.transform(X_val.reshape(X_val.shape[0], -1))\n",
    "    X_train = X_train.reshape(-1, 65, max_length)\n",
    "    X_val = X_val.reshape(-1, 65, max_length)\n",
    "\n",
    "    # Save the scaler to a file\n",
    "    with open(r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\simpleEEGNet_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = EEGDataset(X_train, y_train)\n",
    "    val_dataset = EEGDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = train_and_evaluate(train_loader, val_loader, y_train)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    fold_idx += 1\n",
    "\n",
    "\n",
    "# Calculate additional metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "auc = roc_auc_score(all_labels, all_predictions)\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy},Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "weight decay = 1e-7\n",
    "learning rate = 0.0001\n",
    "epoch = 100\n",
    "batch size = 32\n",
    "early stopping patience = 10\n",
    "standard scaler\n",
    "ReLU\n",
    "cross entropy loss\n",
    "drop out = 0.8\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66654d63-c450-4d9f-a3e3-63701fd1d41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934cb89-5ea1-4eb0-a152-00c71e49e06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
