{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "656cdbc2-da54-45ba-97fc-195b59bd7820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\detectlie\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Convolution.cpp:1032.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.693996528784434, Validation Loss: 0.6935914754867554\n",
      "Epoch 1: Train Loss: 0.6643290718396505, Validation Loss: 0.6902887225151062\n",
      "Epoch 2: Train Loss: 0.6432450612386068, Validation Loss: 0.6884269118309021\n",
      "Epoch 3: Train Loss: 0.6301511327425638, Validation Loss: 0.6905054450035095\n",
      "Epoch 4: Train Loss: 0.6118823687235514, Validation Loss: 0.6926698684692383\n",
      "Epoch 5: Train Loss: 0.6103557348251343, Validation Loss: 0.6950153708457947\n",
      "Epoch 6: Train Loss: 0.5919282635052999, Validation Loss: 0.6973549723625183\n",
      "Epoch 7: Train Loss: 0.5904016296068827, Validation Loss: 0.6991558074951172\n",
      "Epoch 8: Train Loss: 0.5979834397633871, Validation Loss: 0.6993719935417175\n",
      "Epoch 9: Train Loss: 0.5930612285931905, Validation Loss: 0.6999982595443726\n",
      "Epoch 10: Train Loss: 0.5715248783429464, Validation Loss: 0.6943541169166565\n",
      "Epoch 11: Train Loss: 0.5566069086392721, Validation Loss: 0.6972734332084656\n",
      "Epoch 12: Train Loss: 0.5417172710100809, Validation Loss: 0.6952484846115112\n",
      "Epoch 13: Train Loss: 0.5297817091147105, Validation Loss: 0.6784602403640747\n",
      "Epoch 14: Train Loss: 0.5231937865416209, Validation Loss: 0.6653235554695129\n",
      "Epoch 15: Train Loss: 0.5043079853057861, Validation Loss: 0.661503255367279\n",
      "Epoch 16: Train Loss: 0.5108510255813599, Validation Loss: 0.6586697101593018\n",
      "Epoch 17: Train Loss: 0.48739131291707355, Validation Loss: 0.6565800905227661\n",
      "Epoch 18: Train Loss: 0.49838600556055707, Validation Loss: 0.6546937823295593\n",
      "Epoch 19: Train Loss: 0.48716559012730914, Validation Loss: 0.6553989052772522\n",
      "Epoch 20: Train Loss: 0.49073095122973126, Validation Loss: 0.6459977626800537\n",
      "Epoch 21: Train Loss: 0.49121803045272827, Validation Loss: 0.6295221447944641\n",
      "Epoch 22: Train Loss: 0.4514746367931366, Validation Loss: 0.6284791827201843\n",
      "Epoch 23: Train Loss: 0.471916526556015, Validation Loss: 0.6253767013549805\n",
      "Epoch 24: Train Loss: 0.455928514401118, Validation Loss: 0.6372425556182861\n",
      "Epoch 25: Train Loss: 0.4445534149805705, Validation Loss: 0.6336150169372559\n",
      "Epoch 26: Train Loss: 0.45067669947942096, Validation Loss: 0.6272753477096558\n",
      "Epoch 27: Train Loss: 0.4383918344974518, Validation Loss: 0.624032735824585\n",
      "Epoch 28: Train Loss: 0.42635785539944965, Validation Loss: 0.6229337453842163\n",
      "Epoch 29: Train Loss: 0.4261362850666046, Validation Loss: 0.6231886744499207\n",
      "Epoch 30: Train Loss: 0.4258117079734802, Validation Loss: 0.6057221293449402\n",
      "Epoch 31: Train Loss: 0.43408851822217304, Validation Loss: 0.6115439534187317\n",
      "Epoch 32: Train Loss: 0.42309990525245667, Validation Loss: 0.6187000870704651\n",
      "Epoch 33: Train Loss: 0.415523221095403, Validation Loss: 0.6257290244102478\n",
      "Epoch 34: Train Loss: 0.4126966993014018, Validation Loss: 0.6234588027000427\n",
      "Epoch 35: Train Loss: 0.418411244948705, Validation Loss: 0.6206585764884949\n",
      "Epoch 36: Train Loss: 0.4124002556006114, Validation Loss: 0.6210111975669861\n",
      "Epoch 37: Train Loss: 0.40531452496846515, Validation Loss: 0.6181336045265198\n",
      "Epoch 38: Train Loss: 0.4293259084224701, Validation Loss: 0.6176615357398987\n",
      "Epoch 39: Train Loss: 0.40887416402498883, Validation Loss: 0.6174524426460266\n",
      "Epoch 40: Train Loss: 0.41463568806648254, Validation Loss: 0.5980464220046997\n",
      "Epoch 41: Train Loss: 0.4022488296031952, Validation Loss: 0.5873101353645325\n",
      "Epoch 42: Train Loss: 0.4066295127073924, Validation Loss: 0.607293963432312\n",
      "Epoch 43: Train Loss: 0.40402278304100037, Validation Loss: 0.609825074672699\n",
      "Epoch 44: Train Loss: 0.3997134764989217, Validation Loss: 0.5958125591278076\n",
      "Epoch 45: Train Loss: 0.3911927243073781, Validation Loss: 0.5718101263046265\n",
      "Epoch 46: Train Loss: 0.4026774764060974, Validation Loss: 0.5805906653404236\n",
      "Epoch 47: Train Loss: 0.39940422773361206, Validation Loss: 0.5899908542633057\n",
      "Epoch 48: Train Loss: 0.40395888686180115, Validation Loss: 0.5973824858665466\n",
      "Epoch 49: Train Loss: 0.3993075489997864, Validation Loss: 0.6001935601234436\n",
      "Epoch 50: Train Loss: 0.4120073616504669, Validation Loss: 0.6323214769363403\n",
      "Epoch 51: Train Loss: 0.3973537087440491, Validation Loss: 0.6534256339073181\n",
      "Epoch 52: Train Loss: 0.39455408851305646, Validation Loss: 0.6493805050849915\n",
      "Epoch 53: Train Loss: 0.37628453969955444, Validation Loss: 0.6375414729118347\n",
      "Epoch 54: Train Loss: 0.382232000430425, Validation Loss: 0.6242520809173584\n",
      "Epoch 55: Train Loss: 0.38755614558855694, Validation Loss: 0.6074365377426147\n",
      "Epoch 56: Train Loss: 0.37936492760976154, Validation Loss: 0.5972087979316711\n",
      "Epoch 57: Train Loss: 0.38633445898691815, Validation Loss: 0.5953683257102966\n",
      "Epoch 58: Train Loss: 0.3958805004755656, Validation Loss: 0.5950049161911011\n",
      "Epoch 59: Train Loss: 0.3946549395720164, Validation Loss: 0.5957342982292175\n",
      "Epoch 60: Train Loss: 0.3901906708876292, Validation Loss: 0.6048671007156372\n",
      "Epoch 61: Train Loss: 0.38301217555999756, Validation Loss: 0.6101238131523132\n",
      "Epoch 62: Train Loss: 0.3788959284623464, Validation Loss: 0.608439564704895\n",
      "Epoch 63: Train Loss: 0.37828344106674194, Validation Loss: 0.599575936794281\n",
      "Epoch 64: Train Loss: 0.3776608109474182, Validation Loss: 0.5881388783454895\n",
      "Epoch 65: Train Loss: 0.3617091675599416, Validation Loss: 0.5793825387954712\n",
      "Epoch 66: Train Loss: 0.38060156504313153, Validation Loss: 0.5756786465644836\n",
      "Epoch 67: Train Loss: 0.37052979071935016, Validation Loss: 0.575384259223938\n",
      "Epoch 68: Train Loss: 0.3981809119383494, Validation Loss: 0.5778684020042419\n",
      "Epoch 69: Train Loss: 0.390233318010966, Validation Loss: 0.5776797533035278\n",
      "Epoch 70: Train Loss: 0.37087217966715497, Validation Loss: 0.5753539800643921\n",
      "Epoch 71: Train Loss: 0.3709877630074819, Validation Loss: 0.5812799334526062\n",
      "Epoch 72: Train Loss: 0.35898595054944354, Validation Loss: 0.5864353775978088\n",
      "Epoch 73: Train Loss: 0.3718407452106476, Validation Loss: 0.5892218351364136\n",
      "Epoch 74: Train Loss: 0.3586615025997162, Validation Loss: 0.5939242839813232\n",
      "Epoch 75: Train Loss: 0.3664729396502177, Validation Loss: 0.5929747819900513\n",
      "Epoch 76: Train Loss: 0.36613429586092633, Validation Loss: 0.5918933749198914\n",
      "Epoch 77: Train Loss: 0.3630952338377635, Validation Loss: 0.5899112820625305\n",
      "Epoch 78: Train Loss: 0.3693033556143443, Validation Loss: 0.5918482542037964\n",
      "Epoch 79: Train Loss: 0.36189887921015423, Validation Loss: 0.5913898348808289\n",
      "Epoch 80: Train Loss: 0.3562012314796448, Validation Loss: 0.5910376310348511\n",
      "Epoch 81: Train Loss: 0.3565471867720286, Validation Loss: 0.5837962031364441\n",
      "Epoch 82: Train Loss: 0.36364808678627014, Validation Loss: 0.5791535973548889\n",
      "Epoch 83: Train Loss: 0.3560916880766551, Validation Loss: 0.5785452723503113\n",
      "Epoch 84: Train Loss: 0.3622802098592122, Validation Loss: 0.590225875377655\n",
      "Epoch 85: Train Loss: 0.35523290435473126, Validation Loss: 0.5880954265594482\n",
      "Epoch 86: Train Loss: 0.36062533656756085, Validation Loss: 0.5777091383934021\n",
      "Epoch 87: Train Loss: 0.36191727717717487, Validation Loss: 0.574849545955658\n",
      "Epoch 88: Train Loss: 0.36302184065183, Validation Loss: 0.5737800002098083\n",
      "Epoch 89: Train Loss: 0.39398014545440674, Validation Loss: 0.5725376009941101\n",
      "Epoch 90: Train Loss: 0.34955937663714093, Validation Loss: 0.5737372040748596\n",
      "Epoch 91: Train Loss: 0.36383108297983807, Validation Loss: 0.5753720998764038\n",
      "Epoch 92: Train Loss: 0.3603205780188243, Validation Loss: 0.5888895392417908\n",
      "Epoch 93: Train Loss: 0.37730713685353595, Validation Loss: 0.6116951107978821\n",
      "Epoch 94: Train Loss: 0.3600085874398549, Validation Loss: 0.6150757670402527\n",
      "Epoch 95: Train Loss: 0.3540444076061249, Validation Loss: 0.6114928126335144\n",
      "Epoch 96: Train Loss: 0.3560085594654083, Validation Loss: 0.6073023676872253\n",
      "Epoch 97: Train Loss: 0.3596581319967906, Validation Loss: 0.6028488278388977\n",
      "Epoch 98: Train Loss: 0.3499971330165863, Validation Loss: 0.6000998020172119\n",
      "Epoch 99: Train Loss: 0.348681499560674, Validation Loss: 0.5994925498962402\n",
      "Epoch 100: Train Loss: 0.35801999767621356, Validation Loss: 0.5825323462486267\n",
      "Epoch 101: Train Loss: 0.35896824796994525, Validation Loss: 0.5695151090621948\n",
      "Epoch 102: Train Loss: 0.3639347453912099, Validation Loss: 0.555997371673584\n",
      "Epoch 103: Train Loss: 0.35192455848058063, Validation Loss: 0.5474936962127686\n",
      "Epoch 104: Train Loss: 0.3579586942990621, Validation Loss: 0.5539032220840454\n",
      "Epoch 105: Train Loss: 0.3677865266799927, Validation Loss: 0.5671241283416748\n",
      "Epoch 106: Train Loss: 0.34434840083122253, Validation Loss: 0.5718809962272644\n",
      "Epoch 107: Train Loss: 0.3566243350505829, Validation Loss: 0.5711672306060791\n",
      "Epoch 108: Train Loss: 0.34688151876131695, Validation Loss: 0.5728037357330322\n",
      "Epoch 109: Train Loss: 0.34463350971539813, Validation Loss: 0.575047492980957\n",
      "Epoch 110: Train Loss: 0.36126387119293213, Validation Loss: 0.5775675773620605\n",
      "Epoch 111: Train Loss: 0.3450862069924672, Validation Loss: 0.586009681224823\n",
      "Epoch 112: Train Loss: 0.3497253855069478, Validation Loss: 0.5817859768867493\n",
      "Epoch 113: Train Loss: 0.3398486872514089, Validation Loss: 0.5842216610908508\n",
      "Epoch 114: Train Loss: 0.3411206205685933, Validation Loss: 0.5820257067680359\n",
      "Epoch 115: Train Loss: 0.3525967299938202, Validation Loss: 0.5803353786468506\n",
      "Epoch 116: Train Loss: 0.3447074194749196, Validation Loss: 0.580668032169342\n",
      "Epoch 117: Train Loss: 0.34380584955215454, Validation Loss: 0.5819172859191895\n",
      "Epoch 118: Train Loss: 0.34548747539520264, Validation Loss: 0.5820004343986511\n",
      "Epoch 119: Train Loss: 0.3479784627755483, Validation Loss: 0.5814701318740845\n",
      "Epoch 120: Train Loss: 0.344651718934377, Validation Loss: 0.5800603032112122\n",
      "Epoch 121: Train Loss: 0.3381216327349345, Validation Loss: 0.5656533241271973\n",
      "Epoch 122: Train Loss: 0.3402370512485504, Validation Loss: 0.563665509223938\n",
      "Epoch 123: Train Loss: 0.35738444328308105, Validation Loss: 0.5752618312835693\n",
      "Epoch 124: Train Loss: 0.34353535374005634, Validation Loss: 0.5741161704063416\n",
      "Epoch 125: Train Loss: 0.3404724995295207, Validation Loss: 0.5601488947868347\n",
      "Epoch 126: Train Loss: 0.34610063831011456, Validation Loss: 0.547727644443512\n",
      "Epoch 127: Train Loss: 0.34004783630371094, Validation Loss: 0.5428377389907837\n",
      "Epoch 128: Train Loss: 0.3552657961845398, Validation Loss: 0.5422638058662415\n",
      "Epoch 129: Train Loss: 0.34101816018422443, Validation Loss: 0.5420426726341248\n",
      "Epoch 130: Train Loss: 0.3455226421356201, Validation Loss: 0.5415388941764832\n",
      "Epoch 131: Train Loss: 0.34339181582132977, Validation Loss: 0.5491656064987183\n",
      "Epoch 132: Train Loss: 0.3426101903120677, Validation Loss: 0.5628140568733215\n",
      "Epoch 133: Train Loss: 0.3367205460866292, Validation Loss: 0.5770625472068787\n",
      "Epoch 134: Train Loss: 0.3413999577363332, Validation Loss: 0.5823882222175598\n",
      "Epoch 135: Train Loss: 0.3402230441570282, Validation Loss: 0.5863613486289978\n",
      "Epoch 136: Train Loss: 0.3353392382462819, Validation Loss: 0.5875816345214844\n",
      "Epoch 137: Train Loss: 0.34601149956385296, Validation Loss: 0.587932288646698\n",
      "Epoch 138: Train Loss: 0.3419017394383748, Validation Loss: 0.5875051617622375\n",
      "Epoch 139: Train Loss: 0.33414242664972943, Validation Loss: 0.5875394940376282\n",
      "Epoch 140: Train Loss: 0.33670295278231305, Validation Loss: 0.5954535603523254\n",
      "Epoch 141: Train Loss: 0.3390630582968394, Validation Loss: 0.5877611637115479\n",
      "Epoch 142: Train Loss: 0.3427809774875641, Validation Loss: 0.5859502553939819\n",
      "Epoch 143: Train Loss: 0.3368096351623535, Validation Loss: 0.5861579775810242\n",
      "Epoch 144: Train Loss: 0.33617661396662396, Validation Loss: 0.5849994421005249\n",
      "Epoch 145: Train Loss: 0.3435613711675008, Validation Loss: 0.5884674191474915\n",
      "Epoch 146: Train Loss: 0.3458016713460286, Validation Loss: 0.5881738066673279\n",
      "Epoch 147: Train Loss: 0.34416963656743366, Validation Loss: 0.5876376628875732\n",
      "Epoch 148: Train Loss: 0.3393438657124837, Validation Loss: 0.5881876945495605\n",
      "Epoch 149: Train Loss: 0.33125390609105426, Validation Loss: 0.5883352756500244\n",
      "Epoch 150: Train Loss: 0.33606646458307904, Validation Loss: 0.5925314426422119\n",
      "Epoch 151: Train Loss: 0.3328650891780853, Validation Loss: 0.5942744612693787\n",
      "Epoch 152: Train Loss: 0.34127304951349896, Validation Loss: 0.581462025642395\n",
      "Epoch 153: Train Loss: 0.3398006657759349, Validation Loss: 0.5958176851272583\n",
      "Epoch 154: Train Loss: 0.34633644421895343, Validation Loss: 0.5835203528404236\n",
      "Epoch 155: Train Loss: 0.3358510037263234, Validation Loss: 0.568182110786438\n",
      "Epoch 156: Train Loss: 0.33374688029289246, Validation Loss: 0.5586995482444763\n",
      "Epoch 157: Train Loss: 0.33630964159965515, Validation Loss: 0.5567302703857422\n",
      "Epoch 158: Train Loss: 0.3409168521563212, Validation Loss: 0.5567414164543152\n",
      "Epoch 159: Train Loss: 0.3481917877991994, Validation Loss: 0.557283341884613\n",
      "Epoch 160: Train Loss: 0.3376619319121043, Validation Loss: 0.5373162031173706\n",
      "Epoch 161: Train Loss: 0.3356049954891205, Validation Loss: 0.5180795788764954\n",
      "Epoch 162: Train Loss: 0.34749212861061096, Validation Loss: 0.5360442399978638\n",
      "Epoch 163: Train Loss: 0.33602044979731244, Validation Loss: 0.5528577566146851\n",
      "Epoch 164: Train Loss: 0.3324171801408132, Validation Loss: 0.5509618520736694\n",
      "Epoch 165: Train Loss: 0.3499547839164734, Validation Loss: 0.5498966574668884\n",
      "Epoch 166: Train Loss: 0.33383555213610333, Validation Loss: 0.5461001396179199\n",
      "Epoch 167: Train Loss: 0.33465491731961566, Validation Loss: 0.5666693449020386\n",
      "Epoch 168: Train Loss: 0.34018505613009137, Validation Loss: 0.5733039975166321\n",
      "Epoch 169: Train Loss: 0.33940089742342633, Validation Loss: 0.575238049030304\n",
      "Epoch 170: Train Loss: 0.3349422911802928, Validation Loss: 0.597509503364563\n",
      "Epoch 171: Train Loss: 0.33978691697120667, Validation Loss: 0.5854423642158508\n",
      "Epoch 172: Train Loss: 0.336605836947759, Validation Loss: 0.5731639266014099\n",
      "Epoch 173: Train Loss: 0.3449993034203847, Validation Loss: 0.572028398513794\n",
      "Epoch 174: Train Loss: 0.3357166151205699, Validation Loss: 0.5781981348991394\n",
      "Epoch 175: Train Loss: 0.3339139421780904, Validation Loss: 0.5860145688056946\n",
      "Epoch 176: Train Loss: 0.33871569236119586, Validation Loss: 0.5918943881988525\n",
      "Epoch 177: Train Loss: 0.33299520611763, Validation Loss: 0.5919214487075806\n",
      "Epoch 178: Train Loss: 0.33988908926645917, Validation Loss: 0.5918024778366089\n",
      "Epoch 179: Train Loss: 0.33261850476264954, Validation Loss: 0.5903281569480896\n",
      "Epoch 180: Train Loss: 0.3359817365805308, Validation Loss: 0.5726527571678162\n",
      "Epoch 181: Train Loss: 0.3295281032721202, Validation Loss: 0.5691584944725037\n",
      "Epoch 182: Train Loss: 0.334945688645045, Validation Loss: 0.564089298248291\n",
      "Epoch 183: Train Loss: 0.34879950682322186, Validation Loss: 0.5593633055686951\n",
      "Epoch 184: Train Loss: 0.3339315752188365, Validation Loss: 0.5717473030090332\n",
      "Epoch 185: Train Loss: 0.3320690492788951, Validation Loss: 0.5824310779571533\n",
      "Epoch 186: Train Loss: 0.3409869571526845, Validation Loss: 0.5716195702552795\n",
      "Epoch 187: Train Loss: 0.35826805233955383, Validation Loss: 0.5673503279685974\n",
      "Epoch 188: Train Loss: 0.3435254196325938, Validation Loss: 0.5664904117584229\n",
      "Epoch 189: Train Loss: 0.33092738191286725, Validation Loss: 0.5666057467460632\n",
      "Epoch 190: Train Loss: 0.33393482367197674, Validation Loss: 0.5871827006340027\n",
      "Epoch 191: Train Loss: 0.3411865035692851, Validation Loss: 0.5948325395584106\n",
      "Epoch 192: Train Loss: 0.3487289845943451, Validation Loss: 0.5972532629966736\n",
      "Epoch 193: Train Loss: 0.34041211009025574, Validation Loss: 0.600638210773468\n",
      "Epoch 194: Train Loss: 0.33698874711990356, Validation Loss: 0.5740247368812561\n",
      "Epoch 195: Train Loss: 0.3365438977877299, Validation Loss: 0.5612679719924927\n",
      "Epoch 196: Train Loss: 0.3367703855037689, Validation Loss: 0.5594015121459961\n",
      "Epoch 197: Train Loss: 0.3317607541879018, Validation Loss: 0.5570239424705505\n",
      "Epoch 198: Train Loss: 0.3322993417580922, Validation Loss: 0.5561222434043884\n",
      "Epoch 199: Train Loss: 0.3473554452260335, Validation Loss: 0.5544002056121826\n",
      "Epoch 200: Train Loss: 0.3317965666453044, Validation Loss: 0.5552484393119812\n",
      "Epoch 201: Train Loss: 0.3311638633410136, Validation Loss: 0.5627370476722717\n",
      "Epoch 202: Train Loss: 0.3370011548201243, Validation Loss: 0.5749075412750244\n",
      "Epoch 203: Train Loss: 0.3381802936395009, Validation Loss: 0.5957404971122742\n",
      "Epoch 204: Train Loss: 0.35370967785517377, Validation Loss: 0.5872957110404968\n",
      "Epoch 205: Train Loss: 0.34384632110595703, Validation Loss: 0.5748735070228577\n",
      "Epoch 206: Train Loss: 0.3342372675736745, Validation Loss: 0.5649594664573669\n",
      "Epoch 207: Train Loss: 0.3629223008950551, Validation Loss: 0.5613952279090881\n",
      "Epoch 208: Train Loss: 0.33776335914929706, Validation Loss: 0.5607619881629944\n",
      "Epoch 209: Train Loss: 0.3329379657904307, Validation Loss: 0.5608353614807129\n",
      "Epoch 210: Train Loss: 0.3339991768201192, Validation Loss: 0.5667309761047363\n",
      "Epoch 211: Train Loss: 0.33195336659749347, Validation Loss: 0.5683398842811584\n",
      "Epoch 212: Train Loss: 0.3367745876312256, Validation Loss: 0.5813746452331543\n",
      "Epoch 213: Train Loss: 0.337053914864858, Validation Loss: 0.587546706199646\n",
      "Epoch 214: Train Loss: 0.32770119110743207, Validation Loss: 0.5898444652557373\n",
      "Epoch 215: Train Loss: 0.34594406684239704, Validation Loss: 0.5725002288818359\n",
      "Epoch 216: Train Loss: 0.333484023809433, Validation Loss: 0.5562018156051636\n",
      "Epoch 217: Train Loss: 0.3388025164604187, Validation Loss: 0.5536580085754395\n",
      "Epoch 218: Train Loss: 0.33178700009981793, Validation Loss: 0.5538915395736694\n",
      "Epoch 219: Train Loss: 0.3284972310066223, Validation Loss: 0.5541463494300842\n",
      "Epoch 220: Train Loss: 0.33029024799664813, Validation Loss: 0.5589702129364014\n",
      "Epoch 221: Train Loss: 0.3313997487227122, Validation Loss: 0.5556764602661133\n",
      "Epoch 222: Train Loss: 0.3373858531316121, Validation Loss: 0.547699511051178\n",
      "Epoch 223: Train Loss: 0.33055952191352844, Validation Loss: 0.5558428168296814\n",
      "Epoch 224: Train Loss: 0.3311234811941783, Validation Loss: 0.5606769919395447\n",
      "Epoch 225: Train Loss: 0.3294815421104431, Validation Loss: 0.5567098259925842\n",
      "Epoch 226: Train Loss: 0.3337826728820801, Validation Loss: 0.5505907535552979\n",
      "Epoch 227: Train Loss: 0.3318090836207072, Validation Loss: 0.548228919506073\n",
      "Epoch 228: Train Loss: 0.3348421057065328, Validation Loss: 0.5474320650100708\n",
      "Epoch 229: Train Loss: 0.33115952213605243, Validation Loss: 0.5476733446121216\n",
      "Epoch 230: Train Loss: 0.3325001100699107, Validation Loss: 0.5427747368812561\n",
      "Epoch 231: Train Loss: 0.3310323754946391, Validation Loss: 0.5421561598777771\n",
      "Epoch 232: Train Loss: 0.33484920859336853, Validation Loss: 0.5423098206520081\n",
      "Epoch 233: Train Loss: 0.33216577768325806, Validation Loss: 0.5496546030044556\n",
      "Epoch 234: Train Loss: 0.33018919825553894, Validation Loss: 0.5631117820739746\n",
      "Epoch 235: Train Loss: 0.3348226845264435, Validation Loss: 0.5688903331756592\n",
      "Epoch 236: Train Loss: 0.33292216062545776, Validation Loss: 0.5692653059959412\n",
      "Epoch 237: Train Loss: 0.32932321230570477, Validation Loss: 0.5683978796005249\n",
      "Epoch 238: Train Loss: 0.3306065797805786, Validation Loss: 0.5666822791099548\n",
      "Epoch 239: Train Loss: 0.3292388319969177, Validation Loss: 0.5649896264076233\n",
      "Epoch 240: Train Loss: 0.3277996977170308, Validation Loss: 0.5460063219070435\n",
      "Epoch 241: Train Loss: 0.3369824290275574, Validation Loss: 0.5424331426620483\n",
      "Epoch 242: Train Loss: 0.3293030957380931, Validation Loss: 0.5431132316589355\n",
      "Epoch 243: Train Loss: 0.3304076890150706, Validation Loss: 0.5537035465240479\n",
      "Epoch 244: Train Loss: 0.3309025565783183, Validation Loss: 0.568756103515625\n",
      "Epoch 245: Train Loss: 0.328606257836024, Validation Loss: 0.5628690719604492\n",
      "Epoch 246: Train Loss: 0.3282991945743561, Validation Loss: 0.5581831932067871\n",
      "Epoch 247: Train Loss: 0.3324366708596547, Validation Loss: 0.5514809489250183\n",
      "Epoch 248: Train Loss: 0.32931751012802124, Validation Loss: 0.5498798489570618\n",
      "Epoch 249: Train Loss: 0.3288879891236623, Validation Loss: 0.5481632947921753\n",
      "Epoch 250: Train Loss: 0.32811184724171955, Validation Loss: 0.5496626496315002\n",
      "Epoch 251: Train Loss: 0.3340364893277486, Validation Loss: 0.5600907206535339\n",
      "Epoch 252: Train Loss: 0.33008084694544476, Validation Loss: 0.5695258975028992\n",
      "Epoch 253: Train Loss: 0.33265156547228497, Validation Loss: 0.5608027577400208\n",
      "Epoch 254: Train Loss: 0.32848625381787616, Validation Loss: 0.5544754862785339\n",
      "Epoch 255: Train Loss: 0.32469289501508075, Validation Loss: 0.5515972971916199\n",
      "Epoch 256: Train Loss: 0.3315246601899465, Validation Loss: 0.5505340695381165\n",
      "Epoch 257: Train Loss: 0.33272355794906616, Validation Loss: 0.5519934892654419\n",
      "Epoch 258: Train Loss: 0.3308999836444855, Validation Loss: 0.5524453520774841\n",
      "Epoch 259: Train Loss: 0.32942453026771545, Validation Loss: 0.5531958341598511\n",
      "Epoch 260: Train Loss: 0.34105269114176434, Validation Loss: 0.570376455783844\n",
      "Early stopping at epoch 261\n",
      "Fold 2\n",
      "Epoch 0: Train Loss: 0.7083051204681396, Validation Loss: 0.6876772046089172\n",
      "Epoch 1: Train Loss: 0.6975756486256918, Validation Loss: 0.678443193435669\n",
      "Epoch 2: Train Loss: 0.6452837983767191, Validation Loss: 0.6694794297218323\n",
      "Epoch 3: Train Loss: 0.6375590364138285, Validation Loss: 0.6559498906135559\n",
      "Epoch 4: Train Loss: 0.6318533619244894, Validation Loss: 0.6415467858314514\n",
      "Epoch 5: Train Loss: 0.6257990002632141, Validation Loss: 0.6268607378005981\n",
      "Epoch 6: Train Loss: 0.6286297043164571, Validation Loss: 0.6137608885765076\n",
      "Epoch 7: Train Loss: 0.5921107729276022, Validation Loss: 0.6034621596336365\n",
      "Epoch 8: Train Loss: 0.6005419294039408, Validation Loss: 0.5972685217857361\n",
      "Epoch 9: Train Loss: 0.5864925980567932, Validation Loss: 0.5933973789215088\n",
      "Epoch 10: Train Loss: 0.5893583496411642, Validation Loss: 0.582186758518219\n",
      "Epoch 11: Train Loss: 0.5998836358388265, Validation Loss: 0.5698761940002441\n",
      "Epoch 12: Train Loss: 0.5412251750628153, Validation Loss: 0.5620810389518738\n",
      "Epoch 13: Train Loss: 0.5423207879066467, Validation Loss: 0.5488688349723816\n",
      "Epoch 14: Train Loss: 0.5299593508243561, Validation Loss: 0.5344935655593872\n",
      "Epoch 15: Train Loss: 0.5159178177515665, Validation Loss: 0.5276209712028503\n",
      "Epoch 16: Train Loss: 0.5181868672370911, Validation Loss: 0.5277063846588135\n",
      "Epoch 17: Train Loss: 0.5119544267654419, Validation Loss: 0.5307735800743103\n",
      "Epoch 18: Train Loss: 0.5060538550217947, Validation Loss: 0.5334901809692383\n",
      "Epoch 19: Train Loss: 0.49265767137209576, Validation Loss: 0.5343216061592102\n",
      "Epoch 20: Train Loss: 0.4932665228843689, Validation Loss: 0.546204686164856\n",
      "Epoch 21: Train Loss: 0.5253147880236307, Validation Loss: 0.533266544342041\n",
      "Epoch 22: Train Loss: 0.4691592852274577, Validation Loss: 0.49772873520851135\n",
      "Epoch 23: Train Loss: 0.4561014274756114, Validation Loss: 0.5051097273826599\n",
      "Epoch 24: Train Loss: 0.46265212694803876, Validation Loss: 0.5230379104614258\n",
      "Epoch 25: Train Loss: 0.4398232698440552, Validation Loss: 0.5433065295219421\n",
      "Epoch 26: Train Loss: 0.4505857030550639, Validation Loss: 0.5505935549736023\n",
      "Epoch 27: Train Loss: 0.45746230085690814, Validation Loss: 0.5502026677131653\n",
      "Epoch 28: Train Loss: 0.4327652057011922, Validation Loss: 0.5472785830497742\n",
      "Epoch 29: Train Loss: 0.4387921392917633, Validation Loss: 0.5458477139472961\n",
      "Epoch 30: Train Loss: 0.429734339316686, Validation Loss: 0.5113880634307861\n",
      "Epoch 31: Train Loss: 0.4403643210728963, Validation Loss: 0.4999924600124359\n",
      "Epoch 32: Train Loss: 0.4335880974928538, Validation Loss: 0.5195896625518799\n",
      "Epoch 33: Train Loss: 0.4289584557215373, Validation Loss: 0.5319496989250183\n",
      "Epoch 34: Train Loss: 0.4213958481947581, Validation Loss: 0.533699095249176\n",
      "Epoch 35: Train Loss: 0.41577179233233136, Validation Loss: 0.5231064558029175\n",
      "Epoch 36: Train Loss: 0.41229472557703656, Validation Loss: 0.5172582268714905\n",
      "Epoch 37: Train Loss: 0.4220539430777232, Validation Loss: 0.5151455998420715\n",
      "Epoch 38: Train Loss: 0.4134032229582469, Validation Loss: 0.5143817663192749\n",
      "Epoch 39: Train Loss: 0.40516921877861023, Validation Loss: 0.5133640766143799\n",
      "Epoch 40: Train Loss: 0.4421356717745463, Validation Loss: 0.5119589567184448\n",
      "Epoch 41: Train Loss: 0.41380536556243896, Validation Loss: 0.5013118386268616\n",
      "Epoch 42: Train Loss: 0.41299377878506977, Validation Loss: 0.4935937225818634\n",
      "Epoch 43: Train Loss: 0.39857375621795654, Validation Loss: 0.4953356087207794\n",
      "Epoch 44: Train Loss: 0.4046393831570943, Validation Loss: 0.4991130530834198\n",
      "Epoch 45: Train Loss: 0.38686437408129376, Validation Loss: 0.5037643909454346\n",
      "Epoch 46: Train Loss: 0.3916050096352895, Validation Loss: 0.5031601786613464\n",
      "Epoch 47: Train Loss: 0.3886185884475708, Validation Loss: 0.5008040070533752\n",
      "Epoch 48: Train Loss: 0.3810459574063619, Validation Loss: 0.5005165338516235\n",
      "Epoch 49: Train Loss: 0.38598355650901794, Validation Loss: 0.5001159906387329\n",
      "Epoch 50: Train Loss: 0.3884389301141103, Validation Loss: 0.5196167230606079\n",
      "Epoch 51: Train Loss: 0.38066450754801434, Validation Loss: 0.5053197145462036\n",
      "Epoch 52: Train Loss: 0.38357600569725037, Validation Loss: 0.48607808351516724\n",
      "Epoch 53: Train Loss: 0.4000583589076996, Validation Loss: 0.4752851724624634\n",
      "Epoch 54: Train Loss: 0.37364740173021954, Validation Loss: 0.47763505578041077\n",
      "Epoch 55: Train Loss: 0.37299348910649616, Validation Loss: 0.49043411016464233\n",
      "Epoch 56: Train Loss: 0.37668634454409283, Validation Loss: 0.4957125186920166\n",
      "Epoch 57: Train Loss: 0.36552323897679645, Validation Loss: 0.4982025623321533\n",
      "Epoch 58: Train Loss: 0.37210215131441754, Validation Loss: 0.4982963502407074\n",
      "Epoch 59: Train Loss: 0.36973996957143146, Validation Loss: 0.4977702498435974\n",
      "Epoch 60: Train Loss: 0.37277979652086896, Validation Loss: 0.5369387269020081\n",
      "Epoch 61: Train Loss: 0.3926496207714081, Validation Loss: 0.5166776776313782\n",
      "Epoch 62: Train Loss: 0.3863238791624705, Validation Loss: 0.4846715033054352\n",
      "Epoch 63: Train Loss: 0.3767353097597758, Validation Loss: 0.47836601734161377\n",
      "Epoch 64: Train Loss: 0.3713642756144206, Validation Loss: 0.4847969114780426\n",
      "Epoch 65: Train Loss: 0.3637492557366689, Validation Loss: 0.4938518702983856\n",
      "Epoch 66: Train Loss: 0.35303791364034015, Validation Loss: 0.4952585995197296\n",
      "Epoch 67: Train Loss: 0.3592613935470581, Validation Loss: 0.4950839579105377\n",
      "Epoch 68: Train Loss: 0.34962629278500873, Validation Loss: 0.4944702684879303\n",
      "Epoch 69: Train Loss: 0.3582351605097453, Validation Loss: 0.49429038166999817\n",
      "Epoch 70: Train Loss: 0.36311285694440204, Validation Loss: 0.4934541583061218\n",
      "Epoch 71: Train Loss: 0.36088792483011883, Validation Loss: 0.4946463406085968\n",
      "Epoch 72: Train Loss: 0.3578113913536072, Validation Loss: 0.49543696641921997\n",
      "Epoch 73: Train Loss: 0.35173388322194415, Validation Loss: 0.501552164554596\n",
      "Epoch 74: Train Loss: 0.405624787012736, Validation Loss: 0.5013086795806885\n",
      "Epoch 75: Train Loss: 0.35374895731608075, Validation Loss: 0.5129080414772034\n",
      "Epoch 76: Train Loss: 0.3550797601540883, Validation Loss: 0.5039768218994141\n",
      "Epoch 77: Train Loss: 0.3445679148038228, Validation Loss: 0.498762309551239\n",
      "Epoch 78: Train Loss: 0.34966347614924115, Validation Loss: 0.49911370873451233\n",
      "Epoch 79: Train Loss: 0.3690056999524434, Validation Loss: 0.49938279390335083\n",
      "Epoch 80: Train Loss: 0.3703741729259491, Validation Loss: 0.5094365477561951\n",
      "Epoch 81: Train Loss: 0.3669070204099019, Validation Loss: 0.49104511737823486\n",
      "Epoch 82: Train Loss: 0.3513272702693939, Validation Loss: 0.4894225001335144\n",
      "Epoch 83: Train Loss: 0.3605882426102956, Validation Loss: 0.4905869662761688\n",
      "Epoch 84: Train Loss: 0.3517194092273712, Validation Loss: 0.4970465302467346\n",
      "Epoch 85: Train Loss: 0.3576177954673767, Validation Loss: 0.5036402344703674\n",
      "Epoch 86: Train Loss: 0.36332542697588605, Validation Loss: 0.5068221092224121\n",
      "Epoch 87: Train Loss: 0.37363041440645856, Validation Loss: 0.5087617039680481\n",
      "Epoch 88: Train Loss: 0.34657509128252667, Validation Loss: 0.5109941363334656\n",
      "Epoch 89: Train Loss: 0.3525840938091278, Validation Loss: 0.5116283893585205\n",
      "Epoch 90: Train Loss: 0.357390691836675, Validation Loss: 0.5191749334335327\n",
      "Epoch 91: Train Loss: 0.3504188259442647, Validation Loss: 0.5175251960754395\n",
      "Epoch 92: Train Loss: 0.34889209270477295, Validation Loss: 0.5390208959579468\n",
      "Epoch 93: Train Loss: 0.3476494749387105, Validation Loss: 0.5465867519378662\n",
      "Epoch 94: Train Loss: 0.3522080183029175, Validation Loss: 0.5400944352149963\n",
      "Epoch 95: Train Loss: 0.34681211908658344, Validation Loss: 0.5283594727516174\n",
      "Epoch 96: Train Loss: 0.34683865308761597, Validation Loss: 0.5144895911216736\n",
      "Epoch 97: Train Loss: 0.34358761707941693, Validation Loss: 0.5103534460067749\n",
      "Epoch 98: Train Loss: 0.34570054213205975, Validation Loss: 0.509365439414978\n",
      "Epoch 99: Train Loss: 0.3593018750349681, Validation Loss: 0.5087435245513916\n",
      "Epoch 100: Train Loss: 0.34867457548777264, Validation Loss: 0.5083212852478027\n",
      "Epoch 101: Train Loss: 0.3444230258464813, Validation Loss: 0.5017240643501282\n",
      "Epoch 102: Train Loss: 0.3748711049556732, Validation Loss: 0.5104258060455322\n",
      "Epoch 103: Train Loss: 0.35106953978538513, Validation Loss: 0.5317590832710266\n",
      "Epoch 104: Train Loss: 0.35010865330696106, Validation Loss: 0.5432257652282715\n",
      "Epoch 105: Train Loss: 0.3394923706849416, Validation Loss: 0.5401144027709961\n",
      "Epoch 106: Train Loss: 0.3403107424577077, Validation Loss: 0.5344243049621582\n",
      "Epoch 107: Train Loss: 0.3450447420279185, Validation Loss: 0.5316216945648193\n",
      "Epoch 108: Train Loss: 0.33617499470710754, Validation Loss: 0.5307861566543579\n",
      "Epoch 109: Train Loss: 0.3355313241481781, Validation Loss: 0.5310227274894714\n",
      "Epoch 110: Train Loss: 0.34369879961013794, Validation Loss: 0.5353796482086182\n",
      "Epoch 111: Train Loss: 0.37114372849464417, Validation Loss: 0.5280293226242065\n",
      "Epoch 112: Train Loss: 0.34312817454338074, Validation Loss: 0.5555397868156433\n",
      "Epoch 113: Train Loss: 0.34244002898534137, Validation Loss: 0.5164419412612915\n",
      "Epoch 114: Train Loss: 0.33853084842363995, Validation Loss: 0.5101544857025146\n",
      "Epoch 115: Train Loss: 0.3483240107695262, Validation Loss: 0.49732962250709534\n",
      "Epoch 116: Train Loss: 0.34521156549453735, Validation Loss: 0.5026372075080872\n",
      "Epoch 117: Train Loss: 0.3366028269131978, Validation Loss: 0.5089063048362732\n",
      "Epoch 118: Train Loss: 0.36712459723154706, Validation Loss: 0.5116721987724304\n",
      "Epoch 119: Train Loss: 0.3418324490388234, Validation Loss: 0.5120269656181335\n",
      "Epoch 120: Train Loss: 0.34982167681058246, Validation Loss: 0.5210666060447693\n",
      "Epoch 121: Train Loss: 0.34855032960573834, Validation Loss: 0.5572044253349304\n",
      "Epoch 122: Train Loss: 0.34389378627141315, Validation Loss: 0.5323907732963562\n",
      "Epoch 123: Train Loss: 0.33548033237457275, Validation Loss: 0.5204890966415405\n",
      "Epoch 124: Train Loss: 0.3516897161801656, Validation Loss: 0.5173596143722534\n",
      "Epoch 125: Train Loss: 0.33532116810480755, Validation Loss: 0.5168901085853577\n",
      "Epoch 126: Train Loss: 0.37747739752133685, Validation Loss: 0.5181158781051636\n",
      "Epoch 127: Train Loss: 0.3305727442105611, Validation Loss: 0.5183271765708923\n",
      "Epoch 128: Train Loss: 0.3250340322653453, Validation Loss: 0.5184333920478821\n",
      "Epoch 129: Train Loss: 0.3312297463417053, Validation Loss: 0.5184038877487183\n",
      "Epoch 130: Train Loss: 0.3342184325059255, Validation Loss: 0.5185085535049438\n",
      "Epoch 131: Train Loss: 0.32635608315467834, Validation Loss: 0.5194993019104004\n",
      "Epoch 132: Train Loss: 0.33776723345120746, Validation Loss: 0.5262384414672852\n",
      "Epoch 133: Train Loss: 0.3438042203585307, Validation Loss: 0.5244630575180054\n",
      "Epoch 134: Train Loss: 0.3365767300128937, Validation Loss: 0.5166829824447632\n",
      "Epoch 135: Train Loss: 0.3307486375172933, Validation Loss: 0.5144729614257812\n",
      "Epoch 136: Train Loss: 0.34220001101493835, Validation Loss: 0.5137457251548767\n",
      "Epoch 137: Train Loss: 0.35496432582537335, Validation Loss: 0.5127646327018738\n",
      "Epoch 138: Train Loss: 0.3369964063167572, Validation Loss: 0.5126803517341614\n",
      "Epoch 139: Train Loss: 0.3401862680912018, Validation Loss: 0.5125141739845276\n",
      "Epoch 140: Train Loss: 0.3269907732804616, Validation Loss: 0.5160194635391235\n",
      "Epoch 141: Train Loss: 0.3299379249413808, Validation Loss: 0.518216073513031\n",
      "Epoch 142: Train Loss: 0.3332963287830353, Validation Loss: 0.5160763263702393\n",
      "Epoch 143: Train Loss: 0.3323537806669871, Validation Loss: 0.5163273811340332\n",
      "Epoch 144: Train Loss: 0.33300243814786273, Validation Loss: 0.5185795426368713\n",
      "Epoch 145: Train Loss: 0.33776694536209106, Validation Loss: 0.5207880735397339\n",
      "Epoch 146: Train Loss: 0.3378024597962697, Validation Loss: 0.5240771174430847\n",
      "Epoch 147: Train Loss: 0.32983742157618207, Validation Loss: 0.5238076448440552\n",
      "Epoch 148: Train Loss: 0.32975585261980694, Validation Loss: 0.5231667160987854\n",
      "Epoch 149: Train Loss: 0.3250206212202708, Validation Loss: 0.5227128267288208\n",
      "Epoch 150: Train Loss: 0.3294792373975118, Validation Loss: 0.522047221660614\n",
      "Epoch 151: Train Loss: 0.3384629786014557, Validation Loss: 0.5173118114471436\n",
      "Epoch 152: Train Loss: 0.330696980158488, Validation Loss: 0.5107432007789612\n",
      "Early stopping at epoch 153\n",
      "Fold 3\n",
      "Epoch 0: Train Loss: 0.7019999424616495, Validation Loss: 0.6927950978279114\n",
      "Epoch 1: Train Loss: 0.6621474822362264, Validation Loss: 0.6890231370925903\n",
      "Epoch 2: Train Loss: 0.6191515127817789, Validation Loss: 0.6824383735656738\n",
      "Epoch 3: Train Loss: 0.6053307056427002, Validation Loss: 0.674854576587677\n",
      "Epoch 4: Train Loss: 0.5933709144592285, Validation Loss: 0.6644116044044495\n",
      "Epoch 5: Train Loss: 0.5799201329549154, Validation Loss: 0.6560257077217102\n",
      "Epoch 6: Train Loss: 0.5578224857648214, Validation Loss: 0.6465904116630554\n",
      "Epoch 7: Train Loss: 0.5563542048136393, Validation Loss: 0.6399958729743958\n",
      "Epoch 8: Train Loss: 0.5514944593111674, Validation Loss: 0.6369102597236633\n",
      "Epoch 9: Train Loss: 0.5471930106480917, Validation Loss: 0.6343766450881958\n",
      "Epoch 10: Train Loss: 0.5378936330477396, Validation Loss: 0.6200687885284424\n",
      "Epoch 11: Train Loss: 0.5367120901743571, Validation Loss: 0.6021193861961365\n",
      "Epoch 12: Train Loss: 0.516340027252833, Validation Loss: 0.5962591767311096\n",
      "Epoch 13: Train Loss: 0.509298582871755, Validation Loss: 0.5888779163360596\n",
      "Epoch 14: Train Loss: 0.501778781414032, Validation Loss: 0.5847193002700806\n",
      "Epoch 15: Train Loss: 0.4716568887233734, Validation Loss: 0.5832699537277222\n",
      "Epoch 16: Train Loss: 0.48349348704020184, Validation Loss: 0.5840874314308167\n",
      "Epoch 17: Train Loss: 0.4798878828684489, Validation Loss: 0.585759699344635\n",
      "Epoch 18: Train Loss: 0.47081611553827923, Validation Loss: 0.5874878168106079\n",
      "Epoch 19: Train Loss: 0.4732693632443746, Validation Loss: 0.5880104303359985\n",
      "Epoch 20: Train Loss: 0.4640348056952159, Validation Loss: 0.5922399163246155\n",
      "Epoch 21: Train Loss: 0.46088366707166034, Validation Loss: 0.5997813940048218\n",
      "Epoch 22: Train Loss: 0.4424713949362437, Validation Loss: 0.6035497784614563\n",
      "Epoch 23: Train Loss: 0.4320521553357442, Validation Loss: 0.6066625118255615\n",
      "Epoch 24: Train Loss: 0.42775800824165344, Validation Loss: 0.6089227199554443\n",
      "Epoch 25: Train Loss: 0.43327386180559796, Validation Loss: 0.6064782738685608\n",
      "Epoch 26: Train Loss: 0.4649454752604167, Validation Loss: 0.6043245196342468\n",
      "Epoch 27: Train Loss: 0.4268423517545064, Validation Loss: 0.6032910346984863\n",
      "Epoch 28: Train Loss: 0.42679956555366516, Validation Loss: 0.6023402214050293\n",
      "Epoch 29: Train Loss: 0.41915667057037354, Validation Loss: 0.6022541522979736\n",
      "Epoch 30: Train Loss: 0.425862451394399, Validation Loss: 0.6041580438613892\n",
      "Epoch 31: Train Loss: 0.41997525095939636, Validation Loss: 0.6008700132369995\n",
      "Epoch 32: Train Loss: 0.41181322932243347, Validation Loss: 0.5992405414581299\n",
      "Epoch 33: Train Loss: 0.40675022204717, Validation Loss: 0.5973743796348572\n",
      "Epoch 34: Train Loss: 0.4038694004217784, Validation Loss: 0.5968024730682373\n",
      "Epoch 35: Train Loss: 0.40076292554537457, Validation Loss: 0.5972691774368286\n",
      "Epoch 36: Train Loss: 0.3992306888103485, Validation Loss: 0.5962736010551453\n",
      "Epoch 37: Train Loss: 0.4010022083918254, Validation Loss: 0.5963816046714783\n",
      "Epoch 38: Train Loss: 0.4059846301873525, Validation Loss: 0.597626805305481\n",
      "Epoch 39: Train Loss: 0.40018264452616376, Validation Loss: 0.5980392098426819\n",
      "Epoch 40: Train Loss: 0.39665962258974713, Validation Loss: 0.6061794757843018\n",
      "Epoch 41: Train Loss: 0.4054500460624695, Validation Loss: 0.6084715723991394\n",
      "Epoch 42: Train Loss: 0.39524643619855243, Validation Loss: 0.6048649549484253\n",
      "Epoch 43: Train Loss: 0.4031192362308502, Validation Loss: 0.6019302010536194\n",
      "Epoch 44: Train Loss: 0.3902447124322255, Validation Loss: 0.6053456664085388\n",
      "Epoch 45: Train Loss: 0.39874495069185895, Validation Loss: 0.6047154664993286\n",
      "Epoch 46: Train Loss: 0.3924104670683543, Validation Loss: 0.6055668592453003\n",
      "Epoch 47: Train Loss: 0.3895805279413859, Validation Loss: 0.6057490110397339\n",
      "Epoch 48: Train Loss: 0.38623879353205365, Validation Loss: 0.6059198379516602\n",
      "Epoch 49: Train Loss: 0.3836294710636139, Validation Loss: 0.6059359908103943\n",
      "Epoch 50: Train Loss: 0.4106126129627228, Validation Loss: 0.6117344498634338\n",
      "Epoch 51: Train Loss: 0.40512720743815106, Validation Loss: 0.5997044444084167\n",
      "Epoch 52: Train Loss: 0.3881538410981496, Validation Loss: 0.5909066796302795\n",
      "Epoch 53: Train Loss: 0.39216614762942, Validation Loss: 0.5949951410293579\n",
      "Epoch 54: Train Loss: 0.3721431791782379, Validation Loss: 0.5980017781257629\n",
      "Epoch 55: Train Loss: 0.3811492919921875, Validation Loss: 0.5994082093238831\n",
      "Epoch 56: Train Loss: 0.39121079444885254, Validation Loss: 0.5992703437805176\n",
      "Epoch 57: Train Loss: 0.3749805986881256, Validation Loss: 0.6008783578872681\n",
      "Epoch 58: Train Loss: 0.3930506507555644, Validation Loss: 0.6008418202400208\n",
      "Epoch 59: Train Loss: 0.3743103047211965, Validation Loss: 0.6006587147712708\n",
      "Epoch 60: Train Loss: 0.38168713450431824, Validation Loss: 0.6047273278236389\n",
      "Epoch 61: Train Loss: 0.3767123619715373, Validation Loss: 0.5998371243476868\n",
      "Epoch 62: Train Loss: 0.3744292954603831, Validation Loss: 0.6112143397331238\n",
      "Epoch 63: Train Loss: 0.37170276045799255, Validation Loss: 0.6153625845909119\n",
      "Epoch 64: Train Loss: 0.39329831798871356, Validation Loss: 0.6078611016273499\n",
      "Epoch 65: Train Loss: 0.37385589877764386, Validation Loss: 0.6014286279678345\n",
      "Epoch 66: Train Loss: 0.3768116732438405, Validation Loss: 0.5973055362701416\n",
      "Epoch 67: Train Loss: 0.3731156090895335, Validation Loss: 0.5947341918945312\n",
      "Epoch 68: Train Loss: 0.39626137415568036, Validation Loss: 0.5922638177871704\n",
      "Epoch 69: Train Loss: 0.37606074412663776, Validation Loss: 0.5908851623535156\n",
      "Epoch 70: Train Loss: 0.37729724248250324, Validation Loss: 0.5860381126403809\n",
      "Epoch 71: Train Loss: 0.3631916344165802, Validation Loss: 0.5798518657684326\n",
      "Epoch 72: Train Loss: 0.3736943105856578, Validation Loss: 0.5768382549285889\n",
      "Epoch 73: Train Loss: 0.38260817527770996, Validation Loss: 0.5773665904998779\n",
      "Epoch 74: Train Loss: 0.3656374514102936, Validation Loss: 0.5780516862869263\n",
      "Epoch 75: Train Loss: 0.3734610974788666, Validation Loss: 0.5763209462165833\n",
      "Epoch 76: Train Loss: 0.4048989415168762, Validation Loss: 0.5778170228004456\n",
      "Epoch 77: Train Loss: 0.36387046178181964, Validation Loss: 0.5801088809967041\n",
      "Epoch 78: Train Loss: 0.3738265534241994, Validation Loss: 0.5806456208229065\n",
      "Epoch 79: Train Loss: 0.3617696166038513, Validation Loss: 0.5812438130378723\n",
      "Epoch 80: Train Loss: 0.39416177074114483, Validation Loss: 0.5852219462394714\n",
      "Epoch 81: Train Loss: 0.3738788962364197, Validation Loss: 0.5941032767295837\n",
      "Epoch 82: Train Loss: 0.3740508755048116, Validation Loss: 0.5887050032615662\n",
      "Epoch 83: Train Loss: 0.3603760898113251, Validation Loss: 0.5855314135551453\n",
      "Epoch 84: Train Loss: 0.37598979473114014, Validation Loss: 0.5867586135864258\n",
      "Epoch 85: Train Loss: 0.36937205990155536, Validation Loss: 0.5858855247497559\n",
      "Epoch 86: Train Loss: 0.3664754629135132, Validation Loss: 0.5867862105369568\n",
      "Epoch 87: Train Loss: 0.3578185737133026, Validation Loss: 0.5881280899047852\n",
      "Epoch 88: Train Loss: 0.3580658932526906, Validation Loss: 0.5886020660400391\n",
      "Epoch 89: Train Loss: 0.3580966095129649, Validation Loss: 0.5883222222328186\n",
      "Epoch 90: Train Loss: 0.3572014669577281, Validation Loss: 0.5908777117729187\n",
      "Epoch 91: Train Loss: 0.3629430631796519, Validation Loss: 0.5904436707496643\n",
      "Epoch 92: Train Loss: 0.3582065502802531, Validation Loss: 0.5945350527763367\n",
      "Epoch 93: Train Loss: 0.3552139401435852, Validation Loss: 0.5920770168304443\n",
      "Epoch 94: Train Loss: 0.35329585274060565, Validation Loss: 0.5904680490493774\n",
      "Epoch 95: Train Loss: 0.3722078800201416, Validation Loss: 0.586225688457489\n",
      "Epoch 96: Train Loss: 0.3577580451965332, Validation Loss: 0.5831536054611206\n",
      "Epoch 97: Train Loss: 0.35591092705726624, Validation Loss: 0.5813053846359253\n",
      "Epoch 98: Train Loss: 0.3587542275587718, Validation Loss: 0.5806147456169128\n",
      "Epoch 99: Train Loss: 0.3480943739414215, Validation Loss: 0.5802450776100159\n",
      "Epoch 100: Train Loss: 0.3489585518836975, Validation Loss: 0.5748851895332336\n",
      "Epoch 101: Train Loss: 0.3726692199707031, Validation Loss: 0.5778919458389282\n",
      "Epoch 102: Train Loss: 0.35397955775260925, Validation Loss: 0.5764652490615845\n",
      "Epoch 103: Train Loss: 0.3456238905588786, Validation Loss: 0.583970308303833\n",
      "Epoch 104: Train Loss: 0.3472095529238383, Validation Loss: 0.5812876224517822\n",
      "Epoch 105: Train Loss: 0.36281679073969525, Validation Loss: 0.5760943293571472\n",
      "Epoch 106: Train Loss: 0.3564063608646393, Validation Loss: 0.5747971534729004\n",
      "Epoch 107: Train Loss: 0.3497503201166789, Validation Loss: 0.5722776651382446\n",
      "Epoch 108: Train Loss: 0.3457510968049367, Validation Loss: 0.5722915530204773\n",
      "Epoch 109: Train Loss: 0.3760612408320109, Validation Loss: 0.5738154053688049\n",
      "Epoch 110: Train Loss: 0.353823721408844, Validation Loss: 0.5926326513290405\n",
      "Epoch 111: Train Loss: 0.35222267111142475, Validation Loss: 0.5978325009346008\n",
      "Epoch 112: Train Loss: 0.3530008892218272, Validation Loss: 0.5925946235656738\n",
      "Epoch 113: Train Loss: 0.3671574294567108, Validation Loss: 0.5856162905693054\n",
      "Epoch 114: Train Loss: 0.3493357201417287, Validation Loss: 0.5811068415641785\n",
      "Epoch 115: Train Loss: 0.3661775092283885, Validation Loss: 0.5779902935028076\n",
      "Epoch 116: Train Loss: 0.3563414712746938, Validation Loss: 0.5722768902778625\n",
      "Epoch 117: Train Loss: 0.3421504298845927, Validation Loss: 0.5683929920196533\n",
      "Epoch 118: Train Loss: 0.35967503984769184, Validation Loss: 0.5679827332496643\n",
      "Epoch 119: Train Loss: 0.34458614389101666, Validation Loss: 0.5686829090118408\n",
      "Epoch 120: Train Loss: 0.3812496066093445, Validation Loss: 0.5873507857322693\n",
      "Epoch 121: Train Loss: 0.35632272561391193, Validation Loss: 0.5733799934387207\n",
      "Epoch 122: Train Loss: 0.3394533693790436, Validation Loss: 0.5586659908294678\n",
      "Epoch 123: Train Loss: 0.361194650332133, Validation Loss: 0.5632046461105347\n",
      "Epoch 124: Train Loss: 0.3521150251229604, Validation Loss: 0.5731484889984131\n",
      "Epoch 125: Train Loss: 0.34957117835680646, Validation Loss: 0.5839244723320007\n",
      "Epoch 126: Train Loss: 0.3422550360361735, Validation Loss: 0.5924821496009827\n",
      "Epoch 127: Train Loss: 0.35431260863939923, Validation Loss: 0.5981864929199219\n",
      "Epoch 128: Train Loss: 0.3487708369890849, Validation Loss: 0.5972790122032166\n",
      "Epoch 129: Train Loss: 0.33876381317774457, Validation Loss: 0.5985917448997498\n",
      "Epoch 130: Train Loss: 0.3468060294787089, Validation Loss: 0.5869263410568237\n",
      "Epoch 131: Train Loss: 0.3359559973080953, Validation Loss: 0.5806201100349426\n",
      "Epoch 132: Train Loss: 0.3480522135893504, Validation Loss: 0.5814134478569031\n",
      "Epoch 133: Train Loss: 0.3712160289287567, Validation Loss: 0.5949223637580872\n",
      "Epoch 134: Train Loss: 0.35739649335543316, Validation Loss: 0.6106593608856201\n",
      "Epoch 135: Train Loss: 0.35050280888875324, Validation Loss: 0.6212891340255737\n",
      "Epoch 136: Train Loss: 0.3471622169017792, Validation Loss: 0.6157891154289246\n",
      "Epoch 137: Train Loss: 0.3441852231820424, Validation Loss: 0.6132605671882629\n",
      "Epoch 138: Train Loss: 0.3454280396302541, Validation Loss: 0.6120845675468445\n",
      "Epoch 139: Train Loss: 0.3392334779103597, Validation Loss: 0.6106566786766052\n",
      "Epoch 140: Train Loss: 0.3410613238811493, Validation Loss: 0.5922988057136536\n",
      "Epoch 141: Train Loss: 0.36008795102437335, Validation Loss: 0.5931239128112793\n",
      "Epoch 142: Train Loss: 0.35054760177930194, Validation Loss: 0.5846397280693054\n",
      "Epoch 143: Train Loss: 0.3413514792919159, Validation Loss: 0.610490620136261\n",
      "Epoch 144: Train Loss: 0.34545037150382996, Validation Loss: 0.6189274787902832\n",
      "Epoch 145: Train Loss: 0.3447745442390442, Validation Loss: 0.6119599938392639\n",
      "Epoch 146: Train Loss: 0.3441261053085327, Validation Loss: 0.605284571647644\n",
      "Epoch 147: Train Loss: 0.3486511806646983, Validation Loss: 0.599484920501709\n",
      "Epoch 148: Train Loss: 0.3480859597524007, Validation Loss: 0.5952944755554199\n",
      "Epoch 149: Train Loss: 0.3561051885286967, Validation Loss: 0.592832624912262\n",
      "Epoch 150: Train Loss: 0.3379176656405131, Validation Loss: 0.5806397199630737\n",
      "Epoch 151: Train Loss: 0.3449120819568634, Validation Loss: 0.5720327496528625\n",
      "Epoch 152: Train Loss: 0.3433630367120107, Validation Loss: 0.5744655132293701\n",
      "Epoch 153: Train Loss: 0.32961274186770123, Validation Loss: 0.5794538855552673\n",
      "Epoch 154: Train Loss: 0.3314688007036845, Validation Loss: 0.581727147102356\n",
      "Epoch 155: Train Loss: 0.3415723343690236, Validation Loss: 0.5819456577301025\n",
      "Epoch 156: Train Loss: 0.34720877806345624, Validation Loss: 0.5779640078544617\n",
      "Epoch 157: Train Loss: 0.3477448622385661, Validation Loss: 0.5736764073371887\n",
      "Epoch 158: Train Loss: 0.344743549823761, Validation Loss: 0.5724914073944092\n",
      "Epoch 159: Train Loss: 0.32857945561408997, Validation Loss: 0.5725646018981934\n",
      "Epoch 160: Train Loss: 0.3441503445307414, Validation Loss: 0.5686984658241272\n",
      "Epoch 161: Train Loss: 0.3432173828283946, Validation Loss: 0.5631023049354553\n",
      "Epoch 162: Train Loss: 0.3375791708628337, Validation Loss: 0.5580666065216064\n",
      "Epoch 163: Train Loss: 0.37276212374369305, Validation Loss: 0.5537732243537903\n",
      "Epoch 164: Train Loss: 0.33350934584935504, Validation Loss: 0.5573447346687317\n",
      "Epoch 165: Train Loss: 0.34819453954696655, Validation Loss: 0.5586625337600708\n",
      "Epoch 166: Train Loss: 0.3334578176339467, Validation Loss: 0.5639844536781311\n",
      "Epoch 167: Train Loss: 0.332641065120697, Validation Loss: 0.5667464733123779\n",
      "Epoch 168: Train Loss: 0.3268131911754608, Validation Loss: 0.5691872835159302\n",
      "Epoch 169: Train Loss: 0.3314845661322276, Validation Loss: 0.5710288882255554\n",
      "Epoch 170: Train Loss: 0.3296221097310384, Validation Loss: 0.5688984394073486\n",
      "Epoch 171: Train Loss: 0.342679629723231, Validation Loss: 0.5698494911193848\n",
      "Epoch 172: Train Loss: 0.3258301615715027, Validation Loss: 0.5703434348106384\n",
      "Epoch 173: Train Loss: 0.34410332640012103, Validation Loss: 0.5777349472045898\n",
      "Epoch 174: Train Loss: 0.33598950505256653, Validation Loss: 0.5807011723518372\n",
      "Epoch 175: Train Loss: 0.3337911864121755, Validation Loss: 0.5812391638755798\n",
      "Epoch 176: Train Loss: 0.33675925930341083, Validation Loss: 0.5818834900856018\n",
      "Epoch 177: Train Loss: 0.33202023307482403, Validation Loss: 0.5805603861808777\n",
      "Epoch 178: Train Loss: 0.32553624113400775, Validation Loss: 0.5797184109687805\n",
      "Epoch 179: Train Loss: 0.331695020198822, Validation Loss: 0.579582691192627\n",
      "Epoch 180: Train Loss: 0.3420483370621999, Validation Loss: 0.5744836330413818\n",
      "Epoch 181: Train Loss: 0.3270869553089142, Validation Loss: 0.5788109302520752\n",
      "Epoch 182: Train Loss: 0.3615213930606842, Validation Loss: 0.5810908079147339\n",
      "Epoch 183: Train Loss: 0.3216015299161275, Validation Loss: 0.580005943775177\n",
      "Epoch 184: Train Loss: 0.3380170663197835, Validation Loss: 0.5813306570053101\n",
      "Epoch 185: Train Loss: 0.33131123582522076, Validation Loss: 0.5776501893997192\n",
      "Epoch 186: Train Loss: 0.32920725146929425, Validation Loss: 0.5734168291091919\n",
      "Epoch 187: Train Loss: 0.32301146785418194, Validation Loss: 0.571084201335907\n",
      "Epoch 188: Train Loss: 0.333380530277888, Validation Loss: 0.5698185563087463\n",
      "Epoch 189: Train Loss: 0.34176917870839435, Validation Loss: 0.568081796169281\n",
      "Epoch 190: Train Loss: 0.3277941147486369, Validation Loss: 0.5596318244934082\n",
      "Epoch 191: Train Loss: 0.32975701491038006, Validation Loss: 0.5545433759689331\n",
      "Epoch 192: Train Loss: 0.33864439527193707, Validation Loss: 0.5513361096382141\n",
      "Epoch 193: Train Loss: 0.3277057508627574, Validation Loss: 0.558987021446228\n",
      "Epoch 194: Train Loss: 0.3498644133408864, Validation Loss: 0.5900408625602722\n",
      "Epoch 195: Train Loss: 0.3249928057193756, Validation Loss: 0.6081016659736633\n",
      "Epoch 196: Train Loss: 0.32265253861745197, Validation Loss: 0.6123445630073547\n",
      "Epoch 197: Train Loss: 0.32616106669108075, Validation Loss: 0.6124880313873291\n",
      "Epoch 198: Train Loss: 0.34346431493759155, Validation Loss: 0.6135503053665161\n",
      "Epoch 199: Train Loss: 0.33586440483729046, Validation Loss: 0.6115230321884155\n",
      "Epoch 200: Train Loss: 0.3411706785360972, Validation Loss: 0.5881208777427673\n",
      "Epoch 201: Train Loss: 0.3228129545847575, Validation Loss: 0.5583561658859253\n",
      "Epoch 202: Train Loss: 0.3242197434107463, Validation Loss: 0.5476915836334229\n",
      "Epoch 203: Train Loss: 0.3451590637365977, Validation Loss: 0.5371798872947693\n",
      "Epoch 204: Train Loss: 0.3267466326554616, Validation Loss: 0.5320995450019836\n",
      "Epoch 205: Train Loss: 0.33346818884213764, Validation Loss: 0.5417701601982117\n",
      "Epoch 206: Train Loss: 0.33141125241915387, Validation Loss: 0.5493553280830383\n",
      "Epoch 207: Train Loss: 0.3317393660545349, Validation Loss: 0.5558973550796509\n",
      "Epoch 208: Train Loss: 0.3231785794099172, Validation Loss: 0.5582384467124939\n",
      "Epoch 209: Train Loss: 0.3322073022524516, Validation Loss: 0.5582082867622375\n",
      "Epoch 210: Train Loss: 0.3321351210276286, Validation Loss: 0.5981336832046509\n",
      "Epoch 211: Train Loss: 0.32007290919621784, Validation Loss: 0.6158867478370667\n",
      "Epoch 212: Train Loss: 0.33305912216504413, Validation Loss: 0.6287062764167786\n",
      "Epoch 213: Train Loss: 0.33342865109443665, Validation Loss: 0.6115870475769043\n",
      "Epoch 214: Train Loss: 0.32430700461069745, Validation Loss: 0.6075602769851685\n",
      "Epoch 215: Train Loss: 0.3340403338273366, Validation Loss: 0.6055092811584473\n",
      "Epoch 216: Train Loss: 0.329887052377065, Validation Loss: 0.6046773791313171\n",
      "Epoch 217: Train Loss: 0.3228204349676768, Validation Loss: 0.5995877981185913\n",
      "Epoch 218: Train Loss: 0.3228439688682556, Validation Loss: 0.597235381603241\n",
      "Epoch 219: Train Loss: 0.32107284665107727, Validation Loss: 0.5997239351272583\n",
      "Epoch 220: Train Loss: 0.32287415862083435, Validation Loss: 0.5964171886444092\n",
      "Epoch 221: Train Loss: 0.31872377792994183, Validation Loss: 0.5938961505889893\n",
      "Epoch 222: Train Loss: 0.32171862324078876, Validation Loss: 0.5981666445732117\n",
      "Epoch 223: Train Loss: 0.3192112644513448, Validation Loss: 0.6001325249671936\n",
      "Epoch 224: Train Loss: 0.33326272169748944, Validation Loss: 0.5908405184745789\n",
      "Epoch 225: Train Loss: 0.32310378551483154, Validation Loss: 0.5917869210243225\n",
      "Epoch 226: Train Loss: 0.3193015952905019, Validation Loss: 0.5954156517982483\n",
      "Epoch 227: Train Loss: 0.31853195031483966, Validation Loss: 0.5964100360870361\n",
      "Epoch 228: Train Loss: 0.32079405585924786, Validation Loss: 0.5978050231933594\n",
      "Epoch 229: Train Loss: 0.31976879636446637, Validation Loss: 0.6014116406440735\n",
      "Epoch 230: Train Loss: 0.3309420049190521, Validation Loss: 0.5701647996902466\n",
      "Epoch 231: Train Loss: 0.3287876447041829, Validation Loss: 0.5587248206138611\n",
      "Epoch 232: Train Loss: 0.32420651117960614, Validation Loss: 0.551144540309906\n",
      "Epoch 233: Train Loss: 0.3219566543896993, Validation Loss: 0.5465342998504639\n",
      "Epoch 234: Train Loss: 0.31947068373362225, Validation Loss: 0.5445937514305115\n",
      "Epoch 235: Train Loss: 0.3189121087392171, Validation Loss: 0.5471093654632568\n",
      "Epoch 236: Train Loss: 0.32088183363278705, Validation Loss: 0.550295889377594\n",
      "Epoch 237: Train Loss: 0.3242754439512889, Validation Loss: 0.5498666167259216\n",
      "Epoch 238: Train Loss: 0.3232232630252838, Validation Loss: 0.5478475093841553\n",
      "Epoch 239: Train Loss: 0.3298616011937459, Validation Loss: 0.5467916131019592\n",
      "Epoch 240: Train Loss: 0.31658242146174115, Validation Loss: 0.5505825281143188\n",
      "Epoch 241: Train Loss: 0.3220420877138774, Validation Loss: 0.578766942024231\n",
      "Epoch 242: Train Loss: 0.33106016119321185, Validation Loss: 0.5983965396881104\n",
      "Epoch 243: Train Loss: 0.3304300606250763, Validation Loss: 0.6188520789146423\n",
      "Epoch 244: Train Loss: 0.32891207933425903, Validation Loss: 0.6255868077278137\n",
      "Epoch 245: Train Loss: 0.322532594203949, Validation Loss: 0.6218316555023193\n",
      "Epoch 246: Train Loss: 0.3237018783887227, Validation Loss: 0.6164186596870422\n",
      "Epoch 247: Train Loss: 0.3187556266784668, Validation Loss: 0.6098800301551819\n",
      "Epoch 248: Train Loss: 0.32038338979085285, Validation Loss: 0.6079453825950623\n",
      "Epoch 249: Train Loss: 0.32051817576090497, Validation Loss: 0.6034435033798218\n",
      "Epoch 250: Train Loss: 0.32236170768737793, Validation Loss: 0.5975371599197388\n",
      "Epoch 251: Train Loss: 0.3202047348022461, Validation Loss: 0.5919741988182068\n",
      "Epoch 252: Train Loss: 0.3233211934566498, Validation Loss: 0.5918793678283691\n",
      "Epoch 253: Train Loss: 0.3200990657011668, Validation Loss: 0.5845423936843872\n",
      "Epoch 254: Train Loss: 0.32756495475769043, Validation Loss: 0.5732700824737549\n",
      "Epoch 255: Train Loss: 0.3181861340999603, Validation Loss: 0.5708515048027039\n",
      "Epoch 256: Train Loss: 0.31846946477890015, Validation Loss: 0.5700908899307251\n",
      "Epoch 257: Train Loss: 0.3256649672985077, Validation Loss: 0.5706610083580017\n",
      "Epoch 258: Train Loss: 0.31734304626782733, Validation Loss: 0.5709172487258911\n",
      "Epoch 259: Train Loss: 0.3201335271199544, Validation Loss: 0.5714276432991028\n",
      "Epoch 260: Train Loss: 0.321485956509908, Validation Loss: 0.5648439526557922\n",
      "Epoch 261: Train Loss: 0.3186364273230235, Validation Loss: 0.5652236938476562\n",
      "Epoch 262: Train Loss: 0.32030050953229267, Validation Loss: 0.5651288628578186\n",
      "Epoch 263: Train Loss: 0.3161005477110545, Validation Loss: 0.5662060976028442\n",
      "Epoch 264: Train Loss: 0.3200492064158122, Validation Loss: 0.5684241652488708\n",
      "Epoch 265: Train Loss: 0.3184625804424286, Validation Loss: 0.5721814632415771\n",
      "Epoch 266: Train Loss: 0.3189030687014262, Validation Loss: 0.574787437915802\n",
      "Epoch 267: Train Loss: 0.318121204773585, Validation Loss: 0.5769515037536621\n",
      "Epoch 268: Train Loss: 0.3225935995578766, Validation Loss: 0.5774657726287842\n",
      "Epoch 269: Train Loss: 0.31551648179690045, Validation Loss: 0.5783064961433411\n",
      "Epoch 270: Train Loss: 0.3201228578885396, Validation Loss: 0.5843073725700378\n",
      "Epoch 271: Train Loss: 0.31717370947202045, Validation Loss: 0.5881136059761047\n",
      "Epoch 272: Train Loss: 0.32031845053037006, Validation Loss: 0.5918749570846558\n",
      "Epoch 273: Train Loss: 0.3328687051932017, Validation Loss: 0.6054131984710693\n",
      "Epoch 274: Train Loss: 0.3164273003737132, Validation Loss: 0.6162780523300171\n",
      "Epoch 275: Train Loss: 0.3167276879151662, Validation Loss: 0.6187891960144043\n",
      "Epoch 276: Train Loss: 0.3257584075133006, Validation Loss: 0.6168598532676697\n",
      "Epoch 277: Train Loss: 0.3212394913037618, Validation Loss: 0.6162391901016235\n",
      "Epoch 278: Train Loss: 0.3176612953344981, Validation Loss: 0.6174649596214294\n",
      "Epoch 279: Train Loss: 0.32062239448229474, Validation Loss: 0.617243766784668\n",
      "Epoch 280: Train Loss: 0.3187506894270579, Validation Loss: 0.6197627782821655\n",
      "Epoch 281: Train Loss: 0.318459818760554, Validation Loss: 0.6191204786300659\n",
      "Epoch 282: Train Loss: 0.3223910331726074, Validation Loss: 0.5980823040008545\n",
      "Epoch 283: Train Loss: 0.32136812806129456, Validation Loss: 0.5952585935592651\n",
      "Epoch 284: Train Loss: 0.3195870320002238, Validation Loss: 0.5959230661392212\n",
      "Epoch 285: Train Loss: 0.3262270788351695, Validation Loss: 0.6042234301567078\n",
      "Epoch 286: Train Loss: 0.3228375315666199, Validation Loss: 0.607969343662262\n",
      "Epoch 287: Train Loss: 0.32320424914360046, Validation Loss: 0.6094987988471985\n",
      "Epoch 288: Train Loss: 0.3218240936597188, Validation Loss: 0.6090924739837646\n",
      "Epoch 289: Train Loss: 0.31745003660519916, Validation Loss: 0.6088104248046875\n",
      "Epoch 290: Train Loss: 0.3196117877960205, Validation Loss: 0.6048150062561035\n",
      "Epoch 291: Train Loss: 0.3180213173230489, Validation Loss: 0.6037478446960449\n",
      "Epoch 292: Train Loss: 0.31852541367212933, Validation Loss: 0.602555513381958\n",
      "Epoch 293: Train Loss: 0.3251166045665741, Validation Loss: 0.6021023392677307\n",
      "Epoch 294: Train Loss: 0.3241429626941681, Validation Loss: 0.5898670554161072\n",
      "Epoch 295: Train Loss: 0.31721606850624084, Validation Loss: 0.5816574096679688\n",
      "Epoch 296: Train Loss: 0.31665492057800293, Validation Loss: 0.5787929892539978\n",
      "Epoch 297: Train Loss: 0.33756665388743085, Validation Loss: 0.5741881728172302\n",
      "Epoch 298: Train Loss: 0.31761277715365094, Validation Loss: 0.5713746547698975\n",
      "Epoch 299: Train Loss: 0.32414066791534424, Validation Loss: 0.5689637660980225\n",
      "Epoch 300: Train Loss: 0.32045920689900714, Validation Loss: 0.5703859925270081\n",
      "Epoch 301: Train Loss: 0.3186005850632985, Validation Loss: 0.5744771361351013\n",
      "Epoch 302: Train Loss: 0.326993465423584, Validation Loss: 0.5750769376754761\n",
      "Epoch 303: Train Loss: 0.31871949632962543, Validation Loss: 0.5776531100273132\n",
      "Early stopping at epoch 304\n",
      "Fold 4\n",
      "Epoch 0: Train Loss: 0.695459802945455, Validation Loss: 0.7044510245323181\n",
      "Epoch 1: Train Loss: 0.6451988220214844, Validation Loss: 0.709977924823761\n",
      "Epoch 2: Train Loss: 0.6203339099884033, Validation Loss: 0.7149766683578491\n",
      "Epoch 3: Train Loss: 0.6038055419921875, Validation Loss: 0.7146453261375427\n",
      "Epoch 4: Train Loss: 0.584957997004191, Validation Loss: 0.7114267349243164\n",
      "Epoch 5: Train Loss: 0.5673114856084188, Validation Loss: 0.704084038734436\n",
      "Epoch 6: Train Loss: 0.5662915507952372, Validation Loss: 0.699821949005127\n",
      "Epoch 7: Train Loss: 0.5622806549072266, Validation Loss: 0.6981973648071289\n",
      "Epoch 8: Train Loss: 0.5648395617802938, Validation Loss: 0.6982486248016357\n",
      "Epoch 9: Train Loss: 0.5468153953552246, Validation Loss: 0.6987856030464172\n",
      "Epoch 10: Train Loss: 0.5549031098683676, Validation Loss: 0.6924822330474854\n",
      "Epoch 11: Train Loss: 0.5313790043195089, Validation Loss: 0.6928173303604126\n",
      "Epoch 12: Train Loss: 0.5325332482655843, Validation Loss: 0.6885746717453003\n",
      "Epoch 13: Train Loss: 0.521456797917684, Validation Loss: 0.6826639175415039\n",
      "Epoch 14: Train Loss: 0.5073381861050924, Validation Loss: 0.6778581142425537\n",
      "Epoch 15: Train Loss: 0.5189042389392853, Validation Loss: 0.6747422814369202\n",
      "Epoch 16: Train Loss: 0.4956844747066498, Validation Loss: 0.673271656036377\n",
      "Epoch 17: Train Loss: 0.4896915555000305, Validation Loss: 0.6733518838882446\n",
      "Epoch 18: Train Loss: 0.4926300843556722, Validation Loss: 0.673124372959137\n",
      "Epoch 19: Train Loss: 0.48664135734240216, Validation Loss: 0.6741411685943604\n",
      "Epoch 20: Train Loss: 0.503745307524999, Validation Loss: 0.6784984469413757\n",
      "Epoch 21: Train Loss: 0.48170151313145954, Validation Loss: 0.6679055690765381\n",
      "Epoch 22: Train Loss: 0.49234147866566974, Validation Loss: 0.6597113609313965\n",
      "Epoch 23: Train Loss: 0.4527198175589244, Validation Loss: 0.6594813466072083\n",
      "Epoch 24: Train Loss: 0.4540160596370697, Validation Loss: 0.6589433550834656\n",
      "Epoch 25: Train Loss: 0.44558533032735187, Validation Loss: 0.6584616899490356\n",
      "Epoch 26: Train Loss: 0.47702770431836444, Validation Loss: 0.6553254723548889\n",
      "Epoch 27: Train Loss: 0.4588371217250824, Validation Loss: 0.6510204076766968\n",
      "Epoch 28: Train Loss: 0.4622480273246765, Validation Loss: 0.6485620737075806\n",
      "Epoch 29: Train Loss: 0.448096623023351, Validation Loss: 0.6501119136810303\n",
      "Epoch 30: Train Loss: 0.4329568048318227, Validation Loss: 0.6582475900650024\n",
      "Epoch 31: Train Loss: 0.4496974249680837, Validation Loss: 0.6609413623809814\n",
      "Epoch 32: Train Loss: 0.4330740074316661, Validation Loss: 0.6630325317382812\n",
      "Epoch 33: Train Loss: 0.42194029688835144, Validation Loss: 0.6547974944114685\n",
      "Epoch 34: Train Loss: 0.4265452027320862, Validation Loss: 0.6546376943588257\n",
      "Epoch 35: Train Loss: 0.41443463166554767, Validation Loss: 0.639132022857666\n",
      "Epoch 36: Train Loss: 0.41694604357083637, Validation Loss: 0.630534827709198\n",
      "Epoch 37: Train Loss: 0.4214017490545909, Validation Loss: 0.6241408586502075\n",
      "Epoch 38: Train Loss: 0.4177665015061696, Validation Loss: 0.6223185062408447\n",
      "Epoch 39: Train Loss: 0.40794628858566284, Validation Loss: 0.6223049163818359\n",
      "Epoch 40: Train Loss: 0.42314931750297546, Validation Loss: 0.6342613697052002\n",
      "Epoch 41: Train Loss: 0.42856531341870624, Validation Loss: 0.6187799572944641\n",
      "Epoch 42: Train Loss: 0.43193166454633075, Validation Loss: 0.5846912264823914\n",
      "Epoch 43: Train Loss: 0.40215879678726196, Validation Loss: 0.5951378345489502\n",
      "Epoch 44: Train Loss: 0.40296945969263714, Validation Loss: 0.6027097105979919\n",
      "Epoch 45: Train Loss: 0.40565024813016254, Validation Loss: 0.6030038595199585\n",
      "Epoch 46: Train Loss: 0.40977218747138977, Validation Loss: 0.5966982245445251\n",
      "Epoch 47: Train Loss: 0.4150666892528534, Validation Loss: 0.59499192237854\n",
      "Epoch 48: Train Loss: 0.40046852827072144, Validation Loss: 0.5948306918144226\n",
      "Epoch 49: Train Loss: 0.3919561207294464, Validation Loss: 0.594709038734436\n",
      "Epoch 50: Train Loss: 0.40580230951309204, Validation Loss: 0.5923725962638855\n",
      "Epoch 51: Train Loss: 0.40159453948338825, Validation Loss: 0.5716639757156372\n",
      "Epoch 52: Train Loss: 0.3944254120190938, Validation Loss: 0.5652588605880737\n",
      "Epoch 53: Train Loss: 0.4096539815266927, Validation Loss: 0.5920018553733826\n",
      "Epoch 54: Train Loss: 0.3874342441558838, Validation Loss: 0.6074635982513428\n",
      "Epoch 55: Train Loss: 0.3937111794948578, Validation Loss: 0.60671466588974\n",
      "Epoch 56: Train Loss: 0.3938768804073334, Validation Loss: 0.5973551869392395\n",
      "Epoch 57: Train Loss: 0.3752496639887492, Validation Loss: 0.5914138555526733\n",
      "Epoch 58: Train Loss: 0.3726687530676524, Validation Loss: 0.5892822742462158\n",
      "Epoch 59: Train Loss: 0.396466185649236, Validation Loss: 0.5897336006164551\n",
      "Epoch 60: Train Loss: 0.3949992259343465, Validation Loss: 0.5871707797050476\n",
      "Epoch 61: Train Loss: 0.37733901540438336, Validation Loss: 0.608940839767456\n",
      "Epoch 62: Train Loss: 0.3862727681795756, Validation Loss: 0.6075323820114136\n",
      "Epoch 63: Train Loss: 0.40153124928474426, Validation Loss: 0.5914235711097717\n",
      "Epoch 64: Train Loss: 0.37336523334185284, Validation Loss: 0.5987406969070435\n",
      "Epoch 65: Train Loss: 0.38035916288693744, Validation Loss: 0.5946388840675354\n",
      "Epoch 66: Train Loss: 0.3700697422027588, Validation Loss: 0.5909756422042847\n",
      "Epoch 67: Train Loss: 0.37329856554667157, Validation Loss: 0.591482937335968\n",
      "Epoch 68: Train Loss: 0.387646347284317, Validation Loss: 0.5945323705673218\n",
      "Epoch 69: Train Loss: 0.3672611117362976, Validation Loss: 0.5940089821815491\n",
      "Epoch 70: Train Loss: 0.3742989202340444, Validation Loss: 0.6590076684951782\n",
      "Epoch 71: Train Loss: 0.3835584223270416, Validation Loss: 0.6592295169830322\n",
      "Epoch 72: Train Loss: 0.36812905470530194, Validation Loss: 0.6362408399581909\n",
      "Epoch 73: Train Loss: 0.3701518376668294, Validation Loss: 0.6107388138771057\n",
      "Epoch 74: Train Loss: 0.3690253992875417, Validation Loss: 0.620099663734436\n",
      "Epoch 75: Train Loss: 0.3700881004333496, Validation Loss: 0.635356605052948\n",
      "Epoch 76: Train Loss: 0.3815993070602417, Validation Loss: 0.6372857093811035\n",
      "Epoch 77: Train Loss: 0.36800018946329754, Validation Loss: 0.6401903033256531\n",
      "Epoch 78: Train Loss: 0.36465389529863995, Validation Loss: 0.6350528597831726\n",
      "Epoch 79: Train Loss: 0.3610884447892507, Validation Loss: 0.6331198811531067\n",
      "Epoch 80: Train Loss: 0.3552350203196208, Validation Loss: 0.5937210917472839\n",
      "Epoch 81: Train Loss: 0.3625894288221995, Validation Loss: 0.587682843208313\n",
      "Epoch 82: Train Loss: 0.36128785212834674, Validation Loss: 0.6081851720809937\n",
      "Epoch 83: Train Loss: 0.37849944829940796, Validation Loss: 0.6198816299438477\n",
      "Epoch 84: Train Loss: 0.38526633381843567, Validation Loss: 0.6389253735542297\n",
      "Epoch 85: Train Loss: 0.3581302762031555, Validation Loss: 0.634925901889801\n",
      "Epoch 86: Train Loss: 0.3712000052134196, Validation Loss: 0.6207807064056396\n",
      "Epoch 87: Train Loss: 0.36519328753153485, Validation Loss: 0.6106701493263245\n",
      "Epoch 88: Train Loss: 0.3696333666642507, Validation Loss: 0.608068585395813\n",
      "Epoch 89: Train Loss: 0.3687014977137248, Validation Loss: 0.6116341948509216\n",
      "Epoch 90: Train Loss: 0.35695747534434, Validation Loss: 0.6113333106040955\n",
      "Epoch 91: Train Loss: 0.3717799484729767, Validation Loss: 0.6094639301300049\n",
      "Epoch 92: Train Loss: 0.3905736406644185, Validation Loss: 0.6211093664169312\n",
      "Epoch 93: Train Loss: 0.37847347060839337, Validation Loss: 0.6084728837013245\n",
      "Epoch 94: Train Loss: 0.35366547107696533, Validation Loss: 0.6005772352218628\n",
      "Epoch 95: Train Loss: 0.37537531057993573, Validation Loss: 0.5975832939147949\n",
      "Epoch 96: Train Loss: 0.35468222697575885, Validation Loss: 0.59075528383255\n",
      "Epoch 97: Train Loss: 0.3604627549648285, Validation Loss: 0.5881221294403076\n",
      "Epoch 98: Train Loss: 0.35661176840464276, Validation Loss: 0.5893895626068115\n",
      "Epoch 99: Train Loss: 0.34709962209065753, Validation Loss: 0.5903866291046143\n",
      "Epoch 100: Train Loss: 0.3487534523010254, Validation Loss: 0.5710713863372803\n",
      "Epoch 101: Train Loss: 0.3584718902905782, Validation Loss: 0.5874109864234924\n",
      "Epoch 102: Train Loss: 0.35828495025634766, Validation Loss: 0.6029943823814392\n",
      "Epoch 103: Train Loss: 0.3633419672648112, Validation Loss: 0.5996590852737427\n",
      "Epoch 104: Train Loss: 0.36683256427447003, Validation Loss: 0.5927930474281311\n",
      "Epoch 105: Train Loss: 0.36617473761240643, Validation Loss: 0.5987367033958435\n",
      "Epoch 106: Train Loss: 0.3548650046189626, Validation Loss: 0.6085302829742432\n",
      "Epoch 107: Train Loss: 0.3572765489419301, Validation Loss: 0.6050066351890564\n",
      "Epoch 108: Train Loss: 0.3630884091059367, Validation Loss: 0.606521487236023\n",
      "Epoch 109: Train Loss: 0.3810136616230011, Validation Loss: 0.6034563779830933\n",
      "Epoch 110: Train Loss: 0.35562379161516827, Validation Loss: 0.636256754398346\n",
      "Epoch 111: Train Loss: 0.3428508937358856, Validation Loss: 0.633054792881012\n",
      "Epoch 112: Train Loss: 0.36233145991961163, Validation Loss: 0.6178739666938782\n",
      "Epoch 113: Train Loss: 0.354968657096227, Validation Loss: 0.617900013923645\n",
      "Epoch 114: Train Loss: 0.36227665344874066, Validation Loss: 0.6228986978530884\n",
      "Epoch 115: Train Loss: 0.3504933516184489, Validation Loss: 0.6208009123802185\n",
      "Epoch 116: Train Loss: 0.34427876273790997, Validation Loss: 0.6224753260612488\n",
      "Epoch 117: Train Loss: 0.3565763235092163, Validation Loss: 0.6202135682106018\n",
      "Epoch 118: Train Loss: 0.3425029218196869, Validation Loss: 0.6159136295318604\n",
      "Epoch 119: Train Loss: 0.3514194091161092, Validation Loss: 0.612507700920105\n",
      "Epoch 120: Train Loss: 0.34575847784678143, Validation Loss: 0.5717771053314209\n",
      "Epoch 121: Train Loss: 0.3662493626276652, Validation Loss: 0.5966055393218994\n",
      "Epoch 122: Train Loss: 0.363414466381073, Validation Loss: 0.6569762825965881\n",
      "Epoch 123: Train Loss: 0.34880730509757996, Validation Loss: 0.6735731959342957\n",
      "Epoch 124: Train Loss: 0.3545244137446086, Validation Loss: 0.6763116717338562\n",
      "Epoch 125: Train Loss: 0.34899723529815674, Validation Loss: 0.6601699590682983\n",
      "Epoch 126: Train Loss: 0.3556266228357951, Validation Loss: 0.6444553732872009\n",
      "Epoch 127: Train Loss: 0.34698952237764996, Validation Loss: 0.6312976479530334\n",
      "Epoch 128: Train Loss: 0.33912548422813416, Validation Loss: 0.6258631944656372\n",
      "Epoch 129: Train Loss: 0.34067706267038983, Validation Loss: 0.6208298802375793\n",
      "Epoch 130: Train Loss: 0.34933451811472577, Validation Loss: 0.5881784558296204\n",
      "Epoch 131: Train Loss: 0.3493471642335256, Validation Loss: 0.587700605392456\n",
      "Epoch 132: Train Loss: 0.3411323626836141, Validation Loss: 0.5846924781799316\n",
      "Epoch 133: Train Loss: 0.3453161319096883, Validation Loss: 0.5789346694946289\n",
      "Epoch 134: Train Loss: 0.3542343278725942, Validation Loss: 0.5992894768714905\n",
      "Epoch 135: Train Loss: 0.34918320178985596, Validation Loss: 0.6122780442237854\n",
      "Epoch 136: Train Loss: 0.3472601075967153, Validation Loss: 0.6034621000289917\n",
      "Epoch 137: Train Loss: 0.3443786601225535, Validation Loss: 0.5972000360488892\n",
      "Epoch 138: Train Loss: 0.34965593616167706, Validation Loss: 0.5952029228210449\n",
      "Epoch 139: Train Loss: 0.33752772212028503, Validation Loss: 0.5950753688812256\n",
      "Epoch 140: Train Loss: 0.34139183163642883, Validation Loss: 0.5776529312133789\n",
      "Epoch 141: Train Loss: 0.3412189582983653, Validation Loss: 0.5752494931221008\n",
      "Epoch 142: Train Loss: 0.35973607500394184, Validation Loss: 0.593964159488678\n",
      "Epoch 143: Train Loss: 0.36234774192174274, Validation Loss: 0.6197962760925293\n",
      "Epoch 144: Train Loss: 0.3506900668144226, Validation Loss: 0.6374607086181641\n",
      "Epoch 145: Train Loss: 0.34827062487602234, Validation Loss: 0.6452845931053162\n",
      "Epoch 146: Train Loss: 0.3483616014321645, Validation Loss: 0.6481866836547852\n",
      "Epoch 147: Train Loss: 0.33754246433575946, Validation Loss: 0.6424828767776489\n",
      "Epoch 148: Train Loss: 0.3411487936973572, Validation Loss: 0.6366168856620789\n",
      "Epoch 149: Train Loss: 0.34300218025843304, Validation Loss: 0.6344743371009827\n",
      "Epoch 150: Train Loss: 0.34135733048121136, Validation Loss: 0.6102129817008972\n",
      "Epoch 151: Train Loss: 0.34401193261146545, Validation Loss: 0.6018438339233398\n",
      "Early stopping at epoch 152\n",
      "Fold 5\n",
      "Epoch 0: Train Loss: 0.7120851476987203, Validation Loss: 0.7089218497276306\n",
      "Epoch 1: Train Loss: 0.6760190526644388, Validation Loss: 0.713548481464386\n",
      "Epoch 2: Train Loss: 0.6309847633043925, Validation Loss: 0.7180134654045105\n",
      "Epoch 3: Train Loss: 0.6200993855794271, Validation Loss: 0.7228421568870544\n",
      "Epoch 4: Train Loss: 0.5945566495259603, Validation Loss: 0.7259042263031006\n",
      "Epoch 5: Train Loss: 0.6011505524317423, Validation Loss: 0.7243286967277527\n",
      "Epoch 6: Train Loss: 0.5680165688196818, Validation Loss: 0.7210971117019653\n",
      "Epoch 7: Train Loss: 0.584343671798706, Validation Loss: 0.7185527682304382\n",
      "Epoch 8: Train Loss: 0.56327885389328, Validation Loss: 0.7146907448768616\n",
      "Epoch 9: Train Loss: 0.5612302819887797, Validation Loss: 0.7101320624351501\n",
      "Epoch 10: Train Loss: 0.5562689701716105, Validation Loss: 0.6895220279693604\n",
      "Epoch 11: Train Loss: 0.5425088405609131, Validation Loss: 0.6705929636955261\n",
      "Epoch 12: Train Loss: 0.5265973011652628, Validation Loss: 0.6406577825546265\n",
      "Epoch 13: Train Loss: 0.5044876635074615, Validation Loss: 0.6444969773292542\n",
      "Epoch 14: Train Loss: 0.5185566147168478, Validation Loss: 0.6242069005966187\n",
      "Epoch 15: Train Loss: 0.5070125460624695, Validation Loss: 0.6154186725616455\n",
      "Epoch 16: Train Loss: 0.49684744079907733, Validation Loss: 0.6201226711273193\n",
      "Epoch 17: Train Loss: 0.4918678104877472, Validation Loss: 0.620216429233551\n",
      "Epoch 18: Train Loss: 0.4943465789159139, Validation Loss: 0.620424747467041\n",
      "Epoch 19: Train Loss: 0.4992930789788564, Validation Loss: 0.621616542339325\n",
      "Epoch 20: Train Loss: 0.4807084600130717, Validation Loss: 0.6011815071105957\n",
      "Epoch 21: Train Loss: 0.47586265206336975, Validation Loss: 0.5958435535430908\n",
      "Epoch 22: Train Loss: 0.4814753731091817, Validation Loss: 0.6107307076454163\n",
      "Epoch 23: Train Loss: 0.4831036925315857, Validation Loss: 0.6127947568893433\n",
      "Epoch 24: Train Loss: 0.4615742762883504, Validation Loss: 0.6095439195632935\n",
      "Epoch 25: Train Loss: 0.4611279567082723, Validation Loss: 0.6100412607192993\n",
      "Epoch 26: Train Loss: 0.44135721524556476, Validation Loss: 0.6099079251289368\n",
      "Epoch 27: Train Loss: 0.4515291651089986, Validation Loss: 0.6074568033218384\n",
      "Epoch 28: Train Loss: 0.4630064070224762, Validation Loss: 0.6078000068664551\n",
      "Epoch 29: Train Loss: 0.44552462299664813, Validation Loss: 0.6081163287162781\n",
      "Epoch 30: Train Loss: 0.44805000225702923, Validation Loss: 0.5940418839454651\n",
      "Epoch 31: Train Loss: 0.4445216457049052, Validation Loss: 0.5988938808441162\n",
      "Epoch 32: Train Loss: 0.4446803132692973, Validation Loss: 0.6199018359184265\n",
      "Epoch 33: Train Loss: 0.45582492152849835, Validation Loss: 0.6062248349189758\n",
      "Epoch 34: Train Loss: 0.45185871918996173, Validation Loss: 0.6081822514533997\n",
      "Epoch 35: Train Loss: 0.43461231390635174, Validation Loss: 0.6144790649414062\n",
      "Epoch 36: Train Loss: 0.4314853052298228, Validation Loss: 0.6202363967895508\n",
      "Epoch 37: Train Loss: 0.4452147086461385, Validation Loss: 0.6292392015457153\n",
      "Epoch 38: Train Loss: 0.45210989316304523, Validation Loss: 0.6302626729011536\n",
      "Epoch 39: Train Loss: 0.43361226717631024, Validation Loss: 0.6294363737106323\n",
      "Epoch 40: Train Loss: 0.42250139514605206, Validation Loss: 0.6492772102355957\n",
      "Epoch 41: Train Loss: 0.4209229548772176, Validation Loss: 0.6536607146263123\n",
      "Epoch 42: Train Loss: 0.44411933422088623, Validation Loss: 0.6305640935897827\n",
      "Epoch 43: Train Loss: 0.40735655029614765, Validation Loss: 0.6299028396606445\n",
      "Epoch 44: Train Loss: 0.41536565621693927, Validation Loss: 0.622340977191925\n",
      "Epoch 45: Train Loss: 0.41394853591918945, Validation Loss: 0.6078502535820007\n",
      "Epoch 46: Train Loss: 0.4098966320355733, Validation Loss: 0.6006972789764404\n",
      "Epoch 47: Train Loss: 0.40762409567832947, Validation Loss: 0.5983465909957886\n",
      "Epoch 48: Train Loss: 0.4134894609451294, Validation Loss: 0.6001694798469543\n",
      "Epoch 49: Train Loss: 0.4194507400194804, Validation Loss: 0.6023423075675964\n",
      "Epoch 50: Train Loss: 0.41772763927777606, Validation Loss: 0.6604701280593872\n",
      "Epoch 51: Train Loss: 0.4465905527273814, Validation Loss: 0.6617522835731506\n",
      "Epoch 52: Train Loss: 0.43318066994349164, Validation Loss: 0.5751860737800598\n",
      "Epoch 53: Train Loss: 0.4115973909695943, Validation Loss: 0.5277205109596252\n",
      "Epoch 54: Train Loss: 0.38999177018801373, Validation Loss: 0.5268555879592896\n",
      "Epoch 55: Train Loss: 0.3927965859572093, Validation Loss: 0.5348432064056396\n",
      "Epoch 56: Train Loss: 0.38867934544881183, Validation Loss: 0.5269955396652222\n",
      "Epoch 57: Train Loss: 0.38442912697792053, Validation Loss: 0.524043083190918\n",
      "Epoch 58: Train Loss: 0.4050696790218353, Validation Loss: 0.5255753397941589\n",
      "Epoch 59: Train Loss: 0.3936014274756114, Validation Loss: 0.5288822054862976\n",
      "Epoch 60: Train Loss: 0.3815492292245229, Validation Loss: 0.545132040977478\n",
      "Epoch 61: Train Loss: 0.4020623366038005, Validation Loss: 0.5762783885002136\n",
      "Epoch 62: Train Loss: 0.3790999948978424, Validation Loss: 0.5796339511871338\n",
      "Epoch 63: Train Loss: 0.39081663886706036, Validation Loss: 0.5550452470779419\n",
      "Epoch 64: Train Loss: 0.38190556565920514, Validation Loss: 0.5430686473846436\n",
      "Epoch 65: Train Loss: 0.3935667673746745, Validation Loss: 0.5378047227859497\n",
      "Epoch 66: Train Loss: 0.3891516129175822, Validation Loss: 0.5415241718292236\n",
      "Epoch 67: Train Loss: 0.36354095737139386, Validation Loss: 0.5480807423591614\n",
      "Epoch 68: Train Loss: 0.378839373588562, Validation Loss: 0.5490574240684509\n",
      "Epoch 69: Train Loss: 0.37571924924850464, Validation Loss: 0.5504880547523499\n",
      "Epoch 70: Train Loss: 0.3687167763710022, Validation Loss: 0.570158064365387\n",
      "Epoch 71: Train Loss: 0.3762584626674652, Validation Loss: 0.6021755933761597\n",
      "Epoch 72: Train Loss: 0.3653278549512227, Validation Loss: 0.6223890781402588\n",
      "Epoch 73: Train Loss: 0.3686479131380717, Validation Loss: 0.6071776747703552\n",
      "Epoch 74: Train Loss: 0.3688243230183919, Validation Loss: 0.5673391222953796\n",
      "Epoch 75: Train Loss: 0.35594211022059125, Validation Loss: 0.5588402152061462\n",
      "Epoch 76: Train Loss: 0.3706447680791219, Validation Loss: 0.5522608160972595\n",
      "Epoch 77: Train Loss: 0.35947830478350323, Validation Loss: 0.5537233948707581\n",
      "Epoch 78: Train Loss: 0.36670440435409546, Validation Loss: 0.5535684823989868\n",
      "Epoch 79: Train Loss: 0.3757186532020569, Validation Loss: 0.5538628697395325\n",
      "Epoch 80: Train Loss: 0.3760516146818797, Validation Loss: 0.6199790835380554\n",
      "Epoch 81: Train Loss: 0.37883540987968445, Validation Loss: 0.6224395036697388\n",
      "Epoch 82: Train Loss: 0.3759993314743042, Validation Loss: 0.5967354774475098\n",
      "Epoch 83: Train Loss: 0.40348078807195026, Validation Loss: 0.5415295362472534\n",
      "Epoch 84: Train Loss: 0.3571197986602783, Validation Loss: 0.5330264568328857\n",
      "Epoch 85: Train Loss: 0.38682084282239276, Validation Loss: 0.521248996257782\n",
      "Epoch 86: Train Loss: 0.35259150465329486, Validation Loss: 0.5180373787879944\n",
      "Epoch 87: Train Loss: 0.35606732964515686, Validation Loss: 0.5170881748199463\n",
      "Epoch 88: Train Loss: 0.3740151723225911, Validation Loss: 0.5184075236320496\n",
      "Epoch 89: Train Loss: 0.35396722952524823, Validation Loss: 0.518423318862915\n",
      "Epoch 90: Train Loss: 0.3603579004605611, Validation Loss: 0.5397591590881348\n",
      "Epoch 91: Train Loss: 0.36884135007858276, Validation Loss: 0.5419965982437134\n",
      "Epoch 92: Train Loss: 0.36132948597272235, Validation Loss: 0.5661060810089111\n",
      "Epoch 93: Train Loss: 0.3763466477394104, Validation Loss: 0.5606018304824829\n",
      "Epoch 94: Train Loss: 0.3632851839065552, Validation Loss: 0.5417127013206482\n",
      "Epoch 95: Train Loss: 0.3494628369808197, Validation Loss: 0.5355942845344543\n",
      "Epoch 96: Train Loss: 0.3484707474708557, Validation Loss: 0.5320846438407898\n",
      "Epoch 97: Train Loss: 0.36458753546079, Validation Loss: 0.5322403907775879\n",
      "Epoch 98: Train Loss: 0.34024421374003094, Validation Loss: 0.5330949425697327\n",
      "Epoch 99: Train Loss: 0.3404054840405782, Validation Loss: 0.5337836146354675\n",
      "Epoch 100: Train Loss: 0.34862397114435834, Validation Loss: 0.5486548542976379\n",
      "Epoch 101: Train Loss: 0.3447835644086202, Validation Loss: 0.5816711783409119\n",
      "Epoch 102: Train Loss: 0.36105867226918537, Validation Loss: 0.5563397407531738\n",
      "Epoch 103: Train Loss: 0.3697555859883626, Validation Loss: 0.5542731881141663\n",
      "Epoch 104: Train Loss: 0.3526288866996765, Validation Loss: 0.5723908543586731\n",
      "Epoch 105: Train Loss: 0.3610943754514058, Validation Loss: 0.559755802154541\n",
      "Epoch 106: Train Loss: 0.35007862250010174, Validation Loss: 0.5572010278701782\n",
      "Epoch 107: Train Loss: 0.3462017277876536, Validation Loss: 0.5473991632461548\n",
      "Epoch 108: Train Loss: 0.3424762388070424, Validation Loss: 0.543651819229126\n",
      "Epoch 109: Train Loss: 0.35283613204956055, Validation Loss: 0.542132556438446\n",
      "Epoch 110: Train Loss: 0.3558649222056071, Validation Loss: 0.5455853939056396\n",
      "Epoch 111: Train Loss: 0.37108773986498517, Validation Loss: 0.573110044002533\n",
      "Epoch 112: Train Loss: 0.3748207588990529, Validation Loss: 0.5766833424568176\n",
      "Epoch 113: Train Loss: 0.3462065060933431, Validation Loss: 0.5744988322257996\n",
      "Epoch 114: Train Loss: 0.35138652722040814, Validation Loss: 0.5622630715370178\n",
      "Epoch 115: Train Loss: 0.3537144064903259, Validation Loss: 0.5517387390136719\n",
      "Epoch 116: Train Loss: 0.3472319344679515, Validation Loss: 0.5526796579360962\n",
      "Epoch 117: Train Loss: 0.3460306028525035, Validation Loss: 0.5477160215377808\n",
      "Epoch 118: Train Loss: 0.4021054108937581, Validation Loss: 0.5368641018867493\n",
      "Epoch 119: Train Loss: 0.33601827422777814, Validation Loss: 0.5335060358047485\n",
      "Epoch 120: Train Loss: 0.3437874416510264, Validation Loss: 0.5072453022003174\n",
      "Epoch 121: Train Loss: 0.351261039574941, Validation Loss: 0.5084928274154663\n",
      "Epoch 122: Train Loss: 0.34442291657129925, Validation Loss: 0.542563796043396\n",
      "Epoch 123: Train Loss: 0.34956308205922443, Validation Loss: 0.5444663166999817\n",
      "Epoch 124: Train Loss: 0.3938446044921875, Validation Loss: 0.5534589290618896\n",
      "Epoch 125: Train Loss: 0.3364337583382924, Validation Loss: 0.5494635105133057\n",
      "Epoch 126: Train Loss: 0.3410673836867015, Validation Loss: 0.5448564887046814\n",
      "Epoch 127: Train Loss: 0.33732133110364276, Validation Loss: 0.5419211387634277\n",
      "Epoch 128: Train Loss: 0.34878599643707275, Validation Loss: 0.5487810969352722\n",
      "Epoch 129: Train Loss: 0.34455233812332153, Validation Loss: 0.5502054691314697\n",
      "Epoch 130: Train Loss: 0.33925997217496234, Validation Loss: 0.5637937784194946\n",
      "Epoch 131: Train Loss: 0.337069571018219, Validation Loss: 0.5611060261726379\n",
      "Epoch 132: Train Loss: 0.3400420844554901, Validation Loss: 0.5465494394302368\n",
      "Epoch 133: Train Loss: 0.34377919634183246, Validation Loss: 0.5247399210929871\n",
      "Epoch 134: Train Loss: 0.3536483844121297, Validation Loss: 0.5160325765609741\n",
      "Epoch 135: Train Loss: 0.3485054175059001, Validation Loss: 0.5225831866264343\n",
      "Epoch 136: Train Loss: 0.3477798104286194, Validation Loss: 0.5321809649467468\n",
      "Epoch 137: Train Loss: 0.3323412537574768, Validation Loss: 0.5432000756263733\n",
      "Epoch 138: Train Loss: 0.33194485306739807, Validation Loss: 0.5480139851570129\n",
      "Epoch 139: Train Loss: 0.342029333114624, Validation Loss: 0.5478328466415405\n",
      "Epoch 140: Train Loss: 0.3393372694651286, Validation Loss: 0.5783703923225403\n",
      "Epoch 141: Train Loss: 0.3493506908416748, Validation Loss: 0.5993881225585938\n",
      "Epoch 142: Train Loss: 0.356767604748408, Validation Loss: 0.5881404280662537\n",
      "Epoch 143: Train Loss: 0.3337576687335968, Validation Loss: 0.5783267617225647\n",
      "Epoch 144: Train Loss: 0.337053785721461, Validation Loss: 0.5708880424499512\n",
      "Epoch 145: Train Loss: 0.3384660283724467, Validation Loss: 0.5609425902366638\n",
      "Epoch 146: Train Loss: 0.34644779562950134, Validation Loss: 0.5535853505134583\n",
      "Epoch 147: Train Loss: 0.3371562163035075, Validation Loss: 0.5516431927680969\n",
      "Epoch 148: Train Loss: 0.3384418586889903, Validation Loss: 0.5517138838768005\n",
      "Epoch 149: Train Loss: 0.3382667501767476, Validation Loss: 0.5523532629013062\n",
      "Epoch 150: Train Loss: 0.34632524847984314, Validation Loss: 0.5681758522987366\n",
      "Epoch 151: Train Loss: 0.3340467115243276, Validation Loss: 0.5796250700950623\n",
      "Epoch 152: Train Loss: 0.33386872212092084, Validation Loss: 0.5687142610549927\n",
      "Epoch 153: Train Loss: 0.3464849889278412, Validation Loss: 0.5501323938369751\n",
      "Epoch 154: Train Loss: 0.33982305725415546, Validation Loss: 0.5430908203125\n",
      "Epoch 155: Train Loss: 0.3444070915381114, Validation Loss: 0.5432353019714355\n",
      "Epoch 156: Train Loss: 0.339007039864858, Validation Loss: 0.5472625494003296\n",
      "Epoch 157: Train Loss: 0.33450307448705036, Validation Loss: 0.5491654276847839\n",
      "Epoch 158: Train Loss: 0.3382003406683604, Validation Loss: 0.5508481860160828\n",
      "Epoch 159: Train Loss: 0.33062654733657837, Validation Loss: 0.5519154071807861\n",
      "Epoch 160: Train Loss: 0.3409699996312459, Validation Loss: 0.5561962127685547\n",
      "Epoch 161: Train Loss: 0.3440501093864441, Validation Loss: 0.5517762303352356\n",
      "Epoch 162: Train Loss: 0.3326198657353719, Validation Loss: 0.5470789670944214\n",
      "Epoch 163: Train Loss: 0.3419894774754842, Validation Loss: 0.5458768606185913\n",
      "Epoch 164: Train Loss: 0.35109300414721173, Validation Loss: 0.5714139938354492\n",
      "Epoch 165: Train Loss: 0.33977484703063965, Validation Loss: 0.570380687713623\n",
      "Epoch 166: Train Loss: 0.3345535099506378, Validation Loss: 0.5540952086448669\n",
      "Epoch 167: Train Loss: 0.33112287521362305, Validation Loss: 0.5498304963111877\n",
      "Epoch 168: Train Loss: 0.3357262810071309, Validation Loss: 0.5476964712142944\n",
      "Epoch 169: Train Loss: 0.3373553951581319, Validation Loss: 0.5471183657646179\n",
      "Epoch 170: Train Loss: 0.3302530845006307, Validation Loss: 0.5387543439865112\n",
      "Epoch 171: Train Loss: 0.3343075116475423, Validation Loss: 0.5186641216278076\n",
      "Epoch 172: Train Loss: 0.33284977078437805, Validation Loss: 0.5284757018089294\n",
      "Epoch 173: Train Loss: 0.3354058464368184, Validation Loss: 0.5429718494415283\n",
      "Epoch 174: Train Loss: 0.33506203691164654, Validation Loss: 0.5539773106575012\n",
      "Epoch 175: Train Loss: 0.35343149304389954, Validation Loss: 0.5494356155395508\n",
      "Epoch 176: Train Loss: 0.3317960997422536, Validation Loss: 0.5423471927642822\n",
      "Epoch 177: Train Loss: 0.334670494000117, Validation Loss: 0.5374245643615723\n",
      "Epoch 178: Train Loss: 0.33452598253885907, Validation Loss: 0.5342701077461243\n",
      "Epoch 179: Train Loss: 0.3381968041261037, Validation Loss: 0.5318343043327332\n",
      "Epoch 180: Train Loss: 0.3268076479434967, Validation Loss: 0.49002939462661743\n",
      "Epoch 181: Train Loss: 0.32969756921132404, Validation Loss: 0.5515091419219971\n",
      "Epoch 182: Train Loss: 0.3555358052253723, Validation Loss: 0.5593500137329102\n",
      "Epoch 183: Train Loss: 0.3672784666220347, Validation Loss: 0.5614158511161804\n",
      "Epoch 184: Train Loss: 0.3366408944129944, Validation Loss: 0.5432160496711731\n",
      "Epoch 185: Train Loss: 0.339488406976064, Validation Loss: 0.5299492478370667\n",
      "Epoch 186: Train Loss: 0.34589122732480365, Validation Loss: 0.5207692980766296\n",
      "Epoch 187: Train Loss: 0.33342061440149945, Validation Loss: 0.516838014125824\n",
      "Epoch 188: Train Loss: 0.3251773615678151, Validation Loss: 0.5127237439155579\n",
      "Epoch 189: Train Loss: 0.32423845926920575, Validation Loss: 0.5121952295303345\n",
      "Epoch 190: Train Loss: 0.3351972798506419, Validation Loss: 0.4948680102825165\n",
      "Epoch 191: Train Loss: 0.3470190763473511, Validation Loss: 0.5072717666625977\n",
      "Epoch 192: Train Loss: 0.33385221163431805, Validation Loss: 0.534863293170929\n",
      "Epoch 193: Train Loss: 0.365984171628952, Validation Loss: 0.5559463500976562\n",
      "Epoch 194: Train Loss: 0.32673224806785583, Validation Loss: 0.534837007522583\n",
      "Epoch 195: Train Loss: 0.3296203315258026, Validation Loss: 0.525743305683136\n",
      "Epoch 196: Train Loss: 0.33529937267303467, Validation Loss: 0.5146641135215759\n",
      "Epoch 197: Train Loss: 0.33315621813138324, Validation Loss: 0.5063226222991943\n",
      "Epoch 198: Train Loss: 0.32942548394203186, Validation Loss: 0.5024774670600891\n",
      "Epoch 199: Train Loss: 0.3354092240333557, Validation Loss: 0.5004064440727234\n",
      "Epoch 200: Train Loss: 0.3315296669801076, Validation Loss: 0.5050813555717468\n",
      "Epoch 201: Train Loss: 0.3413541316986084, Validation Loss: 0.4962445795536041\n",
      "Epoch 202: Train Loss: 0.34778239329655963, Validation Loss: 0.4856117069721222\n",
      "Epoch 203: Train Loss: 0.3303292195002238, Validation Loss: 0.4836038053035736\n",
      "Epoch 204: Train Loss: 0.3291211823622386, Validation Loss: 0.4744777977466583\n",
      "Epoch 205: Train Loss: 0.33652834097544354, Validation Loss: 0.48401665687561035\n",
      "Epoch 206: Train Loss: 0.3295662800470988, Validation Loss: 0.5048331618309021\n",
      "Epoch 207: Train Loss: 0.33366527160008747, Validation Loss: 0.5216297507286072\n",
      "Epoch 208: Train Loss: 0.3270227909088135, Validation Loss: 0.5253490805625916\n",
      "Epoch 209: Train Loss: 0.3311786651611328, Validation Loss: 0.5262627005577087\n",
      "Epoch 210: Train Loss: 0.3293446401755015, Validation Loss: 0.5501413345336914\n",
      "Epoch 211: Train Loss: 0.3318415582180023, Validation Loss: 0.5615997910499573\n",
      "Epoch 212: Train Loss: 0.3235882719357808, Validation Loss: 0.564386785030365\n",
      "Epoch 213: Train Loss: 0.3268741965293884, Validation Loss: 0.5636832118034363\n",
      "Epoch 214: Train Loss: 0.33023889859517414, Validation Loss: 0.5533445477485657\n",
      "Epoch 215: Train Loss: 0.33443957567214966, Validation Loss: 0.5403370261192322\n",
      "Epoch 216: Train Loss: 0.33284224073092145, Validation Loss: 0.5365921258926392\n",
      "Epoch 217: Train Loss: 0.331902136405309, Validation Loss: 0.5361180901527405\n",
      "Epoch 218: Train Loss: 0.33132627606391907, Validation Loss: 0.5347168445587158\n",
      "Epoch 219: Train Loss: 0.3301648298899333, Validation Loss: 0.5340057015419006\n",
      "Epoch 220: Train Loss: 0.33171820640563965, Validation Loss: 0.5272254943847656\n",
      "Epoch 221: Train Loss: 0.3323552906513214, Validation Loss: 0.5174383521080017\n",
      "Epoch 222: Train Loss: 0.3437179724375407, Validation Loss: 0.5298978090286255\n",
      "Epoch 223: Train Loss: 0.3286868731180827, Validation Loss: 0.5327849388122559\n",
      "Epoch 224: Train Loss: 0.3297394613424937, Validation Loss: 0.5141608119010925\n",
      "Epoch 225: Train Loss: 0.3398498793443044, Validation Loss: 0.5113338828086853\n",
      "Epoch 226: Train Loss: 0.32215194900830585, Validation Loss: 0.5091428756713867\n",
      "Epoch 227: Train Loss: 0.33050288756688434, Validation Loss: 0.5078641772270203\n",
      "Epoch 228: Train Loss: 0.35321948925654095, Validation Loss: 0.5053519010543823\n",
      "Epoch 229: Train Loss: 0.3317437171936035, Validation Loss: 0.5038666129112244\n",
      "Epoch 230: Train Loss: 0.33498966693878174, Validation Loss: 0.5374112725257874\n",
      "Epoch 231: Train Loss: 0.3270689845085144, Validation Loss: 0.5592312216758728\n",
      "Epoch 232: Train Loss: 0.3320321540037791, Validation Loss: 0.5628740787506104\n",
      "Epoch 233: Train Loss: 0.3300568461418152, Validation Loss: 0.5684807896614075\n",
      "Epoch 234: Train Loss: 0.32889755566914874, Validation Loss: 0.5569130778312683\n",
      "Epoch 235: Train Loss: 0.32791923483212787, Validation Loss: 0.542041003704071\n",
      "Epoch 236: Train Loss: 0.33910245696703595, Validation Loss: 0.5309632420539856\n",
      "Epoch 237: Train Loss: 0.33425239721934, Validation Loss: 0.5224627256393433\n",
      "Epoch 238: Train Loss: 0.3300008674462636, Validation Loss: 0.518492579460144\n",
      "Epoch 239: Train Loss: 0.3275367220242818, Validation Loss: 0.5185788869857788\n",
      "Epoch 240: Train Loss: 0.323931356271108, Validation Loss: 0.5008768439292908\n",
      "Epoch 241: Train Loss: 0.325769970814387, Validation Loss: 0.48335450887680054\n",
      "Epoch 242: Train Loss: 0.32616191109021503, Validation Loss: 0.47701478004455566\n",
      "Epoch 243: Train Loss: 0.3292686442534129, Validation Loss: 0.4798277020454407\n",
      "Epoch 244: Train Loss: 0.3310452699661255, Validation Loss: 0.5044811367988586\n",
      "Epoch 245: Train Loss: 0.3280886709690094, Validation Loss: 0.5164773464202881\n",
      "Epoch 246: Train Loss: 0.33182528614997864, Validation Loss: 0.5161541104316711\n",
      "Epoch 247: Train Loss: 0.33444710572560626, Validation Loss: 0.5167371034622192\n",
      "Epoch 248: Train Loss: 0.33034660418828327, Validation Loss: 0.5165034532546997\n",
      "Epoch 249: Train Loss: 0.33252806464831036, Validation Loss: 0.51703280210495\n",
      "Epoch 250: Train Loss: 0.3288217882315318, Validation Loss: 0.5299230813980103\n",
      "Epoch 251: Train Loss: 0.3247886498769124, Validation Loss: 0.5361655950546265\n",
      "Epoch 252: Train Loss: 0.32694785793622333, Validation Loss: 0.5343906879425049\n",
      "Epoch 253: Train Loss: 0.3308490018049876, Validation Loss: 0.5427243709564209\n",
      "Epoch 254: Train Loss: 0.32852434118588764, Validation Loss: 0.55082768201828\n",
      "Epoch 255: Train Loss: 0.3307725091775258, Validation Loss: 0.5535130500793457\n",
      "Epoch 256: Train Loss: 0.33413941661516827, Validation Loss: 0.5547028183937073\n",
      "Epoch 257: Train Loss: 0.32580336928367615, Validation Loss: 0.5519708395004272\n",
      "Epoch 258: Train Loss: 0.3306329647699992, Validation Loss: 0.5523810386657715\n",
      "Epoch 259: Train Loss: 0.32861385742823285, Validation Loss: 0.5515968799591064\n",
      "Epoch 260: Train Loss: 0.32491829991340637, Validation Loss: 0.5403509736061096\n",
      "Epoch 261: Train Loss: 0.332449992497762, Validation Loss: 0.5355615019798279\n",
      "Epoch 262: Train Loss: 0.3256991505622864, Validation Loss: 0.5216345191001892\n",
      "Epoch 263: Train Loss: 0.3254573444525401, Validation Loss: 0.5122683048248291\n",
      "Epoch 264: Train Loss: 0.3346208830674489, Validation Loss: 0.507761538028717\n",
      "Epoch 265: Train Loss: 0.33487872282663983, Validation Loss: 0.5149846076965332\n",
      "Epoch 266: Train Loss: 0.336442232131958, Validation Loss: 0.5166266560554504\n",
      "Epoch 267: Train Loss: 0.32107354203859967, Validation Loss: 0.5203715562820435\n",
      "Epoch 268: Train Loss: 0.33075301845868427, Validation Loss: 0.5208516716957092\n",
      "Epoch 269: Train Loss: 0.3263207773367564, Validation Loss: 0.522140622138977\n",
      "Epoch 270: Train Loss: 0.3289824426174164, Validation Loss: 0.5483322143554688\n",
      "Epoch 271: Train Loss: 0.330255667368571, Validation Loss: 0.5633903741836548\n",
      "Epoch 272: Train Loss: 0.33261316021283466, Validation Loss: 0.5606920719146729\n",
      "Epoch 273: Train Loss: 0.3331410884857178, Validation Loss: 0.5689642429351807\n",
      "Epoch 274: Train Loss: 0.35645347833633423, Validation Loss: 0.5623739361763\n",
      "Epoch 275: Train Loss: 0.33205782373746234, Validation Loss: 0.5312524437904358\n",
      "Epoch 276: Train Loss: 0.3344479203224182, Validation Loss: 0.5152210593223572\n",
      "Epoch 277: Train Loss: 0.32973532875378925, Validation Loss: 0.4989456534385681\n",
      "Epoch 278: Train Loss: 0.35057157278060913, Validation Loss: 0.49100154638290405\n",
      "Epoch 279: Train Loss: 0.3304585615793864, Validation Loss: 0.48996445536613464\n",
      "Epoch 280: Train Loss: 0.3309655984242757, Validation Loss: 0.5144758820533752\n",
      "Epoch 281: Train Loss: 0.3286117414633433, Validation Loss: 0.5258622169494629\n",
      "Epoch 282: Train Loss: 0.345703383286794, Validation Loss: 0.5335003137588501\n",
      "Epoch 283: Train Loss: 0.33705485860506695, Validation Loss: 0.5375449061393738\n",
      "Epoch 284: Train Loss: 0.32269882162412006, Validation Loss: 0.5413044691085815\n",
      "Epoch 285: Train Loss: 0.3313623567422231, Validation Loss: 0.5340166687965393\n",
      "Epoch 286: Train Loss: 0.3391733964284261, Validation Loss: 0.5300309658050537\n",
      "Epoch 287: Train Loss: 0.3465009729067485, Validation Loss: 0.5305725336074829\n",
      "Epoch 288: Train Loss: 0.3218375543753306, Validation Loss: 0.5320340991020203\n",
      "Epoch 289: Train Loss: 0.32806851466496784, Validation Loss: 0.5322200655937195\n",
      "Epoch 290: Train Loss: 0.3225075801213582, Validation Loss: 0.5210952758789062\n",
      "Epoch 291: Train Loss: 0.32680946588516235, Validation Loss: 0.5162877440452576\n",
      "Epoch 292: Train Loss: 0.3322148521741231, Validation Loss: 0.5112362504005432\n",
      "Epoch 293: Train Loss: 0.3436153431733449, Validation Loss: 0.5113456845283508\n",
      "Epoch 294: Train Loss: 0.32464249928792316, Validation Loss: 0.518375813961029\n",
      "Epoch 295: Train Loss: 0.3353383441766103, Validation Loss: 0.525312066078186\n",
      "Epoch 296: Train Loss: 0.3233336806297302, Validation Loss: 0.5298832654953003\n",
      "Epoch 297: Train Loss: 0.3243248263994853, Validation Loss: 0.5316546559333801\n",
      "Epoch 298: Train Loss: 0.3284576137860616, Validation Loss: 0.5326842665672302\n",
      "Epoch 299: Train Loss: 0.3240475654602051, Validation Loss: 0.5321544408798218\n",
      "Epoch 300: Train Loss: 0.3246939579645793, Validation Loss: 0.5400676131248474\n",
      "Epoch 301: Train Loss: 0.3225007851918538, Validation Loss: 0.5447162985801697\n",
      "Epoch 302: Train Loss: 0.3338561952114105, Validation Loss: 0.5604066252708435\n",
      "Epoch 303: Train Loss: 0.3283432225386302, Validation Loss: 0.5811952948570251\n",
      "Early stopping at epoch 304\n",
      "Accuracy: 0.7333333333333333,Precision: 0.8043478260869565, Recall: 0.6166666666666667, F1-score: 0.6981132075471698, AUC: 0.7333333333333334\n",
      "Confusion Matrix:\n",
      "[[51  9]\n",
      " [23 37]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "\n",
    "model_save_dir = r'C:\\Users\\User\\Documents\\Lie detect data\\Model'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Define a function to load and pad data\n",
    "def load_data(data_dir, max_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    file_list = os.listdir(data_dir)\n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        label = 1 if 'truth' in file else 0\n",
    "        if data.shape[1] > max_length:\n",
    "            processed_data = data[:, :max_length]  # Cut data if it exceeds max_length\n",
    "        else:\n",
    "            processed_data = np.zeros((data.shape[0], max_length))\n",
    "            processed_data[:, :data.shape[1]] = data  # Pad data if it is shorter than max_length\n",
    "        X.append(processed_data)\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load dataset and pad the data\n",
    "data_dir = \"C:\\\\Users\\\\User\\\\Documents\\\\Lie detect data\\\\56M_DWTEEGData\"\n",
    "max_length = 500 # Define maximum length for padding\n",
    "X, y = load_data(data_dir, max_length)\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure the data is reshaped to [1, Chans, Samples]\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32).unsqueeze(0), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# Define EEGNet model\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, nb_classes, Chans=65, Samples=500, dropoutRate=0.5, \n",
    "                 kernLength=125, F1=8, D=2, F2=None, norm_rate=0.25, dropoutType='Dropout'):\n",
    "        super(EEGNet, self).__init__()\n",
    "        if F2 is None:\n",
    "            F2 = F1 * D\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, F1, (1, kernLength), padding='same', bias=False)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(F1)\n",
    "        \n",
    "        self.depthwiseConv = nn.Conv2d(F1, F1 * D, (Chans, 1), groups=F1, bias=False)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(F1 * D)\n",
    "        \n",
    "        self.averagePool1 = nn.AvgPool2d((1, 4))\n",
    "        \n",
    "        if dropoutType == 'SpatialDropout2D':\n",
    "            self.dropout1 = nn.Dropout2d(dropoutRate)\n",
    "        elif dropoutType == 'Dropout':\n",
    "            self.dropout1 = nn.Dropout(dropoutRate)\n",
    "        else:\n",
    "            raise ValueError('dropoutType must be one of SpatialDropout2D or Dropout')\n",
    "        \n",
    "        self.separableConv1 = nn.Conv2d(F1 * D, F2, (1, 16), padding='same', bias=False)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(F2)\n",
    "        \n",
    "        self.averagePool2 = nn.AvgPool2d((1, 8))\n",
    "        \n",
    "        if dropoutType == 'SpatialDropout2D':\n",
    "            self.dropout2 = nn.Dropout2d(dropoutRate)\n",
    "        elif dropoutType == 'Dropout':\n",
    "            self.dropout2 = nn.Dropout(dropoutRate)\n",
    "        \n",
    "        # Add more depth by adding another separable convolutional block\n",
    "        self.separableConv2 = nn.Conv2d(F2, F2, (1, 16), padding='same', bias=False)\n",
    "        self.batchnorm4 = nn.BatchNorm2d(F2)\n",
    "        self.averagePool3 = nn.AvgPool2d((1, 8))\n",
    "        \n",
    "        if dropoutType == 'SpatialDropout2D':\n",
    "            self.dropout3 = nn.Dropout2d(dropoutRate)\n",
    "        elif dropoutType == 'Dropout':\n",
    "            self.dropout3 = nn.Dropout(dropoutRate)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Linear(F2 * (Samples // 256), nb_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.depthwiseConv(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.averagePool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.separableConv1(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.averagePool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Pass through the additional depth layers\n",
    "        x = self.separableConv2(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.averagePool3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_and_evaluate(train_loader, val_loader, y_train):\n",
    "    nb_classes = 2\n",
    "    model = EEGNet(nb_classes=nb_classes).to(device)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-8)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "    num_epochs = 500\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 100\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "            fold_model_path = os.path.join(model_save_dir, f'fold3_model_fold_{fold_idx}.pth')\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': best_val_loss,\n",
    "            }, fold_model_path)\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_idx = 0\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "for train_index, val_index in kf.split(X, y):\n",
    "    print(f'Fold {fold_idx + 1}')\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Normalize data using scaler fitted on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_val = scaler.transform(X_val.reshape(X_val.shape[0], -1))\n",
    "    X_train = X_train.reshape(-1, 65, max_length)\n",
    "    X_val = X_val.reshape(-1, 65, max_length)\n",
    "\n",
    "    # Save the scaler to a file\n",
    "    with open(r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\simpleEEGNet_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = EEGDataset(X_train, y_train)\n",
    "    val_dataset = EEGDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = train_and_evaluate(train_loader, val_loader, y_train)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    fold_idx += 1\n",
    "\n",
    "\n",
    "# Calculate additional metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "auc = roc_auc_score(all_labels, all_predictions)\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy},Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66654d63-c450-4d9f-a3e3-63701fd1d41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934cb89-5ea1-4eb0-a152-00c71e49e06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
