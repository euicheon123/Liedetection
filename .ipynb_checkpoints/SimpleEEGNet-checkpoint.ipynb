{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "656cdbc2-da54-45ba-97fc-195b59bd7820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 0: Train Loss: 0.6728512446085612, Validation Loss: 0.6797606945037842\n",
      "Epoch 1: Train Loss: 0.6055499315261841, Validation Loss: 0.6625386476516724\n",
      "Epoch 2: Train Loss: 0.5764322082201639, Validation Loss: 0.643312394618988\n",
      "Epoch 3: Train Loss: 0.5431099732716879, Validation Loss: 0.6237540245056152\n",
      "Epoch 4: Train Loss: 0.5385176340738932, Validation Loss: 0.6039524674415588\n",
      "Epoch 5: Train Loss: 0.5193821390469869, Validation Loss: 0.5849659442901611\n",
      "Epoch 6: Train Loss: 0.49567264318466187, Validation Loss: 0.5678061246871948\n",
      "Epoch 7: Train Loss: 0.5001585682233175, Validation Loss: 0.5525394678115845\n",
      "Epoch 8: Train Loss: 0.49632829427719116, Validation Loss: 0.5389334559440613\n",
      "Epoch 9: Train Loss: 0.4819478591283162, Validation Loss: 0.5286773443222046\n",
      "Epoch 10: Train Loss: 0.48006851474444073, Validation Loss: 0.5099048018455505\n",
      "Epoch 11: Train Loss: 0.4680226643880208, Validation Loss: 0.496318519115448\n",
      "Epoch 12: Train Loss: 0.47031835714975995, Validation Loss: 0.4854574501514435\n",
      "Epoch 13: Train Loss: 0.4297721286614736, Validation Loss: 0.4757198691368103\n",
      "Epoch 14: Train Loss: 0.4048228959242503, Validation Loss: 0.46742716431617737\n",
      "Epoch 15: Train Loss: 0.4015778601169586, Validation Loss: 0.4600060284137726\n",
      "Epoch 16: Train Loss: 0.42057528098424274, Validation Loss: 0.455873966217041\n",
      "Epoch 17: Train Loss: 0.3795862893263499, Validation Loss: 0.45264872908592224\n",
      "Epoch 18: Train Loss: 0.37383269270261127, Validation Loss: 0.4505689740180969\n",
      "Epoch 19: Train Loss: 0.3742383619149526, Validation Loss: 0.4493306577205658\n",
      "Epoch 20: Train Loss: 0.3810616334279378, Validation Loss: 0.44240081310272217\n",
      "Epoch 21: Train Loss: 0.36229703823725384, Validation Loss: 0.4406622648239136\n",
      "Epoch 22: Train Loss: 0.3383539815743764, Validation Loss: 0.44114524126052856\n",
      "Epoch 23: Train Loss: 0.31267473101615906, Validation Loss: 0.4497753381729126\n",
      "Epoch 24: Train Loss: 0.30909793575604755, Validation Loss: 0.4581405520439148\n",
      "Epoch 25: Train Loss: 0.29803499579429626, Validation Loss: 0.4609624147415161\n",
      "Epoch 26: Train Loss: 0.30352382858594257, Validation Loss: 0.4618823826313019\n",
      "Epoch 27: Train Loss: 0.2944636543591817, Validation Loss: 0.4599175453186035\n",
      "Epoch 28: Train Loss: 0.2910470167795817, Validation Loss: 0.45776674151420593\n",
      "Epoch 29: Train Loss: 0.29935797055562335, Validation Loss: 0.4557037353515625\n",
      "Epoch 30: Train Loss: 0.28050480286280316, Validation Loss: 0.45281434059143066\n",
      "Epoch 31: Train Loss: 0.2579834113518397, Validation Loss: 0.4595491886138916\n",
      "Epoch 32: Train Loss: 0.26869335273901623, Validation Loss: 0.4724175035953522\n",
      "Epoch 33: Train Loss: 0.269096314907074, Validation Loss: 0.4842950403690338\n",
      "Epoch 34: Train Loss: 0.26112601161003113, Validation Loss: 0.48108598589897156\n",
      "Epoch 35: Train Loss: 0.2366799314816793, Validation Loss: 0.4750545620918274\n",
      "Epoch 36: Train Loss: 0.22931542495886484, Validation Loss: 0.47209134697914124\n",
      "Epoch 37: Train Loss: 0.22803961733977, Validation Loss: 0.4734377861022949\n",
      "Epoch 38: Train Loss: 0.23598618805408478, Validation Loss: 0.47503092885017395\n",
      "Epoch 39: Train Loss: 0.26080281535784405, Validation Loss: 0.47359567880630493\n",
      "Epoch 40: Train Loss: 0.22973004480202994, Validation Loss: 0.4908900856971741\n",
      "Epoch 41: Train Loss: 0.22411642968654633, Validation Loss: 0.4925324022769928\n",
      "Epoch 42: Train Loss: 0.22736517091592154, Validation Loss: 0.46525368094444275\n",
      "Epoch 43: Train Loss: 0.21847867965698242, Validation Loss: 0.46587592363357544\n",
      "Epoch 44: Train Loss: 0.2333220640818278, Validation Loss: 0.4796634018421173\n",
      "Epoch 45: Train Loss: 0.20233807464440665, Validation Loss: 0.4773407280445099\n",
      "Epoch 46: Train Loss: 0.20944814880688986, Validation Loss: 0.47563764452934265\n",
      "Epoch 47: Train Loss: 0.20210368931293488, Validation Loss: 0.4766968786716461\n",
      "Epoch 48: Train Loss: 0.19095909098784128, Validation Loss: 0.47242921590805054\n",
      "Epoch 49: Train Loss: 0.18378561238447824, Validation Loss: 0.47008857131004333\n",
      "Epoch 50: Train Loss: 0.20973411202430725, Validation Loss: 0.473355770111084\n",
      "Epoch 51: Train Loss: 0.18383113046487173, Validation Loss: 0.5153461694717407\n",
      "Epoch 52: Train Loss: 0.17301986614863077, Validation Loss: 0.5210666656494141\n",
      "Epoch 53: Train Loss: 0.17948144674301147, Validation Loss: 0.5167768001556396\n",
      "Epoch 54: Train Loss: 0.16357793907324472, Validation Loss: 0.510560929775238\n",
      "Epoch 55: Train Loss: 0.18820291260878244, Validation Loss: 0.5034435391426086\n",
      "Epoch 56: Train Loss: 0.1601606160402298, Validation Loss: 0.5023174285888672\n",
      "Epoch 57: Train Loss: 0.18031749626000723, Validation Loss: 0.5018208026885986\n",
      "Epoch 58: Train Loss: 0.1511536737283071, Validation Loss: 0.5026308298110962\n",
      "Epoch 59: Train Loss: 0.16521031161149344, Validation Loss: 0.5023972392082214\n",
      "Epoch 60: Train Loss: 0.16090847551822662, Validation Loss: 0.5270783305168152\n",
      "Early stopping at epoch 61\n",
      "Fold 2\n",
      "Epoch 0: Train Loss: 0.7470125953356425, Validation Loss: 0.6901670098304749\n",
      "Epoch 1: Train Loss: 0.6352793176968893, Validation Loss: 0.6786351799964905\n",
      "Epoch 2: Train Loss: 0.6074485778808594, Validation Loss: 0.668980598449707\n",
      "Epoch 3: Train Loss: 0.5510974923769633, Validation Loss: 0.6552607417106628\n",
      "Epoch 4: Train Loss: 0.5342436830202738, Validation Loss: 0.6412838697433472\n",
      "Epoch 5: Train Loss: 0.5136440992355347, Validation Loss: 0.6258048415184021\n",
      "Epoch 6: Train Loss: 0.4939589699109395, Validation Loss: 0.6127337217330933\n",
      "Epoch 7: Train Loss: 0.49683605631192523, Validation Loss: 0.6009674668312073\n",
      "Epoch 8: Train Loss: 0.4881315529346466, Validation Loss: 0.5909883975982666\n",
      "Epoch 9: Train Loss: 0.4823063711325328, Validation Loss: 0.5836327075958252\n",
      "Epoch 10: Train Loss: 0.4929000437259674, Validation Loss: 0.5580304265022278\n",
      "Epoch 11: Train Loss: 0.44532260298728943, Validation Loss: 0.5393150448799133\n",
      "Epoch 12: Train Loss: 0.44291648268699646, Validation Loss: 0.5172335505485535\n",
      "Epoch 13: Train Loss: 0.40504347284634906, Validation Loss: 0.48537832498550415\n",
      "Epoch 14: Train Loss: 0.3873627483844757, Validation Loss: 0.4648782014846802\n",
      "Epoch 15: Train Loss: 0.37675060828526813, Validation Loss: 0.4580775201320648\n",
      "Epoch 16: Train Loss: 0.3677557706832886, Validation Loss: 0.4551544189453125\n",
      "Epoch 17: Train Loss: 0.3618691563606262, Validation Loss: 0.45636042952537537\n",
      "Epoch 18: Train Loss: 0.3618051012357076, Validation Loss: 0.4553089439868927\n",
      "Epoch 19: Train Loss: 0.3554811378320058, Validation Loss: 0.4541526138782501\n",
      "Epoch 20: Train Loss: 0.36508045593897503, Validation Loss: 0.4555775225162506\n",
      "Epoch 21: Train Loss: 0.35991249481836957, Validation Loss: 0.4506949186325073\n",
      "Epoch 22: Train Loss: 0.3102905750274658, Validation Loss: 0.4342416226863861\n",
      "Epoch 23: Train Loss: 0.30584216117858887, Validation Loss: 0.4262237846851349\n",
      "Epoch 24: Train Loss: 0.29676931103070575, Validation Loss: 0.42513859272003174\n",
      "Epoch 25: Train Loss: 0.2817215671141942, Validation Loss: 0.4319516718387604\n",
      "Epoch 26: Train Loss: 0.26591625809669495, Validation Loss: 0.42799630761146545\n",
      "Epoch 27: Train Loss: 0.276157150665919, Validation Loss: 0.4239354431629181\n",
      "Epoch 28: Train Loss: 0.26685333251953125, Validation Loss: 0.4234035611152649\n",
      "Epoch 29: Train Loss: 0.2816389898459117, Validation Loss: 0.42227014899253845\n",
      "Epoch 30: Train Loss: 0.26419107615947723, Validation Loss: 0.42712196707725525\n",
      "Epoch 31: Train Loss: 0.24885159730911255, Validation Loss: 0.42010438442230225\n",
      "Epoch 32: Train Loss: 0.24967393775780997, Validation Loss: 0.4100315272808075\n",
      "Epoch 33: Train Loss: 0.23135443528493246, Validation Loss: 0.409506231546402\n",
      "Epoch 34: Train Loss: 0.21560976902643839, Validation Loss: 0.42509108781814575\n",
      "Epoch 35: Train Loss: 0.21128510932127634, Validation Loss: 0.4240798056125641\n",
      "Epoch 36: Train Loss: 0.21952536205450693, Validation Loss: 0.4170062243938446\n",
      "Epoch 37: Train Loss: 0.2264877806107203, Validation Loss: 0.41653263568878174\n",
      "Epoch 38: Train Loss: 0.19632562498251596, Validation Loss: 0.41419723629951477\n",
      "Epoch 39: Train Loss: 0.20031889776388803, Validation Loss: 0.41318657994270325\n",
      "Epoch 40: Train Loss: 0.19380618631839752, Validation Loss: 0.3792797327041626\n",
      "Epoch 41: Train Loss: 0.2237780640522639, Validation Loss: 0.3959210216999054\n",
      "Epoch 42: Train Loss: 0.18643799920876822, Validation Loss: 0.4411070644855499\n",
      "Epoch 43: Train Loss: 0.19338328639666238, Validation Loss: 0.4199455678462982\n",
      "Epoch 44: Train Loss: 0.20117479066054025, Validation Loss: 0.3873004913330078\n",
      "Epoch 45: Train Loss: 0.15943258007367453, Validation Loss: 0.3913423418998718\n",
      "Epoch 46: Train Loss: 0.15216238796710968, Validation Loss: 0.3985801935195923\n",
      "Epoch 47: Train Loss: 0.15356830259164175, Validation Loss: 0.407771497964859\n",
      "Epoch 48: Train Loss: 0.2137683928012848, Validation Loss: 0.41117793321609497\n",
      "Epoch 49: Train Loss: 0.15960009396076202, Validation Loss: 0.41079622507095337\n",
      "Epoch 50: Train Loss: 0.15586307644844055, Validation Loss: 0.4055947959423065\n",
      "Epoch 51: Train Loss: 0.15615171690781912, Validation Loss: 0.37909868359565735\n",
      "Epoch 52: Train Loss: 0.14518505334854126, Validation Loss: 0.3834242820739746\n",
      "Epoch 53: Train Loss: 0.1724946747223536, Validation Loss: 0.43999043107032776\n",
      "Epoch 54: Train Loss: 0.14601371685663858, Validation Loss: 0.44343632459640503\n",
      "Epoch 55: Train Loss: 0.1415924852093061, Validation Loss: 0.4007156193256378\n",
      "Epoch 56: Train Loss: 0.13316131134827933, Validation Loss: 0.40363234281539917\n",
      "Epoch 57: Train Loss: 0.1203545331954956, Validation Loss: 0.4135192930698395\n",
      "Epoch 58: Train Loss: 0.1274559199810028, Validation Loss: 0.42048922181129456\n",
      "Epoch 59: Train Loss: 0.11659483363231023, Validation Loss: 0.42263323068618774\n",
      "Epoch 60: Train Loss: 0.13249566157658896, Validation Loss: 0.49978306889533997\n",
      "Epoch 61: Train Loss: 0.15679691980282465, Validation Loss: 0.44225528836250305\n",
      "Epoch 62: Train Loss: 0.12629898389180502, Validation Loss: 0.38613513112068176\n",
      "Epoch 63: Train Loss: 0.11746057122945786, Validation Loss: 0.39933738112449646\n",
      "Epoch 64: Train Loss: 0.10191189249356587, Validation Loss: 0.42298585176467896\n",
      "Epoch 65: Train Loss: 0.11928785343964894, Validation Loss: 0.4461608827114105\n",
      "Epoch 66: Train Loss: 0.11473789562781651, Validation Loss: 0.4662511944770813\n",
      "Epoch 67: Train Loss: 0.12085895488659541, Validation Loss: 0.4649631977081299\n",
      "Epoch 68: Train Loss: 0.11147267868121465, Validation Loss: 0.4625352621078491\n",
      "Epoch 69: Train Loss: 0.11395643651485443, Validation Loss: 0.4626769721508026\n",
      "Epoch 70: Train Loss: 0.10660961270332336, Validation Loss: 0.421726793050766\n",
      "Epoch 71: Train Loss: 0.12251145144303639, Validation Loss: 0.40847572684288025\n",
      "Epoch 72: Train Loss: 0.1313497150937716, Validation Loss: 0.4785110056400299\n",
      "Epoch 73: Train Loss: 0.10342122117678325, Validation Loss: 0.4273909330368042\n",
      "Epoch 74: Train Loss: 0.11073481291532516, Validation Loss: 0.4321702718734741\n",
      "Epoch 75: Train Loss: 0.09380679080883662, Validation Loss: 0.4548022449016571\n",
      "Epoch 76: Train Loss: 0.09371747076511383, Validation Loss: 0.46293702721595764\n",
      "Epoch 77: Train Loss: 0.08744923522075017, Validation Loss: 0.4642101526260376\n",
      "Epoch 78: Train Loss: 0.08551459014415741, Validation Loss: 0.465557336807251\n",
      "Epoch 79: Train Loss: 0.07892567416032155, Validation Loss: 0.46611812710762024\n",
      "Epoch 80: Train Loss: 0.08162588129440944, Validation Loss: 0.49341943860054016\n",
      "Epoch 81: Train Loss: 0.10042629142602284, Validation Loss: 0.47862401604652405\n",
      "Epoch 82: Train Loss: 0.08799350758393605, Validation Loss: 0.4784930944442749\n",
      "Epoch 83: Train Loss: 0.08311476310094197, Validation Loss: 0.463837206363678\n",
      "Epoch 84: Train Loss: 0.09999740123748779, Validation Loss: 0.46244603395462036\n",
      "Epoch 85: Train Loss: 0.07503590236107509, Validation Loss: 0.4860171377658844\n",
      "Epoch 86: Train Loss: 0.11281886448462804, Validation Loss: 0.49718886613845825\n",
      "Epoch 87: Train Loss: 0.09378080566724141, Validation Loss: 0.4985032379627228\n",
      "Epoch 88: Train Loss: 0.07098823909958203, Validation Loss: 0.49837547540664673\n",
      "Epoch 89: Train Loss: 0.0791843297580878, Validation Loss: 0.497446209192276\n",
      "Epoch 90: Train Loss: 0.07164798428614934, Validation Loss: 0.4424227774143219\n",
      "Early stopping at epoch 91\n",
      "Fold 3\n",
      "Epoch 0: Train Loss: 0.7435210347175598, Validation Loss: 0.688220202922821\n",
      "Epoch 1: Train Loss: 0.6301254232724508, Validation Loss: 0.6830360293388367\n",
      "Epoch 2: Train Loss: 0.5831573009490967, Validation Loss: 0.6779305338859558\n",
      "Epoch 3: Train Loss: 0.5485806663831075, Validation Loss: 0.6703627109527588\n",
      "Epoch 4: Train Loss: 0.5410495599110922, Validation Loss: 0.6612920165061951\n",
      "Epoch 5: Train Loss: 0.5186078945795695, Validation Loss: 0.6511105298995972\n",
      "Epoch 6: Train Loss: 0.4977823595205943, Validation Loss: 0.6402624249458313\n",
      "Epoch 7: Train Loss: 0.5208184619744619, Validation Loss: 0.6299282908439636\n",
      "Epoch 8: Train Loss: 0.47875486811002094, Validation Loss: 0.6215689778327942\n",
      "Epoch 9: Train Loss: 0.4851645628611247, Validation Loss: 0.6148819327354431\n",
      "Epoch 10: Train Loss: 0.49382027983665466, Validation Loss: 0.5995844602584839\n",
      "Epoch 11: Train Loss: 0.4565050005912781, Validation Loss: 0.5835970044136047\n",
      "Epoch 12: Train Loss: 0.4489812950293223, Validation Loss: 0.5654611587524414\n",
      "Epoch 13: Train Loss: 0.42174452543258667, Validation Loss: 0.5499199032783508\n",
      "Epoch 14: Train Loss: 0.40410521626472473, Validation Loss: 0.535820484161377\n",
      "Epoch 15: Train Loss: 0.3972612222035726, Validation Loss: 0.5249835848808289\n",
      "Epoch 16: Train Loss: 0.3866666754086812, Validation Loss: 0.5162984132766724\n",
      "Epoch 17: Train Loss: 0.3880301018555959, Validation Loss: 0.5103375315666199\n",
      "Epoch 18: Train Loss: 0.38275932272275287, Validation Loss: 0.5071872472763062\n",
      "Epoch 19: Train Loss: 0.3697372575600942, Validation Loss: 0.5043731331825256\n",
      "Epoch 20: Train Loss: 0.34430911143620807, Validation Loss: 0.49625837802886963\n",
      "Epoch 21: Train Loss: 0.34628363450368244, Validation Loss: 0.48936769366264343\n",
      "Epoch 22: Train Loss: 0.3153007924556732, Validation Loss: 0.48399245738983154\n",
      "Epoch 23: Train Loss: 0.31078394254048664, Validation Loss: 0.48070546984672546\n",
      "Epoch 24: Train Loss: 0.3047313193480174, Validation Loss: 0.4780319929122925\n",
      "Epoch 25: Train Loss: 0.2676527152458827, Validation Loss: 0.47660142183303833\n",
      "Epoch 26: Train Loss: 0.2930314739545186, Validation Loss: 0.47458896040916443\n",
      "Epoch 27: Train Loss: 0.2759254475434621, Validation Loss: 0.473405659198761\n",
      "Epoch 28: Train Loss: 0.2757515609264374, Validation Loss: 0.47223132848739624\n",
      "Epoch 29: Train Loss: 0.26742610335350037, Validation Loss: 0.4715597331523895\n",
      "Epoch 30: Train Loss: 0.25756725668907166, Validation Loss: 0.47423139214515686\n",
      "Epoch 31: Train Loss: 0.2502703567345937, Validation Loss: 0.47762230038642883\n",
      "Epoch 32: Train Loss: 0.25398992498715717, Validation Loss: 0.47192275524139404\n",
      "Epoch 33: Train Loss: 0.23234560588995615, Validation Loss: 0.47069063782691956\n",
      "Epoch 34: Train Loss: 0.22910205523173013, Validation Loss: 0.47128602862358093\n",
      "Epoch 35: Train Loss: 0.22573458154996237, Validation Loss: 0.47194239497184753\n",
      "Epoch 36: Train Loss: 0.21316962937513986, Validation Loss: 0.4713486135005951\n",
      "Epoch 37: Train Loss: 0.20684188604354858, Validation Loss: 0.4709818363189697\n",
      "Epoch 38: Train Loss: 0.22079106668631235, Validation Loss: 0.46994519233703613\n",
      "Epoch 39: Train Loss: 0.2136913239955902, Validation Loss: 0.46941471099853516\n",
      "Epoch 40: Train Loss: 0.20485840241114298, Validation Loss: 0.45823603868484497\n",
      "Epoch 41: Train Loss: 0.2033100575208664, Validation Loss: 0.4575425684452057\n",
      "Epoch 42: Train Loss: 0.20552478730678558, Validation Loss: 0.46039652824401855\n",
      "Epoch 43: Train Loss: 0.1883570154507955, Validation Loss: 0.46621665358543396\n",
      "Epoch 44: Train Loss: 0.18342981239159903, Validation Loss: 0.46449291706085205\n",
      "Epoch 45: Train Loss: 0.20601884524027506, Validation Loss: 0.46539607644081116\n",
      "Epoch 46: Train Loss: 0.18064183990160623, Validation Loss: 0.4671423137187958\n",
      "Epoch 47: Train Loss: 0.1994120975335439, Validation Loss: 0.46946075558662415\n",
      "Epoch 48: Train Loss: 0.19770640631516775, Validation Loss: 0.4699883460998535\n",
      "Epoch 49: Train Loss: 0.19301827996969223, Validation Loss: 0.4686145782470703\n",
      "Epoch 50: Train Loss: 0.1761868198712667, Validation Loss: 0.45914340019226074\n",
      "Epoch 51: Train Loss: 0.18521526455879211, Validation Loss: 0.46285101771354675\n",
      "Epoch 52: Train Loss: 0.16291729112466177, Validation Loss: 0.46678003668785095\n",
      "Epoch 53: Train Loss: 0.17047935724258423, Validation Loss: 0.4639008045196533\n",
      "Epoch 54: Train Loss: 0.15419285744428635, Validation Loss: 0.46875062584877014\n",
      "Epoch 55: Train Loss: 0.15066187580426535, Validation Loss: 0.471274733543396\n",
      "Epoch 56: Train Loss: 0.15818354735771814, Validation Loss: 0.4741102457046509\n",
      "Epoch 57: Train Loss: 0.16993377606074014, Validation Loss: 0.4733867943286896\n",
      "Epoch 58: Train Loss: 0.15801163017749786, Validation Loss: 0.4731304943561554\n",
      "Epoch 59: Train Loss: 0.15087549885114035, Validation Loss: 0.4728246331214905\n",
      "Epoch 60: Train Loss: 0.1558998078107834, Validation Loss: 0.47083619236946106\n",
      "Epoch 61: Train Loss: 0.1592823714017868, Validation Loss: 0.49247026443481445\n",
      "Epoch 62: Train Loss: 0.14739711582660675, Validation Loss: 0.4987349510192871\n",
      "Epoch 63: Train Loss: 0.13187194367249808, Validation Loss: 0.49487411975860596\n",
      "Epoch 64: Train Loss: 0.1453351378440857, Validation Loss: 0.4806329905986786\n",
      "Epoch 65: Train Loss: 0.1402753492196401, Validation Loss: 0.47323212027549744\n",
      "Epoch 66: Train Loss: 0.1664756884177526, Validation Loss: 0.45993614196777344\n",
      "Epoch 67: Train Loss: 0.12481142083803813, Validation Loss: 0.4565616250038147\n",
      "Epoch 68: Train Loss: 0.1723120709260305, Validation Loss: 0.45533323287963867\n",
      "Epoch 69: Train Loss: 0.1671522930264473, Validation Loss: 0.4552014172077179\n",
      "Epoch 70: Train Loss: 0.13280136634906134, Validation Loss: 0.45312362909317017\n",
      "Epoch 71: Train Loss: 0.13984361539284387, Validation Loss: 0.4636341333389282\n",
      "Epoch 72: Train Loss: 0.11827792227268219, Validation Loss: 0.4644346833229065\n",
      "Epoch 73: Train Loss: 0.11595844229062398, Validation Loss: 0.46723073720932007\n",
      "Epoch 74: Train Loss: 0.15968076884746552, Validation Loss: 0.47033950686454773\n",
      "Epoch 75: Train Loss: 0.11470655103524525, Validation Loss: 0.46549615263938904\n",
      "Epoch 76: Train Loss: 0.09603242824474971, Validation Loss: 0.4586574137210846\n",
      "Epoch 77: Train Loss: 0.1434629112482071, Validation Loss: 0.4554796516895294\n",
      "Epoch 78: Train Loss: 0.10413411508003871, Validation Loss: 0.4551433324813843\n",
      "Epoch 79: Train Loss: 0.1054560715953509, Validation Loss: 0.4547407925128937\n",
      "Epoch 80: Train Loss: 0.12272831797599792, Validation Loss: 0.466694712638855\n",
      "Epoch 81: Train Loss: 0.10857358326514562, Validation Loss: 0.49200618267059326\n",
      "Epoch 82: Train Loss: 0.16747281948725382, Validation Loss: 0.48583686351776123\n",
      "Epoch 83: Train Loss: 0.10245853662490845, Validation Loss: 0.48054906725883484\n",
      "Epoch 84: Train Loss: 0.13426915556192398, Validation Loss: 0.47128885984420776\n",
      "Epoch 85: Train Loss: 0.1173853725194931, Validation Loss: 0.46439501643180847\n",
      "Epoch 86: Train Loss: 0.09755177050828934, Validation Loss: 0.46289536356925964\n",
      "Epoch 87: Train Loss: 0.09078807880481084, Validation Loss: 0.46193450689315796\n",
      "Epoch 88: Train Loss: 0.10323425630728404, Validation Loss: 0.4632464051246643\n",
      "Epoch 89: Train Loss: 0.11955551554759343, Validation Loss: 0.4639383554458618\n",
      "Epoch 90: Train Loss: 0.1015524963537852, Validation Loss: 0.48993417620658875\n",
      "Epoch 91: Train Loss: 0.13718533515930176, Validation Loss: 0.49300524592399597\n",
      "Epoch 92: Train Loss: 0.10747982064882915, Validation Loss: 0.4850335419178009\n",
      "Epoch 93: Train Loss: 0.08522386103868484, Validation Loss: 0.47175729274749756\n",
      "Epoch 94: Train Loss: 0.09259771307309468, Validation Loss: 0.4637388288974762\n",
      "Epoch 95: Train Loss: 0.10218677669763565, Validation Loss: 0.4677075743675232\n",
      "Epoch 96: Train Loss: 0.09351737797260284, Validation Loss: 0.4681496322154999\n",
      "Epoch 97: Train Loss: 0.09364650895198186, Validation Loss: 0.46705862879753113\n",
      "Epoch 98: Train Loss: 0.122557799021403, Validation Loss: 0.46943172812461853\n",
      "Epoch 99: Train Loss: 0.08138453960418701, Validation Loss: 0.4707035422325134\n",
      "Epoch 100: Train Loss: 0.08267082025607426, Validation Loss: 0.48592105507850647\n",
      "Epoch 101: Train Loss: 0.10512719551722209, Validation Loss: 0.4983251690864563\n",
      "Epoch 102: Train Loss: 0.08226971824963887, Validation Loss: 0.5211886167526245\n",
      "Epoch 103: Train Loss: 0.09136319657166798, Validation Loss: 0.5203696489334106\n",
      "Epoch 104: Train Loss: 0.07787597303589185, Validation Loss: 0.504282534122467\n",
      "Epoch 105: Train Loss: 0.08228922386964162, Validation Loss: 0.48471158742904663\n",
      "Epoch 106: Train Loss: 0.08586370199918747, Validation Loss: 0.4774666428565979\n",
      "Epoch 107: Train Loss: 0.0800209790468216, Validation Loss: 0.4747001826763153\n",
      "Epoch 108: Train Loss: 0.07108233496546745, Validation Loss: 0.47476527094841003\n",
      "Epoch 109: Train Loss: 0.06794528663158417, Validation Loss: 0.47429147362709045\n",
      "Early stopping at epoch 110\n",
      "Fold 4\n",
      "Epoch 0: Train Loss: 0.7166911363601685, Validation Loss: 0.7066628336906433\n",
      "Epoch 1: Train Loss: 0.6366023421287537, Validation Loss: 0.7048442959785461\n",
      "Epoch 2: Train Loss: 0.5721755623817444, Validation Loss: 0.7023708820343018\n",
      "Epoch 3: Train Loss: 0.5647891362508138, Validation Loss: 0.6990721225738525\n",
      "Epoch 4: Train Loss: 0.5533963342507681, Validation Loss: 0.6946954727172852\n",
      "Epoch 5: Train Loss: 0.5159067511558533, Validation Loss: 0.6896926760673523\n",
      "Epoch 6: Train Loss: 0.5070134103298187, Validation Loss: 0.6835107803344727\n",
      "Epoch 7: Train Loss: 0.4980532725652059, Validation Loss: 0.6781319975852966\n",
      "Epoch 8: Train Loss: 0.5118055840333303, Validation Loss: 0.6731176376342773\n",
      "Epoch 9: Train Loss: 0.49041128158569336, Validation Loss: 0.6684316396713257\n",
      "Epoch 10: Train Loss: 0.4880845646063487, Validation Loss: 0.6459348201751709\n",
      "Epoch 11: Train Loss: 0.47016820311546326, Validation Loss: 0.6261350512504578\n",
      "Epoch 12: Train Loss: 0.46411258975664776, Validation Loss: 0.6145408749580383\n",
      "Epoch 13: Train Loss: 0.42316068212191266, Validation Loss: 0.6052769422531128\n",
      "Epoch 14: Train Loss: 0.4028387864430745, Validation Loss: 0.5953681468963623\n",
      "Epoch 15: Train Loss: 0.3961421648661296, Validation Loss: 0.5874496698379517\n",
      "Epoch 16: Train Loss: 0.38870113094647724, Validation Loss: 0.5836565494537354\n",
      "Epoch 17: Train Loss: 0.378354549407959, Validation Loss: 0.5823697447776794\n",
      "Epoch 18: Train Loss: 0.3818480273087819, Validation Loss: 0.5798665881156921\n",
      "Epoch 19: Train Loss: 0.3697890142599742, Validation Loss: 0.5803384780883789\n",
      "Epoch 20: Train Loss: 0.3702166477839152, Validation Loss: 0.5666069984436035\n",
      "Epoch 21: Train Loss: 0.3633173207441966, Validation Loss: 0.5568587183952332\n",
      "Epoch 22: Train Loss: 0.34563719232877094, Validation Loss: 0.5534090399742126\n",
      "Epoch 23: Train Loss: 0.3212152421474457, Validation Loss: 0.5514686703681946\n",
      "Epoch 24: Train Loss: 0.3160730302333832, Validation Loss: 0.541876494884491\n",
      "Epoch 25: Train Loss: 0.30633501211802167, Validation Loss: 0.5364569425582886\n",
      "Epoch 26: Train Loss: 0.30038078625996906, Validation Loss: 0.5341110229492188\n",
      "Epoch 27: Train Loss: 0.3050430119037628, Validation Loss: 0.5301350355148315\n",
      "Epoch 28: Train Loss: 0.28531723221143085, Validation Loss: 0.5294592380523682\n",
      "Epoch 29: Train Loss: 0.2842302719751994, Validation Loss: 0.5294373631477356\n",
      "Epoch 30: Train Loss: 0.29205737511316937, Validation Loss: 0.5266004204750061\n",
      "Epoch 31: Train Loss: 0.2844293067852656, Validation Loss: 0.5174824595451355\n",
      "Epoch 32: Train Loss: 0.2606494178374608, Validation Loss: 0.5057469010353088\n",
      "Epoch 33: Train Loss: 0.25555957357088727, Validation Loss: 0.4980529844760895\n",
      "Epoch 34: Train Loss: 0.24902387956778207, Validation Loss: 0.49712252616882324\n",
      "Epoch 35: Train Loss: 0.25134578347206116, Validation Loss: 0.4970093071460724\n",
      "Epoch 36: Train Loss: 0.24344938496748605, Validation Loss: 0.49848198890686035\n",
      "Epoch 37: Train Loss: 0.23521112898985544, Validation Loss: 0.49817129969596863\n",
      "Epoch 38: Train Loss: 0.22252433995405832, Validation Loss: 0.4994875192642212\n",
      "Epoch 39: Train Loss: 0.23022342721621195, Validation Loss: 0.4995291531085968\n",
      "Epoch 40: Train Loss: 0.2247229516506195, Validation Loss: 0.48480433225631714\n",
      "Epoch 41: Train Loss: 0.21726834774017334, Validation Loss: 0.4902397394180298\n",
      "Epoch 42: Train Loss: 0.2123365749915441, Validation Loss: 0.4863074719905853\n",
      "Epoch 43: Train Loss: 0.21949816246827444, Validation Loss: 0.47300824522972107\n",
      "Epoch 44: Train Loss: 0.20518540839354196, Validation Loss: 0.4625232517719269\n",
      "Epoch 45: Train Loss: 0.21169765293598175, Validation Loss: 0.4656592607498169\n",
      "Epoch 46: Train Loss: 0.20105014741420746, Validation Loss: 0.4671291708946228\n",
      "Epoch 47: Train Loss: 0.18628298739592233, Validation Loss: 0.4689347445964813\n",
      "Epoch 48: Train Loss: 0.1887778490781784, Validation Loss: 0.47163501381874084\n",
      "Epoch 49: Train Loss: 0.18942491710186005, Validation Loss: 0.4728909730911255\n",
      "Epoch 50: Train Loss: 0.18914749721686044, Validation Loss: 0.474591463804245\n",
      "Epoch 51: Train Loss: 0.18952128291130066, Validation Loss: 0.5009628534317017\n",
      "Epoch 52: Train Loss: 0.19075274467468262, Validation Loss: 0.46068325638771057\n",
      "Epoch 53: Train Loss: 0.1735349198182424, Validation Loss: 0.4650232791900635\n",
      "Epoch 54: Train Loss: 0.17529520889123282, Validation Loss: 0.47323301434516907\n",
      "Epoch 55: Train Loss: 0.18585827946662903, Validation Loss: 0.44735610485076904\n",
      "Epoch 56: Train Loss: 0.1874526192744573, Validation Loss: 0.4556786119937897\n",
      "Epoch 57: Train Loss: 0.15489288171132407, Validation Loss: 0.461166650056839\n",
      "Epoch 58: Train Loss: 0.17812389135360718, Validation Loss: 0.46607983112335205\n",
      "Epoch 59: Train Loss: 0.1573522835969925, Validation Loss: 0.4662288427352905\n",
      "Epoch 60: Train Loss: 0.1606887231270472, Validation Loss: 0.4742087125778198\n",
      "Epoch 61: Train Loss: 0.16865715881188711, Validation Loss: 0.4768811762332916\n",
      "Epoch 62: Train Loss: 0.16191420952479044, Validation Loss: 0.5010232329368591\n",
      "Epoch 63: Train Loss: 0.1952776163816452, Validation Loss: 0.4528864622116089\n",
      "Epoch 64: Train Loss: 0.17293057839075723, Validation Loss: 0.40707504749298096\n",
      "Epoch 65: Train Loss: 0.16695818305015564, Validation Loss: 0.42767417430877686\n",
      "Epoch 66: Train Loss: 0.15132297575473785, Validation Loss: 0.4408450722694397\n",
      "Epoch 67: Train Loss: 0.15376150608062744, Validation Loss: 0.4487397372722626\n",
      "Epoch 68: Train Loss: 0.13889380544424057, Validation Loss: 0.4464530348777771\n",
      "Epoch 69: Train Loss: 0.14545248448848724, Validation Loss: 0.4456443786621094\n",
      "Epoch 70: Train Loss: 0.13118159770965576, Validation Loss: 0.4282357692718506\n",
      "Epoch 71: Train Loss: 0.15945024291674295, Validation Loss: 0.4548947215080261\n",
      "Epoch 72: Train Loss: 0.13424250980218252, Validation Loss: 0.4895689785480499\n",
      "Epoch 73: Train Loss: 0.12658787270387015, Validation Loss: 0.48992669582366943\n",
      "Epoch 74: Train Loss: 0.12377810726563136, Validation Loss: 0.4695501923561096\n",
      "Epoch 75: Train Loss: 0.11647972464561462, Validation Loss: 0.44891157746315\n",
      "Epoch 76: Train Loss: 0.11112311979134877, Validation Loss: 0.4328652322292328\n",
      "Epoch 77: Train Loss: 0.11654890825351079, Validation Loss: 0.4288521409034729\n",
      "Epoch 78: Train Loss: 0.11365386843681335, Validation Loss: 0.4279896020889282\n",
      "Epoch 79: Train Loss: 0.12939042349656424, Validation Loss: 0.43156901001930237\n",
      "Epoch 80: Train Loss: 0.11787570267915726, Validation Loss: 0.4487156271934509\n",
      "Epoch 81: Train Loss: 0.11865035196145375, Validation Loss: 0.46577876806259155\n",
      "Epoch 82: Train Loss: 0.12159969906012218, Validation Loss: 0.42133837938308716\n",
      "Epoch 83: Train Loss: 0.12222580115000407, Validation Loss: 0.39842867851257324\n",
      "Epoch 84: Train Loss: 0.11220728357632954, Validation Loss: 0.3738429844379425\n",
      "Epoch 85: Train Loss: 0.1144042859474818, Validation Loss: 0.39396268129348755\n",
      "Epoch 86: Train Loss: 0.10532787690560023, Validation Loss: 0.43103960156440735\n",
      "Epoch 87: Train Loss: 0.10339469959338506, Validation Loss: 0.43081793189048767\n",
      "Epoch 88: Train Loss: 0.11302625884612401, Validation Loss: 0.4256499707698822\n",
      "Epoch 89: Train Loss: 0.10617857426404953, Validation Loss: 0.4297565817832947\n",
      "Epoch 90: Train Loss: 0.09326963126659393, Validation Loss: 0.3796725869178772\n",
      "Epoch 91: Train Loss: 0.10855873425801595, Validation Loss: 0.40830084681510925\n",
      "Epoch 92: Train Loss: 0.11166538049777348, Validation Loss: 0.44282492995262146\n",
      "Epoch 93: Train Loss: 0.11619045833746593, Validation Loss: 0.4816330373287201\n",
      "Epoch 94: Train Loss: 0.09663094331820805, Validation Loss: 0.47389906644821167\n",
      "Epoch 95: Train Loss: 0.08982498695453008, Validation Loss: 0.4575473368167877\n",
      "Epoch 96: Train Loss: 0.09292407830556233, Validation Loss: 0.4379504323005676\n",
      "Epoch 97: Train Loss: 0.08327934145927429, Validation Loss: 0.43740078806877136\n",
      "Epoch 98: Train Loss: 0.1389664113521576, Validation Loss: 0.42624571919441223\n",
      "Epoch 99: Train Loss: 0.08406616747379303, Validation Loss: 0.42980068922042847\n",
      "Epoch 100: Train Loss: 0.08622193584839503, Validation Loss: 0.45360133051872253\n",
      "Epoch 101: Train Loss: 0.1007613589366277, Validation Loss: 0.41562700271606445\n",
      "Epoch 102: Train Loss: 0.08704597999652226, Validation Loss: 0.40384724736213684\n",
      "Epoch 103: Train Loss: 0.09722250948349635, Validation Loss: 0.4687551259994507\n",
      "Epoch 104: Train Loss: 0.08316231518983841, Validation Loss: 0.5098092555999756\n",
      "Epoch 105: Train Loss: 0.08159255981445312, Validation Loss: 0.45270681381225586\n",
      "Epoch 106: Train Loss: 0.09365126987298329, Validation Loss: 0.4306134581565857\n",
      "Epoch 107: Train Loss: 0.09422262261311214, Validation Loss: 0.4126989543437958\n",
      "Epoch 108: Train Loss: 0.0745690514643987, Validation Loss: 0.4166112244129181\n",
      "Epoch 109: Train Loss: 0.0965206374724706, Validation Loss: 0.422756552696228\n",
      "Epoch 110: Train Loss: 0.0732758417725563, Validation Loss: 0.510022759437561\n",
      "Epoch 111: Train Loss: 0.07866852109630902, Validation Loss: 0.4612152576446533\n",
      "Epoch 112: Train Loss: 0.08473117649555206, Validation Loss: 0.3903619945049286\n",
      "Epoch 113: Train Loss: 0.09794206668933232, Validation Loss: 0.4316897988319397\n",
      "Epoch 114: Train Loss: 0.0743274874985218, Validation Loss: 0.4656899869441986\n",
      "Epoch 115: Train Loss: 0.07430081566174825, Validation Loss: 0.45699402689933777\n",
      "Epoch 116: Train Loss: 0.07513345653812091, Validation Loss: 0.4432162344455719\n",
      "Epoch 117: Train Loss: 0.0709620863199234, Validation Loss: 0.44116801023483276\n",
      "Epoch 118: Train Loss: 0.07918647180000941, Validation Loss: 0.4338549077510834\n",
      "Epoch 119: Train Loss: 0.06308773035804431, Validation Loss: 0.43306025862693787\n",
      "Epoch 120: Train Loss: 0.07132617632548015, Validation Loss: 0.42353808879852295\n",
      "Epoch 121: Train Loss: 0.06402148803075154, Validation Loss: 0.3810376822948456\n",
      "Epoch 122: Train Loss: 0.13334585974613825, Validation Loss: 0.3812538981437683\n",
      "Epoch 123: Train Loss: 0.07922462125619252, Validation Loss: 0.4505467414855957\n",
      "Early stopping at epoch 124\n",
      "Fold 5\n",
      "Epoch 0: Train Loss: 0.6893789569536845, Validation Loss: 0.7248177528381348\n",
      "Epoch 1: Train Loss: 0.601054847240448, Validation Loss: 0.7354446649551392\n",
      "Epoch 2: Train Loss: 0.5547809799512228, Validation Loss: 0.7485418319702148\n",
      "Epoch 3: Train Loss: 0.5369959274927775, Validation Loss: 0.7620238661766052\n",
      "Epoch 4: Train Loss: 0.5160213708877563, Validation Loss: 0.7644131779670715\n",
      "Epoch 5: Train Loss: 0.49951005975405377, Validation Loss: 0.7704858183860779\n",
      "Epoch 6: Train Loss: 0.4873788158098857, Validation Loss: 0.7747746109962463\n",
      "Epoch 7: Train Loss: 0.470839262008667, Validation Loss: 0.776464581489563\n",
      "Epoch 8: Train Loss: 0.4801858762900035, Validation Loss: 0.7782866954803467\n",
      "Epoch 9: Train Loss: 0.4700193603833516, Validation Loss: 0.7830855846405029\n",
      "Epoch 10: Train Loss: 0.4722384214401245, Validation Loss: 0.7623972296714783\n",
      "Epoch 11: Train Loss: 0.43354296684265137, Validation Loss: 0.7502514719963074\n",
      "Epoch 12: Train Loss: 0.4408724904060364, Validation Loss: 0.7402612566947937\n",
      "Epoch 13: Train Loss: 0.4024873872598012, Validation Loss: 0.7360472083091736\n",
      "Epoch 14: Train Loss: 0.4064628978570302, Validation Loss: 0.7491801977157593\n",
      "Epoch 15: Train Loss: 0.38141681750615436, Validation Loss: 0.7657343745231628\n",
      "Epoch 16: Train Loss: 0.3918709059556325, Validation Loss: 0.7808706760406494\n",
      "Epoch 17: Train Loss: 0.3780338068803151, Validation Loss: 0.7932636141777039\n",
      "Epoch 18: Train Loss: 0.36614089210828143, Validation Loss: 0.7947794198989868\n",
      "Epoch 19: Train Loss: 0.36156943440437317, Validation Loss: 0.7941124439239502\n",
      "Epoch 20: Train Loss: 0.36550984779993695, Validation Loss: 0.8317221403121948\n",
      "Epoch 21: Train Loss: 0.34032883246739704, Validation Loss: 0.8526806235313416\n",
      "Epoch 22: Train Loss: 0.3139078418413798, Validation Loss: 0.8400442004203796\n",
      "Epoch 23: Train Loss: 0.3111991584300995, Validation Loss: 0.822495698928833\n",
      "Epoch 24: Train Loss: 0.32312721014022827, Validation Loss: 0.8108353018760681\n",
      "Epoch 25: Train Loss: 0.2965481181939443, Validation Loss: 0.8075627684593201\n",
      "Epoch 26: Train Loss: 0.2888153592745463, Validation Loss: 0.8149541616439819\n",
      "Epoch 27: Train Loss: 0.2831864356994629, Validation Loss: 0.8209065794944763\n",
      "Epoch 28: Train Loss: 0.29360679785410565, Validation Loss: 0.8215155005455017\n",
      "Epoch 29: Train Loss: 0.29086026549339294, Validation Loss: 0.8223447203636169\n",
      "Epoch 30: Train Loss: 0.2803680996100108, Validation Loss: 0.7972131371498108\n",
      "Epoch 31: Train Loss: 0.26029955844084424, Validation Loss: 0.770081102848053\n",
      "Epoch 32: Train Loss: 0.26113569736480713, Validation Loss: 0.7247614860534668\n",
      "Epoch 33: Train Loss: 0.24652149279912314, Validation Loss: 0.7059474587440491\n",
      "Epoch 34: Train Loss: 0.23369026680787405, Validation Loss: 0.7078709602355957\n",
      "Epoch 35: Train Loss: 0.239610493183136, Validation Loss: 0.7117493152618408\n",
      "Epoch 36: Train Loss: 0.2304531435171763, Validation Loss: 0.7216546535491943\n",
      "Epoch 37: Train Loss: 0.22959598898887634, Validation Loss: 0.7232323884963989\n",
      "Epoch 38: Train Loss: 0.23049846788247427, Validation Loss: 0.7239816784858704\n",
      "Epoch 39: Train Loss: 0.21766881148020426, Validation Loss: 0.719173014163971\n",
      "Epoch 40: Train Loss: 0.21463618179162344, Validation Loss: 0.7082241177558899\n",
      "Epoch 41: Train Loss: 0.20602867503960928, Validation Loss: 0.6685479283332825\n",
      "Epoch 42: Train Loss: 0.2018189678589503, Validation Loss: 0.6563960909843445\n",
      "Epoch 43: Train Loss: 0.2152676930030187, Validation Loss: 0.6457920074462891\n",
      "Epoch 44: Train Loss: 0.19839767118295035, Validation Loss: 0.6480523347854614\n",
      "Epoch 45: Train Loss: 0.18797786037127176, Validation Loss: 0.6507523655891418\n",
      "Epoch 46: Train Loss: 0.1880942682425181, Validation Loss: 0.6593727469444275\n",
      "Epoch 47: Train Loss: 0.19002035756905875, Validation Loss: 0.6501350998878479\n",
      "Epoch 48: Train Loss: 0.1850406378507614, Validation Loss: 0.6447187066078186\n",
      "Epoch 49: Train Loss: 0.17801361779371896, Validation Loss: 0.6457539796829224\n",
      "Epoch 50: Train Loss: 0.18187201023101807, Validation Loss: 0.6111558079719543\n",
      "Epoch 51: Train Loss: 0.1946951001882553, Validation Loss: 0.6013707518577576\n",
      "Epoch 52: Train Loss: 0.18365887304147085, Validation Loss: 0.6174165606498718\n",
      "Epoch 53: Train Loss: 0.16311092178026834, Validation Loss: 0.6599799394607544\n",
      "Epoch 54: Train Loss: 0.16609320044517517, Validation Loss: 0.6832880973815918\n",
      "Epoch 55: Train Loss: 0.1583333561817805, Validation Loss: 0.6565863490104675\n",
      "Epoch 56: Train Loss: 0.14978032559156418, Validation Loss: 0.6287882924079895\n",
      "Epoch 57: Train Loss: 0.1702136571208636, Validation Loss: 0.6126068234443665\n",
      "Epoch 58: Train Loss: 0.14943797886371613, Validation Loss: 0.6097618937492371\n",
      "Epoch 59: Train Loss: 0.17417130370934805, Validation Loss: 0.6127070188522339\n",
      "Epoch 60: Train Loss: 0.14526723821957907, Validation Loss: 0.5857834219932556\n",
      "Epoch 61: Train Loss: 0.15535359581311545, Validation Loss: 0.5753793716430664\n",
      "Epoch 62: Train Loss: 0.15238364040851593, Validation Loss: 0.6554788947105408\n",
      "Epoch 63: Train Loss: 0.13591798643271127, Validation Loss: 0.6873114109039307\n",
      "Epoch 64: Train Loss: 0.13263894120852152, Validation Loss: 0.6500276327133179\n",
      "Epoch 65: Train Loss: 0.1341452201207479, Validation Loss: 0.6034749746322632\n",
      "Epoch 66: Train Loss: 0.12330822149912517, Validation Loss: 0.576067328453064\n",
      "Epoch 67: Train Loss: 0.12191421538591385, Validation Loss: 0.569130003452301\n",
      "Epoch 68: Train Loss: 0.12689045071601868, Validation Loss: 0.570927619934082\n",
      "Epoch 69: Train Loss: 0.1384605293472608, Validation Loss: 0.5773235559463501\n",
      "Epoch 70: Train Loss: 0.12196914106607437, Validation Loss: 0.601440966129303\n",
      "Epoch 71: Train Loss: 0.12644866108894348, Validation Loss: 0.6196622848510742\n",
      "Epoch 72: Train Loss: 0.12476929028828938, Validation Loss: 0.6458158493041992\n",
      "Epoch 73: Train Loss: 0.10989519208669662, Validation Loss: 0.6311895251274109\n",
      "Epoch 74: Train Loss: 0.11402324835459392, Validation Loss: 0.6110839247703552\n",
      "Epoch 75: Train Loss: 0.11158626278241475, Validation Loss: 0.6025716662406921\n",
      "Epoch 76: Train Loss: 0.10188434024651845, Validation Loss: 0.5931727290153503\n",
      "Epoch 77: Train Loss: 0.11454286177953084, Validation Loss: 0.5885241627693176\n",
      "Epoch 78: Train Loss: 0.11901267617940903, Validation Loss: 0.5894578695297241\n",
      "Epoch 79: Train Loss: 0.10762644062439601, Validation Loss: 0.5859050750732422\n",
      "Epoch 80: Train Loss: 0.11696207523345947, Validation Loss: 0.6002427935600281\n",
      "Epoch 81: Train Loss: 0.11802660177151363, Validation Loss: 0.6057878136634827\n",
      "Epoch 82: Train Loss: 0.11962367097536723, Validation Loss: 0.6065383553504944\n",
      "Epoch 83: Train Loss: 0.1020401269197464, Validation Loss: 0.5955203771591187\n",
      "Epoch 84: Train Loss: 0.12740266074736914, Validation Loss: 0.6273831725120544\n",
      "Epoch 85: Train Loss: 0.11257807910442352, Validation Loss: 0.6496013402938843\n",
      "Epoch 86: Train Loss: 0.1038166011373202, Validation Loss: 0.6697604060173035\n",
      "Epoch 87: Train Loss: 0.09555567304293315, Validation Loss: 0.6654224395751953\n",
      "Epoch 88: Train Loss: 0.10041061043739319, Validation Loss: 0.6598801612854004\n",
      "Epoch 89: Train Loss: 0.08848893642425537, Validation Loss: 0.6551057696342468\n",
      "Epoch 90: Train Loss: 0.08844718833764394, Validation Loss: 0.557386577129364\n",
      "Epoch 91: Train Loss: 0.09437427669763565, Validation Loss: 0.5329923033714294\n",
      "Epoch 92: Train Loss: 0.08209894349177678, Validation Loss: 0.6056239008903503\n",
      "Epoch 93: Train Loss: 0.09522609164317448, Validation Loss: 0.6533756852149963\n",
      "Epoch 94: Train Loss: 0.10370079179604848, Validation Loss: 0.6359120607376099\n",
      "Epoch 95: Train Loss: 0.07500655700763066, Validation Loss: 0.604409396648407\n",
      "Epoch 96: Train Loss: 0.09709152330954869, Validation Loss: 0.598919153213501\n",
      "Epoch 97: Train Loss: 0.11514147867759068, Validation Loss: 0.6148154139518738\n",
      "Epoch 98: Train Loss: 0.07889665539065997, Validation Loss: 0.6235953569412231\n",
      "Epoch 99: Train Loss: 0.07724406321843465, Validation Loss: 0.6282544732093811\n",
      "Epoch 100: Train Loss: 0.08317103733619054, Validation Loss: 0.7030178308486938\n",
      "Epoch 101: Train Loss: 0.07500108083089192, Validation Loss: 0.7466885447502136\n",
      "Epoch 102: Train Loss: 0.08814802517493565, Validation Loss: 0.7168929576873779\n",
      "Epoch 103: Train Loss: 0.08869364360968272, Validation Loss: 0.6700828075408936\n",
      "Epoch 104: Train Loss: 0.09283105532328288, Validation Loss: 0.6878966093063354\n",
      "Epoch 105: Train Loss: 0.08893409371376038, Validation Loss: 0.6987091302871704\n",
      "Epoch 106: Train Loss: 0.07470725973447163, Validation Loss: 0.694480836391449\n",
      "Epoch 107: Train Loss: 0.08216878523429234, Validation Loss: 0.684276819229126\n",
      "Epoch 108: Train Loss: 0.06806684906284015, Validation Loss: 0.6828792095184326\n",
      "Epoch 109: Train Loss: 0.0860861986875534, Validation Loss: 0.6894215941429138\n",
      "Epoch 110: Train Loss: 0.06638551130890846, Validation Loss: 0.705327033996582\n",
      "Epoch 111: Train Loss: 0.07663488388061523, Validation Loss: 0.7265936136245728\n",
      "Epoch 112: Train Loss: 0.0900574338932832, Validation Loss: 0.7190378308296204\n",
      "Epoch 113: Train Loss: 0.07495426883300145, Validation Loss: 0.6974299550056458\n",
      "Epoch 114: Train Loss: 0.07581354056795438, Validation Loss: 0.6643040180206299\n",
      "Epoch 115: Train Loss: 0.07228928804397583, Validation Loss: 0.6694977879524231\n",
      "Epoch 116: Train Loss: 0.07039380942781766, Validation Loss: 0.7003554105758667\n",
      "Epoch 117: Train Loss: 0.06175142526626587, Validation Loss: 0.7112292051315308\n",
      "Epoch 118: Train Loss: 0.07615253701806068, Validation Loss: 0.7081817388534546\n",
      "Epoch 119: Train Loss: 0.07193466151754062, Validation Loss: 0.7033191919326782\n",
      "Epoch 120: Train Loss: 0.0666242887576421, Validation Loss: 0.6796759963035583\n",
      "Epoch 121: Train Loss: 0.1114417314529419, Validation Loss: 0.6302106976509094\n",
      "Epoch 122: Train Loss: 0.10156363745530446, Validation Loss: 0.6623244285583496\n",
      "Epoch 123: Train Loss: 0.06265086680650711, Validation Loss: 0.7360454201698303\n",
      "Epoch 124: Train Loss: 0.07807748888929684, Validation Loss: 0.7516787052154541\n",
      "Epoch 125: Train Loss: 0.07457035531600316, Validation Loss: 0.746607780456543\n",
      "Epoch 126: Train Loss: 0.06959027176102002, Validation Loss: 0.755216658115387\n",
      "Epoch 127: Train Loss: 0.06295900543530782, Validation Loss: 0.7531874179840088\n",
      "Epoch 128: Train Loss: 0.07300183052817981, Validation Loss: 0.7538236975669861\n",
      "Epoch 129: Train Loss: 0.06768698617815971, Validation Loss: 0.7551461458206177\n",
      "Epoch 130: Train Loss: 0.0681736059486866, Validation Loss: 0.7470531463623047\n",
      "Early stopping at epoch 131\n",
      "Accuracy: 0.7416666666666667,Precision: 0.7543859649122807, Recall: 0.7166666666666667, F1-score: 0.7350427350427351, AUC: 0.7416666666666666\n",
      "Confusion Matrix:\n",
      "[[46 14]\n",
      " [17 43]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nweight decay = 1e-7\\nlearning rate = 0.0001\\nepoch = 100\\nbatch size = 32\\nearly stopping patience = 10\\nstandard scaler\\nReLU\\ncross entropy loss\\ndrop out = 0.8\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "model_save_dir = r'C:\\Users\\User\\Documents\\Lie detect data\\Model'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Define a function to load and pad data\n",
    "def load_data(data_dir, max_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    file_list = os.listdir(data_dir)\n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        label = 1 if 'truth' in file else 0\n",
    "        padded_data = np.zeros((65, max_length))\n",
    "        length = min(data.shape[1], max_length)\n",
    "        padded_data[:, :length] = data[:, :length]\n",
    "        X.append(padded_data)\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load dataset and pad the data\n",
    "data_dir = \"C:\\\\Users\\\\User\\\\Documents\\\\Lie detect data\\\\56M_DWTEEGData\"\n",
    "max_length = 500  # Define maximum length for padding\n",
    "X, y = load_data(data_dir, max_length)\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# Define EEGNet model\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(65, 32, kernel_size=63, padding=31)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "        self.depthwiseConv1d = nn.Conv1d(32, 64, kernel_size=65, groups=32, padding=32)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)  # Additional convolutional layer\n",
    "        self.batchnorm3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.pooling = nn.AvgPool1d(kernel_size=4)\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        \n",
    "        self._calculate_num_features()\n",
    "        self.fc = nn.Linear(self.num_features, num_classes)\n",
    "\n",
    "    def _calculate_num_features(self):\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.zeros(1, 65, 500)\n",
    "            sample_output = self._forward_features(sample_input)\n",
    "            self.num_features = sample_output.shape[1]\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.depthwiseConv1d(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.conv2(x)  # Additional convolutional layer\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.pooling(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.global_pool(x)  # Global average pooling layer\n",
    "        x = torch.flatten(x, 1)  # Flatten the output for the fully connected layer\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_and_evaluate(train_loader, val_loader, y_train):\n",
    "    model = EEGNet(num_classes=2).to(device)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-3)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "    num_epochs = 200\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 40\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "            fold_model_path = os.path.join(model_save_dir, f'fold3_model_fold_{fold_idx}.pth')\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': best_val_loss,\n",
    "            }, fold_model_path)\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_idx = 0\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "for train_index, val_index in kf.split(X, y):\n",
    "    print(f'Fold {fold_idx + 1}')\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Normalize data using scaler fitted on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_val = scaler.transform(X_val.reshape(X_val.shape[0], -1))\n",
    "    X_train = X_train.reshape(-1, 65, max_length)\n",
    "    X_val = X_val.reshape(-1, 65, max_length)\n",
    "\n",
    "    # Save the scaler to a file\n",
    "    with open(r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\simpleEEGNet_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = EEGDataset(X_train, y_train)\n",
    "    val_dataset = EEGDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = train_and_evaluate(train_loader, val_loader, y_train)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    fold_idx += 1\n",
    "\n",
    "\n",
    "# Calculate additional metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "auc = roc_auc_score(all_labels, all_predictions)\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy},Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "weight decay = 1e-7\n",
    "learning rate = 0.0001\n",
    "epoch = 100\n",
    "batch size = 32\n",
    "early stopping patience = 10\n",
    "standard scaler\n",
    "ReLU\n",
    "cross entropy loss\n",
    "drop out = 0.8\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66654d63-c450-4d9f-a3e3-63701fd1d41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934cb89-5ea1-4eb0-a152-00c71e49e06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
