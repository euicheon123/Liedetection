{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29d5ee57-cf0b-4330-985e-ada385863b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'lie' samples: 65\n",
      "Number of 'truth' samples: 78\n",
      "Total number of samples: 143\n",
      "Adding 5 lie samples from subject 8 to test set\n",
      "Adding 6 truth samples from subject 8 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 1000)\n",
      "(132, 65, 1000)\n",
      "Epoch 0: Train Loss: 0.7663410446223091, Validation Loss: 0.7279139459133148\n",
      "Epoch 1: Train Loss: 0.7064177008236155, Validation Loss: 0.7337835431098938\n",
      "Epoch 2: Train Loss: 0.6904541552066803, Validation Loss: 0.7259415984153748\n",
      "Epoch 3: Train Loss: 0.643039717393763, Validation Loss: 0.7305749356746674\n",
      "Epoch 4: Train Loss: 0.6550154160050785, Validation Loss: 0.7317310571670532\n",
      "Epoch 5: Train Loss: 0.6874113643870634, Validation Loss: 0.7230466902256012\n",
      "Epoch 6: Train Loss: 0.6528738526736989, Validation Loss: 0.7231185734272003\n",
      "Epoch 7: Train Loss: 0.6349251165109522, Validation Loss: 0.7231727540493011\n",
      "Epoch 8: Train Loss: 0.6366264574667987, Validation Loss: 0.7216062545776367\n",
      "Epoch 9: Train Loss: 0.6385990451363956, Validation Loss: 0.7227805256843567\n",
      "Epoch 10: Train Loss: 0.6390267119688147, Validation Loss: 0.7307238876819611\n",
      "Epoch 11: Train Loss: 0.616481973844416, Validation Loss: 0.7445108890533447\n",
      "Epoch 12: Train Loss: 0.6214579448980444, Validation Loss: 0.7339146435260773\n",
      "Epoch 13: Train Loss: 0.6003472840084749, Validation Loss: 0.7338123917579651\n",
      "Epoch 14: Train Loss: 0.5936615905340981, Validation Loss: 0.7308785617351532\n",
      "Epoch 15: Train Loss: 0.5880611293456134, Validation Loss: 0.7293472588062286\n",
      "Epoch 16: Train Loss: 0.6035056797897115, Validation Loss: 0.7279426753520966\n",
      "Epoch 17: Train Loss: 0.6221574474783504, Validation Loss: 0.7261792421340942\n",
      "Epoch 18: Train Loss: 0.6032831809099983, Validation Loss: 0.7268461585044861\n",
      "Epoch 19: Train Loss: 0.5923349156099207, Validation Loss: 0.7259692251682281\n",
      "Epoch 20: Train Loss: 0.5862913920598871, Validation Loss: 0.7213239371776581\n",
      "Epoch 21: Train Loss: 0.5804625065887675, Validation Loss: 0.714594155550003\n",
      "Epoch 22: Train Loss: 0.5673512255444246, Validation Loss: 0.721291571855545\n",
      "Epoch 23: Train Loss: 0.5444851359900307, Validation Loss: 0.7242691516876221\n",
      "Epoch 24: Train Loss: 0.55609190288712, Validation Loss: 0.7290063202381134\n",
      "Epoch 25: Train Loss: 0.5534178667208728, Validation Loss: 0.7392879128456116\n",
      "Epoch 26: Train Loss: 0.5418694036848405, Validation Loss: 0.740508496761322\n",
      "Epoch 27: Train Loss: 0.52417373306611, Validation Loss: 0.7381647527217865\n",
      "Epoch 28: Train Loss: 0.5520185389939476, Validation Loss: 0.7385293245315552\n",
      "Epoch 29: Train Loss: 0.5245852347682504, Validation Loss: 0.7378564774990082\n",
      "Epoch 30: Train Loss: 0.5358527372865116, Validation Loss: 0.739843875169754\n",
      "Epoch 31: Train Loss: 0.5472811530618107, Validation Loss: 0.7193359732627869\n",
      "Epoch 32: Train Loss: 0.5284490129526924, Validation Loss: 0.7125126123428345\n",
      "Epoch 33: Train Loss: 0.5231321156024933, Validation Loss: 0.7181376218795776\n",
      "Epoch 34: Train Loss: 0.501612629960565, Validation Loss: 0.7232473194599152\n",
      "Epoch 35: Train Loss: 0.49725982371498556, Validation Loss: 0.709915041923523\n",
      "Epoch 36: Train Loss: 0.48257877896813783, Validation Loss: 0.7025318145751953\n",
      "Epoch 37: Train Loss: 0.49229826646692615, Validation Loss: 0.7084099352359772\n",
      "Epoch 38: Train Loss: 0.4756263704860912, Validation Loss: 0.7095926403999329\n",
      "Epoch 39: Train Loss: 0.4510541155057795, Validation Loss: 0.7112532258033752\n",
      "Epoch 40: Train Loss: 0.4627668612143573, Validation Loss: 0.7166513502597809\n",
      "Epoch 41: Train Loss: 0.4712996938649346, Validation Loss: 0.7058639824390411\n",
      "Epoch 42: Train Loss: 0.4439500430051018, Validation Loss: 0.7148025035858154\n",
      "Epoch 43: Train Loss: 0.4233735449173871, Validation Loss: 0.7283833622932434\n",
      "Epoch 44: Train Loss: 0.41721440588726716, Validation Loss: 0.7239502668380737\n",
      "Epoch 45: Train Loss: 0.41397423428647656, Validation Loss: 0.7342022955417633\n",
      "Epoch 46: Train Loss: 0.4325284677393296, Validation Loss: 0.7277415692806244\n",
      "Epoch 47: Train Loss: 0.4028628135428709, Validation Loss: 0.7253140211105347\n",
      "Epoch 48: Train Loss: 0.39320357582148385, Validation Loss: 0.7284328043460846\n",
      "Epoch 49: Train Loss: 0.3871965899186976, Validation Loss: 0.7288947403430939\n",
      "Epoch 50: Train Loss: 0.40875306725502014, Validation Loss: 0.7252321839332581\n",
      "Epoch 51: Train Loss: 0.4007945656776428, Validation Loss: 0.7390807271003723\n",
      "Epoch 52: Train Loss: 0.39831288597163034, Validation Loss: 0.6951223015785217\n",
      "Epoch 53: Train Loss: 0.37824343057239757, Validation Loss: 0.7075954973697662\n",
      "Epoch 54: Train Loss: 0.35644959000980153, Validation Loss: 0.7103075087070465\n",
      "Epoch 55: Train Loss: 0.3562105885323356, Validation Loss: 0.7237622439861298\n",
      "Epoch 56: Train Loss: 0.3376837372779846, Validation Loss: 0.7182644307613373\n",
      "Epoch 57: Train Loss: 0.3165288015323527, Validation Loss: 0.7145377695560455\n",
      "Epoch 58: Train Loss: 0.3230125474579194, Validation Loss: 0.7141141593456268\n",
      "Epoch 59: Train Loss: 0.3130433235098334, Validation Loss: 0.7132278382778168\n",
      "Epoch 60: Train Loss: 0.336197208832292, Validation Loss: 0.7343849539756775\n",
      "Epoch 61: Train Loss: 0.3365573716514251, Validation Loss: 0.7365191876888275\n",
      "Epoch 62: Train Loss: 0.2910185131956549, Validation Loss: 0.7291911840438843\n",
      "Epoch 63: Train Loss: 0.29478378067998323, Validation Loss: 0.7327784299850464\n",
      "Epoch 64: Train Loss: 0.2751174060737385, Validation Loss: 0.7518365681171417\n",
      "Epoch 65: Train Loss: 0.27005073077538433, Validation Loss: 0.7368332147598267\n",
      "Epoch 66: Train Loss: 0.2808263047653086, Validation Loss: 0.7271250486373901\n",
      "Epoch 67: Train Loss: 0.2326658819528187, Validation Loss: 0.7164621651172638\n",
      "Epoch 68: Train Loss: 0.23505635734866648, Validation Loss: 0.714871734380722\n",
      "Epoch 69: Train Loss: 0.24780656178207958, Validation Loss: 0.713427871465683\n",
      "Epoch 70: Train Loss: 0.24581989295342388, Validation Loss: 0.7707777619361877\n",
      "Epoch 71: Train Loss: 0.2685309569625294, Validation Loss: 0.766685426235199\n",
      "Epoch 72: Train Loss: 0.21753594849039526, Validation Loss: 0.7267428338527679\n",
      "Epoch 73: Train Loss: 0.22026063063565424, Validation Loss: 0.6959261000156403\n",
      "Epoch 74: Train Loss: 0.2746876548318302, Validation Loss: 0.6559180915355682\n",
      "Epoch 75: Train Loss: 0.2044151271967327, Validation Loss: 0.6766790449619293\n",
      "Epoch 76: Train Loss: 0.21334310138926788, Validation Loss: 0.6611387133598328\n",
      "Epoch 77: Train Loss: 0.18319308889262817, Validation Loss: 0.6722120642662048\n",
      "Epoch 78: Train Loss: 0.18936397529700222, Validation Loss: 0.6762699782848358\n",
      "Epoch 79: Train Loss: 0.17760419363484664, Validation Loss: 0.6711113750934601\n",
      "Epoch 80: Train Loss: 0.20017464721904082, Validation Loss: 0.6077858805656433\n",
      "Epoch 81: Train Loss: 0.17635781493257074, Validation Loss: 0.5550296753644943\n",
      "Epoch 82: Train Loss: 0.19243458943331943, Validation Loss: 0.6103772521018982\n",
      "Epoch 83: Train Loss: 0.16109401206759846, Validation Loss: 0.5756385028362274\n",
      "Epoch 84: Train Loss: 0.15179261477554545, Validation Loss: 0.5482954531908035\n",
      "Epoch 85: Train Loss: 0.1530651612316861, Validation Loss: 0.5617073029279709\n",
      "Epoch 86: Train Loss: 0.14793581822339227, Validation Loss: 0.5492165088653564\n",
      "Epoch 87: Train Loss: 0.1229785268797594, Validation Loss: 0.530585765838623\n",
      "Epoch 88: Train Loss: 0.137162073789274, Validation Loss: 0.5214778929948807\n",
      "Epoch 89: Train Loss: 0.1563471529413672, Validation Loss: 0.5195320546627045\n",
      "Epoch 90: Train Loss: 0.14141154661774635, Validation Loss: 0.5465623736381531\n",
      "Epoch 91: Train Loss: 0.156670392217005, Validation Loss: 0.4749266803264618\n",
      "Epoch 92: Train Loss: 0.1299684367635671, Validation Loss: 0.518608957529068\n",
      "Epoch 93: Train Loss: 0.14047284336651072, Validation Loss: 0.5075367391109467\n",
      "Epoch 94: Train Loss: 0.1424553236540626, Validation Loss: 0.4405187517404556\n",
      "Epoch 95: Train Loss: 0.1233649287811097, Validation Loss: 0.43700355291366577\n",
      "Epoch 96: Train Loss: 0.12857174719957745, Validation Loss: 0.42984794080257416\n",
      "Epoch 97: Train Loss: 0.11156974590438254, Validation Loss: 0.432527095079422\n",
      "Epoch 98: Train Loss: 0.12070383405422463, Validation Loss: 0.42529016733169556\n",
      "Epoch 99: Train Loss: 0.09283632522120196, Validation Loss: 0.4257306009531021\n",
      "Epoch 100: Train Loss: 0.1006416955634075, Validation Loss: 0.4214276969432831\n",
      "Epoch 101: Train Loss: 0.13536249648998766, Validation Loss: 0.3887613117694855\n",
      "Epoch 102: Train Loss: 0.15159992917495616, Validation Loss: 0.35281434655189514\n",
      "Epoch 103: Train Loss: 0.1349947630044292, Validation Loss: 0.39164498448371887\n",
      "Epoch 104: Train Loss: 0.12630140474613974, Validation Loss: 0.3400908187031746\n",
      "Epoch 105: Train Loss: 0.10628317318418447, Validation Loss: 0.34106970578432083\n",
      "Epoch 106: Train Loss: 0.11654696679290603, Validation Loss: 0.3435627445578575\n",
      "Epoch 107: Train Loss: 0.10102443616179858, Validation Loss: 0.35654959082603455\n",
      "Epoch 108: Train Loss: 0.11952967481578097, Validation Loss: 0.3580698147416115\n",
      "Epoch 109: Train Loss: 0.0961525862050407, Validation Loss: 0.36533233523368835\n",
      "Epoch 110: Train Loss: 0.09075108071898713, Validation Loss: 0.3620995357632637\n",
      "Epoch 111: Train Loss: 0.08700322710415896, Validation Loss: 0.3591419458389282\n",
      "Epoch 112: Train Loss: 0.1060241391772733, Validation Loss: 0.32977499812841415\n",
      "Epoch 113: Train Loss: 0.13290130697629032, Validation Loss: 0.33035141229629517\n",
      "Epoch 114: Train Loss: 0.08897725574891358, Validation Loss: 0.3384542688727379\n",
      "Epoch 115: Train Loss: 0.11304023270221318, Validation Loss: 0.30993804708123207\n",
      "Epoch 116: Train Loss: 0.10873984775560744, Validation Loss: 0.2775793746113777\n",
      "Epoch 117: Train Loss: 0.07765149423742995, Validation Loss: 0.2928847000002861\n",
      "Epoch 118: Train Loss: 0.09901290759444237, Validation Loss: 0.2882445529103279\n",
      "Epoch 119: Train Loss: 0.07967660406275708, Validation Loss: 0.2823798656463623\n",
      "Epoch 120: Train Loss: 0.10229518987676677, Validation Loss: 0.34588997066020966\n",
      "Epoch 121: Train Loss: 0.10676352351027377, Validation Loss: 0.29435305297374725\n",
      "Epoch 122: Train Loss: 0.09846018342410817, Validation Loss: 0.30286262929439545\n",
      "Epoch 123: Train Loss: 0.12499752728378072, Validation Loss: 0.29598468542099\n",
      "Epoch 124: Train Loss: 0.106709058780004, Validation Loss: 0.31925125420093536\n",
      "Epoch 125: Train Loss: 0.11796558045727365, Validation Loss: 0.2929001897573471\n",
      "Epoch 126: Train Loss: 0.09816161838962752, Validation Loss: 0.2994380071759224\n",
      "Epoch 127: Train Loss: 0.09767715260386467, Validation Loss: 0.30379997193813324\n",
      "Epoch 128: Train Loss: 0.09922450967133045, Validation Loss: 0.3038959577679634\n",
      "Epoch 129: Train Loss: 0.08807793654063169, Validation Loss: 0.3012503609061241\n",
      "Epoch 130: Train Loss: 0.13188007276724367, Validation Loss: 0.3029508404433727\n",
      "Epoch 131: Train Loss: 0.12233808998237639, Validation Loss: 0.34768543392419815\n",
      "Epoch 132: Train Loss: 0.10406356200794964, Validation Loss: 0.3399221673607826\n",
      "Epoch 133: Train Loss: 0.1076434864936506, Validation Loss: 0.33574599772691727\n",
      "Epoch 134: Train Loss: 0.0776780097033171, Validation Loss: 0.30693090334534645\n",
      "Epoch 135: Train Loss: 0.09302390328444102, Validation Loss: 0.30111120641231537\n",
      "Epoch 136: Train Loss: 0.08353115464834605, Validation Loss: 0.2775494270026684\n",
      "Epoch 137: Train Loss: 0.0840802926241475, Validation Loss: 0.2819211892783642\n",
      "Epoch 138: Train Loss: 0.09420954115579233, Validation Loss: 0.2887863852083683\n",
      "Epoch 139: Train Loss: 0.08413908564869095, Validation Loss: 0.284440740942955\n",
      "Epoch 140: Train Loss: 0.06928380633540013, Validation Loss: 0.25393209233880043\n",
      "Epoch 141: Train Loss: 0.10018000650383971, Validation Loss: 0.2786046750843525\n",
      "Epoch 142: Train Loss: 0.09417233873596963, Validation Loss: 0.21333569660782814\n",
      "Epoch 143: Train Loss: 0.09926517307758331, Validation Loss: 0.2335142008960247\n",
      "Epoch 144: Train Loss: 0.09889498791273903, Validation Loss: 0.3521315008401871\n",
      "Epoch 145: Train Loss: 0.06968375676147201, Validation Loss: 0.2925793267786503\n",
      "Epoch 146: Train Loss: 0.1025013945010655, Validation Loss: 0.27226242795586586\n",
      "Epoch 147: Train Loss: 0.07790442865670603, Validation Loss: 0.26723312959074974\n",
      "Epoch 148: Train Loss: 0.08302443375920548, Validation Loss: 0.26768694818019867\n",
      "Epoch 149: Train Loss: 0.11098506476949244, Validation Loss: 0.27823811024427414\n",
      "Epoch 150: Train Loss: 0.09401851130978149, Validation Loss: 0.2686239220201969\n",
      "Epoch 151: Train Loss: 0.08269459655617967, Validation Loss: 0.23012875020503998\n",
      "Epoch 152: Train Loss: 0.1081777318545124, Validation Loss: 0.24784164130687714\n",
      "Epoch 153: Train Loss: 0.08277172020033878, Validation Loss: 0.22241102904081345\n",
      "Epoch 154: Train Loss: 0.07122178449678947, Validation Loss: 0.21194919757544994\n",
      "Epoch 155: Train Loss: 0.08460797473569126, Validation Loss: 0.22649466060101986\n",
      "Epoch 156: Train Loss: 0.10962365666294799, Validation Loss: 0.23293979465961456\n",
      "Epoch 157: Train Loss: 0.08239128435140147, Validation Loss: 0.21734494343400002\n",
      "Epoch 158: Train Loss: 0.06243056540980058, Validation Loss: 0.2203945629298687\n",
      "Epoch 159: Train Loss: 0.07417671201641068, Validation Loss: 0.22152267768979073\n",
      "Epoch 160: Train Loss: 0.09305875959313091, Validation Loss: 0.21158050745725632\n",
      "Epoch 161: Train Loss: 0.06449344206382246, Validation Loss: 0.2566181905567646\n",
      "Epoch 162: Train Loss: 0.07138246812802904, Validation Loss: 0.2338819969445467\n",
      "Epoch 163: Train Loss: 0.09293317304485861, Validation Loss: 0.20725437253713608\n",
      "Epoch 164: Train Loss: 0.12104025483131409, Validation Loss: 0.22816833294928074\n",
      "Epoch 165: Train Loss: 0.08117757841725559, Validation Loss: 0.20545130781829357\n",
      "Epoch 166: Train Loss: 0.08137146966970142, Validation Loss: 0.21022790484130383\n",
      "Epoch 167: Train Loss: 0.10513125551754937, Validation Loss: 0.2044074684381485\n",
      "Epoch 168: Train Loss: 0.06774506665875807, Validation Loss: 0.207554679363966\n",
      "Epoch 169: Train Loss: 0.0713836176403086, Validation Loss: 0.22650043666362762\n",
      "Epoch 170: Train Loss: 0.08733420550604076, Validation Loss: 0.20533520728349686\n",
      "Epoch 171: Train Loss: 0.09559246667605989, Validation Loss: 0.2397434851154685\n",
      "Epoch 172: Train Loss: 0.10562263177159954, Validation Loss: 0.29950850922614336\n",
      "Epoch 173: Train Loss: 0.09480221083332949, Validation Loss: 0.28442783281207085\n",
      "Epoch 174: Train Loss: 0.09439223440473571, Validation Loss: 0.24866535142064095\n",
      "Epoch 175: Train Loss: 0.10783675811527406, Validation Loss: 0.25116745196282864\n",
      "Epoch 176: Train Loss: 0.08359848058727734, Validation Loss: 0.24414699524641037\n",
      "Epoch 177: Train Loss: 0.08890605460414115, Validation Loss: 0.2628847509622574\n",
      "Epoch 178: Train Loss: 0.06660697365398793, Validation Loss: 0.2712084762752056\n",
      "Epoch 179: Train Loss: 0.07258311026346158, Validation Loss: 0.2683263700455427\n",
      "Epoch 180: Train Loss: 0.09372474316178876, Validation Loss: 0.25194079242646694\n",
      "Epoch 181: Train Loss: 0.09637352983083795, Validation Loss: 0.24708490446209908\n",
      "Epoch 182: Train Loss: 0.08661810056689907, Validation Loss: 0.287925498560071\n",
      "Epoch 183: Train Loss: 0.07779835865778081, Validation Loss: 0.28930198587477207\n",
      "Epoch 184: Train Loss: 0.07295729134998777, Validation Loss: 0.3080708999186754\n",
      "Epoch 185: Train Loss: 0.08471075760419755, Validation Loss: 0.3021809943020344\n",
      "Epoch 186: Train Loss: 0.11665668749414823, Validation Loss: 0.3426149245351553\n",
      "Epoch 187: Train Loss: 0.06942934353890665, Validation Loss: 0.33534963242709637\n",
      "Epoch 188: Train Loss: 0.10005912475068779, Validation Loss: 0.3334836848080158\n",
      "Epoch 189: Train Loss: 0.06427444235476501, Validation Loss: 0.35129974130541086\n",
      "Epoch 190: Train Loss: 0.07609996443395228, Validation Loss: 0.2395151238888502\n",
      "Epoch 191: Train Loss: 0.07494525668923468, Validation Loss: 0.2627677582204342\n",
      "Epoch 192: Train Loss: 0.0841801796683713, Validation Loss: 0.28410145826637745\n",
      "Epoch 193: Train Loss: 0.1798984254436458, Validation Loss: 0.2183772549033165\n",
      "Epoch 194: Train Loss: 0.087079487138373, Validation Loss: 0.2405588198453188\n",
      "Epoch 195: Train Loss: 0.08711080450345488, Validation Loss: 0.2767029032111168\n",
      "Epoch 196: Train Loss: 0.07671366778531057, Validation Loss: 0.2793290400877595\n",
      "Epoch 197: Train Loss: 0.06416727624395314, Validation Loss: 0.25671603344380856\n",
      "Epoch 198: Train Loss: 0.0635667279472246, Validation Loss: 0.2520643640309572\n",
      "Epoch 199: Train Loss: 0.06375692439649035, Validation Loss: 0.2533450061455369\n",
      "Fold 1 Metrics:\n",
      "Accuracy: 0.8181818181818182, Precision: 0.75, Recall: 1.0, F1-score: 0.8571428571428571, AUC: 0.8\n",
      "Confusion Matrix:\n",
      "[[3 2]\n",
      " [0 6]]\n",
      "Completed fold 1\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples from subject 7 to test set\n",
      "Adding 6 truth samples from subject 7 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 1000)\n",
      "(132, 65, 1000)\n",
      "Epoch 0: Train Loss: 0.7334959401803858, Validation Loss: 0.6844283044338226\n",
      "Epoch 1: Train Loss: 0.6998743309694178, Validation Loss: 0.6806623637676239\n",
      "Epoch 2: Train Loss: 0.6847751631456263, Validation Loss: 0.6731494665145874\n",
      "Epoch 3: Train Loss: 0.6554855493938222, Validation Loss: 0.66098952293396\n",
      "Epoch 4: Train Loss: 0.6339439399102155, Validation Loss: 0.6549221575260162\n",
      "Epoch 5: Train Loss: 0.6390763976994682, Validation Loss: 0.6511832773685455\n",
      "Epoch 6: Train Loss: 0.640796258169062, Validation Loss: 0.647560328245163\n",
      "Epoch 7: Train Loss: 0.6323037147521973, Validation Loss: 0.6467950344085693\n",
      "Epoch 8: Train Loss: 0.6288377151769751, Validation Loss: 0.645422637462616\n",
      "Epoch 9: Train Loss: 0.630434604252086, Validation Loss: 0.6441887319087982\n",
      "Epoch 10: Train Loss: 0.614687558482675, Validation Loss: 0.6423182189464569\n",
      "Epoch 11: Train Loss: 0.6267804748871747, Validation Loss: 0.6459165513515472\n",
      "Epoch 12: Train Loss: 0.6131800623501048, Validation Loss: 0.6351236999034882\n",
      "Epoch 13: Train Loss: 0.5887079799876493, Validation Loss: 0.6233961582183838\n",
      "Epoch 14: Train Loss: 0.6113553292611066, Validation Loss: 0.6194860935211182\n",
      "Epoch 15: Train Loss: 0.5987725590958315, Validation Loss: 0.6158742010593414\n",
      "Epoch 16: Train Loss: 0.5982810293926912, Validation Loss: 0.6175492405891418\n",
      "Epoch 17: Train Loss: 0.5961267439758077, Validation Loss: 0.6145561337471008\n",
      "Epoch 18: Train Loss: 0.5636213719844818, Validation Loss: 0.6117659509181976\n",
      "Epoch 19: Train Loss: 0.5725681623991798, Validation Loss: 0.6144711375236511\n",
      "Epoch 20: Train Loss: 0.5859530252568862, Validation Loss: 0.6123994290828705\n",
      "Epoch 21: Train Loss: 0.5751286920379189, Validation Loss: 0.6128076314926147\n",
      "Epoch 22: Train Loss: 0.5826159785775578, Validation Loss: 0.5954713225364685\n",
      "Epoch 23: Train Loss: 0.5600655815180611, Validation Loss: 0.5974781811237335\n",
      "Epoch 24: Train Loss: 0.5725293965900645, Validation Loss: 0.5921127796173096\n",
      "Epoch 25: Train Loss: 0.5359499752521515, Validation Loss: 0.580857127904892\n",
      "Epoch 26: Train Loss: 0.5553863486822914, Validation Loss: 0.5753366351127625\n",
      "Epoch 27: Train Loss: 0.5453476169530083, Validation Loss: 0.5675126314163208\n",
      "Epoch 28: Train Loss: 0.5336562552872826, Validation Loss: 0.5699460208415985\n",
      "Epoch 29: Train Loss: 0.5537541999536402, Validation Loss: 0.5710009932518005\n",
      "Epoch 30: Train Loss: 0.5351222402909223, Validation Loss: 0.5740927457809448\n",
      "Epoch 31: Train Loss: 0.5311889385475832, Validation Loss: 0.5860190093517303\n",
      "Epoch 32: Train Loss: 0.5159084428759182, Validation Loss: 0.5785953998565674\n",
      "Epoch 33: Train Loss: 0.49776645618326526, Validation Loss: 0.5909483432769775\n",
      "Epoch 34: Train Loss: 0.5090038478374481, Validation Loss: 0.5857795476913452\n",
      "Epoch 35: Train Loss: 0.5046298468814177, Validation Loss: 0.5919865965843201\n",
      "Epoch 36: Train Loss: 0.485008905915653, Validation Loss: 0.5944240391254425\n",
      "Epoch 37: Train Loss: 0.47870345150723176, Validation Loss: 0.6035166084766388\n",
      "Epoch 38: Train Loss: 0.4761318967622869, Validation Loss: 0.6104183495044708\n",
      "Epoch 39: Train Loss: 0.467538924778209, Validation Loss: 0.6134868860244751\n",
      "Epoch 40: Train Loss: 0.4603397460544811, Validation Loss: 0.6214224696159363\n",
      "Epoch 41: Train Loss: 0.4726264950107126, Validation Loss: 0.6548585891723633\n",
      "Epoch 42: Train Loss: 0.4303882385001463, Validation Loss: 0.6762245297431946\n",
      "Epoch 43: Train Loss: 0.4148133344510022, Validation Loss: 0.6851403713226318\n",
      "Epoch 44: Train Loss: 0.40952130626229677, Validation Loss: 0.7471521198749542\n",
      "Epoch 45: Train Loss: 0.3538101613521576, Validation Loss: 0.7874931991100311\n",
      "Epoch 46: Train Loss: 0.34549004365416136, Validation Loss: 0.8226095139980316\n",
      "Epoch 47: Train Loss: 0.3391102771548664, Validation Loss: 0.8115315735340118\n",
      "Epoch 48: Train Loss: 0.36877498205970316, Validation Loss: 0.7901147603988647\n",
      "Epoch 49: Train Loss: 0.35513317497337565, Validation Loss: 0.8140877485275269\n",
      "Epoch 50: Train Loss: 0.35920106663423423, Validation Loss: 0.7713387608528137\n",
      "Epoch 51: Train Loss: 0.33483126759529114, Validation Loss: 0.8582300245761871\n",
      "Epoch 52: Train Loss: 0.34494751779472127, Validation Loss: 0.8258143663406372\n",
      "Epoch 53: Train Loss: 0.2729753571398118, Validation Loss: 0.900664746761322\n",
      "Epoch 54: Train Loss: 0.2757637018666548, Validation Loss: 0.9052168130874634\n",
      "Epoch 55: Train Loss: 0.2464401537881178, Validation Loss: 0.8976088762283325\n",
      "Epoch 56: Train Loss: 0.22367642118650324, Validation Loss: 0.9135257601737976\n",
      "Epoch 57: Train Loss: 0.21920013690696044, Validation Loss: 0.9094147682189941\n",
      "Epoch 58: Train Loss: 0.21521160313311746, Validation Loss: 0.958312064409256\n",
      "Epoch 59: Train Loss: 0.23276070636861465, Validation Loss: 0.9360063374042511\n",
      "Epoch 60: Train Loss: 0.24962562848539913, Validation Loss: 1.0655352175235748\n",
      "Epoch 61: Train Loss: 0.23773901953416712, Validation Loss: 0.9055159986019135\n",
      "Epoch 62: Train Loss: 0.22303609243210623, Validation Loss: 0.8848010003566742\n",
      "Epoch 63: Train Loss: 0.17432789898970547, Validation Loss: 0.8905620574951172\n",
      "Epoch 64: Train Loss: 0.18694010135882042, Validation Loss: 1.058912605047226\n",
      "Epoch 65: Train Loss: 0.2024539573227658, Validation Loss: 1.0235817432403564\n",
      "Epoch 66: Train Loss: 0.15178761127240517, Validation Loss: 1.127788782119751\n",
      "Epoch 67: Train Loss: 0.14956640890415976, Validation Loss: 1.0668376982212067\n",
      "Epoch 68: Train Loss: 0.15812772841137998, Validation Loss: 1.047960638999939\n",
      "Epoch 69: Train Loss: 0.1709131933748722, Validation Loss: 1.0633613616228104\n",
      "Epoch 70: Train Loss: 0.16968292085563436, Validation Loss: 1.0516176223754883\n",
      "Epoch 71: Train Loss: 0.1821804697460988, Validation Loss: 1.1124753654003143\n",
      "Epoch 72: Train Loss: 0.15238030649283352, Validation Loss: 1.155232459306717\n",
      "Epoch 73: Train Loss: 0.1523359873715569, Validation Loss: 1.1567146182060242\n",
      "Epoch 74: Train Loss: 0.17800856907578075, Validation Loss: 1.2071762531995773\n",
      "Epoch 75: Train Loss: 0.15565204839496052, Validation Loss: 1.0827374905347824\n",
      "Epoch 76: Train Loss: 0.17198963529046843, Validation Loss: 1.2286235839128494\n",
      "Epoch 77: Train Loss: 0.13046657677520723, Validation Loss: 1.256341278553009\n",
      "Epoch 78: Train Loss: 0.17954338955528595, Validation Loss: 1.257679283618927\n",
      "Epoch 79: Train Loss: 0.12983523024355664, Validation Loss: 1.2379539161920547\n",
      "Epoch 80: Train Loss: 0.12022563442587852, Validation Loss: 1.1808815598487854\n",
      "Epoch 81: Train Loss: 0.1616932506508687, Validation Loss: 1.2861854434013367\n",
      "Epoch 82: Train Loss: 0.17327717302695794, Validation Loss: 1.256706029176712\n",
      "Epoch 83: Train Loss: 0.2011834968100576, Validation Loss: 1.1464260071516037\n",
      "Epoch 84: Train Loss: 0.13341288005604462, Validation Loss: 1.1518606096506119\n",
      "Epoch 85: Train Loss: 0.16267975200625026, Validation Loss: 1.2322961539030075\n",
      "Epoch 86: Train Loss: 0.1531197560184142, Validation Loss: 1.2027257606387138\n",
      "Epoch 87: Train Loss: 0.12328620057772188, Validation Loss: 1.3227490186691284\n",
      "Epoch 88: Train Loss: 0.13540017276125796, Validation Loss: 1.3089446872472763\n",
      "Epoch 89: Train Loss: 0.13739732028368642, Validation Loss: 1.3004191666841507\n",
      "Epoch 90: Train Loss: 0.1169382500297883, Validation Loss: 1.186572253704071\n",
      "Epoch 91: Train Loss: 0.14961298062082598, Validation Loss: 1.2097293734550476\n",
      "Epoch 92: Train Loss: 0.12322551154476755, Validation Loss: 1.118323728442192\n",
      "Epoch 93: Train Loss: 0.1407340243458748, Validation Loss: 1.233008325099945\n",
      "Epoch 94: Train Loss: 0.13319928703062675, Validation Loss: 1.2913543283939362\n",
      "Epoch 95: Train Loss: 0.11098003836677355, Validation Loss: 1.2132058292627335\n",
      "Epoch 96: Train Loss: 0.15045420365298495, Validation Loss: 1.2409412413835526\n",
      "Epoch 97: Train Loss: 0.10754769700853263, Validation Loss: 1.2047543376684189\n",
      "Epoch 98: Train Loss: 0.10370260690722395, Validation Loss: 1.2452872097492218\n",
      "Epoch 99: Train Loss: 0.13771932210554094, Validation Loss: 1.2054270505905151\n",
      "Epoch 100: Train Loss: 0.12279257824754014, Validation Loss: 1.349492758512497\n",
      "Epoch 101: Train Loss: 0.1428395454388331, Validation Loss: 1.3907423168420792\n",
      "Epoch 102: Train Loss: 0.11119632833801649, Validation Loss: 1.260012686252594\n",
      "Epoch 103: Train Loss: 0.10800244308569852, Validation Loss: 1.0953997671604156\n",
      "Epoch 104: Train Loss: 0.09319316557444193, Validation Loss: 1.3434610664844513\n",
      "Epoch 105: Train Loss: 0.11286793939550133, Validation Loss: 1.3537482619285583\n",
      "Epoch 106: Train Loss: 0.11149180483292132, Validation Loss: 1.3322027921676636\n",
      "Epoch 107: Train Loss: 0.11092005363282036, Validation Loss: 1.3755655139684677\n",
      "Epoch 108: Train Loss: 0.08758393346386797, Validation Loss: 1.393286183476448\n",
      "Epoch 109: Train Loss: 0.10893324132570449, Validation Loss: 1.3906665444374084\n",
      "Epoch 110: Train Loss: 0.1012110566610799, Validation Loss: 1.3268753290176392\n",
      "Epoch 111: Train Loss: 0.12991912054884083, Validation Loss: 1.3063544556498528\n",
      "Epoch 112: Train Loss: 0.08063400268335552, Validation Loss: 1.4190069884061813\n",
      "Epoch 113: Train Loss: 0.12039995423572905, Validation Loss: 1.4345426857471466\n",
      "Epoch 114: Train Loss: 0.10031984828631668, Validation Loss: 1.3634025603532791\n",
      "Epoch 115: Train Loss: 0.1071543840910582, Validation Loss: 1.3794099241495132\n",
      "Epoch 116: Train Loss: 0.10921162081991925, Validation Loss: 1.3999017626047134\n",
      "Epoch 117: Train Loss: 0.11934351186980219, Validation Loss: 1.4262333810329437\n",
      "Epoch 118: Train Loss: 0.07876222399885163, Validation Loss: 1.4524312987923622\n",
      "Epoch 119: Train Loss: 0.1118758558843504, Validation Loss: 1.4618689864873886\n",
      "Epoch 120: Train Loss: 0.12844208600547383, Validation Loss: 1.6181867718696594\n",
      "Epoch 121: Train Loss: 0.12488390998367001, Validation Loss: 1.5739057064056396\n",
      "Epoch 122: Train Loss: 0.0960478263304514, Validation Loss: 1.3049222081899643\n",
      "Epoch 123: Train Loss: 0.09795978621524923, Validation Loss: 1.4067502319812775\n",
      "Epoch 124: Train Loss: 0.08598619502256899, Validation Loss: 1.2719901204109192\n",
      "Epoch 125: Train Loss: 0.10954005786162965, Validation Loss: 1.4553370475769043\n",
      "Epoch 126: Train Loss: 0.09693859309396323, Validation Loss: 1.4862009584903717\n",
      "Epoch 127: Train Loss: 0.09611413178636748, Validation Loss: 1.5080845057964325\n",
      "Epoch 128: Train Loss: 0.1055065381943303, Validation Loss: 1.5639271438121796\n",
      "Epoch 129: Train Loss: 0.10172437328626127, Validation Loss: 1.557035744190216\n",
      "Epoch 130: Train Loss: 0.1285706768360208, Validation Loss: 1.370790883898735\n",
      "Epoch 131: Train Loss: 0.11755487947341274, Validation Loss: 1.325047917664051\n",
      "Epoch 132: Train Loss: 0.1250320545890752, Validation Loss: 1.2673774734139442\n",
      "Epoch 133: Train Loss: 0.09081373292514507, Validation Loss: 1.54047691822052\n",
      "Epoch 134: Train Loss: 0.12471895396490307, Validation Loss: 1.527664601802826\n",
      "Epoch 135: Train Loss: 0.11051004549817127, Validation Loss: 1.5648152232170105\n",
      "Epoch 136: Train Loss: 0.08159663199501879, Validation Loss: 1.5184829533100128\n",
      "Epoch 137: Train Loss: 0.08552780210533563, Validation Loss: 1.519726812839508\n",
      "Epoch 138: Train Loss: 0.10857975611682325, Validation Loss: 1.5187797844409943\n",
      "Epoch 139: Train Loss: 0.07203624994658372, Validation Loss: 1.5288369208574295\n",
      "Epoch 140: Train Loss: 0.12364963568089639, Validation Loss: 1.7492852956056595\n",
      "Epoch 141: Train Loss: 0.13362650326727069, Validation Loss: 1.6440942734479904\n",
      "Epoch 142: Train Loss: 0.14210869141799562, Validation Loss: 1.70076884329319\n",
      "Epoch 143: Train Loss: 0.11515057207468678, Validation Loss: 1.6275728195905685\n",
      "Epoch 144: Train Loss: 0.09408245471251361, Validation Loss: 1.5507700443267822\n",
      "Epoch 145: Train Loss: 0.09605440042693825, Validation Loss: 1.4772238731384277\n",
      "Epoch 146: Train Loss: 0.07696759185808547, Validation Loss: 1.5263239741325378\n",
      "Epoch 147: Train Loss: 0.08582127861240331, Validation Loss: 1.5650902688503265\n",
      "Epoch 148: Train Loss: 0.09150071833830546, Validation Loss: 1.5964964181184769\n",
      "Epoch 149: Train Loss: 0.09832959181136068, Validation Loss: 1.6043480411171913\n",
      "Epoch 150: Train Loss: 0.10230445642681683, Validation Loss: 1.7095993012189865\n",
      "Epoch 151: Train Loss: 0.1008840916568742, Validation Loss: 1.5323854461312294\n",
      "Epoch 152: Train Loss: 0.08860316483632606, Validation Loss: 1.7119539976119995\n",
      "Epoch 153: Train Loss: 0.07252155731925193, Validation Loss: 1.8083709329366684\n",
      "Epoch 154: Train Loss: 0.08536321680773706, Validation Loss: 1.7525402307510376\n",
      "Epoch 155: Train Loss: 0.1123396403649274, Validation Loss: 1.6647899597883224\n",
      "Epoch 156: Train Loss: 0.10511958500479951, Validation Loss: 1.6665962487459183\n",
      "Epoch 157: Train Loss: 0.09068525818121784, Validation Loss: 1.6707160025835037\n",
      "Epoch 158: Train Loss: 0.07485593280152363, Validation Loss: 1.6418586671352386\n",
      "Epoch 159: Train Loss: 0.11948978081893395, Validation Loss: 1.6543584987521172\n",
      "Epoch 160: Train Loss: 0.09539742178886253, Validation Loss: 1.9034178107976913\n",
      "Epoch 161: Train Loss: 0.12350731452598292, Validation Loss: 1.5100596100091934\n",
      "Epoch 162: Train Loss: 0.07526473936570041, Validation Loss: 1.4047059267759323\n",
      "Epoch 163: Train Loss: 0.10167677601908937, Validation Loss: 1.5414978116750717\n",
      "Epoch 164: Train Loss: 0.08568136815858238, Validation Loss: 1.4680735021829605\n",
      "Epoch 165: Train Loss: 0.09255541699445423, Validation Loss: 1.5669965744018555\n",
      "Epoch 166: Train Loss: 0.0939455408438602, Validation Loss: 1.5441277027130127\n",
      "Epoch 167: Train Loss: 0.09200730717138332, Validation Loss: 1.5113464891910553\n",
      "Epoch 168: Train Loss: 0.06312563938691336, Validation Loss: 1.5209294110536575\n",
      "Epoch 169: Train Loss: 0.06916597148622661, Validation Loss: 1.5358036905527115\n",
      "Epoch 170: Train Loss: 0.10205924669828485, Validation Loss: 1.5381498485803604\n",
      "Epoch 171: Train Loss: 0.10979448891628314, Validation Loss: 1.5708120167255402\n",
      "Epoch 172: Train Loss: 0.09339413307059337, Validation Loss: 1.6885481402277946\n",
      "Epoch 173: Train Loss: 0.07856562435079147, Validation Loss: 1.6532949060201645\n",
      "Epoch 174: Train Loss: 0.10595641150961027, Validation Loss: 1.617978647351265\n",
      "Epoch 175: Train Loss: 0.0686763850698138, Validation Loss: 1.767562210559845\n",
      "Epoch 176: Train Loss: 0.05172897836960414, Validation Loss: 1.8551172316074371\n",
      "Epoch 177: Train Loss: 0.10514714731834829, Validation Loss: 1.8681397438049316\n",
      "Epoch 178: Train Loss: 0.07753405892564093, Validation Loss: 1.8454041481018066\n",
      "Epoch 179: Train Loss: 0.098887365859221, Validation Loss: 1.9001233577728271\n",
      "Epoch 180: Train Loss: 0.07365141387152321, Validation Loss: 1.7248267978429794\n",
      "Epoch 181: Train Loss: 0.1360937745639068, Validation Loss: 1.6144501119852066\n",
      "Epoch 182: Train Loss: 0.09017754451590865, Validation Loss: 1.6839968115091324\n",
      "Epoch 183: Train Loss: 0.09468908818876919, Validation Loss: 1.6965058594942093\n",
      "Epoch 184: Train Loss: 0.09607539561522357, Validation Loss: 1.6141158491373062\n",
      "Epoch 185: Train Loss: 0.07115652198519777, Validation Loss: 1.556223064661026\n",
      "Epoch 186: Train Loss: 0.09051051412654273, Validation Loss: 1.5539522618055344\n",
      "Epoch 187: Train Loss: 0.10176668480476912, Validation Loss: 1.5588165670633316\n",
      "Epoch 188: Train Loss: 0.09817138648427584, Validation Loss: 1.557571604847908\n",
      "Epoch 189: Train Loss: 0.09748759263140314, Validation Loss: 1.6175914853811264\n",
      "Epoch 190: Train Loss: 0.095178180565948, Validation Loss: 1.745664045214653\n",
      "Epoch 191: Train Loss: 0.0853502711500315, Validation Loss: 1.8103841841220856\n",
      "Epoch 192: Train Loss: 0.09603726633769624, Validation Loss: 1.8280610144138336\n",
      "Epoch 193: Train Loss: 0.07265729084610939, Validation Loss: 1.6443033665418625\n",
      "Epoch 194: Train Loss: 0.07206890644396052, Validation Loss: 1.7210295051336288\n",
      "Epoch 195: Train Loss: 0.07614337674835149, Validation Loss: 1.8056940138339996\n",
      "Epoch 196: Train Loss: 0.08220380210482023, Validation Loss: 1.7673675119876862\n",
      "Epoch 197: Train Loss: 0.08305398024180356, Validation Loss: 1.7857657223939896\n",
      "Epoch 198: Train Loss: 0.06011078608057955, Validation Loss: 1.7582322657108307\n",
      "Epoch 199: Train Loss: 0.06730638277333449, Validation Loss: 1.7473058104515076\n",
      "Fold 2 Metrics:\n",
      "Accuracy: 0.5454545454545454, Precision: 0.5454545454545454, Recall: 1.0, F1-score: 0.7058823529411765, AUC: 0.5\n",
      "Confusion Matrix:\n",
      "[[0 5]\n",
      " [0 6]]\n",
      "Completed fold 2\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples from subject 3 to test set\n",
      "Adding 6 truth samples from subject 3 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 1000)\n",
      "(132, 65, 1000)\n",
      "Epoch 0: Train Loss: 0.7156912474071279, Validation Loss: 0.7073937058448792\n",
      "Epoch 1: Train Loss: 0.6781822127454421, Validation Loss: 0.7431998550891876\n",
      "Epoch 2: Train Loss: 0.6667834590463078, Validation Loss: 0.7297941446304321\n",
      "Epoch 3: Train Loss: 0.6705255157807294, Validation Loss: 0.6833061575889587\n",
      "Epoch 4: Train Loss: 0.6457456736003652, Validation Loss: 0.6712294518947601\n",
      "Epoch 5: Train Loss: 0.6247350082677954, Validation Loss: 0.6636032462120056\n",
      "Epoch 6: Train Loss: 0.6459735877373639, Validation Loss: 0.6609659492969513\n",
      "Epoch 7: Train Loss: 0.6358023110558005, Validation Loss: 0.6575399339199066\n",
      "Epoch 8: Train Loss: 0.6348390894777635, Validation Loss: 0.6567422449588776\n",
      "Epoch 9: Train Loss: 0.6332683843724868, Validation Loss: 0.6575217247009277\n",
      "Epoch 10: Train Loss: 0.6306188334436977, Validation Loss: 0.6523676514625549\n",
      "Epoch 11: Train Loss: 0.6130707404192757, Validation Loss: 0.6822804510593414\n",
      "Epoch 12: Train Loss: 0.6063916472827687, Validation Loss: 0.7135496139526367\n",
      "Epoch 13: Train Loss: 0.5896773373379427, Validation Loss: 0.7408349514007568\n",
      "Epoch 14: Train Loss: 0.5678650859524222, Validation Loss: 0.7880028188228607\n",
      "Epoch 15: Train Loss: 0.5484969388036167, Validation Loss: 0.8285448849201202\n",
      "Epoch 16: Train Loss: 0.5092247584286858, Validation Loss: 0.8747095763683319\n",
      "Epoch 17: Train Loss: 0.5078145440887002, Validation Loss: 0.8888156712055206\n",
      "Epoch 18: Train Loss: 0.5013265995418325, Validation Loss: 0.8963401317596436\n",
      "Epoch 19: Train Loss: 0.5133348265114952, Validation Loss: 0.9000386297702789\n",
      "Epoch 20: Train Loss: 0.49952169376261096, Validation Loss: 1.0036105513572693\n",
      "Epoch 21: Train Loss: 0.4769925983513103, Validation Loss: 1.112976610660553\n",
      "Epoch 22: Train Loss: 0.4257991296403548, Validation Loss: 1.2665697932243347\n",
      "Epoch 23: Train Loss: 0.398258461671717, Validation Loss: 1.2913604974746704\n",
      "Epoch 24: Train Loss: 0.36915888944092917, Validation Loss: 1.3991808891296387\n",
      "Epoch 25: Train Loss: 0.3402116324971704, Validation Loss: 1.427006721496582\n",
      "Epoch 26: Train Loss: 0.30839072080219493, Validation Loss: 1.4743586778640747\n",
      "Epoch 27: Train Loss: 0.29321379784275503, Validation Loss: 1.5624610781669617\n",
      "Epoch 28: Train Loss: 0.31028959943967704, Validation Loss: 1.611305296421051\n",
      "Epoch 29: Train Loss: 0.2732605565996731, Validation Loss: 1.5776513814926147\n",
      "Epoch 30: Train Loss: 0.2951534048599355, Validation Loss: 1.8097606301307678\n",
      "Epoch 31: Train Loss: 0.26864762604236603, Validation Loss: 1.6399877667427063\n",
      "Epoch 32: Train Loss: 0.25387610714225206, Validation Loss: 2.262945830821991\n",
      "Epoch 33: Train Loss: 0.18532994214226217, Validation Loss: 2.3105944991111755\n",
      "Epoch 34: Train Loss: 0.17332858416964025, Validation Loss: 2.365967273712158\n",
      "Epoch 35: Train Loss: 0.17449922552880118, Validation Loss: 2.4109466075897217\n",
      "Epoch 36: Train Loss: 0.1661015998791246, Validation Loss: 2.435335874557495\n",
      "Epoch 37: Train Loss: 0.20248415452592514, Validation Loss: 2.474009692668915\n",
      "Epoch 38: Train Loss: 0.15659339287701776, Validation Loss: 2.4737313985824585\n",
      "Epoch 39: Train Loss: 0.15847409998669343, Validation Loss: 2.498307704925537\n",
      "Epoch 40: Train Loss: 0.13272202672327266, Validation Loss: 2.879538059234619\n",
      "Epoch 41: Train Loss: 0.14907287477570422, Validation Loss: 2.695504069328308\n",
      "Epoch 42: Train Loss: 0.14887589084751465, Validation Loss: 2.5778859853744507\n",
      "Epoch 43: Train Loss: 0.14370745705331073, Validation Loss: 2.9213207960128784\n",
      "Epoch 44: Train Loss: 0.1401446056278313, Validation Loss: 2.8110698461532593\n",
      "Epoch 45: Train Loss: 0.17250659404432073, Validation Loss: 2.8656173944473267\n",
      "Epoch 46: Train Loss: 0.122435666511164, Validation Loss: 3.1604021787643433\n",
      "Epoch 47: Train Loss: 0.1200457215309143, Validation Loss: 3.352270245552063\n",
      "Epoch 48: Train Loss: 0.13123800999978008, Validation Loss: 3.301010489463806\n",
      "Epoch 49: Train Loss: 0.13169398739495697, Validation Loss: 3.405720591545105\n",
      "Epoch 50: Train Loss: 0.1864565779619357, Validation Loss: 3.277134895324707\n",
      "Epoch 51: Train Loss: 0.13688666943241568, Validation Loss: 3.4396722316741943\n",
      "Epoch 52: Train Loss: 0.1160835549235344, Validation Loss: 3.36452853679657\n",
      "Epoch 53: Train Loss: 0.14970988462514737, Validation Loss: 3.482267141342163\n",
      "Epoch 54: Train Loss: 0.12096272102173637, Validation Loss: 3.12744140625\n",
      "Epoch 55: Train Loss: 0.12492995292824857, Validation Loss: 3.5349422693252563\n",
      "Epoch 56: Train Loss: 0.09854851115275831, Validation Loss: 3.5610101222991943\n",
      "Epoch 57: Train Loss: 0.09883593088563751, Validation Loss: 3.649675726890564\n",
      "Epoch 58: Train Loss: 0.1301456669016796, Validation Loss: 3.608977794647217\n",
      "Epoch 59: Train Loss: 0.1237722048535943, Validation Loss: 3.6463531255722046\n",
      "Epoch 60: Train Loss: 0.11296409795827725, Validation Loss: 3.1995863914489746\n",
      "Epoch 61: Train Loss: 0.10340278387507972, Validation Loss: 3.641790270805359\n",
      "Epoch 62: Train Loss: 0.09626519428018261, Validation Loss: 3.994060516357422\n",
      "Epoch 63: Train Loss: 0.11994120911421145, Validation Loss: 3.7753829956054688\n",
      "Epoch 64: Train Loss: 0.11571939136175548, Validation Loss: 4.020834445953369\n",
      "Epoch 65: Train Loss: 0.0968102305470144, Validation Loss: 3.836552619934082\n",
      "Epoch 66: Train Loss: 0.1165187814656426, Validation Loss: 3.6646840572357178\n",
      "Epoch 67: Train Loss: 0.09602461919626769, Validation Loss: 3.9601457118988037\n",
      "Epoch 68: Train Loss: 0.08334469608962536, Validation Loss: 3.743781328201294\n",
      "Epoch 69: Train Loss: 0.09192237011430894, Validation Loss: 3.83567214012146\n",
      "Epoch 70: Train Loss: 0.10464733093976974, Validation Loss: 4.219754576683044\n",
      "Epoch 71: Train Loss: 0.0990260159048964, Validation Loss: 3.782161235809326\n",
      "Epoch 72: Train Loss: 0.09214086970314384, Validation Loss: 3.8272993564605713\n",
      "Epoch 73: Train Loss: 0.08125759283190265, Validation Loss: 4.412397861480713\n",
      "Epoch 74: Train Loss: 0.11149700911825194, Validation Loss: 4.405580163002014\n",
      "Epoch 75: Train Loss: 0.08007838182589587, Validation Loss: 4.50628662109375\n",
      "Epoch 76: Train Loss: 0.10906213015208349, Validation Loss: 4.455384850502014\n",
      "Epoch 77: Train Loss: 0.09323572822134285, Validation Loss: 4.4996854066848755\n",
      "Epoch 78: Train Loss: 0.11382698577226084, Validation Loss: 4.329033255577087\n",
      "Epoch 79: Train Loss: 0.08515244048527058, Validation Loss: 4.338059902191162\n",
      "Epoch 80: Train Loss: 0.07911961098365924, Validation Loss: 4.456062436103821\n",
      "Epoch 81: Train Loss: 0.08002833901521038, Validation Loss: 4.5544339418411255\n",
      "Epoch 82: Train Loss: 0.07939088136395987, Validation Loss: 4.381129264831543\n",
      "Epoch 83: Train Loss: 0.06591348344569697, Validation Loss: 4.014694690704346\n",
      "Epoch 84: Train Loss: 0.15435628253309167, Validation Loss: 4.4860758781433105\n",
      "Epoch 85: Train Loss: 0.06389138003920808, Validation Loss: 4.114908695220947\n",
      "Epoch 86: Train Loss: 0.07288244399516021, Validation Loss: 4.424704074859619\n",
      "Epoch 87: Train Loss: 0.0892684252877884, Validation Loss: 4.544475793838501\n",
      "Epoch 88: Train Loss: 0.08179609915789436, Validation Loss: 4.53598165512085\n",
      "Epoch 89: Train Loss: 0.08638141830177869, Validation Loss: 4.41842246055603\n",
      "Epoch 90: Train Loss: 0.13852968551766345, Validation Loss: 3.586469531059265\n",
      "Epoch 91: Train Loss: 0.10634626256411567, Validation Loss: 3.5029858350753784\n",
      "Epoch 92: Train Loss: 0.09153396414373727, Validation Loss: 3.7747682332992554\n",
      "Epoch 93: Train Loss: 0.11896344997427043, Validation Loss: 4.568478584289551\n",
      "Epoch 94: Train Loss: 0.1184923947953126, Validation Loss: 4.721164703369141\n",
      "Epoch 95: Train Loss: 0.13577438299270236, Validation Loss: 3.9330906867980957\n",
      "Epoch 96: Train Loss: 0.10061444172306973, Validation Loss: 4.278321385383606\n",
      "Epoch 97: Train Loss: 0.08170337246402222, Validation Loss: 4.079726457595825\n",
      "Epoch 98: Train Loss: 0.08187025412917137, Validation Loss: 4.2361027002334595\n",
      "Epoch 99: Train Loss: 0.09276793571189046, Validation Loss: 4.076944351196289\n",
      "Epoch 100: Train Loss: 0.0982883104999714, Validation Loss: 4.7381837368011475\n",
      "Epoch 101: Train Loss: 0.07685527139726807, Validation Loss: 4.851016283035278\n",
      "Epoch 102: Train Loss: 0.12094682285233456, Validation Loss: 4.542046546936035\n",
      "Epoch 103: Train Loss: 0.08730583232553567, Validation Loss: 4.250456094741821\n",
      "Epoch 104: Train Loss: 0.10073753939393688, Validation Loss: 4.512258529663086\n",
      "Epoch 105: Train Loss: 0.08271031706210445, Validation Loss: 4.843031406402588\n",
      "Epoch 106: Train Loss: 0.0936741986695458, Validation Loss: 4.76806640625\n",
      "Epoch 107: Train Loss: 0.07148210709804997, Validation Loss: 4.5660059452056885\n",
      "Epoch 108: Train Loss: 0.0784219898393049, Validation Loss: 4.634766101837158\n",
      "Epoch 109: Train Loss: 0.07231608192052912, Validation Loss: 4.675416588783264\n",
      "Epoch 110: Train Loss: 0.1095294896787142, Validation Loss: 4.44061803817749\n",
      "Epoch 111: Train Loss: 0.08075151200789739, Validation Loss: 4.266384959220886\n",
      "Epoch 112: Train Loss: 0.08330929698422551, Validation Loss: 5.067539215087891\n",
      "Epoch 113: Train Loss: 0.08063193261349465, Validation Loss: 5.124741792678833\n",
      "Epoch 114: Train Loss: 0.09791578998898759, Validation Loss: 5.017686367034912\n",
      "Epoch 115: Train Loss: 0.09356223646660938, Validation Loss: 4.830793619155884\n",
      "Epoch 116: Train Loss: 0.06733197450418682, Validation Loss: 4.843595027923584\n",
      "Epoch 117: Train Loss: 0.08235476396101363, Validation Loss: 4.7168967723846436\n",
      "Epoch 118: Train Loss: 0.0759002284201629, Validation Loss: 4.558532953262329\n",
      "Epoch 119: Train Loss: 0.08214028004337759, Validation Loss: 4.669718503952026\n",
      "Epoch 120: Train Loss: 0.07481167454491644, Validation Loss: 4.836792945861816\n",
      "Epoch 121: Train Loss: 0.06646925358868697, Validation Loss: 5.086871862411499\n",
      "Epoch 122: Train Loss: 0.12814094016657157, Validation Loss: 5.388951778411865\n",
      "Epoch 123: Train Loss: 0.07188331982230439, Validation Loss: 5.144613742828369\n",
      "Epoch 124: Train Loss: 0.07141880210324683, Validation Loss: 4.933348178863525\n",
      "Epoch 125: Train Loss: 0.08530032440253041, Validation Loss: 5.471177339553833\n",
      "Epoch 126: Train Loss: 0.12777452825513833, Validation Loss: 5.639296054840088\n",
      "Epoch 127: Train Loss: 0.06633175237049513, Validation Loss: 5.469383239746094\n",
      "Epoch 128: Train Loss: 0.06621129494434332, Validation Loss: 5.348426103591919\n",
      "Epoch 129: Train Loss: 0.06560978038674768, Validation Loss: 5.499116659164429\n",
      "Epoch 130: Train Loss: 0.07196833335739725, Validation Loss: 4.776542901992798\n",
      "Epoch 131: Train Loss: 0.0813588158446638, Validation Loss: 4.732430458068848\n",
      "Epoch 132: Train Loss: 0.08332674165585023, Validation Loss: 4.95925235748291\n",
      "Epoch 133: Train Loss: 0.08759183896815076, Validation Loss: 4.74231219291687\n",
      "Epoch 134: Train Loss: 0.10378461435218067, Validation Loss: 4.533704042434692\n",
      "Epoch 135: Train Loss: 0.07452540658414364, Validation Loss: 4.731765985488892\n",
      "Epoch 136: Train Loss: 0.0801061126677429, Validation Loss: 4.71314811706543\n",
      "Epoch 137: Train Loss: 0.07735027794671409, Validation Loss: 4.838251352310181\n",
      "Epoch 138: Train Loss: 0.07733486573595334, Validation Loss: 4.660441637039185\n",
      "Epoch 139: Train Loss: 0.07073422641876866, Validation Loss: 4.787349700927734\n",
      "Epoch 140: Train Loss: 0.08278176591128987, Validation Loss: 4.403559684753418\n",
      "Epoch 141: Train Loss: 0.06461446663803037, Validation Loss: 4.896716594696045\n",
      "Epoch 142: Train Loss: 0.09439669697381117, Validation Loss: 5.155607461929321\n",
      "Epoch 143: Train Loss: 0.11207021318156929, Validation Loss: 5.144481182098389\n",
      "Epoch 144: Train Loss: 0.08627276580013774, Validation Loss: 5.024658441543579\n",
      "Epoch 145: Train Loss: 0.07418302857481382, Validation Loss: 4.849193096160889\n",
      "Epoch 146: Train Loss: 0.08920461246195961, Validation Loss: 4.790066957473755\n",
      "Epoch 147: Train Loss: 0.06849217398420852, Validation Loss: 5.010719537734985\n",
      "Epoch 148: Train Loss: 0.0776238719768384, Validation Loss: 4.8120012283325195\n",
      "Epoch 149: Train Loss: 0.04837192459415425, Validation Loss: 4.914680480957031\n",
      "Epoch 150: Train Loss: 0.07190962954807807, Validation Loss: 5.220319032669067\n",
      "Epoch 151: Train Loss: 0.08281529272961266, Validation Loss: 4.818681240081787\n",
      "Epoch 152: Train Loss: 0.07690089104204055, Validation Loss: 5.365617275238037\n",
      "Epoch 153: Train Loss: 0.0701027607326122, Validation Loss: 5.2894697189331055\n",
      "Epoch 154: Train Loss: 0.10550972564584192, Validation Loss: 4.82916784286499\n",
      "Epoch 155: Train Loss: 0.06554470726234071, Validation Loss: 5.523776054382324\n",
      "Epoch 156: Train Loss: 0.08945641932351624, Validation Loss: 5.4020164012908936\n",
      "Epoch 157: Train Loss: 0.07578352401854799, Validation Loss: 5.331254720687866\n",
      "Epoch 158: Train Loss: 0.058180814992417305, Validation Loss: 5.132166504859924\n",
      "Epoch 159: Train Loss: 0.07655335205387981, Validation Loss: 5.452383279800415\n",
      "Epoch 160: Train Loss: 0.0797386870116872, Validation Loss: 5.07219922542572\n",
      "Epoch 161: Train Loss: 0.08577950287829428, Validation Loss: 5.102154493331909\n",
      "Epoch 162: Train Loss: 0.07750096224138842, Validation Loss: 5.0721434354782104\n",
      "Epoch 163: Train Loss: 0.06328990189906429, Validation Loss: 5.3837409019470215\n",
      "Epoch 164: Train Loss: 0.09873723485233153, Validation Loss: 5.120185375213623\n",
      "Epoch 165: Train Loss: 0.05274366532616755, Validation Loss: 5.258150339126587\n",
      "Epoch 166: Train Loss: 0.07083835240508266, Validation Loss: 5.206942319869995\n",
      "Epoch 167: Train Loss: 0.05717062900828965, Validation Loss: 5.445367097854614\n",
      "Epoch 168: Train Loss: 0.07073168118265183, Validation Loss: 5.41409707069397\n",
      "Epoch 169: Train Loss: 0.06444483105202808, Validation Loss: 5.424968719482422\n",
      "Epoch 170: Train Loss: 0.09869299415731803, Validation Loss: 5.6990861892700195\n",
      "Epoch 171: Train Loss: 0.06462858680306989, Validation Loss: 5.864064455032349\n",
      "Epoch 172: Train Loss: 0.08816728028742706, Validation Loss: 5.917598485946655\n",
      "Epoch 173: Train Loss: 0.09324606125900413, Validation Loss: 5.452055811882019\n",
      "Epoch 174: Train Loss: 0.10696083749644458, Validation Loss: 4.795169234275818\n",
      "Epoch 175: Train Loss: 0.10075202225433554, Validation Loss: 5.119018077850342\n",
      "Epoch 176: Train Loss: 0.05225566091180286, Validation Loss: 5.011265397071838\n",
      "Epoch 177: Train Loss: 0.10877329656196867, Validation Loss: 5.31578803062439\n",
      "Epoch 178: Train Loss: 0.04846253007759943, Validation Loss: 5.119308233261108\n",
      "Epoch 179: Train Loss: 0.06075799752793768, Validation Loss: 5.219351649284363\n",
      "Epoch 180: Train Loss: 0.12323574942317517, Validation Loss: 5.527252674102783\n",
      "Epoch 181: Train Loss: 0.06545685304274015, Validation Loss: 5.604765176773071\n",
      "Epoch 182: Train Loss: 0.09879159348030739, Validation Loss: 5.39699912071228\n",
      "Epoch 183: Train Loss: 0.09439812091124408, Validation Loss: 5.51077127456665\n",
      "Epoch 184: Train Loss: 0.08506502556231092, Validation Loss: 5.213905334472656\n",
      "Epoch 185: Train Loss: 0.0801687661339255, Validation Loss: 5.2314136028289795\n",
      "Epoch 186: Train Loss: 0.09986747115823057, Validation Loss: 4.8763604164123535\n",
      "Epoch 187: Train Loss: 0.044127135674523955, Validation Loss: 4.953086853027344\n",
      "Epoch 188: Train Loss: 0.05303014445063822, Validation Loss: 4.824822187423706\n",
      "Epoch 189: Train Loss: 0.07240369228427024, Validation Loss: 5.0037500858306885\n",
      "Epoch 190: Train Loss: 0.0631338037440882, Validation Loss: 4.680325984954834\n",
      "Epoch 191: Train Loss: 0.07561287493444979, Validation Loss: 4.779644250869751\n",
      "Epoch 192: Train Loss: 0.07259112402029774, Validation Loss: 4.841354608535767\n",
      "Epoch 193: Train Loss: 0.0837975626482683, Validation Loss: 4.50516676902771\n",
      "Epoch 194: Train Loss: 0.08334291978355717, Validation Loss: 4.670719146728516\n",
      "Epoch 195: Train Loss: 0.07460776595946621, Validation Loss: 4.638239741325378\n",
      "Epoch 196: Train Loss: 0.06694607382558067, Validation Loss: 4.8617894649505615\n",
      "Epoch 197: Train Loss: 0.06907847953056369, Validation Loss: 5.026396036148071\n",
      "Epoch 198: Train Loss: 0.059531024698277604, Validation Loss: 4.799982309341431\n",
      "Epoch 199: Train Loss: 0.07253709313569262, Validation Loss: 4.9692912101745605\n",
      "Fold 3 Metrics:\n",
      "Accuracy: 0.09090909090909091, Precision: 0.0, Recall: 0.0, F1-score: 0.0, AUC: 0.09999999999999998\n",
      "Confusion Matrix:\n",
      "[[1 4]\n",
      " [6 0]]\n",
      "Completed fold 3\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples from subject 10 to test set\n",
      "Adding 6 truth samples from subject 10 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 1000)\n",
      "(132, 65, 1000)\n",
      "Epoch 0: Train Loss: 0.7180479168891907, Validation Loss: 0.6933778822422028\n",
      "Epoch 1: Train Loss: 0.6812073132571053, Validation Loss: 0.7215862274169922\n",
      "Epoch 2: Train Loss: 0.636967725613538, Validation Loss: 0.733603298664093\n",
      "Epoch 3: Train Loss: 0.6831696699647343, Validation Loss: 0.7335799932479858\n",
      "Epoch 4: Train Loss: 0.6569930911064148, Validation Loss: 0.7307702302932739\n",
      "Epoch 5: Train Loss: 0.6152936430538402, Validation Loss: 0.735443651676178\n",
      "Epoch 6: Train Loss: 0.6156922613873201, Validation Loss: 0.73769211769104\n",
      "Epoch 7: Train Loss: 0.6245215640348547, Validation Loss: 0.739821195602417\n",
      "Epoch 8: Train Loss: 0.6355720197453218, Validation Loss: 0.7386012077331543\n",
      "Epoch 9: Train Loss: 0.6283372605548185, Validation Loss: 0.7391088604927063\n",
      "Epoch 10: Train Loss: 0.6318439315347111, Validation Loss: 0.738986998796463\n",
      "Epoch 11: Train Loss: 0.5979196008513955, Validation Loss: 0.7480872869491577\n",
      "Epoch 12: Train Loss: 0.6264211745823131, Validation Loss: 0.7454714179039001\n",
      "Epoch 13: Train Loss: 0.5962147817892187, Validation Loss: 0.7406179308891296\n",
      "Epoch 14: Train Loss: 0.5971649885177612, Validation Loss: 0.7454600632190704\n",
      "Epoch 15: Train Loss: 0.5833119837676778, Validation Loss: 0.762582004070282\n",
      "Epoch 16: Train Loss: 0.5597304701805115, Validation Loss: 0.7655807435512543\n",
      "Epoch 17: Train Loss: 0.5736453533172607, Validation Loss: 0.7720781862735748\n",
      "Epoch 18: Train Loss: 0.5684492132242989, Validation Loss: 0.7684052586555481\n",
      "Epoch 19: Train Loss: 0.5719717635827906, Validation Loss: 0.766651064157486\n",
      "Epoch 20: Train Loss: 0.5575928968541762, Validation Loss: 0.7584401071071625\n",
      "Epoch 21: Train Loss: 0.5761643613086027, Validation Loss: 0.7746261954307556\n",
      "Epoch 22: Train Loss: 0.5420333935933954, Validation Loss: 0.773715078830719\n",
      "Epoch 23: Train Loss: 0.5321183642920326, Validation Loss: 0.7692228853702545\n",
      "Epoch 24: Train Loss: 0.5232128781430861, Validation Loss: 0.7771996259689331\n",
      "Epoch 25: Train Loss: 0.49181238518041726, Validation Loss: 0.785782665014267\n",
      "Epoch 26: Train Loss: 0.519316645229564, Validation Loss: 0.7994199693202972\n",
      "Epoch 27: Train Loss: 0.5079641587593976, Validation Loss: 0.8061560392379761\n",
      "Epoch 28: Train Loss: 0.5129179989590364, Validation Loss: 0.8053134977817535\n",
      "Epoch 29: Train Loss: 0.4930172562599182, Validation Loss: 0.8080725371837616\n",
      "Epoch 30: Train Loss: 0.5007510553388035, Validation Loss: 0.8098003268241882\n",
      "Epoch 31: Train Loss: 0.474449543391957, Validation Loss: 0.8156228065490723\n",
      "Epoch 32: Train Loss: 0.4752893377752865, Validation Loss: 0.8222329914569855\n",
      "Epoch 33: Train Loss: 0.47028327864759106, Validation Loss: 0.8019499778747559\n",
      "Epoch 34: Train Loss: 0.428296776378856, Validation Loss: 0.7872538864612579\n",
      "Epoch 35: Train Loss: 0.429010131779839, Validation Loss: 0.7839343845844269\n",
      "Epoch 36: Train Loss: 0.4316658167278065, Validation Loss: 0.7720210254192352\n",
      "Epoch 37: Train Loss: 0.41888539405430064, Validation Loss: 0.7797191441059113\n",
      "Epoch 38: Train Loss: 0.41707267130122466, Validation Loss: 0.7781951278448105\n",
      "Epoch 39: Train Loss: 0.413264695335837, Validation Loss: 0.7771506309509277\n",
      "Epoch 40: Train Loss: 0.3897135450559504, Validation Loss: 0.7415647655725479\n",
      "Epoch 41: Train Loss: 0.37798403817064624, Validation Loss: 0.6749916523694992\n",
      "Epoch 42: Train Loss: 0.3785570141147165, Validation Loss: 0.6535138040781021\n",
      "Epoch 43: Train Loss: 0.3178322648300844, Validation Loss: 0.6112065240740776\n",
      "Epoch 44: Train Loss: 0.3089022399748073, Validation Loss: 0.6381361037492752\n",
      "Epoch 45: Train Loss: 0.291566607706687, Validation Loss: 0.6381588019430637\n",
      "Epoch 46: Train Loss: 0.2666226765688728, Validation Loss: 0.617406602948904\n",
      "Epoch 47: Train Loss: 0.2634868209852892, Validation Loss: 0.6013541668653488\n",
      "Epoch 48: Train Loss: 0.26183751050163717, Validation Loss: 0.6098027676343918\n",
      "Epoch 49: Train Loss: 0.23637313439565547, Validation Loss: 0.618297852575779\n",
      "Epoch 50: Train Loss: 0.2549547780962551, Validation Loss: 0.5773112885653973\n",
      "Epoch 51: Train Loss: 0.2329547133515863, Validation Loss: 0.6201112596318126\n",
      "Epoch 52: Train Loss: 0.20925989922355204, Validation Loss: 0.6336971651762724\n",
      "Epoch 53: Train Loss: 0.1817969872671015, Validation Loss: 0.6734849135391414\n",
      "Epoch 54: Train Loss: 0.16736402932335348, Validation Loss: 0.5663926773704588\n",
      "Epoch 55: Train Loss: 0.1482635859180899, Validation Loss: 0.618485800921917\n",
      "Epoch 56: Train Loss: 0.13093889241709428, Validation Loss: 0.6650058021768928\n",
      "Epoch 57: Train Loss: 0.1233869764734717, Validation Loss: 0.6968098937068135\n",
      "Epoch 58: Train Loss: 0.13254321629510207, Validation Loss: 0.7290871664881706\n",
      "Epoch 59: Train Loss: 0.1167451717397746, Validation Loss: 0.7382682303432375\n",
      "Epoch 60: Train Loss: 0.11475515760043088, Validation Loss: 0.7293661357834935\n",
      "Epoch 61: Train Loss: 0.11937572127755951, Validation Loss: 0.8169937008060515\n",
      "Epoch 62: Train Loss: 0.11084429963546641, Validation Loss: 0.8114899925421923\n",
      "Epoch 63: Train Loss: 0.092865489423275, Validation Loss: 0.8886220261920244\n",
      "Epoch 64: Train Loss: 0.09540002683506292, Validation Loss: 0.7778066543396562\n",
      "Epoch 65: Train Loss: 0.07013924233615398, Validation Loss: 0.9350104999030009\n",
      "Epoch 66: Train Loss: 0.06597991101443768, Validation Loss: 0.949826588970609\n",
      "Epoch 67: Train Loss: 0.06390746967757449, Validation Loss: 0.9151940955780447\n",
      "Epoch 68: Train Loss: 0.0640559691716643, Validation Loss: 0.9753199204569682\n",
      "Epoch 69: Train Loss: 0.08085534590132096, Validation Loss: 0.9144640836166218\n",
      "Epoch 70: Train Loss: 0.07715458510553136, Validation Loss: 1.0106793164741248\n",
      "Epoch 71: Train Loss: 0.07139206918723442, Validation Loss: 0.9250147839775309\n",
      "Epoch 72: Train Loss: 0.0670976130401387, Validation Loss: 1.041467423492577\n",
      "Epoch 73: Train Loss: 0.054556935477782696, Validation Loss: 1.182752485270612\n",
      "Epoch 74: Train Loss: 0.05195636861026287, Validation Loss: 1.1931900951312855\n",
      "Epoch 75: Train Loss: 0.03969735926126733, Validation Loss: 1.222697059973143\n",
      "Epoch 76: Train Loss: 0.04181344308616484, Validation Loss: 1.138856075500371\n",
      "Epoch 77: Train Loss: 0.04760830893236048, Validation Loss: 1.2105832157540135\n",
      "Epoch 78: Train Loss: 0.03646868544028086, Validation Loss: 1.1981755502638407\n",
      "Epoch 79: Train Loss: 0.05143471023834804, Validation Loss: 1.1527327865187544\n",
      "Epoch 80: Train Loss: 0.06064177759210853, Validation Loss: 1.3293883334263228\n",
      "Epoch 81: Train Loss: 0.05362949257387834, Validation Loss: 1.2422934236528818\n",
      "Epoch 82: Train Loss: 0.048147942103883797, Validation Loss: 1.1837955614319071\n",
      "Epoch 83: Train Loss: 0.04146257305846495, Validation Loss: 1.5485401910846122\n",
      "Epoch 84: Train Loss: 0.03839361536152223, Validation Loss: 1.5227233391342452\n",
      "Epoch 85: Train Loss: 0.031582852222901935, Validation Loss: 1.535408389318036\n",
      "Epoch 86: Train Loss: 0.037008037297602964, Validation Loss: 1.47166623758676\n",
      "Epoch 87: Train Loss: 0.02630725582404172, Validation Loss: 1.4554303301556502\n",
      "Epoch 88: Train Loss: 0.022559213945094275, Validation Loss: 1.3913930171402171\n",
      "Epoch 89: Train Loss: 0.025236311861697364, Validation Loss: 1.4371203615446575\n",
      "Epoch 90: Train Loss: 0.034081401084275806, Validation Loss: 1.3700680047186324\n",
      "Epoch 91: Train Loss: 0.025719336453167832, Validation Loss: 1.4021145085898752\n",
      "Epoch 92: Train Loss: 0.03254502365256057, Validation Loss: 1.6341907299283776\n",
      "Epoch 93: Train Loss: 0.038509537246735656, Validation Loss: 1.384513770644844\n",
      "Epoch 94: Train Loss: 0.03605499551357592, Validation Loss: 1.559913275854342\n",
      "Epoch 95: Train Loss: 0.0277043724542155, Validation Loss: 1.5230277904774994\n",
      "Epoch 96: Train Loss: 0.023425302961293387, Validation Loss: 1.6118637100080377\n",
      "Epoch 97: Train Loss: 0.02353272806195652, Validation Loss: 1.6609052798739867\n",
      "Epoch 98: Train Loss: 0.022134409751743078, Validation Loss: 1.662307281367248\n",
      "Epoch 99: Train Loss: 0.022347245529732284, Validation Loss: 1.5860355869263003\n",
      "Epoch 100: Train Loss: 0.018296620753758094, Validation Loss: 1.646582880126516\n",
      "Epoch 101: Train Loss: 0.021261016399983096, Validation Loss: 1.7086667767580366\n",
      "Epoch 102: Train Loss: 0.016314362175762653, Validation Loss: 1.6752418268370093\n",
      "Epoch 103: Train Loss: 0.020916150828056475, Validation Loss: 1.7158567307606063\n",
      "Epoch 104: Train Loss: 0.01906033923082492, Validation Loss: 1.582141438517283\n",
      "Epoch 105: Train Loss: 0.01713266659199315, Validation Loss: 1.7541634006774984\n",
      "Epoch 106: Train Loss: 0.022312475121853984, Validation Loss: 1.6499005509031122\n",
      "Epoch 107: Train Loss: 0.0202036818544216, Validation Loss: 1.7086711664742325\n",
      "Epoch 108: Train Loss: 0.01632217221948154, Validation Loss: 1.853426810088422\n",
      "Epoch 109: Train Loss: 0.013490132877931875, Validation Loss: 1.7580637857608963\n",
      "Epoch 110: Train Loss: 0.02470171410480843, Validation Loss: 1.5716658812525566\n",
      "Epoch 111: Train Loss: 0.0209775661556598, Validation Loss: 1.6576433305817773\n",
      "Epoch 112: Train Loss: 0.027838574393707162, Validation Loss: 1.845336099178894\n",
      "Epoch 113: Train Loss: 0.017580527141142416, Validation Loss: 1.7155628998730208\n",
      "Epoch 114: Train Loss: 0.012974079657236444, Validation Loss: 1.6646123924465428\n",
      "Epoch 115: Train Loss: 0.013578233432353419, Validation Loss: 1.6545829970045816\n",
      "Epoch 116: Train Loss: 0.017915848231710055, Validation Loss: 1.5755035476613557\n",
      "Epoch 117: Train Loss: 0.011055171421235976, Validation Loss: 1.7886425651286117\n",
      "Epoch 118: Train Loss: 0.009141363326788825, Validation Loss: 1.789093831770515\n",
      "Epoch 119: Train Loss: 0.013784089858424576, Validation Loss: 1.7871111427975848\n",
      "Epoch 120: Train Loss: 0.0158672645167612, Validation Loss: 1.9766698079401976\n",
      "Epoch 121: Train Loss: 0.01339897296994048, Validation Loss: 1.9139626588912506\n",
      "Epoch 122: Train Loss: 0.008067808360518777, Validation Loss: 1.8470782030308328\n",
      "Epoch 123: Train Loss: 0.00997272351592341, Validation Loss: 1.6440260803792626\n",
      "Epoch 124: Train Loss: 0.014837824369725935, Validation Loss: 1.6995804504385887\n",
      "Epoch 125: Train Loss: 0.011164611438289285, Validation Loss: 1.7490413573723345\n",
      "Epoch 126: Train Loss: 0.00878560753292678, Validation Loss: 1.7394283601443021\n",
      "Epoch 127: Train Loss: 0.006280296389698325, Validation Loss: 1.7477634737224435\n",
      "Epoch 128: Train Loss: 0.014454545603845926, Validation Loss: 1.7484025347876013\n",
      "Epoch 129: Train Loss: 0.0076965099912794195, Validation Loss: 1.7280721449023986\n",
      "Epoch 130: Train Loss: 0.007948360049768406, Validation Loss: 1.82226614134197\n",
      "Epoch 131: Train Loss: 0.011631054885904579, Validation Loss: 1.882270673875155\n",
      "Epoch 132: Train Loss: 0.015259100165327682, Validation Loss: 1.4380798935889771\n",
      "Epoch 133: Train Loss: 0.02820035383817466, Validation Loss: 1.8210635743816965\n",
      "Epoch 134: Train Loss: 0.018451821428778416, Validation Loss: 1.6422846111872786\n",
      "Epoch 135: Train Loss: 0.011720009271384162, Validation Loss: 1.6463137464861575\n",
      "Epoch 136: Train Loss: 0.01226339363219107, Validation Loss: 1.6072150854852225\n",
      "Epoch 137: Train Loss: 0.008054292281432188, Validation Loss: 1.7238839259243832\n",
      "Epoch 138: Train Loss: 0.00821874413968009, Validation Loss: 1.7266869141039933\n",
      "Epoch 139: Train Loss: 0.01188597658320385, Validation Loss: 1.6773897200400825\n",
      "Epoch 140: Train Loss: 0.008366657058050966, Validation Loss: 1.5919691920223613\n",
      "Epoch 141: Train Loss: 0.015999145833227563, Validation Loss: 1.7775527635139952\n",
      "Epoch 142: Train Loss: 0.010370417347396998, Validation Loss: 2.0093605119009226\n",
      "Epoch 143: Train Loss: 0.009964353035149328, Validation Loss: 2.0305272730456636\n",
      "Epoch 144: Train Loss: 0.011923600084093563, Validation Loss: 2.324062601896003\n",
      "Epoch 145: Train Loss: 0.011031356222434518, Validation Loss: 2.014731399671291\n",
      "Epoch 146: Train Loss: 0.007838758509880042, Validation Loss: 2.041051665382838\n",
      "Epoch 147: Train Loss: 0.0105314625235384, Validation Loss: 2.1319859027678376\n",
      "Epoch 148: Train Loss: 0.009406898499411695, Validation Loss: 1.9611773093183729\n",
      "Epoch 149: Train Loss: 0.010962259613306206, Validation Loss: 1.968660056563749\n",
      "Epoch 150: Train Loss: 0.007585675277582863, Validation Loss: 2.0326968032977675\n",
      "Epoch 151: Train Loss: 0.005611330130384029, Validation Loss: 2.070598204795715\n",
      "Epoch 152: Train Loss: 0.009046183094409677, Validation Loss: 2.3418577908278166\n",
      "Epoch 153: Train Loss: 0.0063772632891093105, Validation Loss: 2.4185304042639473\n",
      "Epoch 154: Train Loss: 0.005679494550670771, Validation Loss: 2.1823806956017506\n",
      "Epoch 155: Train Loss: 0.006605511914719553, Validation Loss: 2.4206376270340115\n",
      "Epoch 156: Train Loss: 0.00634268467204974, Validation Loss: 2.1610009270953014\n",
      "Epoch 157: Train Loss: 0.004952930492650279, Validation Loss: 2.132481594784622\n",
      "Epoch 158: Train Loss: 0.004489366999169921, Validation Loss: 2.1010432440225486\n",
      "Epoch 159: Train Loss: 0.0033945745090022683, Validation Loss: 2.2025699217369947\n",
      "Epoch 160: Train Loss: 0.004906937825794825, Validation Loss: 2.099370360266221\n",
      "Epoch 161: Train Loss: 0.006049470589085317, Validation Loss: 2.156390249114338\n",
      "Epoch 162: Train Loss: 0.006053089677794453, Validation Loss: 2.1430962282729524\n",
      "Epoch 163: Train Loss: 0.0045073413605089575, Validation Loss: 2.1071666080379146\n",
      "Epoch 164: Train Loss: 0.004581433707190787, Validation Loss: 2.1583122807760446\n",
      "Epoch 165: Train Loss: 0.003519572598366615, Validation Loss: 2.1546313761873535\n",
      "Epoch 166: Train Loss: 0.0035470864009659957, Validation Loss: 2.0801730751081777\n",
      "Epoch 167: Train Loss: 0.004953213775640025, Validation Loss: 2.088356832530735\n",
      "Epoch 168: Train Loss: 0.019188156313783324, Validation Loss: 2.0377149976138753\n",
      "Epoch 169: Train Loss: 0.005525079870815663, Validation Loss: 2.0989647104106552\n",
      "Epoch 170: Train Loss: 0.0049903393202625655, Validation Loss: 2.4904546248653787\n",
      "Epoch 171: Train Loss: 0.009806384373565806, Validation Loss: 1.9867074886955436\n",
      "Epoch 172: Train Loss: 0.008600879741339561, Validation Loss: 2.0543285606363497\n",
      "Epoch 173: Train Loss: 0.008298648079610704, Validation Loss: 2.2110736171316603\n",
      "Epoch 174: Train Loss: 0.006382034112261061, Validation Loss: 1.965545276797343\n",
      "Epoch 175: Train Loss: 0.00615762618562097, Validation Loss: 2.1868540247239707\n",
      "Epoch 176: Train Loss: 0.005656811634681243, Validation Loss: 2.1034956574314947\n",
      "Epoch 177: Train Loss: 0.003980414602932904, Validation Loss: 2.235101521005845\n",
      "Epoch 178: Train Loss: 0.003844971888397327, Validation Loss: 2.1337003111680133\n",
      "Epoch 179: Train Loss: 0.0031325345900019303, Validation Loss: 2.1787906288989234\n",
      "Epoch 180: Train Loss: 0.005016659037210047, Validation Loss: 2.104538718738695\n",
      "Epoch 181: Train Loss: 0.002410974439151366, Validation Loss: 2.328242996809422\n",
      "Epoch 182: Train Loss: 0.004028863554327365, Validation Loss: 2.5727671181448386\n",
      "Epoch 183: Train Loss: 0.0026049241800244678, Validation Loss: 2.4701496755697008\n",
      "Epoch 184: Train Loss: 0.004146879199919675, Validation Loss: 2.3978270290444925\n",
      "Epoch 185: Train Loss: 0.0025721674113917876, Validation Loss: 2.4053045709515573\n",
      "Epoch 186: Train Loss: 0.0022725069053087603, Validation Loss: 2.2325630186742274\n",
      "Epoch 187: Train Loss: 0.002837648260859115, Validation Loss: 2.2718009550453644\n",
      "Epoch 188: Train Loss: 0.0030042205334586255, Validation Loss: 2.322698573180787\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 345\u001b[0m\n\u001b[0;32m    342\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model\u001b[39;00m\n\u001b[1;32m--> 345\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;66;03m# Evaluate on the test set and calculate metrics\u001b[39;00m\n\u001b[0;32m    348\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[1;32mIn[17], line 232\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(train_loader, val_loader, y_train)\u001b[0m\n\u001b[0;32m    230\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[0;32m    231\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 232\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m     running_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    235\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m running_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\detectlie\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\detectlie\\Lib\\site-packages\\torch\\optim\\optimizer.py:379\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    378\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 379\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# call optimizer step pre hooks\u001b[39;49;00m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_global_optimizer_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\detectlie\\Lib\\site-packages\\torch\\autograd\\profiler.py:605\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 605\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\detectlie\\Lib\\site-packages\\torch\\_ops.py:854\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(self_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    847\u001b[0m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[0;32m    848\u001b[0m     \u001b[38;5;66;03m# named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "model_save_dir = r'C:\\Users\\User\\Documents\\Lie detect data\\Model'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion Matrix'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='viridis', xticklabels=['lie', 'truth'], yticklabels=['lie', 'truth'])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.show()\n",
    "\n",
    "# Define a function to load and pad data\n",
    "def load_data(data_dir, max_length):\n",
    "    subject_data = {'lie': {}, 'truth': {}}\n",
    "    \n",
    "    file_list = os.listdir(data_dir)\n",
    "    \n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Determine if the file is 'truth' or 'lie'\n",
    "        label_type = 'truth' if 'truth' in file else 'lie'\n",
    "        subj_id = int(file.split('_')[-1].split('.')[0])\n",
    "        \n",
    "        # Grouping logic\n",
    "        if label_type == 'lie':\n",
    "            # Mapping each 5 lie samples to one subject\n",
    "            subject_key = (subj_id - 1) // 5 + 1\n",
    "        else:  # 'truth'\n",
    "            # Mapping each 6 truth samples to one subject\n",
    "            subject_key = (subj_id - 1) // 6 + 1\n",
    "            \n",
    "        # Initialize the subject's list if it doesn't exist\n",
    "        if subject_key not in subject_data[label_type]:\n",
    "            subject_data[label_type][subject_key] = []\n",
    "            \n",
    "\n",
    "        \"\"\"\n",
    "        # Extract only the portion of the data from time length 3000 to 3750\n",
    "        data_subset = data[:, 3000:3750]\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        # Pad or truncate the data to match max_length\n",
    "        if data.shape[1] > max_length:\n",
    "            processed_data = data[:, :max_length]  # Truncate if it exceeds max_length\n",
    "        else:\n",
    "            processed_data = np.zeros((data.shape[0], max_length))\n",
    "            processed_data[:, :data.shape[1]] = data  # Pad if it is shorter than max_length\n",
    "        \n",
    "        # Add the processed data to the appropriate list\n",
    "        subject_data[label_type][subject_key].append(processed_data)\n",
    "    \n",
    "    return subject_data\n",
    "\n",
    "# Load dataset and pad/truncate the data\n",
    "data_dir = \"C:\\\\Users\\\\User\\\\Documents\\\\Lie detect data\\\\7M_EEGData\"\n",
    "max_length = 250 # Define maximum length for padding\n",
    "subject_data = load_data(data_dir, max_length)\n",
    "\n",
    "# Count the total number of samples\n",
    "num_lie_samples = sum(len(subject_data['lie'][subject_key]) for subject_key in subject_data['lie'])\n",
    "num_truth_samples = sum(len(subject_data['truth'][subject_key]) for subject_key in subject_data['truth'])\n",
    "\n",
    "print(f\"Number of 'lie' samples: {num_lie_samples}\")\n",
    "print(f\"Number of 'truth' samples: {num_truth_samples}\")\n",
    "print(f\"Total number of samples: {num_lie_samples + num_truth_samples}\")\n",
    "\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure the data is reshaped to [1, Chans, Samples]\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# Define EEGNet model\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int, kernel_size: tuple, padding: tuple = 0):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.depthwise_conv = nn.Conv2d(self.c_in, self.c_in, kernel_size=self.kernel_size,\n",
    "                                        padding=self.padding, groups=self.c_in)\n",
    "        self.conv2d_1x1 = nn.Conv2d(self.c_in, self.c_out, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y = self.depthwise_conv(x)\n",
    "        y = self.conv2d_1x1(y)\n",
    "        return y\n",
    "\n",
    "class SeparableConv1d(nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int, kernel_size: tuple, padding: tuple = 0):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.depthwise_conv = nn.Conv1d(self.c_in, self.c_in, kernel_size=self.kernel_size,\n",
    "                                        padding=self.padding, groups=self.c_in)\n",
    "        self.conv1d_1x1 = nn.Conv1d(self.c_in, self.c_out, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y = self.depthwise_conv(x)\n",
    "        y = self.conv1d_1x1(y)\n",
    "        return y\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, nb_classes: int = 2, Chans: int = 65, Samples: int = 250,\n",
    "                 dropoutRate: float = 0.5, kernLength: int = 125,\n",
    "                 F1:int = 8, D:int = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        F2 = F1 * D\n",
    "\n",
    "        # Make kernel size and odd number\n",
    "        try:\n",
    "            assert kernLength % 2 != 0\n",
    "        except AssertionError:\n",
    "            raise ValueError(\"ERROR: kernLength must be odd number\")\n",
    "\n",
    "        # In: (B, Chans, Samples, 1)\n",
    "        # Out: (B, F1, Samples, 1)\n",
    "        self.conv1 = nn.Conv1d(Chans, F1, kernLength, padding=(kernLength // 2))\n",
    "        self.bn1 = nn.BatchNorm1d(F1) # (B, F1, Samples, 1)\n",
    "        # In: (B, F1, Samples, 1)\n",
    "        # Out: (B, F2, Samples - Chans + 1, 1)\n",
    "        self.conv2 = nn.Conv1d(F1, F2, Chans, groups=F1)\n",
    "        self.bn2 = nn.BatchNorm1d(F2) # (B, F2, Samples - Chans + 1, 1)\n",
    "        # In: (B, F2, Samples - Chans + 1, 1)\n",
    "        # Out: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        self.avg_pool = nn.AvgPool1d(4)\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "\n",
    "        # In: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        # Out: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        self.conv3 = SeparableConv1d(F2, F2, kernel_size=31, padding=15)\n",
    "        self.bn3 = nn.BatchNorm1d(F2)\n",
    "        # In: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        # Out: (B, F2, (Samples - Chans + 1) / 32, 1)\n",
    "        self.avg_pool2 = nn.AvgPool1d(8)\n",
    "        # In: (B, F2 *  (Samples - Chans + 1) / 32)\n",
    "        self.fc = nn.Linear(F2 * ((Samples - Chans + 1) // 32), nb_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Block 1\n",
    "        y1 = self.conv1(x)\n",
    "        #print(\"conv1: \", y1.shape)\n",
    "        y1 = self.bn1(y1)\n",
    "        #print(\"bn1: \", y1.shape)\n",
    "        y1 = self.conv2(y1)\n",
    "        #print(\"conv2\", y1.shape)\n",
    "        y1 = F.elu(self.bn2(y1))\n",
    "        #print(\"bn2\", y1.shape)\n",
    "        y1 = self.avg_pool(y1)\n",
    "        #print(\"avg_pool\", y1.shape)\n",
    "        y1 = self.dropout(y1)\n",
    "        #print(\"dropout\", y1.shape)\n",
    "\n",
    "        # Block 2\n",
    "        y2 = self.conv3(y1)\n",
    "        #print(\"conv3\", y2.shape)\n",
    "        y2 = F.elu(self.bn3(y2))\n",
    "        #print(\"bn3\", y2.shape)\n",
    "        y2 = self.avg_pool2(y2)\n",
    "        #print(\"avg_pool2\", y2.shape)\n",
    "        y2 = self.dropout(y2)\n",
    "        #print(\"dropout\", y2.shape)\n",
    "        y2 = torch.flatten(y2, 1)\n",
    "        #print(\"flatten\", y2.shape)\n",
    "        y2 = self.fc(y2)\n",
    "        #print(\"fc\", y2.shape)\n",
    "\n",
    "        return y2\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_and_evaluate(train_loader, val_loader, y_train):\n",
    "    model = EEGNet().to(device)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "    num_epochs = 200\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 200\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "            fold_model_path = os.path.join(model_save_dir, f'revise_model_fold_{fold_idx}.pth')\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': best_val_loss,\n",
    "            }, fold_model_path)\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize the cross-validation\n",
    "subject_ids = list(range(1, 14))  # Since there are 13 subjects for lie and truth\n",
    "random.shuffle(subject_ids)  # Shuffle to ensure random selection\n",
    "\n",
    "# Initialize arrays to store all labels, predictions, and metrics\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "fold_accuracies = []\n",
    "fold_precisions = []\n",
    "fold_recalls = []\n",
    "fold_f1_scores = []\n",
    "fold_aucs = []\n",
    "fold_conf_matrices = []\n",
    "\n",
    "for fold_idx in range(13):\n",
    "    # Split subjects into test and training sets\n",
    "    test_subject = subject_ids[fold_idx]\n",
    "\n",
    "    # Prepare training and test data\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for subject_id in subject_ids:\n",
    "        lie_samples = subject_data['lie'].get(subject_id, [])\n",
    "        truth_samples = subject_data['truth'].get(subject_id, [])\n",
    "\n",
    "        if subject_id == test_subject:\n",
    "            X_test.extend(lie_samples)\n",
    "            y_test.extend([0] * len(lie_samples))\n",
    "            print(f\"Adding {len(lie_samples)} lie samples from subject {subject_id} to test set\")\n",
    "            X_test.extend(truth_samples)\n",
    "            y_test.extend([1] * len(truth_samples))\n",
    "            print(f\"Adding {len(truth_samples)} truth samples from subject {subject_id} to test set\")\n",
    "        else:\n",
    "            X_train.extend(lie_samples)\n",
    "            y_train.extend([0] * len(lie_samples))\n",
    "            X_train.extend(truth_samples)\n",
    "            y_train.extend([1] * len(truth_samples))\n",
    "            print(f\"Adding {len(lie_samples)} lie samples and {len(truth_samples)} truth samples from subject {subject_id} to train set\")\n",
    "\n",
    "    print(f\"Total training samples: {len(X_train)}, Total test samples: {len(X_test)}\")\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    # Normalize data using scaler fitted on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_test = scaler.transform(X_test.reshape(X_test.shape[0], -1))\n",
    "    X_train = X_train.reshape(-1, 65, max_length)\n",
    "    X_test = X_test.reshape(-1, 65, max_length)\n",
    "\n",
    "    print(X_test.shape)\n",
    "    print(X_train.shape)\n",
    "\n",
    "    # Save the scaler to a file\n",
    "    with open(r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\RevisedEEGNet_scaler_{}.pkl'.format(fold_idx), 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = EEGDataset(X_train, y_train)\n",
    "    test_dataset = EEGDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    model = train_and_evaluate(train_loader, test_loader, y_train)\n",
    "\n",
    "    # Evaluate on the test set and calculate metrics\n",
    "    model.eval()\n",
    "    fold_labels = []\n",
    "    fold_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            fold_labels.extend(y_batch.cpu().numpy())\n",
    "            fold_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics for the current fold\n",
    "    accuracy = accuracy_score(fold_labels, fold_predictions)\n",
    "    precision = precision_score(fold_labels, fold_predictions)\n",
    "    recall = recall_score(fold_labels, fold_predictions)\n",
    "    f1 = f1_score(fold_labels, fold_predictions)\n",
    "    auc = roc_auc_score(fold_labels, fold_predictions)\n",
    "    conf_matrix = confusion_matrix(fold_labels, fold_predictions)\n",
    "\n",
    "    # Store fold metrics\n",
    "    fold_accuracies.append(accuracy)\n",
    "    fold_precisions.append(precision)\n",
    "    fold_recalls.append(recall)\n",
    "    fold_f1_scores.append(f1)\n",
    "    fold_aucs.append(auc)\n",
    "    fold_conf_matrices.append(conf_matrix)\n",
    "\n",
    "    # Print metrics for the current fold\n",
    "    print(f'Fold {fold_idx + 1} Metrics:')\n",
    "    print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Aggregate all labels and predictions for final evaluation across all folds\n",
    "    all_labels.extend(fold_labels)\n",
    "    all_predictions.extend(fold_predictions)\n",
    "\n",
    "    print(f'Completed fold {fold_idx + 1}')\n",
    "    print('-' * 50)\n",
    "\n",
    "# Calculate overall metrics across all folds\n",
    "overall_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "overall_precision = precision_score(all_labels, all_predictions)\n",
    "overall_recall = recall_score(all_labels, all_predictions)\n",
    "overall_f1 = f1_score(all_labels, all_predictions)\n",
    "overall_auc = roc_auc_score(all_labels, all_predictions)\n",
    "overall_conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(f'Overall Metrics:')\n",
    "print(f'Accuracy: {overall_accuracy}, Precision: {overall_precision}, Recall: {overall_recall}, F1-score: {overall_f1}, AUC: {overall_auc}')\n",
    "print('Overall Confusion Matrix:')\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "plot_confusion_matrix(overall_conf_matrix, title='Overall Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae639325-1800-4763-ad37-b5931e4391c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abbb309-50a0-43fa-bcc0-05e6d62e6cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
