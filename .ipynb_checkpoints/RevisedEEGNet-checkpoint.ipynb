{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29d5ee57-cf0b-4330-985e-ada385863b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'lie' samples: 65\n",
      "Number of 'truth' samples: 78\n",
      "Total number of samples: 143\n",
      "Adding 5 lie samples from subject 8 to test set\n",
      "Adding 6 truth samples from subject 8 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 500)\n",
      "(132, 65, 500)\n",
      "Epoch 0: Train Loss: 0.7792740225791931, Validation Loss: 0.6945955157279968\n",
      "Epoch 1: Train Loss: 0.7596580147743225, Validation Loss: 0.6965886950492859\n",
      "Epoch 2: Train Loss: 0.7139911770820617, Validation Loss: 0.6991142630577087\n",
      "Epoch 3: Train Loss: 0.7236616015434265, Validation Loss: 0.7019389867782593\n",
      "Epoch 4: Train Loss: 0.7117868661880493, Validation Loss: 0.7043501734733582\n",
      "Epoch 5: Train Loss: 0.7031640648841858, Validation Loss: 0.7060231566429138\n",
      "Epoch 6: Train Loss: 0.763560676574707, Validation Loss: 0.7077862024307251\n",
      "Epoch 7: Train Loss: 0.7009199500083924, Validation Loss: 0.7076488733291626\n",
      "Epoch 8: Train Loss: 0.6890848875045776, Validation Loss: 0.7090969681739807\n",
      "Epoch 9: Train Loss: 0.7404416561126709, Validation Loss: 0.7098680138587952\n",
      "Epoch 10: Train Loss: 0.7166086554527282, Validation Loss: 0.7115715742111206\n",
      "Epoch 11: Train Loss: 0.6899951457977295, Validation Loss: 0.7124929428100586\n",
      "Epoch 12: Train Loss: 0.6897172212600708, Validation Loss: 0.7122076749801636\n",
      "Epoch 13: Train Loss: 0.750146484375, Validation Loss: 0.7130354642868042\n",
      "Epoch 14: Train Loss: 0.7069889187812806, Validation Loss: 0.7122679948806763\n",
      "Epoch 15: Train Loss: 0.7275263547897339, Validation Loss: 0.7124959230422974\n",
      "Epoch 16: Train Loss: 0.7139515399932861, Validation Loss: 0.7130450010299683\n",
      "Epoch 17: Train Loss: 0.784557831287384, Validation Loss: 0.7142448425292969\n",
      "Epoch 18: Train Loss: 0.7205701470375061, Validation Loss: 0.7149680852890015\n",
      "Epoch 19: Train Loss: 0.724947190284729, Validation Loss: 0.7152068018913269\n",
      "Epoch 20: Train Loss: 0.6891585230827332, Validation Loss: 0.7162513732910156\n",
      "Epoch 21: Train Loss: 0.6836889386177063, Validation Loss: 0.7171958684921265\n",
      "Epoch 22: Train Loss: 0.6942896008491516, Validation Loss: 0.7187814116477966\n",
      "Epoch 23: Train Loss: 0.7140423059463501, Validation Loss: 0.718985378742218\n",
      "Epoch 24: Train Loss: 0.6984142303466797, Validation Loss: 0.7192646265029907\n",
      "Epoch 25: Train Loss: 0.6729378581047059, Validation Loss: 0.7197089195251465\n",
      "Epoch 26: Train Loss: 0.772995388507843, Validation Loss: 0.7197203636169434\n",
      "Epoch 27: Train Loss: 0.6700331449508667, Validation Loss: 0.7195833921432495\n",
      "Epoch 28: Train Loss: 0.6683362185955047, Validation Loss: 0.7205719351768494\n",
      "Epoch 29: Train Loss: 0.6909751772880555, Validation Loss: 0.7191459536552429\n",
      "Epoch 30: Train Loss: 0.7039472222328186, Validation Loss: 0.7208382487297058\n",
      "Epoch 31: Train Loss: 0.6598507046699524, Validation Loss: 0.7216355204582214\n",
      "Epoch 32: Train Loss: 0.6921362042427063, Validation Loss: 0.7233102917671204\n",
      "Epoch 33: Train Loss: 0.6902746438980103, Validation Loss: 0.7241224050521851\n",
      "Epoch 34: Train Loss: 0.7330500364303589, Validation Loss: 0.7237034440040588\n",
      "Epoch 35: Train Loss: 0.6975308775901794, Validation Loss: 0.7251994013786316\n",
      "Epoch 36: Train Loss: 0.7197994709014892, Validation Loss: 0.7270109057426453\n",
      "Epoch 37: Train Loss: 0.6714549779891967, Validation Loss: 0.7262614369392395\n",
      "Epoch 38: Train Loss: 0.7210677266120911, Validation Loss: 0.727357804775238\n",
      "Epoch 39: Train Loss: 0.6870785474777221, Validation Loss: 0.7257038950920105\n",
      "Epoch 40: Train Loss: 0.737228798866272, Validation Loss: 0.727290153503418\n",
      "Epoch 41: Train Loss: 0.7324815273284913, Validation Loss: 0.7275227904319763\n",
      "Epoch 42: Train Loss: 0.6715953946113586, Validation Loss: 0.7266936302185059\n",
      "Epoch 43: Train Loss: 0.7159088969230651, Validation Loss: 0.7276800274848938\n",
      "Epoch 44: Train Loss: 0.7258575320243835, Validation Loss: 0.728520929813385\n",
      "Epoch 45: Train Loss: 0.7153339624404907, Validation Loss: 0.7287459373474121\n",
      "Epoch 46: Train Loss: 0.6970207214355468, Validation Loss: 0.7291447520256042\n",
      "Epoch 47: Train Loss: 0.6803097367286682, Validation Loss: 0.7300452589988708\n",
      "Epoch 48: Train Loss: 0.654299920797348, Validation Loss: 0.7252208590507507\n",
      "Epoch 49: Train Loss: 0.6801945805549622, Validation Loss: 0.7271642088890076\n",
      "Epoch 50: Train Loss: 0.6660872340202332, Validation Loss: 0.7251317501068115\n",
      "Epoch 51: Train Loss: 0.6607329487800598, Validation Loss: 0.728044867515564\n",
      "Epoch 52: Train Loss: 0.7000179052352905, Validation Loss: 0.7282020449638367\n",
      "Epoch 53: Train Loss: 0.6547900974750519, Validation Loss: 0.7312570214271545\n",
      "Epoch 54: Train Loss: 0.7140556931495666, Validation Loss: 0.7318423390388489\n",
      "Epoch 55: Train Loss: 0.6907824277877808, Validation Loss: 0.730638861656189\n",
      "Epoch 56: Train Loss: 0.7044667840003968, Validation Loss: 0.7324521541595459\n",
      "Epoch 57: Train Loss: 0.6992588758468627, Validation Loss: 0.7318148612976074\n",
      "Epoch 58: Train Loss: 0.7352511405944824, Validation Loss: 0.7317622900009155\n",
      "Epoch 59: Train Loss: 0.6966615200042725, Validation Loss: 0.7330382466316223\n",
      "Epoch 60: Train Loss: 0.6544007062911987, Validation Loss: 0.7340291738510132\n",
      "Epoch 61: Train Loss: 0.6985198616981506, Validation Loss: 0.7356609106063843\n",
      "Epoch 62: Train Loss: 0.7235399484634399, Validation Loss: 0.7371640801429749\n",
      "Epoch 63: Train Loss: 0.6869420409202576, Validation Loss: 0.7380343675613403\n",
      "Epoch 64: Train Loss: 0.7345002293586731, Validation Loss: 0.7374942302703857\n",
      "Epoch 65: Train Loss: 0.6688920736312867, Validation Loss: 0.7381791472434998\n",
      "Epoch 66: Train Loss: 0.7244398593902588, Validation Loss: 0.7383414506912231\n",
      "Epoch 67: Train Loss: 0.6652737140655518, Validation Loss: 0.7384235262870789\n",
      "Epoch 68: Train Loss: 0.6499807715415955, Validation Loss: 0.7369372844696045\n",
      "Epoch 69: Train Loss: 0.6951274633407593, Validation Loss: 0.7377513647079468\n",
      "Epoch 70: Train Loss: 0.6784669756889343, Validation Loss: 0.7386035323143005\n",
      "Epoch 71: Train Loss: 0.7070622563362121, Validation Loss: 0.7400197386741638\n",
      "Epoch 72: Train Loss: 0.6773844718933105, Validation Loss: 0.7397521734237671\n",
      "Epoch 73: Train Loss: 0.7457334041595459, Validation Loss: 0.7372479438781738\n",
      "Epoch 74: Train Loss: 0.6974759340286255, Validation Loss: 0.7352105975151062\n",
      "Epoch 75: Train Loss: 0.6418350815773011, Validation Loss: 0.7357274889945984\n",
      "Epoch 76: Train Loss: 0.6384725928306579, Validation Loss: 0.7371617555618286\n",
      "Epoch 77: Train Loss: 0.6629204392433167, Validation Loss: 0.738797664642334\n",
      "Epoch 78: Train Loss: 0.6826916337013245, Validation Loss: 0.7367189526557922\n",
      "Epoch 79: Train Loss: 0.666746985912323, Validation Loss: 0.7376706004142761\n",
      "Epoch 80: Train Loss: 0.6820573806762695, Validation Loss: 0.739117443561554\n",
      "Epoch 81: Train Loss: 0.7117091298103333, Validation Loss: 0.74265056848526\n",
      "Epoch 82: Train Loss: 0.651410722732544, Validation Loss: 0.7423725128173828\n",
      "Epoch 83: Train Loss: 0.6730820298194885, Validation Loss: 0.7428949475288391\n",
      "Epoch 84: Train Loss: 0.6779059529304504, Validation Loss: 0.740129292011261\n",
      "Epoch 85: Train Loss: 0.662909746170044, Validation Loss: 0.7409650683403015\n",
      "Epoch 86: Train Loss: 0.6958484292030335, Validation Loss: 0.7375362515449524\n",
      "Epoch 87: Train Loss: 0.6755926132202148, Validation Loss: 0.7386980056762695\n",
      "Epoch 88: Train Loss: 0.6847707629203796, Validation Loss: 0.7406097054481506\n",
      "Epoch 89: Train Loss: 0.6528402805328369, Validation Loss: 0.7416620254516602\n",
      "Epoch 90: Train Loss: 0.6405498504638671, Validation Loss: 0.7426300048828125\n",
      "Epoch 91: Train Loss: 0.6672055125236511, Validation Loss: 0.742493212223053\n",
      "Epoch 92: Train Loss: 0.629889577627182, Validation Loss: 0.7364688515663147\n",
      "Epoch 93: Train Loss: 0.7118046641349792, Validation Loss: 0.7370632290840149\n",
      "Epoch 94: Train Loss: 0.7378306269645691, Validation Loss: 0.7393399477005005\n",
      "Epoch 95: Train Loss: 0.6568819999694824, Validation Loss: 0.7419797778129578\n",
      "Epoch 96: Train Loss: 0.6538139581680298, Validation Loss: 0.7432742714881897\n",
      "Epoch 97: Train Loss: 0.6613409996032715, Validation Loss: 0.7415128350257874\n",
      "Epoch 98: Train Loss: 0.6537705302238465, Validation Loss: 0.7432193756103516\n",
      "Epoch 99: Train Loss: 0.6920864939689636, Validation Loss: 0.7431719899177551\n",
      "Epoch 100: Train Loss: 0.7209731817245484, Validation Loss: 0.7425794005393982\n",
      "Epoch 101: Train Loss: 0.7107424736022949, Validation Loss: 0.7442264556884766\n",
      "Epoch 102: Train Loss: 0.7075617909431458, Validation Loss: 0.7447057366371155\n",
      "Epoch 103: Train Loss: 0.7197710037231445, Validation Loss: 0.7457664608955383\n",
      "Epoch 104: Train Loss: 0.6926559805870056, Validation Loss: 0.7462993264198303\n",
      "Epoch 105: Train Loss: 0.6700116753578186, Validation Loss: 0.7461970448493958\n",
      "Epoch 106: Train Loss: 0.6896212220191955, Validation Loss: 0.746174156665802\n",
      "Epoch 107: Train Loss: 0.6932116389274597, Validation Loss: 0.7470402717590332\n",
      "Epoch 108: Train Loss: 0.6634037375450135, Validation Loss: 0.7441830635070801\n",
      "Epoch 109: Train Loss: 0.6405224442481995, Validation Loss: 0.7453410029411316\n",
      "Epoch 110: Train Loss: 0.6849368333816528, Validation Loss: 0.7448335289955139\n",
      "Epoch 111: Train Loss: 0.6604578018188476, Validation Loss: 0.746765673160553\n",
      "Epoch 112: Train Loss: 0.691038715839386, Validation Loss: 0.7488868832588196\n",
      "Epoch 113: Train Loss: 0.6596280455589294, Validation Loss: 0.7511051297187805\n",
      "Epoch 114: Train Loss: 0.726842737197876, Validation Loss: 0.7502161860466003\n",
      "Epoch 115: Train Loss: 0.6911164164543152, Validation Loss: 0.7494041323661804\n",
      "Epoch 116: Train Loss: 0.6766407370567322, Validation Loss: 0.7505706548690796\n",
      "Epoch 117: Train Loss: 0.6761206030845642, Validation Loss: 0.750764012336731\n",
      "Epoch 118: Train Loss: 0.6500256776809692, Validation Loss: 0.7500504851341248\n",
      "Epoch 119: Train Loss: 0.6581282138824462, Validation Loss: 0.7491491436958313\n",
      "Epoch 120: Train Loss: 0.6701531767845154, Validation Loss: 0.7510466575622559\n",
      "Epoch 121: Train Loss: 0.6494334816932679, Validation Loss: 0.750450611114502\n",
      "Epoch 122: Train Loss: 0.6911085367202758, Validation Loss: 0.7491811513900757\n",
      "Epoch 123: Train Loss: 0.6247494220733643, Validation Loss: 0.7510079741477966\n",
      "Epoch 124: Train Loss: 0.6559316515922546, Validation Loss: 0.7514805793762207\n",
      "Epoch 125: Train Loss: 0.6521984457969665, Validation Loss: 0.7495894432067871\n",
      "Epoch 126: Train Loss: 0.6465913534164429, Validation Loss: 0.7486565709114075\n",
      "Epoch 127: Train Loss: 0.6569532990455628, Validation Loss: 0.7469989657402039\n",
      "Epoch 128: Train Loss: 0.6882372260093689, Validation Loss: 0.7497323751449585\n",
      "Epoch 129: Train Loss: 0.6725672960281373, Validation Loss: 0.7525124549865723\n",
      "Epoch 130: Train Loss: 0.6530059814453125, Validation Loss: 0.7522507905960083\n",
      "Epoch 131: Train Loss: 0.6706465721130371, Validation Loss: 0.7527762651443481\n",
      "Epoch 132: Train Loss: 0.609127151966095, Validation Loss: 0.754657506942749\n",
      "Epoch 133: Train Loss: 0.6742014527320862, Validation Loss: 0.7545449137687683\n",
      "Epoch 134: Train Loss: 0.6585233926773071, Validation Loss: 0.7556374669075012\n",
      "Epoch 135: Train Loss: 0.6887515187263489, Validation Loss: 0.7559959292411804\n",
      "Epoch 136: Train Loss: 0.6306329667568207, Validation Loss: 0.7549243569374084\n",
      "Epoch 137: Train Loss: 0.6396635174751282, Validation Loss: 0.7560754418373108\n",
      "Epoch 138: Train Loss: 0.7040510654449463, Validation Loss: 0.751096248626709\n",
      "Epoch 139: Train Loss: 0.7231759786605835, Validation Loss: 0.7524993419647217\n",
      "Epoch 140: Train Loss: 0.6740080118179321, Validation Loss: 0.7531521916389465\n",
      "Epoch 141: Train Loss: 0.6640742301940918, Validation Loss: 0.7479329705238342\n",
      "Epoch 142: Train Loss: 0.6881643414497376, Validation Loss: 0.7474177479743958\n",
      "Epoch 143: Train Loss: 0.7153406500816345, Validation Loss: 0.7525146007537842\n",
      "Epoch 144: Train Loss: 0.6361459612846374, Validation Loss: 0.7529180645942688\n",
      "Epoch 145: Train Loss: 0.6759566783905029, Validation Loss: 0.7541493773460388\n",
      "Epoch 146: Train Loss: 0.6431604862213135, Validation Loss: 0.7575995922088623\n",
      "Epoch 147: Train Loss: 0.6606825828552246, Validation Loss: 0.7600827217102051\n",
      "Epoch 148: Train Loss: 0.676641845703125, Validation Loss: 0.7607056498527527\n",
      "Epoch 149: Train Loss: 0.6519906401634217, Validation Loss: 0.7569015622138977\n",
      "Epoch 150: Train Loss: 0.6866036653518677, Validation Loss: 0.7514653205871582\n",
      "Epoch 151: Train Loss: 0.6667421340942383, Validation Loss: 0.7549278140068054\n",
      "Epoch 152: Train Loss: 0.7035825490951538, Validation Loss: 0.7582589983940125\n",
      "Epoch 153: Train Loss: 0.652888834476471, Validation Loss: 0.7500762343406677\n",
      "Epoch 154: Train Loss: 0.6740107536315918, Validation Loss: 0.7491998076438904\n",
      "Epoch 155: Train Loss: 0.6591073632240295, Validation Loss: 0.7501533627510071\n",
      "Epoch 156: Train Loss: 0.6830948710441589, Validation Loss: 0.7500072121620178\n",
      "Epoch 157: Train Loss: 0.675001859664917, Validation Loss: 0.7540947794914246\n",
      "Epoch 158: Train Loss: 0.6637861371040344, Validation Loss: 0.7564007639884949\n",
      "Epoch 159: Train Loss: 0.677974796295166, Validation Loss: 0.7585441470146179\n",
      "Epoch 160: Train Loss: 0.706066119670868, Validation Loss: 0.7596055865287781\n",
      "Epoch 161: Train Loss: 0.6567048311233521, Validation Loss: 0.7577657103538513\n",
      "Epoch 162: Train Loss: 0.6204039096832276, Validation Loss: 0.7607761025428772\n",
      "Epoch 163: Train Loss: 0.63459312915802, Validation Loss: 0.7660870552062988\n",
      "Epoch 164: Train Loss: 0.6458880662918091, Validation Loss: 0.7660576701164246\n",
      "Epoch 165: Train Loss: 0.6639440655708313, Validation Loss: 0.7687075734138489\n",
      "Epoch 166: Train Loss: 0.6973463892936707, Validation Loss: 0.7699825167655945\n",
      "Epoch 167: Train Loss: 0.6963357567787171, Validation Loss: 0.7686359286308289\n",
      "Epoch 168: Train Loss: 0.6490690350532532, Validation Loss: 0.7638943791389465\n",
      "Epoch 169: Train Loss: 0.6615880846977233, Validation Loss: 0.7671165466308594\n",
      "Epoch 170: Train Loss: 0.6646842241287232, Validation Loss: 0.7654109597206116\n",
      "Epoch 171: Train Loss: 0.6689640522003174, Validation Loss: 0.7659692168235779\n",
      "Epoch 172: Train Loss: 0.6374072670936585, Validation Loss: 0.7609580755233765\n",
      "Epoch 173: Train Loss: 0.6933954000473023, Validation Loss: 0.7648215293884277\n",
      "Epoch 174: Train Loss: 0.650012731552124, Validation Loss: 0.7648391127586365\n",
      "Epoch 175: Train Loss: 0.6545175194740296, Validation Loss: 0.7679776549339294\n",
      "Epoch 176: Train Loss: 0.6380614876747132, Validation Loss: 0.765514075756073\n",
      "Epoch 177: Train Loss: 0.6409765839576721, Validation Loss: 0.7677129507064819\n",
      "Epoch 178: Train Loss: 0.6569985985755921, Validation Loss: 0.76605224609375\n",
      "Epoch 179: Train Loss: 0.6572772860527039, Validation Loss: 0.7676515579223633\n",
      "Epoch 180: Train Loss: 0.6346099853515625, Validation Loss: 0.7620107531547546\n",
      "Epoch 181: Train Loss: 0.6808865547180176, Validation Loss: 0.7637662291526794\n",
      "Epoch 182: Train Loss: 0.6340316295623779, Validation Loss: 0.7655030488967896\n",
      "Epoch 183: Train Loss: 0.678193199634552, Validation Loss: 0.7677238583564758\n",
      "Epoch 184: Train Loss: 0.6398743510246276, Validation Loss: 0.7670590281486511\n",
      "Epoch 185: Train Loss: 0.6528416872024536, Validation Loss: 0.7678257822990417\n",
      "Epoch 186: Train Loss: 0.6314295768737793, Validation Loss: 0.7700183391571045\n",
      "Epoch 187: Train Loss: 0.6445027828216553, Validation Loss: 0.77227783203125\n",
      "Epoch 188: Train Loss: 0.6175159454345703, Validation Loss: 0.7629390358924866\n",
      "Epoch 189: Train Loss: 0.6210038423538208, Validation Loss: 0.7659379243850708\n",
      "Epoch 190: Train Loss: 0.6568138480186463, Validation Loss: 0.7672296166419983\n",
      "Epoch 191: Train Loss: 0.6486951589584351, Validation Loss: 0.7695390582084656\n",
      "Epoch 192: Train Loss: 0.6700530648231506, Validation Loss: 0.7705717086791992\n",
      "Epoch 193: Train Loss: 0.701286768913269, Validation Loss: 0.7750490307807922\n",
      "Epoch 194: Train Loss: 0.6464224219322204, Validation Loss: 0.7753114700317383\n",
      "Epoch 195: Train Loss: 0.6393587827682495, Validation Loss: 0.7696496248245239\n",
      "Epoch 196: Train Loss: 0.6228782176971436, Validation Loss: 0.7619644999504089\n",
      "Epoch 197: Train Loss: 0.630456805229187, Validation Loss: 0.7682733535766602\n",
      "Epoch 198: Train Loss: 0.6872124910354614, Validation Loss: 0.7738384008407593\n",
      "Epoch 199: Train Loss: 0.7136413812637329, Validation Loss: 0.7764527797698975\n",
      "Epoch 200: Train Loss: 0.6068767547607422, Validation Loss: 0.7681592702865601\n",
      "Epoch 201: Train Loss: 0.6127418041229248, Validation Loss: 0.7653571963310242\n",
      "Epoch 202: Train Loss: 0.6679113030433654, Validation Loss: 0.7716425061225891\n",
      "Epoch 203: Train Loss: 0.6818556547164917, Validation Loss: 0.7747194170951843\n",
      "Epoch 204: Train Loss: 0.6442587375640869, Validation Loss: 0.775030791759491\n",
      "Epoch 205: Train Loss: 0.6512372374534607, Validation Loss: 0.7760846614837646\n",
      "Epoch 206: Train Loss: 0.6310868263244629, Validation Loss: 0.776826024055481\n",
      "Epoch 207: Train Loss: 0.5859048783779144, Validation Loss: 0.7769246101379395\n",
      "Epoch 208: Train Loss: 0.672170102596283, Validation Loss: 0.7795534133911133\n",
      "Epoch 209: Train Loss: 0.6456376671791076, Validation Loss: 0.7749438285827637\n",
      "Epoch 210: Train Loss: 0.6114330172538758, Validation Loss: 0.7676571011543274\n",
      "Epoch 211: Train Loss: 0.6840740799903869, Validation Loss: 0.7713735699653625\n",
      "Epoch 212: Train Loss: 0.6562695622444152, Validation Loss: 0.776979386806488\n",
      "Epoch 213: Train Loss: 0.6139931201934814, Validation Loss: 0.7751607894897461\n",
      "Epoch 214: Train Loss: 0.684500229358673, Validation Loss: 0.7749175429344177\n",
      "Epoch 215: Train Loss: 0.6669813871383667, Validation Loss: 0.7740198969841003\n",
      "Epoch 216: Train Loss: 0.668768298625946, Validation Loss: 0.7759796977043152\n",
      "Epoch 217: Train Loss: 0.6580614447593689, Validation Loss: 0.7765844464302063\n",
      "Epoch 218: Train Loss: 0.6811508655548095, Validation Loss: 0.7764359712600708\n",
      "Epoch 219: Train Loss: 0.6531033992767334, Validation Loss: 0.7792870402336121\n",
      "Epoch 220: Train Loss: 0.6340462982654571, Validation Loss: 0.7818611264228821\n",
      "Epoch 221: Train Loss: 0.6355502605438232, Validation Loss: 0.7858632802963257\n",
      "Epoch 222: Train Loss: 0.6318984150886535, Validation Loss: 0.7690628170967102\n",
      "Epoch 223: Train Loss: 0.6237766861915588, Validation Loss: 0.775596022605896\n",
      "Epoch 224: Train Loss: 0.6043971121311188, Validation Loss: 0.7718531489372253\n",
      "Epoch 225: Train Loss: 0.6620905995368958, Validation Loss: 0.7768195867538452\n",
      "Epoch 226: Train Loss: 0.6702327966690064, Validation Loss: 0.775743842124939\n",
      "Epoch 227: Train Loss: 0.6770684599876404, Validation Loss: 0.7828715443611145\n",
      "Epoch 228: Train Loss: 0.6436051368713379, Validation Loss: 0.7849653363227844\n",
      "Epoch 229: Train Loss: 0.608086085319519, Validation Loss: 0.7854404449462891\n",
      "Epoch 230: Train Loss: 0.6869716048240662, Validation Loss: 0.7864106893539429\n",
      "Epoch 231: Train Loss: 0.6675808310508728, Validation Loss: 0.7881507873535156\n",
      "Epoch 232: Train Loss: 0.6324176669120789, Validation Loss: 0.7894366383552551\n",
      "Epoch 233: Train Loss: 0.6247179269790649, Validation Loss: 0.7911250591278076\n",
      "Epoch 234: Train Loss: 0.6696218848228455, Validation Loss: 0.7942370772361755\n",
      "Epoch 235: Train Loss: 0.6699649691581726, Validation Loss: 0.7936373949050903\n",
      "Epoch 236: Train Loss: 0.6356320261955262, Validation Loss: 0.7956174612045288\n",
      "Epoch 237: Train Loss: 0.6825754284858704, Validation Loss: 0.7946584820747375\n",
      "Epoch 238: Train Loss: 0.6366685152053833, Validation Loss: 0.7854427099227905\n",
      "Epoch 239: Train Loss: 0.701019823551178, Validation Loss: 0.789996325969696\n",
      "Epoch 240: Train Loss: 0.6165057420730591, Validation Loss: 0.7891786694526672\n",
      "Epoch 241: Train Loss: 0.6776216506958008, Validation Loss: 0.79285728931427\n",
      "Epoch 242: Train Loss: 0.6359833121299744, Validation Loss: 0.7919310927391052\n",
      "Epoch 243: Train Loss: 0.6595469117164612, Validation Loss: 0.7912507653236389\n",
      "Epoch 244: Train Loss: 0.601784086227417, Validation Loss: 0.7869609594345093\n",
      "Epoch 245: Train Loss: 0.6324578046798706, Validation Loss: 0.7901592254638672\n",
      "Epoch 246: Train Loss: 0.6247599720954895, Validation Loss: 0.7897542715072632\n",
      "Epoch 247: Train Loss: 0.6512468814849853, Validation Loss: 0.791370153427124\n",
      "Epoch 248: Train Loss: 0.6160699367523194, Validation Loss: 0.7935698628425598\n",
      "Epoch 249: Train Loss: 0.5948354184627533, Validation Loss: 0.7963261604309082\n",
      "Epoch 250: Train Loss: 0.6315647006034851, Validation Loss: 0.7960256338119507\n",
      "Epoch 251: Train Loss: 0.6543441772460937, Validation Loss: 0.8013925552368164\n",
      "Epoch 252: Train Loss: 0.6360774278640747, Validation Loss: 0.8031588792800903\n",
      "Epoch 253: Train Loss: 0.605497682094574, Validation Loss: 0.8030394911766052\n",
      "Epoch 254: Train Loss: 0.6420405030250549, Validation Loss: 0.8017266988754272\n",
      "Epoch 255: Train Loss: 0.6032188773155213, Validation Loss: 0.7875331044197083\n",
      "Epoch 256: Train Loss: 0.6114159345626831, Validation Loss: 0.7929168343544006\n",
      "Epoch 257: Train Loss: 0.6478346586227417, Validation Loss: 0.7896286249160767\n",
      "Epoch 258: Train Loss: 0.627101457118988, Validation Loss: 0.7961118221282959\n",
      "Epoch 259: Train Loss: 0.5676304936408997, Validation Loss: 0.7889429330825806\n",
      "Epoch 260: Train Loss: 0.6189105629920959, Validation Loss: 0.7945365309715271\n",
      "Epoch 261: Train Loss: 0.6552814245223999, Validation Loss: 0.7996697425842285\n",
      "Epoch 262: Train Loss: 0.6973340749740601, Validation Loss: 0.7993897795677185\n",
      "Epoch 263: Train Loss: 0.5893491327762603, Validation Loss: 0.7854523658752441\n",
      "Epoch 264: Train Loss: 0.6293758511543274, Validation Loss: 0.7949349880218506\n",
      "Epoch 265: Train Loss: 0.6611016273498536, Validation Loss: 0.8019526600837708\n",
      "Epoch 266: Train Loss: 0.6250848293304443, Validation Loss: 0.8009673953056335\n",
      "Epoch 267: Train Loss: 0.6523007273674011, Validation Loss: 0.8071565628051758\n",
      "Epoch 268: Train Loss: 0.6078948259353638, Validation Loss: 0.8088889718055725\n",
      "Epoch 269: Train Loss: 0.6285731613636016, Validation Loss: 0.8115631341934204\n",
      "Epoch 270: Train Loss: 0.621278989315033, Validation Loss: 0.810674786567688\n",
      "Epoch 271: Train Loss: 0.6263585805892944, Validation Loss: 0.8111940026283264\n",
      "Epoch 272: Train Loss: 0.6504693984985351, Validation Loss: 0.809827983379364\n",
      "Epoch 273: Train Loss: 0.6145871877670288, Validation Loss: 0.8087155818939209\n",
      "Epoch 274: Train Loss: 0.5728028535842895, Validation Loss: 0.8121657967567444\n",
      "Epoch 275: Train Loss: 0.6161998987197876, Validation Loss: 0.8122681975364685\n",
      "Epoch 276: Train Loss: 0.6263841390609741, Validation Loss: 0.8155322074890137\n",
      "Epoch 277: Train Loss: 0.628282082080841, Validation Loss: 0.8136888146400452\n",
      "Epoch 278: Train Loss: 0.6856225609779358, Validation Loss: 0.812090277671814\n",
      "Epoch 279: Train Loss: 0.5920284926891327, Validation Loss: 0.8143036961555481\n",
      "Epoch 280: Train Loss: 0.6024982631206512, Validation Loss: 0.8020898103713989\n",
      "Epoch 281: Train Loss: 0.6055835962295533, Validation Loss: 0.8085030913352966\n",
      "Epoch 282: Train Loss: 0.6203837513923645, Validation Loss: 0.8164286613464355\n",
      "Epoch 283: Train Loss: 0.5945658445358276, Validation Loss: 0.8187025189399719\n",
      "Epoch 284: Train Loss: 0.6522274732589721, Validation Loss: 0.8207719326019287\n",
      "Epoch 285: Train Loss: 0.5707474112510681, Validation Loss: 0.8185256123542786\n",
      "Epoch 286: Train Loss: 0.6092776775360107, Validation Loss: 0.809295654296875\n",
      "Epoch 287: Train Loss: 0.632063913345337, Validation Loss: 0.809175968170166\n",
      "Epoch 288: Train Loss: 0.575219452381134, Validation Loss: 0.8140110969543457\n",
      "Epoch 289: Train Loss: 0.6399912357330322, Validation Loss: 0.8105911016464233\n",
      "Epoch 290: Train Loss: 0.6245568752288818, Validation Loss: 0.8142343759536743\n",
      "Epoch 291: Train Loss: 0.6408952951431275, Validation Loss: 0.8195586204528809\n",
      "Epoch 292: Train Loss: 0.6263092279434204, Validation Loss: 0.8235287666320801\n",
      "Epoch 293: Train Loss: 0.5860123991966247, Validation Loss: 0.8311288952827454\n",
      "Epoch 294: Train Loss: 0.6229654312133789, Validation Loss: 0.8173467516899109\n",
      "Epoch 295: Train Loss: 0.6369727611541748, Validation Loss: 0.8063054084777832\n",
      "Epoch 296: Train Loss: 0.604625141620636, Validation Loss: 0.8128623962402344\n",
      "Epoch 297: Train Loss: 0.6355119705200195, Validation Loss: 0.8182673454284668\n",
      "Epoch 298: Train Loss: 0.6160839915275573, Validation Loss: 0.822161078453064\n",
      "Epoch 299: Train Loss: 0.6541660189628601, Validation Loss: 0.8224388360977173\n",
      "Epoch 300: Train Loss: 0.6040139555931091, Validation Loss: 0.8113943338394165\n",
      "Epoch 301: Train Loss: 0.630387008190155, Validation Loss: 0.8195511102676392\n",
      "Epoch 302: Train Loss: 0.6586085200309754, Validation Loss: 0.8110002279281616\n",
      "Epoch 303: Train Loss: 0.6388548493385315, Validation Loss: 0.8141790628433228\n",
      "Epoch 304: Train Loss: 0.6289305567741394, Validation Loss: 0.8213945031166077\n",
      "Epoch 305: Train Loss: 0.649376904964447, Validation Loss: 0.8218128085136414\n",
      "Epoch 306: Train Loss: 0.5916247248649598, Validation Loss: 0.8252725601196289\n",
      "Epoch 307: Train Loss: 0.5954749703407287, Validation Loss: 0.8306236267089844\n",
      "Epoch 308: Train Loss: 0.638569974899292, Validation Loss: 0.8308946490287781\n",
      "Epoch 309: Train Loss: 0.6077175736427307, Validation Loss: 0.8333292007446289\n",
      "Epoch 310: Train Loss: 0.6482150912284851, Validation Loss: 0.8289045095443726\n",
      "Epoch 311: Train Loss: 0.609000813961029, Validation Loss: 0.828988790512085\n",
      "Epoch 312: Train Loss: 0.594858992099762, Validation Loss: 0.8331233263015747\n",
      "Epoch 313: Train Loss: 0.6366555213928222, Validation Loss: 0.8339053988456726\n",
      "Epoch 314: Train Loss: 0.6171722650527954, Validation Loss: 0.8242937326431274\n",
      "Epoch 315: Train Loss: 0.6334776878356934, Validation Loss: 0.832196056842804\n",
      "Epoch 316: Train Loss: 0.6709426999092102, Validation Loss: 0.8319723010063171\n",
      "Epoch 317: Train Loss: 0.6206542730331421, Validation Loss: 0.8368463516235352\n",
      "Epoch 318: Train Loss: 0.6434828281402588, Validation Loss: 0.8322588205337524\n",
      "Epoch 319: Train Loss: 0.629177474975586, Validation Loss: 0.8346307277679443\n",
      "Epoch 320: Train Loss: 0.6166620850563049, Validation Loss: 0.8343081474304199\n",
      "Epoch 321: Train Loss: 0.6306115031242371, Validation Loss: 0.8337182998657227\n",
      "Epoch 322: Train Loss: 0.6010907411575317, Validation Loss: 0.8324998021125793\n",
      "Epoch 323: Train Loss: 0.5612023293972015, Validation Loss: 0.8325002789497375\n",
      "Epoch 324: Train Loss: 0.664756691455841, Validation Loss: 0.8359043002128601\n",
      "Epoch 325: Train Loss: 0.6446451663970947, Validation Loss: 0.8272736668586731\n",
      "Epoch 326: Train Loss: 0.6084804773330689, Validation Loss: 0.8277956247329712\n",
      "Epoch 327: Train Loss: 0.6494931101799011, Validation Loss: 0.8364273309707642\n",
      "Epoch 328: Train Loss: 0.6127135276794433, Validation Loss: 0.8402259945869446\n",
      "Epoch 329: Train Loss: 0.6807595610618591, Validation Loss: 0.8312203884124756\n",
      "Epoch 330: Train Loss: 0.6035999655723572, Validation Loss: 0.8321643471717834\n",
      "Epoch 331: Train Loss: 0.6359181642532349, Validation Loss: 0.8335976600646973\n",
      "Epoch 332: Train Loss: 0.588844221830368, Validation Loss: 0.8169982433319092\n",
      "Epoch 333: Train Loss: 0.6162444829940796, Validation Loss: 0.8266180157661438\n",
      "Epoch 334: Train Loss: 0.5934538960456848, Validation Loss: 0.8243938684463501\n",
      "Epoch 335: Train Loss: 0.6143605232238769, Validation Loss: 0.8355846405029297\n",
      "Epoch 336: Train Loss: 0.6599638104438782, Validation Loss: 0.8381986021995544\n",
      "Epoch 337: Train Loss: 0.5723668456077575, Validation Loss: 0.8303228616714478\n",
      "Epoch 338: Train Loss: 0.592147421836853, Validation Loss: 0.8326366543769836\n",
      "Epoch 339: Train Loss: 0.5738438487052917, Validation Loss: 0.8353332281112671\n",
      "Epoch 340: Train Loss: 0.6377819418907166, Validation Loss: 0.838640034198761\n",
      "Epoch 341: Train Loss: 0.6431079626083374, Validation Loss: 0.8360991477966309\n",
      "Epoch 342: Train Loss: 0.6419830560684204, Validation Loss: 0.8478425145149231\n",
      "Epoch 343: Train Loss: 0.5651124596595765, Validation Loss: 0.8197702169418335\n",
      "Epoch 344: Train Loss: 0.6001571536064148, Validation Loss: 0.8314875960350037\n",
      "Epoch 345: Train Loss: 0.6046508967876434, Validation Loss: 0.8422372341156006\n",
      "Epoch 346: Train Loss: 0.5644580841064453, Validation Loss: 0.8415883779525757\n",
      "Epoch 347: Train Loss: 0.5993795275688172, Validation Loss: 0.8359266519546509\n",
      "Epoch 348: Train Loss: 0.6482920408248901, Validation Loss: 0.8452561497688293\n",
      "Epoch 349: Train Loss: 0.6032902836799622, Validation Loss: 0.8521202206611633\n",
      "Epoch 350: Train Loss: 0.6784700632095337, Validation Loss: 0.8413276672363281\n",
      "Epoch 351: Train Loss: 0.588012158870697, Validation Loss: 0.8353753089904785\n",
      "Epoch 352: Train Loss: 0.6039409399032593, Validation Loss: 0.8435649275779724\n",
      "Epoch 353: Train Loss: 0.687787127494812, Validation Loss: 0.8502823114395142\n",
      "Epoch 354: Train Loss: 0.6399655699729919, Validation Loss: 0.8526987433433533\n",
      "Epoch 355: Train Loss: 0.6268057227134705, Validation Loss: 0.8367663621902466\n",
      "Epoch 356: Train Loss: 0.5987893462181091, Validation Loss: 0.8376986980438232\n",
      "Epoch 357: Train Loss: 0.6169681310653686, Validation Loss: 0.8345311880111694\n",
      "Epoch 358: Train Loss: 0.6025886476039887, Validation Loss: 0.8227671980857849\n",
      "Epoch 359: Train Loss: 0.6633796215057373, Validation Loss: 0.8343158960342407\n",
      "Epoch 360: Train Loss: 0.5893729448318481, Validation Loss: 0.8341872692108154\n",
      "Epoch 361: Train Loss: 0.6310542345046997, Validation Loss: 0.8364744186401367\n",
      "Epoch 362: Train Loss: 0.6120969533920289, Validation Loss: 0.8475908041000366\n",
      "Epoch 363: Train Loss: 0.6441197752952575, Validation Loss: 0.858498215675354\n",
      "Epoch 364: Train Loss: 0.6185958623886109, Validation Loss: 0.8515053987503052\n",
      "Epoch 365: Train Loss: 0.5875240445137024, Validation Loss: 0.8512532711029053\n",
      "Epoch 366: Train Loss: 0.5951555609703064, Validation Loss: 0.8554710745811462\n",
      "Epoch 367: Train Loss: 0.5467562317848206, Validation Loss: 0.849090039730072\n",
      "Epoch 368: Train Loss: 0.638509726524353, Validation Loss: 0.8546699285507202\n",
      "Epoch 369: Train Loss: 0.6200645208358765, Validation Loss: 0.8589206337928772\n",
      "Epoch 370: Train Loss: 0.570428591966629, Validation Loss: 0.8443488478660583\n",
      "Epoch 371: Train Loss: 0.5919940114021301, Validation Loss: 0.850265622138977\n",
      "Epoch 372: Train Loss: 0.6010445535182953, Validation Loss: 0.8544949889183044\n",
      "Epoch 373: Train Loss: 0.5625110149383545, Validation Loss: 0.8587597012519836\n",
      "Epoch 374: Train Loss: 0.6419971466064454, Validation Loss: 0.8607484698295593\n",
      "Epoch 375: Train Loss: 0.5665756583213806, Validation Loss: 0.8665404915809631\n",
      "Epoch 376: Train Loss: 0.6176395297050477, Validation Loss: 0.8576310873031616\n",
      "Epoch 377: Train Loss: 0.6113285422325134, Validation Loss: 0.8573952913284302\n",
      "Epoch 378: Train Loss: 0.5970770955085755, Validation Loss: 0.8680370450019836\n",
      "Epoch 379: Train Loss: 0.5710409164428711, Validation Loss: 0.8734866976737976\n",
      "Epoch 380: Train Loss: 0.5820642590522767, Validation Loss: 0.8711389303207397\n",
      "Epoch 381: Train Loss: 0.586446487903595, Validation Loss: 0.8601976633071899\n",
      "Epoch 382: Train Loss: 0.6042058229446411, Validation Loss: 0.8647815585136414\n",
      "Epoch 383: Train Loss: 0.5765232622623444, Validation Loss: 0.86289381980896\n",
      "Epoch 384: Train Loss: 0.610851275920868, Validation Loss: 0.8647206425666809\n",
      "Epoch 385: Train Loss: 0.6708076238632202, Validation Loss: 0.8733605742454529\n",
      "Epoch 386: Train Loss: 0.6243685245513916, Validation Loss: 0.8700536489486694\n",
      "Epoch 387: Train Loss: 0.5734601378440857, Validation Loss: 0.8525148034095764\n",
      "Epoch 388: Train Loss: 0.6420588254928589, Validation Loss: 0.8498185873031616\n",
      "Epoch 389: Train Loss: 0.5889067888259888, Validation Loss: 0.8558595180511475\n",
      "Epoch 390: Train Loss: 0.6449882507324218, Validation Loss: 0.8649030923843384\n",
      "Epoch 391: Train Loss: 0.6087302684783935, Validation Loss: 0.8702211380004883\n",
      "Epoch 392: Train Loss: 0.6718529105186463, Validation Loss: 0.8745304346084595\n",
      "Epoch 393: Train Loss: 0.5737038731575013, Validation Loss: 0.8650058507919312\n",
      "Epoch 394: Train Loss: 0.6263184428215027, Validation Loss: 0.8673137426376343\n",
      "Epoch 395: Train Loss: 0.5652776062488556, Validation Loss: 0.8580887913703918\n",
      "Epoch 396: Train Loss: 0.608690059185028, Validation Loss: 0.8588432669639587\n",
      "Epoch 397: Train Loss: 0.6436627626419067, Validation Loss: 0.8696903586387634\n",
      "Epoch 398: Train Loss: 0.5650331258773804, Validation Loss: 0.8681093454360962\n",
      "Epoch 399: Train Loss: 0.6467233419418335, Validation Loss: 0.8737170696258545\n",
      "Epoch 400: Train Loss: 0.605549156665802, Validation Loss: 0.8571705222129822\n",
      "Epoch 401: Train Loss: 0.6475480556488037, Validation Loss: 0.8593059182167053\n",
      "Epoch 402: Train Loss: 0.6294446110725402, Validation Loss: 0.8670061230659485\n",
      "Epoch 403: Train Loss: 0.6198462009429931, Validation Loss: 0.8678935766220093\n",
      "Epoch 404: Train Loss: 0.6274718880653382, Validation Loss: 0.8546197414398193\n",
      "Epoch 405: Train Loss: 0.5806246995925903, Validation Loss: 0.8578516244888306\n",
      "Epoch 406: Train Loss: 0.6024145960807801, Validation Loss: 0.8621495962142944\n",
      "Epoch 407: Train Loss: 0.6203397154808045, Validation Loss: 0.8644054532051086\n",
      "Epoch 408: Train Loss: 0.563273137807846, Validation Loss: 0.8718329668045044\n",
      "Epoch 409: Train Loss: 0.6180192589759826, Validation Loss: 0.8688239455223083\n",
      "Epoch 410: Train Loss: 0.5870243787765503, Validation Loss: 0.873792827129364\n",
      "Epoch 411: Train Loss: 0.5905999660491943, Validation Loss: 0.8606943488121033\n",
      "Epoch 412: Train Loss: 0.5893020033836365, Validation Loss: 0.8643282055854797\n",
      "Epoch 413: Train Loss: 0.5901141107082367, Validation Loss: 0.8557716608047485\n",
      "Epoch 414: Train Loss: 0.5475330770015716, Validation Loss: 0.8648453950881958\n",
      "Epoch 415: Train Loss: 0.6205229043960572, Validation Loss: 0.8710703253746033\n",
      "Epoch 416: Train Loss: 0.6165168881416321, Validation Loss: 0.881678581237793\n",
      "Epoch 417: Train Loss: 0.5904785871505738, Validation Loss: 0.8793865442276001\n",
      "Epoch 418: Train Loss: 0.6448742747306824, Validation Loss: 0.8753944039344788\n",
      "Epoch 419: Train Loss: 0.6547667026519776, Validation Loss: 0.8753163814544678\n",
      "Epoch 420: Train Loss: 0.5709598660469055, Validation Loss: 0.8591682314872742\n",
      "Epoch 421: Train Loss: 0.5982306599617004, Validation Loss: 0.8741170167922974\n",
      "Epoch 422: Train Loss: 0.6385149598121643, Validation Loss: 0.8758355379104614\n",
      "Epoch 423: Train Loss: 0.5464906454086303, Validation Loss: 0.8699551820755005\n",
      "Epoch 424: Train Loss: 0.5949613332748414, Validation Loss: 0.8718782663345337\n",
      "Epoch 425: Train Loss: 0.5738836407661438, Validation Loss: 0.8764703273773193\n",
      "Epoch 426: Train Loss: 0.6030417799949646, Validation Loss: 0.8755108118057251\n",
      "Epoch 427: Train Loss: 0.5822197198867798, Validation Loss: 0.8786868453025818\n",
      "Epoch 428: Train Loss: 0.5999399662017822, Validation Loss: 0.876484215259552\n",
      "Epoch 429: Train Loss: 0.5657864570617676, Validation Loss: 0.8828578591346741\n",
      "Epoch 430: Train Loss: 0.6423465609550476, Validation Loss: 0.8769403696060181\n",
      "Epoch 431: Train Loss: 0.6355437159538269, Validation Loss: 0.8791358470916748\n",
      "Epoch 432: Train Loss: 0.6006907463073731, Validation Loss: 0.8802331686019897\n",
      "Epoch 433: Train Loss: 0.6033249974250794, Validation Loss: 0.885125458240509\n",
      "Epoch 434: Train Loss: 0.6290823936462402, Validation Loss: 0.8914411664009094\n",
      "Epoch 435: Train Loss: 0.6131162166595459, Validation Loss: 0.8799636960029602\n",
      "Epoch 436: Train Loss: 0.5821111381053925, Validation Loss: 0.8553239703178406\n",
      "Epoch 437: Train Loss: 0.6062749266624451, Validation Loss: 0.8604661822319031\n",
      "Epoch 438: Train Loss: 0.582619696855545, Validation Loss: 0.8760266900062561\n",
      "Epoch 439: Train Loss: 0.5930983901023865, Validation Loss: 0.886862576007843\n",
      "Epoch 440: Train Loss: 0.59390207529068, Validation Loss: 0.884878933429718\n",
      "Epoch 441: Train Loss: 0.6058920621871948, Validation Loss: 0.8858829736709595\n",
      "Epoch 442: Train Loss: 0.6012837052345276, Validation Loss: 0.8797174096107483\n",
      "Epoch 443: Train Loss: 0.5343775272369384, Validation Loss: 0.8582156896591187\n",
      "Epoch 444: Train Loss: 0.5964242935180664, Validation Loss: 0.8631038069725037\n",
      "Epoch 445: Train Loss: 0.5475809931755066, Validation Loss: 0.8738340735435486\n",
      "Epoch 446: Train Loss: 0.5990534126758575, Validation Loss: 0.8816160559654236\n",
      "Epoch 447: Train Loss: 0.5995452404022217, Validation Loss: 0.8807124495506287\n",
      "Epoch 448: Train Loss: 0.6123217463493347, Validation Loss: 0.8836095333099365\n",
      "Epoch 449: Train Loss: 0.6496828079223633, Validation Loss: 0.8829659819602966\n",
      "Epoch 450: Train Loss: 0.597927451133728, Validation Loss: 0.8902637362480164\n",
      "Epoch 451: Train Loss: 0.5973602771759033, Validation Loss: 0.8861967325210571\n",
      "Epoch 452: Train Loss: 0.5923987627029419, Validation Loss: 0.8825231790542603\n",
      "Epoch 453: Train Loss: 0.6192586898803711, Validation Loss: 0.8869678378105164\n",
      "Epoch 454: Train Loss: 0.5520432591438293, Validation Loss: 0.8764617443084717\n",
      "Epoch 455: Train Loss: 0.5576524496078491, Validation Loss: 0.8672392964363098\n",
      "Epoch 456: Train Loss: 0.573661994934082, Validation Loss: 0.8780307769775391\n",
      "Epoch 457: Train Loss: 0.5828245759010315, Validation Loss: 0.8813768625259399\n",
      "Epoch 458: Train Loss: 0.6284189105033875, Validation Loss: 0.8878742456436157\n",
      "Epoch 459: Train Loss: 0.5631802141666412, Validation Loss: 0.8805789947509766\n",
      "Epoch 460: Train Loss: 0.6300391435623169, Validation Loss: 0.8755077123641968\n",
      "Epoch 461: Train Loss: 0.5703746914863587, Validation Loss: 0.8585506081581116\n",
      "Epoch 462: Train Loss: 0.6120284914970398, Validation Loss: 0.8621863722801208\n",
      "Epoch 463: Train Loss: 0.5732117891311646, Validation Loss: 0.8672475814819336\n",
      "Epoch 464: Train Loss: 0.6045033216476441, Validation Loss: 0.8647287487983704\n",
      "Epoch 465: Train Loss: 0.5432965934276581, Validation Loss: 0.8755592107772827\n",
      "Epoch 466: Train Loss: 0.6068083286285401, Validation Loss: 0.885349452495575\n",
      "Epoch 467: Train Loss: 0.6046389818191529, Validation Loss: 0.8723679780960083\n",
      "Epoch 468: Train Loss: 0.6392488360404969, Validation Loss: 0.8657262325286865\n",
      "Epoch 469: Train Loss: 0.5469057202339173, Validation Loss: 0.8815642595291138\n",
      "Epoch 470: Train Loss: 0.5658041119575501, Validation Loss: 0.8841962218284607\n",
      "Epoch 471: Train Loss: 0.6219622611999511, Validation Loss: 0.8840010762214661\n",
      "Epoch 472: Train Loss: 0.5354889571666718, Validation Loss: 0.8927165865898132\n",
      "Epoch 473: Train Loss: 0.552618145942688, Validation Loss: 0.9001684188842773\n",
      "Epoch 474: Train Loss: 0.5543843686580658, Validation Loss: 0.8969393968582153\n",
      "Epoch 475: Train Loss: 0.5554440498352051, Validation Loss: 0.8999142050743103\n",
      "Epoch 476: Train Loss: 0.5544133663177491, Validation Loss: 0.9129347205162048\n",
      "Epoch 477: Train Loss: 0.5665924787521363, Validation Loss: 0.9144982695579529\n",
      "Epoch 478: Train Loss: 0.5273628652095794, Validation Loss: 0.9116289019584656\n",
      "Epoch 479: Train Loss: 0.567517775297165, Validation Loss: 0.9138429164886475\n",
      "Epoch 480: Train Loss: 0.5857631087303161, Validation Loss: 0.9070764780044556\n",
      "Epoch 481: Train Loss: 0.5924883842468261, Validation Loss: 0.9159465432167053\n",
      "Epoch 482: Train Loss: 0.5311189532279968, Validation Loss: 0.9135150909423828\n",
      "Epoch 483: Train Loss: 0.6217927932739258, Validation Loss: 0.9059007167816162\n",
      "Epoch 484: Train Loss: 0.5361063718795777, Validation Loss: 0.917567253112793\n",
      "Epoch 485: Train Loss: 0.6033973336219788, Validation Loss: 0.9081607460975647\n",
      "Epoch 486: Train Loss: 0.5534028470516205, Validation Loss: 0.9142341613769531\n",
      "Epoch 487: Train Loss: 0.6136070370674134, Validation Loss: 0.9175254106521606\n",
      "Epoch 488: Train Loss: 0.5629746377468109, Validation Loss: 0.8988164663314819\n",
      "Epoch 489: Train Loss: 0.5589406251907348, Validation Loss: 0.9041877388954163\n",
      "Epoch 490: Train Loss: 0.5786383748054504, Validation Loss: 0.9023585915565491\n",
      "Epoch 491: Train Loss: 0.5683618187904358, Validation Loss: 0.902872622013092\n",
      "Epoch 492: Train Loss: 0.5386435449123382, Validation Loss: 0.9064292907714844\n",
      "Epoch 493: Train Loss: 0.5013580799102784, Validation Loss: 0.9100330471992493\n",
      "Epoch 494: Train Loss: 0.6466134190559387, Validation Loss: 0.9140558242797852\n",
      "Epoch 495: Train Loss: 0.5490642249584198, Validation Loss: 0.908221423625946\n",
      "Epoch 496: Train Loss: 0.5919208884239197, Validation Loss: 0.9256629347801208\n",
      "Epoch 497: Train Loss: 0.6068479776382446, Validation Loss: 0.9231142401695251\n",
      "Epoch 498: Train Loss: 0.5525058269500732, Validation Loss: 0.9277446269989014\n",
      "Epoch 499: Train Loss: 0.5742511987686157, Validation Loss: 0.9226722717285156\n",
      "Fold 1 Metrics:\n",
      "Accuracy: 0.18181818181818182, Precision: 0.0, Recall: 0.0, F1-score: 0.0, AUC: 0.2\n",
      "Confusion Matrix:\n",
      "[[2 3]\n",
      " [6 0]]\n",
      "Completed fold 1\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples from subject 7 to test set\n",
      "Adding 6 truth samples from subject 7 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 500)\n",
      "(132, 65, 500)\n",
      "Epoch 0: Train Loss: 0.6797034859657287, Validation Loss: 0.6882249712944031\n",
      "Epoch 1: Train Loss: 0.7013617038726807, Validation Loss: 0.6844051480293274\n",
      "Epoch 2: Train Loss: 0.7397656798362732, Validation Loss: 0.6796671152114868\n",
      "Epoch 3: Train Loss: 0.7763569474220275, Validation Loss: 0.6748416423797607\n",
      "Epoch 4: Train Loss: 0.6869164228439331, Validation Loss: 0.6703181862831116\n",
      "Epoch 5: Train Loss: 0.7304860472679138, Validation Loss: 0.6687007546424866\n",
      "Epoch 6: Train Loss: 0.7252636671066284, Validation Loss: 0.6668034791946411\n",
      "Epoch 7: Train Loss: 0.7179315328598023, Validation Loss: 0.6661500930786133\n",
      "Epoch 8: Train Loss: 0.7022214889526367, Validation Loss: 0.6650480628013611\n",
      "Epoch 9: Train Loss: 0.7125430703163147, Validation Loss: 0.6648667454719543\n",
      "Epoch 10: Train Loss: 0.7125186324119568, Validation Loss: 0.6650441884994507\n",
      "Epoch 11: Train Loss: 0.6845668673515319, Validation Loss: 0.6649065017700195\n",
      "Epoch 12: Train Loss: 0.690834391117096, Validation Loss: 0.663784384727478\n",
      "Epoch 13: Train Loss: 0.6863085269927979, Validation Loss: 0.6635746359825134\n",
      "Epoch 14: Train Loss: 0.7008539319038392, Validation Loss: 0.6639806032180786\n",
      "Epoch 15: Train Loss: 0.7063214182853699, Validation Loss: 0.665658175945282\n",
      "Epoch 16: Train Loss: 0.7489896416664124, Validation Loss: 0.664931058883667\n",
      "Epoch 17: Train Loss: 0.6987793326377869, Validation Loss: 0.6646572351455688\n",
      "Epoch 18: Train Loss: 0.6935812830924988, Validation Loss: 0.6633418202400208\n",
      "Epoch 19: Train Loss: 0.6494113087654114, Validation Loss: 0.6623314619064331\n",
      "Epoch 20: Train Loss: 0.7146682143211365, Validation Loss: 0.6639171242713928\n",
      "Epoch 21: Train Loss: 0.7523434281349182, Validation Loss: 0.6635186672210693\n",
      "Epoch 22: Train Loss: 0.7320419669151306, Validation Loss: 0.664251983165741\n",
      "Epoch 23: Train Loss: 0.71452317237854, Validation Loss: 0.6647595167160034\n",
      "Epoch 24: Train Loss: 0.7203212380409241, Validation Loss: 0.666995644569397\n",
      "Epoch 25: Train Loss: 0.7099799752235413, Validation Loss: 0.6664460897445679\n",
      "Epoch 26: Train Loss: 0.7013121247291565, Validation Loss: 0.6673112511634827\n",
      "Epoch 27: Train Loss: 0.7080401062965394, Validation Loss: 0.665461003780365\n",
      "Epoch 28: Train Loss: 0.7043480515480042, Validation Loss: 0.667175829410553\n",
      "Epoch 29: Train Loss: 0.6980363965034485, Validation Loss: 0.6658990979194641\n",
      "Epoch 30: Train Loss: 0.6533859133720398, Validation Loss: 0.6650155782699585\n",
      "Epoch 31: Train Loss: 0.7242658853530883, Validation Loss: 0.6642662882804871\n",
      "Epoch 32: Train Loss: 0.7310895562171936, Validation Loss: 0.6639828085899353\n",
      "Epoch 33: Train Loss: 0.7288592696189881, Validation Loss: 0.6635552644729614\n",
      "Epoch 34: Train Loss: 0.7207047343254089, Validation Loss: 0.6628983616828918\n",
      "Epoch 35: Train Loss: 0.7266604781150818, Validation Loss: 0.6629085540771484\n",
      "Epoch 36: Train Loss: 0.6886476516723633, Validation Loss: 0.6636484861373901\n",
      "Epoch 37: Train Loss: 0.663077437877655, Validation Loss: 0.662767767906189\n",
      "Epoch 38: Train Loss: 0.6933396697044373, Validation Loss: 0.6629618406295776\n",
      "Epoch 39: Train Loss: 0.6777161002159119, Validation Loss: 0.6629858613014221\n",
      "Epoch 40: Train Loss: 0.745871388912201, Validation Loss: 0.6619017124176025\n",
      "Epoch 41: Train Loss: 0.6916176557540894, Validation Loss: 0.6610443592071533\n",
      "Epoch 42: Train Loss: 0.7086250066757203, Validation Loss: 0.6630851030349731\n",
      "Epoch 43: Train Loss: 0.7076900601387024, Validation Loss: 0.6632562875747681\n",
      "Epoch 44: Train Loss: 0.7133294820785523, Validation Loss: 0.661519467830658\n",
      "Epoch 45: Train Loss: 0.6792083263397217, Validation Loss: 0.6613359451293945\n",
      "Epoch 46: Train Loss: 0.7015336394309998, Validation Loss: 0.6614436507225037\n",
      "Epoch 47: Train Loss: 0.7326253771781921, Validation Loss: 0.6618134379386902\n",
      "Epoch 48: Train Loss: 0.7169477701187134, Validation Loss: 0.662243127822876\n",
      "Epoch 49: Train Loss: 0.695296049118042, Validation Loss: 0.6632620096206665\n",
      "Epoch 50: Train Loss: 0.6639774560928344, Validation Loss: 0.6633739471435547\n",
      "Epoch 51: Train Loss: 0.6738656997680664, Validation Loss: 0.663831889629364\n",
      "Epoch 52: Train Loss: 0.6783716201782226, Validation Loss: 0.6630291938781738\n",
      "Epoch 53: Train Loss: 0.6863058805465698, Validation Loss: 0.6622146368026733\n",
      "Epoch 54: Train Loss: 0.6831654667854309, Validation Loss: 0.6619638204574585\n",
      "Epoch 55: Train Loss: 0.7011962294578552, Validation Loss: 0.6618821620941162\n",
      "Epoch 56: Train Loss: 0.6900312900543213, Validation Loss: 0.6618183851242065\n",
      "Epoch 57: Train Loss: 0.7119709491729737, Validation Loss: 0.6638116240501404\n",
      "Epoch 58: Train Loss: 0.7116339564323425, Validation Loss: 0.6627733707427979\n",
      "Epoch 59: Train Loss: 0.6694379687309265, Validation Loss: 0.6624906063079834\n",
      "Epoch 60: Train Loss: 0.6782983899116516, Validation Loss: 0.6627675890922546\n",
      "Epoch 61: Train Loss: 0.7075363397598267, Validation Loss: 0.6623702049255371\n",
      "Epoch 62: Train Loss: 0.6848950505256652, Validation Loss: 0.6621320843696594\n",
      "Epoch 63: Train Loss: 0.6727071642875672, Validation Loss: 0.6632445454597473\n",
      "Epoch 64: Train Loss: 0.7106343626976013, Validation Loss: 0.6623077988624573\n",
      "Epoch 65: Train Loss: 0.7371311545372009, Validation Loss: 0.6618372797966003\n",
      "Epoch 66: Train Loss: 0.6897886753082275, Validation Loss: 0.6617385745048523\n",
      "Epoch 67: Train Loss: 0.6978568077087403, Validation Loss: 0.661158561706543\n",
      "Epoch 68: Train Loss: 0.6938582301139832, Validation Loss: 0.6609950065612793\n",
      "Epoch 69: Train Loss: 0.698603093624115, Validation Loss: 0.6614497900009155\n",
      "Epoch 70: Train Loss: 0.7252541780471802, Validation Loss: 0.6617743968963623\n",
      "Epoch 71: Train Loss: 0.7250511527061463, Validation Loss: 0.6625996828079224\n",
      "Epoch 72: Train Loss: 0.6782313704490661, Validation Loss: 0.6620519161224365\n",
      "Epoch 73: Train Loss: 0.6740718603134155, Validation Loss: 0.6617291569709778\n",
      "Epoch 74: Train Loss: 0.6948621273040771, Validation Loss: 0.6617702841758728\n",
      "Epoch 75: Train Loss: 0.7178069114685058, Validation Loss: 0.6620292067527771\n",
      "Epoch 76: Train Loss: 0.6944772481918335, Validation Loss: 0.6616224050521851\n",
      "Epoch 77: Train Loss: 0.7491873383522034, Validation Loss: 0.6621795892715454\n",
      "Epoch 78: Train Loss: 0.711348021030426, Validation Loss: 0.6620294451713562\n",
      "Epoch 79: Train Loss: 0.6364403426647186, Validation Loss: 0.6643490791320801\n",
      "Epoch 80: Train Loss: 0.6955378413200378, Validation Loss: 0.6633604168891907\n",
      "Epoch 81: Train Loss: 0.675613272190094, Validation Loss: 0.6637672781944275\n",
      "Epoch 82: Train Loss: 0.6833903312683105, Validation Loss: 0.6653490662574768\n",
      "Epoch 83: Train Loss: 0.6933886766433716, Validation Loss: 0.6638403534889221\n",
      "Epoch 84: Train Loss: 0.6889399528503418, Validation Loss: 0.662841796875\n",
      "Epoch 85: Train Loss: 0.6615763425827026, Validation Loss: 0.6645328998565674\n",
      "Epoch 86: Train Loss: 0.6902636170387269, Validation Loss: 0.6643528938293457\n",
      "Epoch 87: Train Loss: 0.7122942805290222, Validation Loss: 0.6663029193878174\n",
      "Epoch 88: Train Loss: 0.698001766204834, Validation Loss: 0.6652539968490601\n",
      "Epoch 89: Train Loss: 0.7301640391349793, Validation Loss: 0.6638032793998718\n",
      "Epoch 90: Train Loss: 0.699290108680725, Validation Loss: 0.6633641719818115\n",
      "Epoch 91: Train Loss: 0.738687264919281, Validation Loss: 0.6633147597312927\n",
      "Epoch 92: Train Loss: 0.7308985114097595, Validation Loss: 0.6638514995574951\n",
      "Epoch 93: Train Loss: 0.7107871532440185, Validation Loss: 0.6637851595878601\n",
      "Epoch 94: Train Loss: 0.6994130849838257, Validation Loss: 0.6647933721542358\n",
      "Epoch 95: Train Loss: 0.693375313282013, Validation Loss: 0.6641510725021362\n",
      "Epoch 96: Train Loss: 0.7030701041221619, Validation Loss: 0.6633628606796265\n",
      "Epoch 97: Train Loss: 0.6773292660713196, Validation Loss: 0.6658287048339844\n",
      "Epoch 98: Train Loss: 0.7091778755187989, Validation Loss: 0.665673017501831\n",
      "Epoch 99: Train Loss: 0.6889708518981934, Validation Loss: 0.6657576560974121\n",
      "Epoch 100: Train Loss: 0.659381651878357, Validation Loss: 0.6663954257965088\n",
      "Epoch 101: Train Loss: 0.6552427649497986, Validation Loss: 0.6664213538169861\n",
      "Epoch 102: Train Loss: 0.7043640375137329, Validation Loss: 0.665345311164856\n",
      "Epoch 103: Train Loss: 0.6565967798233032, Validation Loss: 0.6655896902084351\n",
      "Epoch 104: Train Loss: 0.7023726582527161, Validation Loss: 0.6655559539794922\n",
      "Epoch 105: Train Loss: 0.630900251865387, Validation Loss: 0.6654254794120789\n",
      "Epoch 106: Train Loss: 0.6592567801475525, Validation Loss: 0.6676760911941528\n",
      "Epoch 107: Train Loss: 0.7175679326057434, Validation Loss: 0.6678721308708191\n",
      "Epoch 108: Train Loss: 0.6751705884933472, Validation Loss: 0.6674937009811401\n",
      "Epoch 109: Train Loss: 0.6601809561252594, Validation Loss: 0.6667011976242065\n",
      "Epoch 110: Train Loss: 0.6801928758621216, Validation Loss: 0.6655237078666687\n",
      "Epoch 111: Train Loss: 0.6838485717773437, Validation Loss: 0.6659755110740662\n",
      "Epoch 112: Train Loss: 0.6669273734092712, Validation Loss: 0.6653886437416077\n",
      "Epoch 113: Train Loss: 0.6601161122322082, Validation Loss: 0.6661545634269714\n",
      "Epoch 114: Train Loss: 0.6797768592834472, Validation Loss: 0.6657733917236328\n",
      "Epoch 115: Train Loss: 0.698726499080658, Validation Loss: 0.6662444472312927\n",
      "Epoch 116: Train Loss: 0.6745227217674256, Validation Loss: 0.6653570532798767\n",
      "Epoch 117: Train Loss: 0.6838144302368164, Validation Loss: 0.664650559425354\n",
      "Epoch 118: Train Loss: 0.6598395824432373, Validation Loss: 0.6647724509239197\n",
      "Epoch 119: Train Loss: 0.6930304884910583, Validation Loss: 0.6642641425132751\n",
      "Epoch 120: Train Loss: 0.6513630509376526, Validation Loss: 0.6645671129226685\n",
      "Epoch 121: Train Loss: 0.7058819532394409, Validation Loss: 0.6646280884742737\n",
      "Epoch 122: Train Loss: 0.6694803595542907, Validation Loss: 0.6674978733062744\n",
      "Epoch 123: Train Loss: 0.6870789170265198, Validation Loss: 0.6661108732223511\n",
      "Epoch 124: Train Loss: 0.6702603816986084, Validation Loss: 0.6647416353225708\n",
      "Epoch 125: Train Loss: 0.6737631559371948, Validation Loss: 0.6652881503105164\n",
      "Epoch 126: Train Loss: 0.7092315912246704, Validation Loss: 0.6649708151817322\n",
      "Epoch 127: Train Loss: 0.6586580753326416, Validation Loss: 0.6669340133666992\n",
      "Epoch 128: Train Loss: 0.6820886492729187, Validation Loss: 0.6658608317375183\n",
      "Epoch 129: Train Loss: 0.6641549706459046, Validation Loss: 0.6674402356147766\n",
      "Epoch 130: Train Loss: 0.6466418266296386, Validation Loss: 0.6668047904968262\n",
      "Epoch 131: Train Loss: 0.723295021057129, Validation Loss: 0.6654693484306335\n",
      "Epoch 132: Train Loss: 0.7120617032051086, Validation Loss: 0.6648621559143066\n",
      "Epoch 133: Train Loss: 0.68690025806427, Validation Loss: 0.6655929684638977\n",
      "Epoch 134: Train Loss: 0.6535319566726685, Validation Loss: 0.6673616766929626\n",
      "Epoch 135: Train Loss: 0.6590803503990174, Validation Loss: 0.6673733592033386\n",
      "Epoch 136: Train Loss: 0.6803660154342651, Validation Loss: 0.665794312953949\n",
      "Epoch 137: Train Loss: 0.664426326751709, Validation Loss: 0.6651303172111511\n",
      "Epoch 138: Train Loss: 0.6709734082221985, Validation Loss: 0.6643233895301819\n",
      "Epoch 139: Train Loss: 0.682386291027069, Validation Loss: 0.664546549320221\n",
      "Epoch 140: Train Loss: 0.6589269995689392, Validation Loss: 0.6645203828811646\n",
      "Epoch 141: Train Loss: 0.6452265739440918, Validation Loss: 0.6649696230888367\n",
      "Epoch 142: Train Loss: 0.6805956840515137, Validation Loss: 0.6651113033294678\n",
      "Epoch 143: Train Loss: 0.648931908607483, Validation Loss: 0.6661554574966431\n",
      "Epoch 144: Train Loss: 0.6898146510124207, Validation Loss: 0.666020929813385\n",
      "Epoch 145: Train Loss: 0.6533251881599427, Validation Loss: 0.6657103300094604\n",
      "Epoch 146: Train Loss: 0.6722142577171326, Validation Loss: 0.6655221581459045\n",
      "Epoch 147: Train Loss: 0.6466144561767578, Validation Loss: 0.6658733487129211\n",
      "Epoch 148: Train Loss: 0.7515507578849793, Validation Loss: 0.6660467386245728\n",
      "Epoch 149: Train Loss: 0.6568790316581726, Validation Loss: 0.6657117009162903\n",
      "Epoch 150: Train Loss: 0.7110207557678223, Validation Loss: 0.6656430959701538\n",
      "Epoch 151: Train Loss: 0.6885709047317505, Validation Loss: 0.6661756038665771\n",
      "Epoch 152: Train Loss: 0.6544524908065796, Validation Loss: 0.6684377789497375\n",
      "Epoch 153: Train Loss: 0.6643986225128173, Validation Loss: 0.6684674620628357\n",
      "Epoch 154: Train Loss: 0.7275579452514649, Validation Loss: 0.6683748960494995\n",
      "Epoch 155: Train Loss: 0.6195863485336304, Validation Loss: 0.6683130264282227\n",
      "Epoch 156: Train Loss: 0.6852353811264038, Validation Loss: 0.6684837341308594\n",
      "Epoch 157: Train Loss: 0.6251193284988403, Validation Loss: 0.6681769490242004\n",
      "Epoch 158: Train Loss: 0.650644326210022, Validation Loss: 0.6690124273300171\n",
      "Epoch 159: Train Loss: 0.6515949010848999, Validation Loss: 0.6697379946708679\n",
      "Epoch 160: Train Loss: 0.6660189986228943, Validation Loss: 0.6706234812736511\n",
      "Epoch 161: Train Loss: 0.6563657283782959, Validation Loss: 0.670627236366272\n",
      "Epoch 162: Train Loss: 0.6516373515129089, Validation Loss: 0.6694895625114441\n",
      "Epoch 163: Train Loss: 0.6779699921607971, Validation Loss: 0.6682806015014648\n",
      "Epoch 164: Train Loss: 0.6865984439849854, Validation Loss: 0.6681759357452393\n",
      "Epoch 165: Train Loss: 0.6801044583320618, Validation Loss: 0.6686642169952393\n",
      "Epoch 166: Train Loss: 0.6442478418350219, Validation Loss: 0.6687877774238586\n",
      "Epoch 167: Train Loss: 0.6867799043655396, Validation Loss: 0.6690070033073425\n",
      "Epoch 168: Train Loss: 0.6316174507141114, Validation Loss: 0.6683803200721741\n",
      "Epoch 169: Train Loss: 0.6970423579216003, Validation Loss: 0.6685574650764465\n",
      "Epoch 170: Train Loss: 0.7211461424827575, Validation Loss: 0.6686325669288635\n",
      "Epoch 171: Train Loss: 0.6352204203605651, Validation Loss: 0.6689297556877136\n",
      "Epoch 172: Train Loss: 0.6685232520103455, Validation Loss: 0.6705734133720398\n",
      "Epoch 173: Train Loss: 0.6832379937171936, Validation Loss: 0.6706230044364929\n",
      "Epoch 174: Train Loss: 0.6870320796966553, Validation Loss: 0.6710100173950195\n",
      "Epoch 175: Train Loss: 0.6532892465591431, Validation Loss: 0.6700320839881897\n",
      "Epoch 176: Train Loss: 0.6781716704368591, Validation Loss: 0.670124888420105\n",
      "Epoch 177: Train Loss: 0.6729364633560181, Validation Loss: 0.6696355938911438\n",
      "Epoch 178: Train Loss: 0.6433481931686401, Validation Loss: 0.6697158813476562\n",
      "Epoch 179: Train Loss: 0.6966052412986755, Validation Loss: 0.6705535054206848\n",
      "Epoch 180: Train Loss: 0.6667476058006286, Validation Loss: 0.6704487204551697\n",
      "Epoch 181: Train Loss: 0.6595642685890197, Validation Loss: 0.6720496416091919\n",
      "Epoch 182: Train Loss: 0.6823758602142334, Validation Loss: 0.6709297299385071\n",
      "Epoch 183: Train Loss: 0.6478734493255616, Validation Loss: 0.6714138984680176\n",
      "Epoch 184: Train Loss: 0.6713473796844482, Validation Loss: 0.6711221933364868\n",
      "Epoch 185: Train Loss: 0.6600223541259765, Validation Loss: 0.6701216697692871\n",
      "Epoch 186: Train Loss: 0.7094537734985351, Validation Loss: 0.6695852875709534\n",
      "Epoch 187: Train Loss: 0.651386559009552, Validation Loss: 0.6689423322677612\n",
      "Epoch 188: Train Loss: 0.6622116923332214, Validation Loss: 0.6700044870376587\n",
      "Epoch 189: Train Loss: 0.7257614254951477, Validation Loss: 0.6691368222236633\n",
      "Epoch 190: Train Loss: 0.6756328225135804, Validation Loss: 0.6684142351150513\n",
      "Epoch 191: Train Loss: 0.6391793370246888, Validation Loss: 0.6679056882858276\n",
      "Epoch 192: Train Loss: 0.6499624133110047, Validation Loss: 0.6675292253494263\n",
      "Epoch 193: Train Loss: 0.6631656169891358, Validation Loss: 0.6676915287971497\n",
      "Epoch 194: Train Loss: 0.6619587540626526, Validation Loss: 0.668976902961731\n",
      "Epoch 195: Train Loss: 0.6661473155021668, Validation Loss: 0.6691701412200928\n",
      "Epoch 196: Train Loss: 0.6424872875213623, Validation Loss: 0.6691295504570007\n",
      "Epoch 197: Train Loss: 0.6395450234413147, Validation Loss: 0.6688161492347717\n",
      "Epoch 198: Train Loss: 0.6619184732437133, Validation Loss: 0.6683833599090576\n",
      "Epoch 199: Train Loss: 0.6594846367835998, Validation Loss: 0.6689578890800476\n",
      "Epoch 200: Train Loss: 0.6733051657676696, Validation Loss: 0.669396162033081\n",
      "Epoch 201: Train Loss: 0.6691148281097412, Validation Loss: 0.6692054867744446\n",
      "Epoch 202: Train Loss: 0.6708969831466675, Validation Loss: 0.669008731842041\n",
      "Epoch 203: Train Loss: 0.6839172601699829, Validation Loss: 0.6689515709877014\n",
      "Epoch 204: Train Loss: 0.6872481226921081, Validation Loss: 0.6680083274841309\n",
      "Epoch 205: Train Loss: 0.6577702045440674, Validation Loss: 0.6679495573043823\n",
      "Epoch 206: Train Loss: 0.6986012578010559, Validation Loss: 0.6683356165885925\n",
      "Epoch 207: Train Loss: 0.6892019271850586, Validation Loss: 0.6680428385734558\n",
      "Epoch 208: Train Loss: 0.6660657048225402, Validation Loss: 0.667801022529602\n",
      "Epoch 209: Train Loss: 0.6455574274063111, Validation Loss: 0.6673442721366882\n",
      "Epoch 210: Train Loss: 0.6465044379234314, Validation Loss: 0.6684291362762451\n",
      "Epoch 211: Train Loss: 0.7086422801017761, Validation Loss: 0.6689613461494446\n",
      "Epoch 212: Train Loss: 0.6659155130386353, Validation Loss: 0.6690645217895508\n",
      "Epoch 213: Train Loss: 0.6272417187690735, Validation Loss: 0.669201135635376\n",
      "Epoch 214: Train Loss: 0.694773530960083, Validation Loss: 0.6681931018829346\n",
      "Epoch 215: Train Loss: 0.6795175790786743, Validation Loss: 0.6694661974906921\n",
      "Epoch 216: Train Loss: 0.6668079376220704, Validation Loss: 0.6689692735671997\n",
      "Epoch 217: Train Loss: 0.656209659576416, Validation Loss: 0.6684796810150146\n",
      "Epoch 218: Train Loss: 0.6348565697669983, Validation Loss: 0.6680672764778137\n",
      "Epoch 219: Train Loss: 0.6494145274162293, Validation Loss: 0.6676232218742371\n",
      "Epoch 220: Train Loss: 0.6394580960273742, Validation Loss: 0.6684865355491638\n",
      "Epoch 221: Train Loss: 0.6555941939353943, Validation Loss: 0.6675713658332825\n",
      "Epoch 222: Train Loss: 0.6785080194473266, Validation Loss: 0.6673974990844727\n",
      "Epoch 223: Train Loss: 0.6601197600364686, Validation Loss: 0.6675106883049011\n",
      "Epoch 224: Train Loss: 0.6606190800666809, Validation Loss: 0.6680413484573364\n",
      "Epoch 225: Train Loss: 0.6901968121528625, Validation Loss: 0.6663872599601746\n",
      "Epoch 226: Train Loss: 0.666598105430603, Validation Loss: 0.6662828922271729\n",
      "Epoch 227: Train Loss: 0.6886835217475891, Validation Loss: 0.6672630906105042\n",
      "Epoch 228: Train Loss: 0.6400025963783265, Validation Loss: 0.6662408709526062\n",
      "Epoch 229: Train Loss: 0.6795023560523987, Validation Loss: 0.6653611660003662\n",
      "Epoch 230: Train Loss: 0.6696286559104919, Validation Loss: 0.6671766638755798\n",
      "Epoch 231: Train Loss: 0.6860731482505799, Validation Loss: 0.6673146486282349\n",
      "Epoch 232: Train Loss: 0.6487135052680969, Validation Loss: 0.6667205691337585\n",
      "Epoch 233: Train Loss: 0.6816251754760743, Validation Loss: 0.6668573021888733\n",
      "Epoch 234: Train Loss: 0.6385771870613098, Validation Loss: 0.6674422025680542\n",
      "Epoch 235: Train Loss: 0.6436462521553039, Validation Loss: 0.6690575480461121\n",
      "Epoch 236: Train Loss: 0.701884388923645, Validation Loss: 0.6691325902938843\n",
      "Epoch 237: Train Loss: 0.6484783053398132, Validation Loss: 0.668746829032898\n",
      "Epoch 238: Train Loss: 0.6699237108230591, Validation Loss: 0.6679561138153076\n",
      "Epoch 239: Train Loss: 0.6372853636741638, Validation Loss: 0.6689515709877014\n",
      "Epoch 240: Train Loss: 0.6539523363113403, Validation Loss: 0.6681666970252991\n",
      "Epoch 241: Train Loss: 0.6745324492454529, Validation Loss: 0.6676817536354065\n",
      "Epoch 242: Train Loss: 0.6339796662330628, Validation Loss: 0.6676514744758606\n",
      "Epoch 243: Train Loss: 0.6889285683631897, Validation Loss: 0.6682333946228027\n",
      "Epoch 244: Train Loss: 0.6178733944892884, Validation Loss: 0.6676602363586426\n",
      "Epoch 245: Train Loss: 0.6521102070808411, Validation Loss: 0.6680270433425903\n",
      "Epoch 246: Train Loss: 0.6306921362876892, Validation Loss: 0.6685223579406738\n",
      "Epoch 247: Train Loss: 0.686976683139801, Validation Loss: 0.6686969995498657\n",
      "Epoch 248: Train Loss: 0.6886607766151428, Validation Loss: 0.6682478785514832\n",
      "Epoch 249: Train Loss: 0.6287349939346314, Validation Loss: 0.6676777005195618\n",
      "Epoch 250: Train Loss: 0.6618553876876831, Validation Loss: 0.6677365899085999\n",
      "Epoch 251: Train Loss: 0.6311296701431275, Validation Loss: 0.6697801351547241\n",
      "Epoch 252: Train Loss: 0.6348963022232056, Validation Loss: 0.6703348159790039\n",
      "Epoch 253: Train Loss: 0.6961243987083435, Validation Loss: 0.6711230278015137\n",
      "Epoch 254: Train Loss: 0.6483001589775086, Validation Loss: 0.6714709401130676\n",
      "Epoch 255: Train Loss: 0.648369574546814, Validation Loss: 0.670581579208374\n",
      "Epoch 256: Train Loss: 0.626179826259613, Validation Loss: 0.6708815693855286\n",
      "Epoch 257: Train Loss: 0.6598145008087158, Validation Loss: 0.6709645390510559\n",
      "Epoch 258: Train Loss: 0.6813889384269715, Validation Loss: 0.6709901690483093\n",
      "Epoch 259: Train Loss: 0.7009344577789307, Validation Loss: 0.6708998680114746\n",
      "Epoch 260: Train Loss: 0.6314001321792603, Validation Loss: 0.6726573705673218\n",
      "Epoch 261: Train Loss: 0.6470475792884827, Validation Loss: 0.6728562116622925\n",
      "Epoch 262: Train Loss: 0.6548813581466675, Validation Loss: 0.672799289226532\n",
      "Epoch 263: Train Loss: 0.6712075591087341, Validation Loss: 0.6735120415687561\n",
      "Epoch 264: Train Loss: 0.6494112849235535, Validation Loss: 0.6727941036224365\n",
      "Epoch 265: Train Loss: 0.6532723188400269, Validation Loss: 0.6733919978141785\n",
      "Epoch 266: Train Loss: 0.6571437120437622, Validation Loss: 0.6726948618888855\n",
      "Epoch 267: Train Loss: 0.6464316964149475, Validation Loss: 0.6744123101234436\n",
      "Epoch 268: Train Loss: 0.6838893055915832, Validation Loss: 0.674919068813324\n",
      "Epoch 269: Train Loss: 0.6437506675720215, Validation Loss: 0.6746211051940918\n",
      "Epoch 270: Train Loss: 0.6641035795211792, Validation Loss: 0.6752358675003052\n",
      "Epoch 271: Train Loss: 0.6424289584159851, Validation Loss: 0.6745215058326721\n",
      "Epoch 272: Train Loss: 0.6746536254882812, Validation Loss: 0.6743048429489136\n",
      "Epoch 273: Train Loss: 0.6843353986740113, Validation Loss: 0.6739813089370728\n",
      "Epoch 274: Train Loss: 0.6694532871246338, Validation Loss: 0.6738131046295166\n",
      "Epoch 275: Train Loss: 0.6565793514251709, Validation Loss: 0.6735154986381531\n",
      "Epoch 276: Train Loss: 0.6448846697807312, Validation Loss: 0.6749646067619324\n",
      "Epoch 277: Train Loss: 0.678309154510498, Validation Loss: 0.6736158132553101\n",
      "Epoch 278: Train Loss: 0.6415717959403991, Validation Loss: 0.6748948097229004\n",
      "Epoch 279: Train Loss: 0.6573109030723572, Validation Loss: 0.6756635904312134\n",
      "Epoch 280: Train Loss: 0.6627694129943847, Validation Loss: 0.6747210621833801\n",
      "Epoch 281: Train Loss: 0.626075005531311, Validation Loss: 0.6731746792793274\n",
      "Epoch 282: Train Loss: 0.667449152469635, Validation Loss: 0.6719448566436768\n",
      "Epoch 283: Train Loss: 0.6445772886276245, Validation Loss: 0.6699415445327759\n",
      "Epoch 284: Train Loss: 0.6409129619598388, Validation Loss: 0.6695138812065125\n",
      "Epoch 285: Train Loss: 0.6421732544898987, Validation Loss: 0.6696052551269531\n",
      "Epoch 286: Train Loss: 0.6556796073913574, Validation Loss: 0.6693078279495239\n",
      "Epoch 287: Train Loss: 0.664045262336731, Validation Loss: 0.6694737672805786\n",
      "Epoch 288: Train Loss: 0.6587353467941284, Validation Loss: 0.668125569820404\n",
      "Epoch 289: Train Loss: 0.6348527193069458, Validation Loss: 0.6690149307250977\n",
      "Epoch 290: Train Loss: 0.6217738270759583, Validation Loss: 0.6684253811836243\n",
      "Epoch 291: Train Loss: 0.6609790802001954, Validation Loss: 0.6689593195915222\n",
      "Epoch 292: Train Loss: 0.6940707564353943, Validation Loss: 0.6690805554389954\n",
      "Epoch 293: Train Loss: 0.6343319892883301, Validation Loss: 0.6709420084953308\n",
      "Epoch 294: Train Loss: 0.6333614230155945, Validation Loss: 0.6703668832778931\n",
      "Epoch 295: Train Loss: 0.6046811521053315, Validation Loss: 0.6703346967697144\n",
      "Epoch 296: Train Loss: 0.636625099182129, Validation Loss: 0.6696235537528992\n",
      "Epoch 297: Train Loss: 0.6558864951133728, Validation Loss: 0.6708810329437256\n",
      "Epoch 298: Train Loss: 0.6287688732147216, Validation Loss: 0.6703775525093079\n",
      "Epoch 299: Train Loss: 0.6414507508277894, Validation Loss: 0.6688977479934692\n",
      "Epoch 300: Train Loss: 0.6536293864250183, Validation Loss: 0.6699554324150085\n",
      "Epoch 301: Train Loss: 0.6502585768699646, Validation Loss: 0.6717824339866638\n",
      "Epoch 302: Train Loss: 0.6863041758537293, Validation Loss: 0.6709046363830566\n",
      "Epoch 303: Train Loss: 0.6190457820892334, Validation Loss: 0.6712125539779663\n",
      "Epoch 304: Train Loss: 0.6802826881408691, Validation Loss: 0.6707690954208374\n",
      "Epoch 305: Train Loss: 0.6952944040298462, Validation Loss: 0.6707746982574463\n",
      "Epoch 306: Train Loss: 0.6485922574996948, Validation Loss: 0.6716676950454712\n",
      "Epoch 307: Train Loss: 0.6288033366203308, Validation Loss: 0.6724383234977722\n",
      "Epoch 308: Train Loss: 0.6515838623046875, Validation Loss: 0.6718167662620544\n",
      "Epoch 309: Train Loss: 0.6576048016548157, Validation Loss: 0.6723732948303223\n",
      "Epoch 310: Train Loss: 0.6745407938957214, Validation Loss: 0.6719474792480469\n",
      "Epoch 311: Train Loss: 0.6674982309341431, Validation Loss: 0.6728781461715698\n",
      "Epoch 312: Train Loss: 0.6548835039138794, Validation Loss: 0.672844409942627\n",
      "Epoch 313: Train Loss: 0.6560050487518311, Validation Loss: 0.6728472709655762\n",
      "Epoch 314: Train Loss: 0.5991742968559265, Validation Loss: 0.6733593940734863\n",
      "Epoch 315: Train Loss: 0.6918859362602234, Validation Loss: 0.6743413805961609\n",
      "Epoch 316: Train Loss: 0.6448263883590698, Validation Loss: 0.674802303314209\n",
      "Epoch 317: Train Loss: 0.6089921474456788, Validation Loss: 0.6758752465248108\n",
      "Epoch 318: Train Loss: 0.6399872183799744, Validation Loss: 0.6747706532478333\n",
      "Epoch 319: Train Loss: 0.6474898219108581, Validation Loss: 0.6745802760124207\n",
      "Epoch 320: Train Loss: 0.6536414384841919, Validation Loss: 0.676075279712677\n",
      "Epoch 321: Train Loss: 0.6526391744613648, Validation Loss: 0.6749826073646545\n",
      "Epoch 322: Train Loss: 0.6167831778526306, Validation Loss: 0.673662543296814\n",
      "Epoch 323: Train Loss: 0.6720400333404541, Validation Loss: 0.6738429665565491\n",
      "Epoch 324: Train Loss: 0.6808146953582763, Validation Loss: 0.6742552518844604\n",
      "Epoch 325: Train Loss: 0.6128281235694886, Validation Loss: 0.6751406788825989\n",
      "Epoch 326: Train Loss: 0.6649824976921082, Validation Loss: 0.6746348142623901\n",
      "Epoch 327: Train Loss: 0.6103919625282288, Validation Loss: 0.6756554245948792\n",
      "Epoch 328: Train Loss: 0.6176770091056824, Validation Loss: 0.6753963828086853\n",
      "Epoch 329: Train Loss: 0.6908398866653442, Validation Loss: 0.6745551824569702\n",
      "Epoch 330: Train Loss: 0.6382067322731018, Validation Loss: 0.6745586395263672\n",
      "Epoch 331: Train Loss: 0.619065237045288, Validation Loss: 0.6756049990653992\n",
      "Epoch 332: Train Loss: 0.6049721837043762, Validation Loss: 0.6755945086479187\n",
      "Epoch 333: Train Loss: 0.6171281576156616, Validation Loss: 0.6752945184707642\n",
      "Epoch 334: Train Loss: 0.6781335711479187, Validation Loss: 0.6754601001739502\n",
      "Epoch 335: Train Loss: 0.6642566323280334, Validation Loss: 0.6754709482192993\n",
      "Epoch 336: Train Loss: 0.6468843460083008, Validation Loss: 0.6754165291786194\n",
      "Epoch 337: Train Loss: 0.643565011024475, Validation Loss: 0.6756610870361328\n",
      "Epoch 338: Train Loss: 0.6593636274337769, Validation Loss: 0.6777722239494324\n",
      "Epoch 339: Train Loss: 0.6497237682342529, Validation Loss: 0.6770491600036621\n",
      "Epoch 340: Train Loss: 0.6260171413421631, Validation Loss: 0.677027702331543\n",
      "Epoch 341: Train Loss: 0.6908244252204895, Validation Loss: 0.6770543456077576\n",
      "Epoch 342: Train Loss: 0.6270605683326721, Validation Loss: 0.6770024299621582\n",
      "Epoch 343: Train Loss: 0.6277968525886536, Validation Loss: 0.6788606643676758\n",
      "Epoch 344: Train Loss: 0.6513828754425048, Validation Loss: 0.6774888634681702\n",
      "Epoch 345: Train Loss: 0.6279819726943969, Validation Loss: 0.678866446018219\n",
      "Epoch 346: Train Loss: 0.6044339895248413, Validation Loss: 0.6779221296310425\n",
      "Epoch 347: Train Loss: 0.6227824449539184, Validation Loss: 0.6777828931808472\n",
      "Epoch 348: Train Loss: 0.6715765595436096, Validation Loss: 0.678852915763855\n",
      "Epoch 349: Train Loss: 0.6370040297508239, Validation Loss: 0.678106427192688\n",
      "Epoch 350: Train Loss: 0.6463877081871032, Validation Loss: 0.6772781610488892\n",
      "Epoch 351: Train Loss: 0.6384679317474365, Validation Loss: 0.6770111918449402\n",
      "Epoch 352: Train Loss: 0.6235035300254822, Validation Loss: 0.6762042045593262\n",
      "Epoch 353: Train Loss: 0.6327755928039551, Validation Loss: 0.6755422949790955\n",
      "Epoch 354: Train Loss: 0.6726231455802918, Validation Loss: 0.6760134100914001\n",
      "Epoch 355: Train Loss: 0.6165553450584411, Validation Loss: 0.6754235625267029\n",
      "Epoch 356: Train Loss: 0.6649954438209533, Validation Loss: 0.6756739616394043\n",
      "Epoch 357: Train Loss: 0.6393975377082824, Validation Loss: 0.6754584908485413\n",
      "Epoch 358: Train Loss: 0.6331362724304199, Validation Loss: 0.6749504804611206\n",
      "Epoch 359: Train Loss: 0.620804488658905, Validation Loss: 0.6744214296340942\n",
      "Epoch 360: Train Loss: 0.6398886799812317, Validation Loss: 0.6742687821388245\n",
      "Epoch 361: Train Loss: 0.6529684662818909, Validation Loss: 0.6745648384094238\n",
      "Epoch 362: Train Loss: 0.689758563041687, Validation Loss: 0.6742017269134521\n",
      "Epoch 363: Train Loss: 0.5978078782558441, Validation Loss: 0.6750636696815491\n",
      "Epoch 364: Train Loss: 0.6430913925170898, Validation Loss: 0.6762217283248901\n",
      "Epoch 365: Train Loss: 0.6173081517219543, Validation Loss: 0.6748924255371094\n",
      "Epoch 366: Train Loss: 0.5996120154857636, Validation Loss: 0.674140453338623\n",
      "Epoch 367: Train Loss: 0.6670679450035095, Validation Loss: 0.6742963790893555\n",
      "Epoch 368: Train Loss: 0.6179563283920289, Validation Loss: 0.6745249032974243\n",
      "Epoch 369: Train Loss: 0.5904467284679413, Validation Loss: 0.6759868264198303\n",
      "Epoch 370: Train Loss: 0.6283505201339722, Validation Loss: 0.6756598949432373\n",
      "Epoch 371: Train Loss: 0.6330743074417114, Validation Loss: 0.6775386333465576\n",
      "Epoch 372: Train Loss: 0.6447965383529664, Validation Loss: 0.678099513053894\n",
      "Epoch 373: Train Loss: 0.6600830078125, Validation Loss: 0.678529679775238\n",
      "Epoch 374: Train Loss: 0.6197191834449768, Validation Loss: 0.6774561405181885\n",
      "Epoch 375: Train Loss: 0.6760103583335877, Validation Loss: 0.6796340942382812\n",
      "Epoch 376: Train Loss: 0.612522542476654, Validation Loss: 0.679183304309845\n",
      "Epoch 377: Train Loss: 0.5790228009223938, Validation Loss: 0.6796470880508423\n",
      "Epoch 378: Train Loss: 0.6175860524177551, Validation Loss: 0.6779630780220032\n",
      "Epoch 379: Train Loss: 0.6259791612625122, Validation Loss: 0.6780742406845093\n",
      "Epoch 380: Train Loss: 0.6147132873535156, Validation Loss: 0.6785502433776855\n",
      "Epoch 381: Train Loss: 0.6247068047523499, Validation Loss: 0.6789900064468384\n",
      "Epoch 382: Train Loss: 0.7436058998107911, Validation Loss: 0.6805823445320129\n",
      "Epoch 383: Train Loss: 0.6220084309577942, Validation Loss: 0.6821357607841492\n",
      "Epoch 384: Train Loss: 0.609753954410553, Validation Loss: 0.6810740232467651\n",
      "Epoch 385: Train Loss: 0.6714619517326355, Validation Loss: 0.6804857850074768\n",
      "Epoch 386: Train Loss: 0.6189308047294617, Validation Loss: 0.6817951202392578\n",
      "Epoch 387: Train Loss: 0.6151503682136535, Validation Loss: 0.6820041537284851\n",
      "Epoch 388: Train Loss: 0.5975576400756836, Validation Loss: 0.6809045672416687\n",
      "Epoch 389: Train Loss: 0.5889307856559753, Validation Loss: 0.6820926666259766\n",
      "Epoch 390: Train Loss: 0.6184173941612243, Validation Loss: 0.6828033328056335\n",
      "Epoch 391: Train Loss: 0.6067322134971619, Validation Loss: 0.682773768901825\n",
      "Epoch 392: Train Loss: 0.6407323241233825, Validation Loss: 0.6839441061019897\n",
      "Epoch 393: Train Loss: 0.6394516229629517, Validation Loss: 0.6824686527252197\n",
      "Epoch 394: Train Loss: 0.6082562923431396, Validation Loss: 0.6819756627082825\n",
      "Epoch 395: Train Loss: 0.6095850944519043, Validation Loss: 0.6819984912872314\n",
      "Epoch 396: Train Loss: 0.6384904980659485, Validation Loss: 0.6830034255981445\n",
      "Epoch 397: Train Loss: 0.6135967016220093, Validation Loss: 0.6824576258659363\n",
      "Epoch 398: Train Loss: 0.6465589523315429, Validation Loss: 0.6822358965873718\n",
      "Epoch 399: Train Loss: 0.6202649354934693, Validation Loss: 0.6821395754814148\n",
      "Epoch 400: Train Loss: 0.6499138116836548, Validation Loss: 0.6826561093330383\n",
      "Epoch 401: Train Loss: 0.5864322304725647, Validation Loss: 0.6830875277519226\n",
      "Epoch 402: Train Loss: 0.6778200984001159, Validation Loss: 0.6843764185905457\n",
      "Epoch 403: Train Loss: 0.6660575866699219, Validation Loss: 0.6861111521720886\n",
      "Epoch 404: Train Loss: 0.6238086104393006, Validation Loss: 0.6840653419494629\n",
      "Epoch 405: Train Loss: 0.6308718085289001, Validation Loss: 0.6842591762542725\n",
      "Epoch 406: Train Loss: 0.5969853520393371, Validation Loss: 0.6848493218421936\n",
      "Epoch 407: Train Loss: 0.6344001650810241, Validation Loss: 0.685019850730896\n",
      "Epoch 408: Train Loss: 0.6112536191940308, Validation Loss: 0.6848055124282837\n",
      "Epoch 409: Train Loss: 0.6354032754898071, Validation Loss: 0.6839045286178589\n",
      "Epoch 410: Train Loss: 0.6341105222702026, Validation Loss: 0.6833718419075012\n",
      "Epoch 411: Train Loss: 0.657716965675354, Validation Loss: 0.682269275188446\n",
      "Epoch 412: Train Loss: 0.6244221448898315, Validation Loss: 0.6807805895805359\n",
      "Epoch 413: Train Loss: 0.6223247408866882, Validation Loss: 0.6795989871025085\n",
      "Epoch 414: Train Loss: 0.6455849766731262, Validation Loss: 0.6811698079109192\n",
      "Epoch 415: Train Loss: 0.580477648973465, Validation Loss: 0.6837496757507324\n",
      "Epoch 416: Train Loss: 0.6379175901412963, Validation Loss: 0.682439386844635\n",
      "Epoch 417: Train Loss: 0.5819090604782104, Validation Loss: 0.6829605102539062\n",
      "Epoch 418: Train Loss: 0.6109267234802246, Validation Loss: 0.6822263598442078\n",
      "Epoch 419: Train Loss: 0.5915879368782043, Validation Loss: 0.6816295385360718\n",
      "Epoch 420: Train Loss: 0.6101698994636535, Validation Loss: 0.6831987500190735\n",
      "Epoch 421: Train Loss: 0.6041313529014587, Validation Loss: 0.6829599142074585\n",
      "Epoch 422: Train Loss: 0.6229336977005004, Validation Loss: 0.6831907033920288\n",
      "Epoch 423: Train Loss: 0.6246210813522339, Validation Loss: 0.6853287220001221\n",
      "Epoch 424: Train Loss: 0.6407150506973267, Validation Loss: 0.6872454881668091\n",
      "Epoch 425: Train Loss: 0.6245092749595642, Validation Loss: 0.6871253252029419\n",
      "Epoch 426: Train Loss: 0.6071039199829101, Validation Loss: 0.6872721910476685\n",
      "Epoch 427: Train Loss: 0.6682759165763855, Validation Loss: 0.686568021774292\n",
      "Epoch 428: Train Loss: 0.5933308362960815, Validation Loss: 0.6871705055236816\n",
      "Epoch 429: Train Loss: 0.6470963716506958, Validation Loss: 0.6891515254974365\n",
      "Epoch 430: Train Loss: 0.6029453754425049, Validation Loss: 0.689350426197052\n",
      "Epoch 431: Train Loss: 0.602413558959961, Validation Loss: 0.687849760055542\n",
      "Epoch 432: Train Loss: 0.6223435521125793, Validation Loss: 0.6866105794906616\n",
      "Epoch 433: Train Loss: 0.6341980814933776, Validation Loss: 0.6856678128242493\n",
      "Epoch 434: Train Loss: 0.5858215034008026, Validation Loss: 0.6869462728500366\n",
      "Epoch 435: Train Loss: 0.5939799666404724, Validation Loss: 0.68869948387146\n",
      "Epoch 436: Train Loss: 0.5707340955734252, Validation Loss: 0.689416766166687\n",
      "Epoch 437: Train Loss: 0.6209871053695679, Validation Loss: 0.6900366544723511\n",
      "Epoch 438: Train Loss: 0.6548158645629882, Validation Loss: 0.6898341178894043\n",
      "Epoch 439: Train Loss: 0.6349716067314148, Validation Loss: 0.6894640922546387\n",
      "Epoch 440: Train Loss: 0.594826853275299, Validation Loss: 0.6917800903320312\n",
      "Epoch 441: Train Loss: 0.6085445046424866, Validation Loss: 0.6922876834869385\n",
      "Epoch 442: Train Loss: 0.5837975263595581, Validation Loss: 0.6928118467330933\n",
      "Epoch 443: Train Loss: 0.603336763381958, Validation Loss: 0.6935555934906006\n",
      "Epoch 444: Train Loss: 0.6210930705070495, Validation Loss: 0.6939287781715393\n",
      "Epoch 445: Train Loss: 0.628398883342743, Validation Loss: 0.6946384906768799\n",
      "Epoch 446: Train Loss: 0.6206999778747558, Validation Loss: 0.6941773891448975\n",
      "Epoch 447: Train Loss: 0.6103424429893494, Validation Loss: 0.6942085027694702\n",
      "Epoch 448: Train Loss: 0.6116771340370178, Validation Loss: 0.6928446292877197\n",
      "Epoch 449: Train Loss: 0.574043071269989, Validation Loss: 0.6938249468803406\n",
      "Epoch 450: Train Loss: 0.6316682696342468, Validation Loss: 0.6954424977302551\n",
      "Epoch 451: Train Loss: 0.6122591853141784, Validation Loss: 0.6957007646560669\n",
      "Epoch 452: Train Loss: 0.6461602330207825, Validation Loss: 0.6982366442680359\n",
      "Epoch 453: Train Loss: 0.6409140467643738, Validation Loss: 0.7001297473907471\n",
      "Epoch 454: Train Loss: 0.5910974860191345, Validation Loss: 0.7002056837081909\n",
      "Epoch 455: Train Loss: 0.6288906455039978, Validation Loss: 0.7026202082633972\n",
      "Epoch 456: Train Loss: 0.6061100482940673, Validation Loss: 0.7031001448631287\n",
      "Epoch 457: Train Loss: 0.6191622853279114, Validation Loss: 0.7035050988197327\n",
      "Epoch 458: Train Loss: 0.5964873313903809, Validation Loss: 0.7028512954711914\n",
      "Epoch 459: Train Loss: 0.5897315740585327, Validation Loss: 0.7029767036437988\n",
      "Epoch 460: Train Loss: 0.5481056690216064, Validation Loss: 0.703957736492157\n",
      "Epoch 461: Train Loss: 0.5928872942924499, Validation Loss: 0.705195426940918\n",
      "Epoch 462: Train Loss: 0.592773163318634, Validation Loss: 0.703080952167511\n",
      "Epoch 463: Train Loss: 0.5743915915489197, Validation Loss: 0.7009952068328857\n",
      "Epoch 464: Train Loss: 0.5972800850868225, Validation Loss: 0.7016026377677917\n",
      "Epoch 465: Train Loss: 0.5959847807884217, Validation Loss: 0.7031991481781006\n",
      "Epoch 466: Train Loss: 0.5626168608665466, Validation Loss: 0.7039515376091003\n",
      "Epoch 467: Train Loss: 0.584191107749939, Validation Loss: 0.7044955492019653\n",
      "Epoch 468: Train Loss: 0.6227646231651306, Validation Loss: 0.706697940826416\n",
      "Epoch 469: Train Loss: 0.5723717331886291, Validation Loss: 0.7075297236442566\n",
      "Epoch 470: Train Loss: 0.6262313246726989, Validation Loss: 0.707512617111206\n",
      "Epoch 471: Train Loss: 0.595067834854126, Validation Loss: 0.7071030139923096\n",
      "Epoch 472: Train Loss: 0.58177250623703, Validation Loss: 0.7077025771141052\n",
      "Epoch 473: Train Loss: 0.6188357949256897, Validation Loss: 0.7105314135551453\n",
      "Epoch 474: Train Loss: 0.5705732107162476, Validation Loss: 0.7142005562782288\n",
      "Epoch 475: Train Loss: 0.5895396113395691, Validation Loss: 0.7151269912719727\n",
      "Epoch 476: Train Loss: 0.6288131952285767, Validation Loss: 0.7137687802314758\n",
      "Epoch 477: Train Loss: 0.6103720545768738, Validation Loss: 0.7135818600654602\n",
      "Epoch 478: Train Loss: 0.5789467394351959, Validation Loss: 0.716099739074707\n",
      "Epoch 479: Train Loss: 0.5752215445041656, Validation Loss: 0.7171632051467896\n",
      "Epoch 480: Train Loss: 0.6027461767196656, Validation Loss: 0.7158048152923584\n",
      "Epoch 481: Train Loss: 0.5660768628120423, Validation Loss: 0.7174390554428101\n",
      "Epoch 482: Train Loss: 0.5920256495475769, Validation Loss: 0.7199349403381348\n",
      "Epoch 483: Train Loss: 0.5471108138561249, Validation Loss: 0.719925045967102\n",
      "Epoch 484: Train Loss: 0.5421896874904633, Validation Loss: 0.7242415547370911\n",
      "Epoch 485: Train Loss: 0.5841940402984619, Validation Loss: 0.7254628539085388\n",
      "Epoch 486: Train Loss: 0.6059588551521301, Validation Loss: 0.7246044874191284\n",
      "Epoch 487: Train Loss: 0.5593016743659973, Validation Loss: 0.7260482907295227\n",
      "Epoch 488: Train Loss: 0.5509621798992157, Validation Loss: 0.7246440052986145\n",
      "Epoch 489: Train Loss: 0.5594397366046906, Validation Loss: 0.7227325439453125\n",
      "Epoch 490: Train Loss: 0.5705377697944641, Validation Loss: 0.7263825535774231\n",
      "Epoch 491: Train Loss: 0.5919554829597473, Validation Loss: 0.7273896336555481\n",
      "Epoch 492: Train Loss: 0.5962974548339843, Validation Loss: 0.7255318760871887\n",
      "Epoch 493: Train Loss: 0.54456467628479, Validation Loss: 0.7286281585693359\n",
      "Epoch 494: Train Loss: 0.5951196074485778, Validation Loss: 0.7310691475868225\n",
      "Epoch 495: Train Loss: 0.5471558153629303, Validation Loss: 0.7332596778869629\n",
      "Epoch 496: Train Loss: 0.5883580088615418, Validation Loss: 0.7329457998275757\n",
      "Epoch 497: Train Loss: 0.5769498825073243, Validation Loss: 0.7331570982933044\n",
      "Epoch 498: Train Loss: 0.5473927855491638, Validation Loss: 0.7364795207977295\n",
      "Epoch 499: Train Loss: 0.5531771063804627, Validation Loss: 0.7387580275535583\n",
      "Fold 2 Metrics:\n",
      "Accuracy: 0.36363636363636365, Precision: 0.0, Recall: 0.0, F1-score: 0.0, AUC: 0.4\n",
      "Confusion Matrix:\n",
      "[[4 1]\n",
      " [6 0]]\n",
      "Completed fold 2\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples from subject 3 to test set\n",
      "Adding 6 truth samples from subject 3 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 500)\n",
      "(132, 65, 500)\n",
      "Epoch 0: Train Loss: 0.7457680106163025, Validation Loss: 0.7003558874130249\n",
      "Epoch 1: Train Loss: 0.6792281627655029, Validation Loss: 0.7047181725502014\n",
      "Epoch 2: Train Loss: 0.7166374802589417, Validation Loss: 0.7101748585700989\n",
      "Epoch 3: Train Loss: 0.7137948274612427, Validation Loss: 0.7149153351783752\n",
      "Epoch 4: Train Loss: 0.6952681183815003, Validation Loss: 0.7197057604789734\n",
      "Epoch 5: Train Loss: 0.6816253304481507, Validation Loss: 0.7220438718795776\n",
      "Epoch 6: Train Loss: 0.7026024580001831, Validation Loss: 0.7237312197685242\n",
      "Epoch 7: Train Loss: 0.689506995677948, Validation Loss: 0.7272591590881348\n",
      "Epoch 8: Train Loss: 0.6611291766166687, Validation Loss: 0.7295033931732178\n",
      "Epoch 9: Train Loss: 0.6748136579990387, Validation Loss: 0.7311809659004211\n",
      "Epoch 10: Train Loss: 0.7075719237327576, Validation Loss: 0.7300112843513489\n",
      "Epoch 11: Train Loss: 0.70469172000885, Validation Loss: 0.7301162481307983\n",
      "Epoch 12: Train Loss: 0.7200459480285645, Validation Loss: 0.7297704219818115\n",
      "Epoch 13: Train Loss: 0.741655683517456, Validation Loss: 0.7286713719367981\n",
      "Epoch 14: Train Loss: 0.7405425667762756, Validation Loss: 0.7293288111686707\n",
      "Epoch 15: Train Loss: 0.7123993396759033, Validation Loss: 0.7292149662971497\n",
      "Epoch 16: Train Loss: 0.7079800248146058, Validation Loss: 0.7306554913520813\n",
      "Epoch 17: Train Loss: 0.6863319993019104, Validation Loss: 0.7281387448310852\n",
      "Epoch 18: Train Loss: 0.7866106867790222, Validation Loss: 0.7279649376869202\n",
      "Epoch 19: Train Loss: 0.6765926003456115, Validation Loss: 0.7265160083770752\n",
      "Epoch 20: Train Loss: 0.6711074590682984, Validation Loss: 0.7265913486480713\n",
      "Epoch 21: Train Loss: 0.7303344249725342, Validation Loss: 0.7275582551956177\n",
      "Epoch 22: Train Loss: 0.6888870120048523, Validation Loss: 0.7265245318412781\n",
      "Epoch 23: Train Loss: 0.6488690972328186, Validation Loss: 0.7254770994186401\n",
      "Epoch 24: Train Loss: 0.7297275424003601, Validation Loss: 0.725828230381012\n",
      "Epoch 25: Train Loss: 0.7060814023017883, Validation Loss: 0.7256312370300293\n",
      "Epoch 26: Train Loss: 0.6790290951728821, Validation Loss: 0.7245901226997375\n",
      "Epoch 27: Train Loss: 0.6661165595054627, Validation Loss: 0.7256577610969543\n",
      "Epoch 28: Train Loss: 0.7149288892745972, Validation Loss: 0.7257604002952576\n",
      "Epoch 29: Train Loss: 0.6917371988296509, Validation Loss: 0.72559654712677\n",
      "Epoch 30: Train Loss: 0.6997225642204284, Validation Loss: 0.726143479347229\n",
      "Epoch 31: Train Loss: 0.6893571376800537, Validation Loss: 0.7262197136878967\n",
      "Epoch 32: Train Loss: 0.6949416875839234, Validation Loss: 0.7270516157150269\n",
      "Epoch 33: Train Loss: 0.6921393871307373, Validation Loss: 0.7244933247566223\n",
      "Epoch 34: Train Loss: 0.6910389423370361, Validation Loss: 0.7258175015449524\n",
      "Epoch 35: Train Loss: 0.7393547415733337, Validation Loss: 0.7243849635124207\n",
      "Epoch 36: Train Loss: 0.7122397661209107, Validation Loss: 0.7255713939666748\n",
      "Epoch 37: Train Loss: 0.7198793530464173, Validation Loss: 0.7254427671432495\n",
      "Epoch 38: Train Loss: 0.6879804372787476, Validation Loss: 0.7244657278060913\n",
      "Epoch 39: Train Loss: 0.6749308586120606, Validation Loss: 0.7258653044700623\n",
      "Epoch 40: Train Loss: 0.7348812580108642, Validation Loss: 0.7256240844726562\n",
      "Epoch 41: Train Loss: 0.680924916267395, Validation Loss: 0.7268972396850586\n",
      "Epoch 42: Train Loss: 0.6819425344467163, Validation Loss: 0.7271455526351929\n",
      "Epoch 43: Train Loss: 0.6916045069694519, Validation Loss: 0.7281604409217834\n",
      "Epoch 44: Train Loss: 0.7243129730224609, Validation Loss: 0.725966215133667\n",
      "Epoch 45: Train Loss: 0.715238893032074, Validation Loss: 0.7267736196517944\n",
      "Epoch 46: Train Loss: 0.7368507266044617, Validation Loss: 0.7273005843162537\n",
      "Epoch 47: Train Loss: 0.7385558128356934, Validation Loss: 0.7284741401672363\n",
      "Epoch 48: Train Loss: 0.66376793384552, Validation Loss: 0.7287644743919373\n",
      "Epoch 49: Train Loss: 0.6924715995788574, Validation Loss: 0.7260136604309082\n",
      "Epoch 50: Train Loss: 0.7352980852127076, Validation Loss: 0.7264258861541748\n",
      "Epoch 51: Train Loss: 0.6510946989059448, Validation Loss: 0.7263181209564209\n",
      "Epoch 52: Train Loss: 0.7299789071083069, Validation Loss: 0.7260819673538208\n",
      "Epoch 53: Train Loss: 0.684490418434143, Validation Loss: 0.7256082892417908\n",
      "Epoch 54: Train Loss: 0.698989725112915, Validation Loss: 0.7257083058357239\n",
      "Epoch 55: Train Loss: 0.69000164270401, Validation Loss: 0.7270046472549438\n",
      "Epoch 56: Train Loss: 0.7176580309867859, Validation Loss: 0.7275035977363586\n",
      "Epoch 57: Train Loss: 0.6760390996932983, Validation Loss: 0.7257814407348633\n",
      "Epoch 58: Train Loss: 0.7032817721366882, Validation Loss: 0.7272341251373291\n",
      "Epoch 59: Train Loss: 0.7081882953643799, Validation Loss: 0.7274171113967896\n",
      "Epoch 60: Train Loss: 0.6673104643821717, Validation Loss: 0.7274358868598938\n",
      "Epoch 61: Train Loss: 0.7475774168968201, Validation Loss: 0.7267659306526184\n",
      "Epoch 62: Train Loss: 0.6867148995399475, Validation Loss: 0.7257611155509949\n",
      "Epoch 63: Train Loss: 0.686135184764862, Validation Loss: 0.7267243266105652\n",
      "Epoch 64: Train Loss: 0.7316450715065003, Validation Loss: 0.7269282937049866\n",
      "Epoch 65: Train Loss: 0.686917495727539, Validation Loss: 0.7273725271224976\n",
      "Epoch 66: Train Loss: 0.6915985703468323, Validation Loss: 0.7274282574653625\n",
      "Epoch 67: Train Loss: 0.7268553853034974, Validation Loss: 0.72837233543396\n",
      "Epoch 68: Train Loss: 0.7492321372032166, Validation Loss: 0.7284684181213379\n",
      "Epoch 69: Train Loss: 0.7103541254997253, Validation Loss: 0.7279181480407715\n",
      "Epoch 70: Train Loss: 0.6564463019371033, Validation Loss: 0.7266531586647034\n",
      "Epoch 71: Train Loss: 0.6814629077911377, Validation Loss: 0.7276620268821716\n",
      "Epoch 72: Train Loss: 0.6821109414100647, Validation Loss: 0.7263388633728027\n",
      "Epoch 73: Train Loss: 0.7293052911758423, Validation Loss: 0.7272146344184875\n",
      "Epoch 74: Train Loss: 0.6617242097854614, Validation Loss: 0.7285372614860535\n",
      "Epoch 75: Train Loss: 0.7054647326469421, Validation Loss: 0.730234682559967\n",
      "Epoch 76: Train Loss: 0.7180323243141175, Validation Loss: 0.7294632792472839\n",
      "Epoch 77: Train Loss: 0.7387643575668335, Validation Loss: 0.7296332716941833\n",
      "Epoch 78: Train Loss: 0.7141436815261841, Validation Loss: 0.7299904823303223\n",
      "Epoch 79: Train Loss: 0.6983277082443238, Validation Loss: 0.7303189635276794\n",
      "Epoch 80: Train Loss: 0.6774622082710267, Validation Loss: 0.7297506332397461\n",
      "Epoch 81: Train Loss: 0.6636909127235413, Validation Loss: 0.7288390398025513\n",
      "Epoch 82: Train Loss: 0.6856472611427307, Validation Loss: 0.7294213771820068\n",
      "Epoch 83: Train Loss: 0.6753843307495118, Validation Loss: 0.7285598516464233\n",
      "Epoch 84: Train Loss: 0.713069224357605, Validation Loss: 0.7297231554985046\n",
      "Epoch 85: Train Loss: 0.6535338997840882, Validation Loss: 0.7294211387634277\n",
      "Epoch 86: Train Loss: 0.6663397431373597, Validation Loss: 0.7292934060096741\n",
      "Epoch 87: Train Loss: 0.7039221286773681, Validation Loss: 0.7280198931694031\n",
      "Epoch 88: Train Loss: 0.6820768356323242, Validation Loss: 0.72678142786026\n",
      "Epoch 89: Train Loss: 0.6797410130500794, Validation Loss: 0.7267439961433411\n",
      "Epoch 90: Train Loss: 0.7023336172103882, Validation Loss: 0.725423276424408\n",
      "Epoch 91: Train Loss: 0.6998560547828674, Validation Loss: 0.7248835563659668\n",
      "Epoch 92: Train Loss: 0.6678288102149963, Validation Loss: 0.7253939509391785\n",
      "Epoch 93: Train Loss: 0.6646877765655518, Validation Loss: 0.7253372669219971\n",
      "Epoch 94: Train Loss: 0.686428678035736, Validation Loss: 0.7245531678199768\n",
      "Epoch 95: Train Loss: 0.6375714659690856, Validation Loss: 0.7237332463264465\n",
      "Epoch 96: Train Loss: 0.6609043002128601, Validation Loss: 0.7252222299575806\n",
      "Epoch 97: Train Loss: 0.7010396957397461, Validation Loss: 0.7262768745422363\n",
      "Epoch 98: Train Loss: 0.6691481590270996, Validation Loss: 0.725770890712738\n",
      "Epoch 99: Train Loss: 0.66168612241745, Validation Loss: 0.7268170714378357\n",
      "Epoch 100: Train Loss: 0.7646955490112305, Validation Loss: 0.7270066738128662\n",
      "Epoch 101: Train Loss: 0.7241986870765686, Validation Loss: 0.7278133034706116\n",
      "Epoch 102: Train Loss: 0.6903292655944824, Validation Loss: 0.7271469831466675\n",
      "Epoch 103: Train Loss: 0.7204566597938538, Validation Loss: 0.7267832159996033\n",
      "Epoch 104: Train Loss: 0.6862941741943359, Validation Loss: 0.7263178825378418\n",
      "Epoch 105: Train Loss: 0.7043344855308533, Validation Loss: 0.7269194722175598\n",
      "Epoch 106: Train Loss: 0.713625144958496, Validation Loss: 0.7235684394836426\n",
      "Epoch 107: Train Loss: 0.7062574625015259, Validation Loss: 0.7247588038444519\n",
      "Epoch 108: Train Loss: 0.6735689878463745, Validation Loss: 0.7263919115066528\n",
      "Epoch 109: Train Loss: 0.679533040523529, Validation Loss: 0.7279950380325317\n",
      "Epoch 110: Train Loss: 0.686067271232605, Validation Loss: 0.7277811169624329\n",
      "Epoch 111: Train Loss: 0.6950111865997315, Validation Loss: 0.7279369235038757\n",
      "Epoch 112: Train Loss: 0.6872390270233154, Validation Loss: 0.7276528477668762\n",
      "Epoch 113: Train Loss: 0.703661847114563, Validation Loss: 0.7252397537231445\n",
      "Epoch 114: Train Loss: 0.6457529067993164, Validation Loss: 0.7263498306274414\n",
      "Epoch 115: Train Loss: 0.7061700105667115, Validation Loss: 0.7278920412063599\n",
      "Epoch 116: Train Loss: 0.6600569009780883, Validation Loss: 0.7296502590179443\n",
      "Epoch 117: Train Loss: 0.6860018849372864, Validation Loss: 0.7278487086296082\n",
      "Epoch 118: Train Loss: 0.6404141902923584, Validation Loss: 0.727008044719696\n",
      "Epoch 119: Train Loss: 0.7006802201271057, Validation Loss: 0.7263044118881226\n",
      "Epoch 120: Train Loss: 0.6679675817489624, Validation Loss: 0.7271481156349182\n",
      "Epoch 121: Train Loss: 0.7011133432388306, Validation Loss: 0.7263153195381165\n",
      "Epoch 122: Train Loss: 0.6675187587738037, Validation Loss: 0.7275975942611694\n",
      "Epoch 123: Train Loss: 0.6643423914909363, Validation Loss: 0.7269642949104309\n",
      "Epoch 124: Train Loss: 0.639972448348999, Validation Loss: 0.7289400100708008\n",
      "Epoch 125: Train Loss: 0.6997543931007385, Validation Loss: 0.7290940880775452\n",
      "Epoch 126: Train Loss: 0.6738581776618957, Validation Loss: 0.7253120541572571\n",
      "Epoch 127: Train Loss: 0.6788572430610657, Validation Loss: 0.7265151143074036\n",
      "Epoch 128: Train Loss: 0.6754374623298645, Validation Loss: 0.7276639938354492\n",
      "Epoch 129: Train Loss: 0.6964526772499084, Validation Loss: 0.727885901927948\n",
      "Epoch 130: Train Loss: 0.6893605470657349, Validation Loss: 0.7283212542533875\n",
      "Epoch 131: Train Loss: 0.6739320993423462, Validation Loss: 0.7290183305740356\n",
      "Epoch 132: Train Loss: 0.6498015761375427, Validation Loss: 0.7293593287467957\n",
      "Epoch 133: Train Loss: 0.731032133102417, Validation Loss: 0.7287824153900146\n",
      "Epoch 134: Train Loss: 0.6436302781105041, Validation Loss: 0.7293390035629272\n",
      "Epoch 135: Train Loss: 0.6770413279533386, Validation Loss: 0.7299214601516724\n",
      "Epoch 136: Train Loss: 0.6797190546989441, Validation Loss: 0.7287846803665161\n",
      "Epoch 137: Train Loss: 0.661622154712677, Validation Loss: 0.7272308468818665\n",
      "Epoch 138: Train Loss: 0.6477811694145202, Validation Loss: 0.7292473912239075\n",
      "Epoch 139: Train Loss: 0.6847860217094421, Validation Loss: 0.7290051579475403\n",
      "Epoch 140: Train Loss: 0.6955028653144837, Validation Loss: 0.7292599678039551\n",
      "Epoch 141: Train Loss: 0.643982183933258, Validation Loss: 0.72740238904953\n",
      "Epoch 142: Train Loss: 0.6764844655990601, Validation Loss: 0.7258965969085693\n",
      "Epoch 143: Train Loss: 0.6671785950660706, Validation Loss: 0.7275107502937317\n",
      "Epoch 144: Train Loss: 0.6540707468986511, Validation Loss: 0.7281781435012817\n",
      "Epoch 145: Train Loss: 0.6869284510612488, Validation Loss: 0.7290149331092834\n",
      "Epoch 146: Train Loss: 0.6416979670524597, Validation Loss: 0.7295640707015991\n",
      "Epoch 147: Train Loss: 0.6764177680015564, Validation Loss: 0.7286723256111145\n",
      "Epoch 148: Train Loss: 0.6385593056678772, Validation Loss: 0.729180634021759\n",
      "Epoch 149: Train Loss: 0.7001616477966308, Validation Loss: 0.7284951210021973\n",
      "Epoch 150: Train Loss: 0.7266919255256653, Validation Loss: 0.7273171544075012\n",
      "Epoch 151: Train Loss: 0.7141646027565003, Validation Loss: 0.7264358401298523\n",
      "Epoch 152: Train Loss: 0.6554559946060181, Validation Loss: 0.7268210649490356\n",
      "Epoch 153: Train Loss: 0.6683834671974183, Validation Loss: 0.7259988188743591\n",
      "Epoch 154: Train Loss: 0.6818384408950806, Validation Loss: 0.7234809994697571\n",
      "Epoch 155: Train Loss: 0.6570812582969665, Validation Loss: 0.7244142889976501\n",
      "Epoch 156: Train Loss: 0.6645075440406799, Validation Loss: 0.7247805595397949\n",
      "Epoch 157: Train Loss: 0.6537588953971862, Validation Loss: 0.725939929485321\n",
      "Epoch 158: Train Loss: 0.6742455363273621, Validation Loss: 0.7271440029144287\n",
      "Epoch 159: Train Loss: 0.6061546921730041, Validation Loss: 0.7267324328422546\n",
      "Epoch 160: Train Loss: 0.6318536520004272, Validation Loss: 0.7266518473625183\n",
      "Epoch 161: Train Loss: 0.6697454571723938, Validation Loss: 0.7274770736694336\n",
      "Epoch 162: Train Loss: 0.6513380646705628, Validation Loss: 0.7248905301094055\n",
      "Epoch 163: Train Loss: 0.6878950953483581, Validation Loss: 0.7263930439949036\n",
      "Epoch 164: Train Loss: 0.6912261009216308, Validation Loss: 0.7268801927566528\n",
      "Epoch 165: Train Loss: 0.6307195782661438, Validation Loss: 0.7256438136100769\n",
      "Epoch 166: Train Loss: 0.6709388136863709, Validation Loss: 0.7270949482917786\n",
      "Epoch 167: Train Loss: 0.7033130645751953, Validation Loss: 0.7280113101005554\n",
      "Epoch 168: Train Loss: 0.6547006487846374, Validation Loss: 0.7287025451660156\n",
      "Epoch 169: Train Loss: 0.6599264502525329, Validation Loss: 0.7281249165534973\n",
      "Epoch 170: Train Loss: 0.64876708984375, Validation Loss: 0.7287054061889648\n",
      "Epoch 171: Train Loss: 0.6973204970359802, Validation Loss: 0.7301562428474426\n",
      "Epoch 172: Train Loss: 0.6475039958953858, Validation Loss: 0.7275794148445129\n",
      "Epoch 173: Train Loss: 0.617560613155365, Validation Loss: 0.7293405532836914\n",
      "Epoch 174: Train Loss: 0.6884378790855408, Validation Loss: 0.7299189567565918\n",
      "Epoch 175: Train Loss: 0.6685884952545166, Validation Loss: 0.7317933440208435\n",
      "Epoch 176: Train Loss: 0.6438822746276855, Validation Loss: 0.7300521731376648\n",
      "Epoch 177: Train Loss: 0.6541546702384948, Validation Loss: 0.7307088375091553\n",
      "Epoch 178: Train Loss: 0.6850820302963256, Validation Loss: 0.7309087514877319\n",
      "Epoch 179: Train Loss: 0.7071627020835877, Validation Loss: 0.7319806814193726\n",
      "Epoch 180: Train Loss: 0.6385947108268738, Validation Loss: 0.7323873043060303\n",
      "Epoch 181: Train Loss: 0.609224271774292, Validation Loss: 0.729458749294281\n",
      "Epoch 182: Train Loss: 0.6189449787139892, Validation Loss: 0.7305490374565125\n",
      "Epoch 183: Train Loss: 0.6773896098136902, Validation Loss: 0.7309571504592896\n",
      "Epoch 184: Train Loss: 0.6765432953834534, Validation Loss: 0.7321019768714905\n",
      "Epoch 185: Train Loss: 0.6554132699966431, Validation Loss: 0.732261061668396\n",
      "Epoch 186: Train Loss: 0.6559517025947571, Validation Loss: 0.7331604957580566\n",
      "Epoch 187: Train Loss: 0.6227232694625855, Validation Loss: 0.7300294041633606\n",
      "Epoch 188: Train Loss: 0.7010478258132935, Validation Loss: 0.7313920259475708\n",
      "Epoch 189: Train Loss: 0.7211374163627624, Validation Loss: 0.7303428649902344\n",
      "Epoch 190: Train Loss: 0.6944652795791626, Validation Loss: 0.7314260005950928\n",
      "Epoch 191: Train Loss: 0.6266459465026856, Validation Loss: 0.7321698069572449\n",
      "Epoch 192: Train Loss: 0.6989209175109863, Validation Loss: 0.732814371585846\n",
      "Epoch 193: Train Loss: 0.6712544083595275, Validation Loss: 0.731987714767456\n",
      "Epoch 194: Train Loss: 0.6440467953681945, Validation Loss: 0.7298883199691772\n",
      "Epoch 195: Train Loss: 0.6757591366767883, Validation Loss: 0.732064962387085\n",
      "Epoch 196: Train Loss: 0.6665884613990783, Validation Loss: 0.7321609258651733\n",
      "Epoch 197: Train Loss: 0.6809583306312561, Validation Loss: 0.7337169051170349\n",
      "Epoch 198: Train Loss: 0.6394798278808593, Validation Loss: 0.7342801690101624\n",
      "Epoch 199: Train Loss: 0.6535388469696045, Validation Loss: 0.7339879870414734\n",
      "Epoch 200: Train Loss: 0.6548559904098511, Validation Loss: 0.734650194644928\n",
      "Epoch 201: Train Loss: 0.6639235258102417, Validation Loss: 0.7346575856208801\n",
      "Epoch 202: Train Loss: 0.6066354036331176, Validation Loss: 0.7324984669685364\n",
      "Epoch 203: Train Loss: 0.6589178323745728, Validation Loss: 0.7343901991844177\n",
      "Epoch 204: Train Loss: 0.6694396615028382, Validation Loss: 0.7353068590164185\n",
      "Epoch 205: Train Loss: 0.7139790773391723, Validation Loss: 0.7368054986000061\n",
      "Epoch 206: Train Loss: 0.6178776621818542, Validation Loss: 0.735146701335907\n",
      "Epoch 207: Train Loss: 0.6769349455833436, Validation Loss: 0.7354413270950317\n",
      "Epoch 208: Train Loss: 0.6911230683326721, Validation Loss: 0.7357568740844727\n",
      "Epoch 209: Train Loss: 0.710522472858429, Validation Loss: 0.7363332509994507\n",
      "Epoch 210: Train Loss: 0.6354697346687317, Validation Loss: 0.7326124310493469\n",
      "Epoch 211: Train Loss: 0.6319598972797393, Validation Loss: 0.7344041466712952\n",
      "Epoch 212: Train Loss: 0.6419464230537415, Validation Loss: 0.7353588938713074\n",
      "Epoch 213: Train Loss: 0.6475852727890015, Validation Loss: 0.7356526255607605\n",
      "Epoch 214: Train Loss: 0.6393683075904846, Validation Loss: 0.7358480095863342\n",
      "Epoch 215: Train Loss: 0.6839537501335144, Validation Loss: 0.7378098964691162\n",
      "Epoch 216: Train Loss: 0.6338833332061767, Validation Loss: 0.7388293743133545\n",
      "Epoch 217: Train Loss: 0.6490400671958924, Validation Loss: 0.7384527325630188\n",
      "Epoch 218: Train Loss: 0.7044413566589356, Validation Loss: 0.7398673892021179\n",
      "Epoch 219: Train Loss: 0.6696004390716552, Validation Loss: 0.7398710250854492\n",
      "Epoch 220: Train Loss: 0.6712791442871093, Validation Loss: 0.7398388981819153\n",
      "Epoch 221: Train Loss: 0.6538524150848388, Validation Loss: 0.7383506894111633\n",
      "Epoch 222: Train Loss: 0.6249294281005859, Validation Loss: 0.7322432398796082\n",
      "Epoch 223: Train Loss: 0.6161953091621399, Validation Loss: 0.7359585762023926\n",
      "Epoch 224: Train Loss: 0.6535621762275696, Validation Loss: 0.738609790802002\n",
      "Epoch 225: Train Loss: 0.7243499636650086, Validation Loss: 0.7363134026527405\n",
      "Epoch 226: Train Loss: 0.6672202587127686, Validation Loss: 0.7385131120681763\n",
      "Epoch 227: Train Loss: 0.7093718886375427, Validation Loss: 0.7397764325141907\n",
      "Epoch 228: Train Loss: 0.6936870574951172, Validation Loss: 0.7420587539672852\n",
      "Epoch 229: Train Loss: 0.5937078416347503, Validation Loss: 0.7402597069740295\n",
      "Epoch 230: Train Loss: 0.6383318901062012, Validation Loss: 0.7422942519187927\n",
      "Epoch 231: Train Loss: 0.6165081858634949, Validation Loss: 0.7427724599838257\n",
      "Epoch 232: Train Loss: 0.6491340517997741, Validation Loss: 0.745773434638977\n",
      "Epoch 233: Train Loss: 0.6096949100494384, Validation Loss: 0.7441233992576599\n",
      "Epoch 234: Train Loss: 0.6229441285133361, Validation Loss: 0.744769275188446\n",
      "Epoch 235: Train Loss: 0.6676296830177307, Validation Loss: 0.7460271716117859\n",
      "Epoch 236: Train Loss: 0.6635297060012817, Validation Loss: 0.7457787394523621\n",
      "Epoch 237: Train Loss: 0.6356086015701294, Validation Loss: 0.7476688027381897\n",
      "Epoch 238: Train Loss: 0.6688969016075135, Validation Loss: 0.7500997185707092\n",
      "Epoch 239: Train Loss: 0.6576478242874145, Validation Loss: 0.7444064021110535\n",
      "Epoch 240: Train Loss: 0.6562139511108398, Validation Loss: 0.7468966841697693\n",
      "Epoch 241: Train Loss: 0.6538100600242615, Validation Loss: 0.7477856874465942\n",
      "Epoch 242: Train Loss: 0.6498881101608276, Validation Loss: 0.7505449056625366\n",
      "Epoch 243: Train Loss: 0.6172379136085511, Validation Loss: 0.7443442344665527\n",
      "Epoch 244: Train Loss: 0.666321063041687, Validation Loss: 0.7468318343162537\n",
      "Epoch 245: Train Loss: 0.6656720161437988, Validation Loss: 0.7501571178436279\n",
      "Epoch 246: Train Loss: 0.6406876802444458, Validation Loss: 0.7515907287597656\n",
      "Epoch 247: Train Loss: 0.6907269597053528, Validation Loss: 0.7522532939910889\n",
      "Epoch 248: Train Loss: 0.6261768221855164, Validation Loss: 0.7521203756332397\n",
      "Epoch 249: Train Loss: 0.6766842365264892, Validation Loss: 0.752003014087677\n",
      "Epoch 250: Train Loss: 0.6552941799163818, Validation Loss: 0.7519451975822449\n",
      "Epoch 251: Train Loss: 0.6596499919891358, Validation Loss: 0.7525308728218079\n",
      "Epoch 252: Train Loss: 0.6708127975463867, Validation Loss: 0.7556607127189636\n",
      "Epoch 253: Train Loss: 0.5940249025821686, Validation Loss: 0.7517072558403015\n",
      "Epoch 254: Train Loss: 0.6888360261917115, Validation Loss: 0.7545647621154785\n",
      "Epoch 255: Train Loss: 0.6202287077903748, Validation Loss: 0.7565496563911438\n",
      "Epoch 256: Train Loss: 0.6605809092521667, Validation Loss: 0.7572643160820007\n",
      "Epoch 257: Train Loss: 0.6255217671394349, Validation Loss: 0.7515460848808289\n",
      "Epoch 258: Train Loss: 0.6048653423786163, Validation Loss: 0.75281822681427\n",
      "Epoch 259: Train Loss: 0.6972264528274537, Validation Loss: 0.756350040435791\n",
      "Epoch 260: Train Loss: 0.6751647472381592, Validation Loss: 0.7576225399971008\n",
      "Epoch 261: Train Loss: 0.659665334224701, Validation Loss: 0.7587212920188904\n",
      "Epoch 262: Train Loss: 0.6036064743995666, Validation Loss: 0.7593151926994324\n",
      "Epoch 263: Train Loss: 0.6185480833053589, Validation Loss: 0.7599018216133118\n",
      "Epoch 264: Train Loss: 0.6274242162704468, Validation Loss: 0.7541683316230774\n",
      "Epoch 265: Train Loss: 0.6250165700912476, Validation Loss: 0.7555993795394897\n",
      "Epoch 266: Train Loss: 0.6582414507865906, Validation Loss: 0.7559720277786255\n",
      "Epoch 267: Train Loss: 0.6290850877761841, Validation Loss: 0.7578092217445374\n",
      "Epoch 268: Train Loss: 0.6718432426452636, Validation Loss: 0.7585711479187012\n",
      "Epoch 269: Train Loss: 0.6836604714393616, Validation Loss: 0.7579343914985657\n",
      "Epoch 270: Train Loss: 0.6749556422233581, Validation Loss: 0.7590274214744568\n",
      "Epoch 271: Train Loss: 0.6392264008522034, Validation Loss: 0.7579411864280701\n",
      "Epoch 272: Train Loss: 0.6614278197288513, Validation Loss: 0.7598598599433899\n",
      "Epoch 273: Train Loss: 0.6971671223640442, Validation Loss: 0.7597233653068542\n",
      "Epoch 274: Train Loss: 0.6366811871528626, Validation Loss: 0.758884608745575\n",
      "Epoch 275: Train Loss: 0.6343082666397095, Validation Loss: 0.7619290351867676\n",
      "Epoch 276: Train Loss: 0.6506158471107483, Validation Loss: 0.7591105699539185\n",
      "Epoch 277: Train Loss: 0.6721877217292785, Validation Loss: 0.7589858770370483\n",
      "Epoch 278: Train Loss: 0.664948308467865, Validation Loss: 0.760320782661438\n",
      "Epoch 279: Train Loss: 0.6298892855644226, Validation Loss: 0.7591130137443542\n",
      "Epoch 280: Train Loss: 0.6282931566238403, Validation Loss: 0.7592424154281616\n",
      "Epoch 281: Train Loss: 0.5966810941696167, Validation Loss: 0.7567042708396912\n",
      "Epoch 282: Train Loss: 0.6196385085582733, Validation Loss: 0.7535941004753113\n",
      "Epoch 283: Train Loss: 0.6176045894622803, Validation Loss: 0.7522429823875427\n",
      "Epoch 284: Train Loss: 0.6245084524154663, Validation Loss: 0.7530370354652405\n",
      "Epoch 285: Train Loss: 0.7184777379035949, Validation Loss: 0.7564078569412231\n",
      "Epoch 286: Train Loss: 0.6416595578193665, Validation Loss: 0.759221076965332\n",
      "Epoch 287: Train Loss: 0.6231434106826782, Validation Loss: 0.7617208361625671\n",
      "Epoch 288: Train Loss: 0.60091912150383, Validation Loss: 0.7622336745262146\n",
      "Epoch 289: Train Loss: 0.6321274280548096, Validation Loss: 0.7570985555648804\n",
      "Epoch 290: Train Loss: 0.6071085453033447, Validation Loss: 0.7605597972869873\n",
      "Epoch 291: Train Loss: 0.6675673842430114, Validation Loss: 0.7648071050643921\n",
      "Epoch 292: Train Loss: 0.5765684723854065, Validation Loss: 0.7687259316444397\n",
      "Epoch 293: Train Loss: 0.6769402861595154, Validation Loss: 0.7721022963523865\n",
      "Epoch 294: Train Loss: 0.6722177028656006, Validation Loss: 0.7738224864006042\n",
      "Epoch 295: Train Loss: 0.5875243186950684, Validation Loss: 0.7743412852287292\n",
      "Epoch 296: Train Loss: 0.604953384399414, Validation Loss: 0.77405846118927\n",
      "Epoch 297: Train Loss: 0.6142714023590088, Validation Loss: 0.7745444178581238\n",
      "Epoch 298: Train Loss: 0.6120978116989135, Validation Loss: 0.7722419500350952\n",
      "Epoch 299: Train Loss: 0.6220451235771179, Validation Loss: 0.77475905418396\n",
      "Epoch 300: Train Loss: 0.6515808939933777, Validation Loss: 0.7687478065490723\n",
      "Epoch 301: Train Loss: 0.6718176484107972, Validation Loss: 0.7729878425598145\n",
      "Epoch 302: Train Loss: 0.647282886505127, Validation Loss: 0.7727099657058716\n",
      "Epoch 303: Train Loss: 0.6655726790428161, Validation Loss: 0.7693243026733398\n",
      "Epoch 304: Train Loss: 0.6107541084289551, Validation Loss: 0.768899142742157\n",
      "Epoch 305: Train Loss: 0.5895579993724823, Validation Loss: 0.7673692107200623\n",
      "Epoch 306: Train Loss: 0.6245346069335938, Validation Loss: 0.7706916928291321\n",
      "Epoch 307: Train Loss: 0.6990346312522888, Validation Loss: 0.7748551964759827\n",
      "Epoch 308: Train Loss: 0.6537065982818604, Validation Loss: 0.7767751216888428\n",
      "Epoch 309: Train Loss: 0.5931261539459228, Validation Loss: 0.777398407459259\n",
      "Epoch 310: Train Loss: 0.6696437120437622, Validation Loss: 0.7766589522361755\n",
      "Epoch 311: Train Loss: 0.6063868522644043, Validation Loss: 0.7781283855438232\n",
      "Epoch 312: Train Loss: 0.6210942983627319, Validation Loss: 0.7791334986686707\n",
      "Epoch 313: Train Loss: 0.6558153748512268, Validation Loss: 0.7793725728988647\n",
      "Epoch 314: Train Loss: 0.6030161023139954, Validation Loss: 0.7818541526794434\n",
      "Epoch 315: Train Loss: 0.602107048034668, Validation Loss: 0.7815704345703125\n",
      "Epoch 316: Train Loss: 0.6444207787513733, Validation Loss: 0.7844361662864685\n",
      "Epoch 317: Train Loss: 0.5845933139324189, Validation Loss: 0.783122181892395\n",
      "Epoch 318: Train Loss: 0.6091121912002564, Validation Loss: 0.7808757424354553\n",
      "Epoch 319: Train Loss: 0.5989567518234253, Validation Loss: 0.782375156879425\n",
      "Epoch 320: Train Loss: 0.6578130960464478, Validation Loss: 0.7814725041389465\n",
      "Epoch 321: Train Loss: 0.6356133937835693, Validation Loss: 0.7779791951179504\n",
      "Epoch 322: Train Loss: 0.6623090505599976, Validation Loss: 0.7841621041297913\n",
      "Epoch 323: Train Loss: 0.6065679669380188, Validation Loss: 0.7770430445671082\n",
      "Epoch 324: Train Loss: 0.5925460815429687, Validation Loss: 0.7838870882987976\n",
      "Epoch 325: Train Loss: 0.6341788172721863, Validation Loss: 0.7837872505187988\n",
      "Epoch 326: Train Loss: 0.611195969581604, Validation Loss: 0.7874078750610352\n",
      "Epoch 327: Train Loss: 0.6073597371578217, Validation Loss: 0.7913724184036255\n",
      "Epoch 328: Train Loss: 0.5730472862720489, Validation Loss: 0.7804839611053467\n",
      "Epoch 329: Train Loss: 0.632285761833191, Validation Loss: 0.7843786478042603\n",
      "Epoch 330: Train Loss: 0.6440839648246766, Validation Loss: 0.7895038723945618\n",
      "Epoch 331: Train Loss: 0.6400110363960266, Validation Loss: 0.7873426675796509\n",
      "Epoch 332: Train Loss: 0.6317749977111816, Validation Loss: 0.7893627285957336\n",
      "Epoch 333: Train Loss: 0.6212386965751648, Validation Loss: 0.790004312992096\n",
      "Epoch 334: Train Loss: 0.6252762079238892, Validation Loss: 0.7920659184455872\n",
      "Epoch 335: Train Loss: 0.6127027571201324, Validation Loss: 0.7846047878265381\n",
      "Epoch 336: Train Loss: 0.6359068512916565, Validation Loss: 0.7868552207946777\n",
      "Epoch 337: Train Loss: 0.6118352174758911, Validation Loss: 0.7862964272499084\n",
      "Epoch 338: Train Loss: 0.5979834079742432, Validation Loss: 0.7865650057792664\n",
      "Epoch 339: Train Loss: 0.6220879197120667, Validation Loss: 0.7889156341552734\n",
      "Epoch 340: Train Loss: 0.6259220838546753, Validation Loss: 0.7928836345672607\n",
      "Epoch 341: Train Loss: 0.6337728023529052, Validation Loss: 0.7933753132820129\n",
      "Epoch 342: Train Loss: 0.6239514231681824, Validation Loss: 0.7974458336830139\n",
      "Epoch 343: Train Loss: 0.6195050716400147, Validation Loss: 0.7970548272132874\n",
      "Epoch 344: Train Loss: 0.6056307196617127, Validation Loss: 0.7855319380760193\n",
      "Epoch 345: Train Loss: 0.606915557384491, Validation Loss: 0.787366509437561\n",
      "Epoch 346: Train Loss: 0.6146851301193237, Validation Loss: 0.7930663228034973\n",
      "Epoch 347: Train Loss: 0.6696345448493958, Validation Loss: 0.7989267110824585\n",
      "Epoch 348: Train Loss: 0.6680598258972168, Validation Loss: 0.7986404299736023\n",
      "Epoch 349: Train Loss: 0.6163488149642944, Validation Loss: 0.7994548082351685\n",
      "Epoch 350: Train Loss: 0.6698421955108642, Validation Loss: 0.8000814318656921\n",
      "Epoch 351: Train Loss: 0.6275842428207398, Validation Loss: 0.801618754863739\n",
      "Epoch 352: Train Loss: 0.6428417325019836, Validation Loss: 0.8033230900764465\n",
      "Epoch 353: Train Loss: 0.6207820057868958, Validation Loss: 0.7887862920761108\n",
      "Epoch 354: Train Loss: 0.6474242687225342, Validation Loss: 0.7939967513084412\n",
      "Epoch 355: Train Loss: 0.6264123678207397, Validation Loss: 0.7982288002967834\n",
      "Epoch 356: Train Loss: 0.5978959321975708, Validation Loss: 0.8008597493171692\n",
      "Epoch 357: Train Loss: 0.6185644745826722, Validation Loss: 0.8037299513816833\n",
      "Epoch 358: Train Loss: 0.5814995348453522, Validation Loss: 0.7953081130981445\n",
      "Epoch 359: Train Loss: 0.596128499507904, Validation Loss: 0.8014463186264038\n",
      "Epoch 360: Train Loss: 0.6046287059783936, Validation Loss: 0.7999686598777771\n",
      "Epoch 361: Train Loss: 0.59436194896698, Validation Loss: 0.7998498678207397\n",
      "Epoch 362: Train Loss: 0.5968881964683532, Validation Loss: 0.8038787841796875\n",
      "Epoch 363: Train Loss: 0.6333657026290893, Validation Loss: 0.8084426522254944\n",
      "Epoch 364: Train Loss: 0.6094963192939759, Validation Loss: 0.8109308481216431\n",
      "Epoch 365: Train Loss: 0.6661765456199646, Validation Loss: 0.812082052230835\n",
      "Epoch 366: Train Loss: 0.6173470377922058, Validation Loss: 0.8059310913085938\n",
      "Epoch 367: Train Loss: 0.6998072981834411, Validation Loss: 0.810499906539917\n",
      "Epoch 368: Train Loss: 0.6716495990753174, Validation Loss: 0.8117257952690125\n",
      "Epoch 369: Train Loss: 0.6601012945175171, Validation Loss: 0.8116039037704468\n",
      "Epoch 370: Train Loss: 0.593168568611145, Validation Loss: 0.8124146461486816\n",
      "Epoch 371: Train Loss: 0.6490467548370361, Validation Loss: 0.8144950270652771\n",
      "Epoch 372: Train Loss: 0.6219452261924744, Validation Loss: 0.8128633499145508\n",
      "Epoch 373: Train Loss: 0.5816363096237183, Validation Loss: 0.8170872926712036\n",
      "Epoch 374: Train Loss: 0.5926800012588501, Validation Loss: 0.8188855648040771\n",
      "Epoch 375: Train Loss: 0.6197062730789185, Validation Loss: 0.8147646188735962\n",
      "Epoch 376: Train Loss: 0.6909667491912842, Validation Loss: 0.8154168725013733\n",
      "Epoch 377: Train Loss: 0.6427281022071838, Validation Loss: 0.8166381120681763\n",
      "Epoch 378: Train Loss: 0.587160712480545, Validation Loss: 0.8151906728744507\n",
      "Epoch 379: Train Loss: 0.6539088726043701, Validation Loss: 0.8187759518623352\n",
      "Epoch 380: Train Loss: 0.7331149458885193, Validation Loss: 0.8227072954177856\n",
      "Epoch 381: Train Loss: 0.5981803059577941, Validation Loss: 0.8245319128036499\n",
      "Epoch 382: Train Loss: 0.6263083100318909, Validation Loss: 0.8272510766983032\n",
      "Epoch 383: Train Loss: 0.5967246532440186, Validation Loss: 0.8224359750747681\n",
      "Epoch 384: Train Loss: 0.5781857371330261, Validation Loss: 0.8237159252166748\n",
      "Epoch 385: Train Loss: 0.6356471538543701, Validation Loss: 0.8184199929237366\n",
      "Epoch 386: Train Loss: 0.6171130657196044, Validation Loss: 0.820348858833313\n",
      "Epoch 387: Train Loss: 0.5621210932731628, Validation Loss: 0.822431743144989\n",
      "Epoch 388: Train Loss: 0.5906568109989166, Validation Loss: 0.8237015008926392\n",
      "Epoch 389: Train Loss: 0.5676346302032471, Validation Loss: 0.8108766674995422\n",
      "Epoch 390: Train Loss: 0.6199577212333679, Validation Loss: 0.8140369653701782\n",
      "Epoch 391: Train Loss: 0.59870365858078, Validation Loss: 0.8193341493606567\n",
      "Epoch 392: Train Loss: 0.5794939815998077, Validation Loss: 0.8137601613998413\n",
      "Epoch 393: Train Loss: 0.6614227771759034, Validation Loss: 0.8228594064712524\n",
      "Epoch 394: Train Loss: 0.6139498710632324, Validation Loss: 0.8255918025970459\n",
      "Epoch 395: Train Loss: 0.570418381690979, Validation Loss: 0.8255186080932617\n",
      "Epoch 396: Train Loss: 0.5946440935134888, Validation Loss: 0.8287301063537598\n",
      "Epoch 397: Train Loss: 0.6793368458747864, Validation Loss: 0.833548903465271\n",
      "Epoch 398: Train Loss: 0.5993840336799622, Validation Loss: 0.8336077928543091\n",
      "Epoch 399: Train Loss: 0.5960419476032257, Validation Loss: 0.832638680934906\n",
      "Epoch 400: Train Loss: 0.6119136333465576, Validation Loss: 0.831696629524231\n",
      "Epoch 401: Train Loss: 0.6186523199081421, Validation Loss: 0.8321590423583984\n",
      "Epoch 402: Train Loss: 0.6226645231246948, Validation Loss: 0.8379315733909607\n",
      "Epoch 403: Train Loss: 0.6471321821212769, Validation Loss: 0.8365621566772461\n",
      "Epoch 404: Train Loss: 0.6152594566345215, Validation Loss: 0.828499972820282\n",
      "Epoch 405: Train Loss: 0.6072793960571289, Validation Loss: 0.8337435722351074\n",
      "Epoch 406: Train Loss: 0.5717901885509491, Validation Loss: 0.8313758969306946\n",
      "Epoch 407: Train Loss: 0.5968852877616883, Validation Loss: 0.818003237247467\n",
      "Epoch 408: Train Loss: 0.5739625096321106, Validation Loss: 0.8265062570571899\n",
      "Epoch 409: Train Loss: 0.6128730297088623, Validation Loss: 0.8279634118080139\n",
      "Epoch 410: Train Loss: 0.611845862865448, Validation Loss: 0.8292199373245239\n",
      "Epoch 411: Train Loss: 0.6481129288673401, Validation Loss: 0.8364078402519226\n",
      "Epoch 412: Train Loss: 0.5615783572196961, Validation Loss: 0.8390186429023743\n",
      "Epoch 413: Train Loss: 0.6772063732147217, Validation Loss: 0.8427141308784485\n",
      "Epoch 414: Train Loss: 0.5656460046768188, Validation Loss: 0.8481325507164001\n",
      "Epoch 415: Train Loss: 0.5911590218544006, Validation Loss: 0.8366044759750366\n",
      "Epoch 416: Train Loss: 0.5936784982681275, Validation Loss: 0.8387628793716431\n",
      "Epoch 417: Train Loss: 0.6868391156196594, Validation Loss: 0.840920627117157\n",
      "Epoch 418: Train Loss: 0.5886570811271667, Validation Loss: 0.8437036275863647\n",
      "Epoch 419: Train Loss: 0.6400595545768738, Validation Loss: 0.8450520634651184\n",
      "Epoch 420: Train Loss: 0.6290754199028015, Validation Loss: 0.8487443327903748\n",
      "Epoch 421: Train Loss: 0.5789230585098266, Validation Loss: 0.8506456017494202\n",
      "Epoch 422: Train Loss: 0.5961497783660888, Validation Loss: 0.8537231683731079\n",
      "Epoch 423: Train Loss: 0.6509564042091369, Validation Loss: 0.8540380001068115\n",
      "Epoch 424: Train Loss: 0.576879906654358, Validation Loss: 0.8529258370399475\n",
      "Epoch 425: Train Loss: 0.586064875125885, Validation Loss: 0.856895387172699\n",
      "Epoch 426: Train Loss: 0.5866738080978393, Validation Loss: 0.8534976840019226\n",
      "Epoch 427: Train Loss: 0.5821503520011901, Validation Loss: 0.8417456746101379\n",
      "Epoch 428: Train Loss: 0.6403771877288819, Validation Loss: 0.8461410403251648\n",
      "Epoch 429: Train Loss: 0.6154636859893798, Validation Loss: 0.8502983450889587\n",
      "Epoch 430: Train Loss: 0.5933852910995483, Validation Loss: 0.8535581231117249\n",
      "Epoch 431: Train Loss: 0.6335107922554016, Validation Loss: 0.8622412085533142\n",
      "Epoch 432: Train Loss: 0.5448216080665589, Validation Loss: 0.8475403189659119\n",
      "Epoch 433: Train Loss: 0.6123145878314972, Validation Loss: 0.8567367196083069\n",
      "Epoch 434: Train Loss: 0.632559311389923, Validation Loss: 0.8588838577270508\n",
      "Epoch 435: Train Loss: 0.6095780968666077, Validation Loss: 0.8567798137664795\n",
      "Epoch 436: Train Loss: 0.6032766580581665, Validation Loss: 0.8434625864028931\n",
      "Epoch 437: Train Loss: 0.6378398537635803, Validation Loss: 0.8510225415229797\n",
      "Epoch 438: Train Loss: 0.567108976840973, Validation Loss: 0.8587290048599243\n",
      "Epoch 439: Train Loss: 0.6301348090171814, Validation Loss: 0.865102231502533\n",
      "Epoch 440: Train Loss: 0.6089048743247986, Validation Loss: 0.8738468289375305\n",
      "Epoch 441: Train Loss: 0.6022242307662964, Validation Loss: 0.8866185545921326\n",
      "Epoch 442: Train Loss: 0.5798663258552551, Validation Loss: 0.8889625668525696\n",
      "Epoch 443: Train Loss: 0.5554833173751831, Validation Loss: 0.8908557295799255\n",
      "Epoch 444: Train Loss: 0.6086782217025757, Validation Loss: 0.8964970707893372\n",
      "Epoch 445: Train Loss: 0.6074485421180725, Validation Loss: 0.8692101836204529\n",
      "Epoch 446: Train Loss: 0.5960126161575318, Validation Loss: 0.8808469176292419\n",
      "Epoch 447: Train Loss: 0.5469425797462464, Validation Loss: 0.8873840570449829\n",
      "Epoch 448: Train Loss: 0.6192711591720581, Validation Loss: 0.8825538754463196\n",
      "Epoch 449: Train Loss: 0.548513650894165, Validation Loss: 0.8768323659896851\n",
      "Epoch 450: Train Loss: 0.5793042182922363, Validation Loss: 0.8841228485107422\n",
      "Epoch 451: Train Loss: 0.5564971387386322, Validation Loss: 0.8901373147964478\n",
      "Epoch 452: Train Loss: 0.5276514768600464, Validation Loss: 0.8904836773872375\n",
      "Epoch 453: Train Loss: 0.5516774415969848, Validation Loss: 0.8968155980110168\n",
      "Epoch 454: Train Loss: 0.5942012310028076, Validation Loss: 0.9026666283607483\n",
      "Epoch 455: Train Loss: 0.6004082441329956, Validation Loss: 0.9046031832695007\n",
      "Epoch 456: Train Loss: 0.5465922176837921, Validation Loss: 0.9013952612876892\n",
      "Epoch 457: Train Loss: 0.5990079641342163, Validation Loss: 0.8961699604988098\n",
      "Epoch 458: Train Loss: 0.5642431735992431, Validation Loss: 0.8970111012458801\n",
      "Epoch 459: Train Loss: 0.579354989528656, Validation Loss: 0.9091128706932068\n",
      "Epoch 460: Train Loss: 0.5717244029045105, Validation Loss: 0.8964328169822693\n",
      "Epoch 461: Train Loss: 0.5605204820632934, Validation Loss: 0.8756818771362305\n",
      "Epoch 462: Train Loss: 0.5760605692863464, Validation Loss: 0.8841090798377991\n",
      "Epoch 463: Train Loss: 0.5826452136039734, Validation Loss: 0.8918739557266235\n",
      "Epoch 464: Train Loss: 0.5625529825687409, Validation Loss: 0.8824281692504883\n",
      "Epoch 465: Train Loss: 0.6004919052124024, Validation Loss: 0.8883947134017944\n",
      "Epoch 466: Train Loss: 0.6154878199100494, Validation Loss: 0.8933046460151672\n",
      "Epoch 467: Train Loss: 0.5809579849243164, Validation Loss: 0.8984328508377075\n",
      "Epoch 468: Train Loss: 0.5052487134933472, Validation Loss: 0.9042240381240845\n",
      "Epoch 469: Train Loss: 0.6346694827079773, Validation Loss: 0.9050356149673462\n",
      "Epoch 470: Train Loss: 0.5297387182712555, Validation Loss: 0.9075211882591248\n",
      "Epoch 471: Train Loss: 0.5762730956077575, Validation Loss: 0.9179078340530396\n",
      "Epoch 472: Train Loss: 0.630619752407074, Validation Loss: 0.901829719543457\n",
      "Epoch 473: Train Loss: 0.5806081652641296, Validation Loss: 0.8930496573448181\n",
      "Epoch 474: Train Loss: 0.5869667291641235, Validation Loss: 0.8973756432533264\n",
      "Epoch 475: Train Loss: 0.5767533779144287, Validation Loss: 0.9059220552444458\n",
      "Epoch 476: Train Loss: 0.6184056401252747, Validation Loss: 0.9143211245536804\n",
      "Epoch 477: Train Loss: 0.5106501758098603, Validation Loss: 0.9070690274238586\n",
      "Epoch 478: Train Loss: 0.5159970581531524, Validation Loss: 0.9011877179145813\n",
      "Epoch 479: Train Loss: 0.5263408720493317, Validation Loss: 0.9070087671279907\n",
      "Epoch 480: Train Loss: 0.5521787643432617, Validation Loss: 0.9039737582206726\n",
      "Epoch 481: Train Loss: 0.5603631019592286, Validation Loss: 0.9095037579536438\n",
      "Epoch 482: Train Loss: 0.5422051668167114, Validation Loss: 0.9033016562461853\n",
      "Epoch 483: Train Loss: 0.5944841265678406, Validation Loss: 0.9116297960281372\n",
      "Epoch 484: Train Loss: 0.5839468359947204, Validation Loss: 0.9234300255775452\n",
      "Epoch 485: Train Loss: 0.6064507961273193, Validation Loss: 0.9309413433074951\n",
      "Epoch 486: Train Loss: 0.5741193652153015, Validation Loss: 0.9367877840995789\n",
      "Epoch 487: Train Loss: 0.5043152332305908, Validation Loss: 0.9268617630004883\n",
      "Epoch 488: Train Loss: 0.5643343687057495, Validation Loss: 0.9330141544342041\n",
      "Epoch 489: Train Loss: 0.5433639645576477, Validation Loss: 0.9349990487098694\n",
      "Epoch 490: Train Loss: 0.5873652219772338, Validation Loss: 0.9387295246124268\n",
      "Epoch 491: Train Loss: 0.5254247844219208, Validation Loss: 0.9349544048309326\n",
      "Epoch 492: Train Loss: 0.5355801105499267, Validation Loss: 0.9298840761184692\n",
      "Epoch 493: Train Loss: 0.5986788868904114, Validation Loss: 0.9397008419036865\n",
      "Epoch 494: Train Loss: 0.5369882464408875, Validation Loss: 0.937615156173706\n",
      "Epoch 495: Train Loss: 0.6210979521274567, Validation Loss: 0.9475362300872803\n",
      "Epoch 496: Train Loss: 0.5522506475448609, Validation Loss: 0.9507582187652588\n",
      "Epoch 497: Train Loss: 0.6279378950595855, Validation Loss: 0.9461475610733032\n",
      "Epoch 498: Train Loss: 0.5632047355175018, Validation Loss: 0.9491811394691467\n",
      "Epoch 499: Train Loss: 0.5286485433578492, Validation Loss: 0.9509994387626648\n",
      "Fold 3 Metrics:\n",
      "Accuracy: 0.45454545454545453, Precision: 0.5, Recall: 0.5, F1-score: 0.5, AUC: 0.45000000000000007\n",
      "Confusion Matrix:\n",
      "[[2 3]\n",
      " [3 3]]\n",
      "Completed fold 3\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples from subject 10 to test set\n",
      "Adding 6 truth samples from subject 10 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 500)\n",
      "(132, 65, 500)\n",
      "Epoch 0: Train Loss: 0.6942812919616699, Validation Loss: 0.6987677812576294\n",
      "Epoch 1: Train Loss: 0.7138457655906677, Validation Loss: 0.7015492916107178\n",
      "Epoch 2: Train Loss: 0.7097591757774353, Validation Loss: 0.7051029205322266\n",
      "Epoch 3: Train Loss: 0.7259985566139221, Validation Loss: 0.7082038521766663\n",
      "Epoch 4: Train Loss: 0.7418055891990661, Validation Loss: 0.7113974094390869\n",
      "Epoch 5: Train Loss: 0.6978631734848022, Validation Loss: 0.7128859758377075\n",
      "Epoch 6: Train Loss: 0.7123038411140442, Validation Loss: 0.7148895859718323\n",
      "Epoch 7: Train Loss: 0.6955327093601227, Validation Loss: 0.715772271156311\n",
      "Epoch 8: Train Loss: 0.7132941365242005, Validation Loss: 0.71593177318573\n",
      "Epoch 9: Train Loss: 0.7258050322532654, Validation Loss: 0.716912031173706\n",
      "Epoch 10: Train Loss: 0.7244604706764222, Validation Loss: 0.7177353501319885\n",
      "Epoch 11: Train Loss: 0.7208434462547302, Validation Loss: 0.7180574536323547\n",
      "Epoch 12: Train Loss: 0.7113519787788392, Validation Loss: 0.7174707055091858\n",
      "Epoch 13: Train Loss: 0.7115511775016785, Validation Loss: 0.7175918817520142\n",
      "Epoch 14: Train Loss: 0.6998067021369934, Validation Loss: 0.7181126475334167\n",
      "Epoch 15: Train Loss: 0.752810025215149, Validation Loss: 0.7177025675773621\n",
      "Epoch 16: Train Loss: 0.7420617461204528, Validation Loss: 0.7178672552108765\n",
      "Epoch 17: Train Loss: 0.7303808689117431, Validation Loss: 0.7178549766540527\n",
      "Epoch 18: Train Loss: 0.7012222409248352, Validation Loss: 0.7177574634552002\n",
      "Epoch 19: Train Loss: 0.7243762254714966, Validation Loss: 0.717054009437561\n",
      "Epoch 20: Train Loss: 0.6604514479637146, Validation Loss: 0.7166997194290161\n",
      "Epoch 21: Train Loss: 0.6878460884094239, Validation Loss: 0.7159920334815979\n",
      "Epoch 22: Train Loss: 0.7040798187255859, Validation Loss: 0.7155866026878357\n",
      "Epoch 23: Train Loss: 0.7352888703346252, Validation Loss: 0.7161177396774292\n",
      "Epoch 24: Train Loss: 0.68228999376297, Validation Loss: 0.7168971300125122\n",
      "Epoch 25: Train Loss: 0.7493892073631286, Validation Loss: 0.7176365256309509\n",
      "Epoch 26: Train Loss: 0.7550032734870911, Validation Loss: 0.7179723381996155\n",
      "Epoch 27: Train Loss: 0.7751018166542053, Validation Loss: 0.7177071571350098\n",
      "Epoch 28: Train Loss: 0.7389358401298523, Validation Loss: 0.7175431847572327\n",
      "Epoch 29: Train Loss: 0.6861007809638977, Validation Loss: 0.7170903086662292\n",
      "Epoch 30: Train Loss: 0.660865581035614, Validation Loss: 0.7171952128410339\n",
      "Epoch 31: Train Loss: 0.7520761609077453, Validation Loss: 0.7170156240463257\n",
      "Epoch 32: Train Loss: 0.7278001308441162, Validation Loss: 0.7173290848731995\n",
      "Epoch 33: Train Loss: 0.7075016856193542, Validation Loss: 0.7166155576705933\n",
      "Epoch 34: Train Loss: 0.6724626064300537, Validation Loss: 0.7165817618370056\n",
      "Epoch 35: Train Loss: 0.6871744871139527, Validation Loss: 0.7171326279640198\n",
      "Epoch 36: Train Loss: 0.7219706296920776, Validation Loss: 0.7171878218650818\n",
      "Epoch 37: Train Loss: 0.7225942969322204, Validation Loss: 0.7180380821228027\n",
      "Epoch 38: Train Loss: 0.7005235195159912, Validation Loss: 0.7179434299468994\n",
      "Epoch 39: Train Loss: 0.7710323572158814, Validation Loss: 0.7177273035049438\n",
      "Epoch 40: Train Loss: 0.7123559474945068, Validation Loss: 0.7174517512321472\n",
      "Epoch 41: Train Loss: 0.7183726549148559, Validation Loss: 0.7169234156608582\n",
      "Epoch 42: Train Loss: 0.6881897568702697, Validation Loss: 0.7165946960449219\n",
      "Epoch 43: Train Loss: 0.7045480012893677, Validation Loss: 0.7163779735565186\n",
      "Epoch 44: Train Loss: 0.731408154964447, Validation Loss: 0.7161970734596252\n",
      "Epoch 45: Train Loss: 0.6862492442131043, Validation Loss: 0.7162657976150513\n",
      "Epoch 46: Train Loss: 0.6909774303436279, Validation Loss: 0.7165570855140686\n",
      "Epoch 47: Train Loss: 0.6802148222923279, Validation Loss: 0.7165489196777344\n",
      "Epoch 48: Train Loss: 0.7450902342796326, Validation Loss: 0.7161548137664795\n",
      "Epoch 49: Train Loss: 0.6509264469146728, Validation Loss: 0.7159290909767151\n",
      "Epoch 50: Train Loss: 0.7040538430213928, Validation Loss: 0.7167025208473206\n",
      "Epoch 51: Train Loss: 0.718264126777649, Validation Loss: 0.7163592576980591\n",
      "Epoch 52: Train Loss: 0.7015078783035278, Validation Loss: 0.71628737449646\n",
      "Epoch 53: Train Loss: 0.7125853776931763, Validation Loss: 0.7164140939712524\n",
      "Epoch 54: Train Loss: 0.6911133050918579, Validation Loss: 0.7155565023422241\n",
      "Epoch 55: Train Loss: 0.718532133102417, Validation Loss: 0.7157493233680725\n",
      "Epoch 56: Train Loss: 0.6926239371299744, Validation Loss: 0.715874969959259\n",
      "Epoch 57: Train Loss: 0.686817979812622, Validation Loss: 0.7160641551017761\n",
      "Epoch 58: Train Loss: 0.7049568176269532, Validation Loss: 0.7163180112838745\n",
      "Epoch 59: Train Loss: 0.7265234470367432, Validation Loss: 0.7165604829788208\n",
      "Epoch 60: Train Loss: 0.7396785974502563, Validation Loss: 0.7158306837081909\n",
      "Epoch 61: Train Loss: 0.6918187856674194, Validation Loss: 0.7156451344490051\n",
      "Epoch 62: Train Loss: 0.7248732447624207, Validation Loss: 0.7162148356437683\n",
      "Epoch 63: Train Loss: 0.697383189201355, Validation Loss: 0.7161797285079956\n",
      "Epoch 64: Train Loss: 0.7472493886947632, Validation Loss: 0.7166541814804077\n",
      "Epoch 65: Train Loss: 0.7218270778656006, Validation Loss: 0.7166453003883362\n",
      "Epoch 66: Train Loss: 0.7416052460670471, Validation Loss: 0.7164701223373413\n",
      "Epoch 67: Train Loss: 0.7527993321418762, Validation Loss: 0.7162734866142273\n",
      "Epoch 68: Train Loss: 0.6801138520240784, Validation Loss: 0.7163740992546082\n",
      "Epoch 69: Train Loss: 0.7138907551765442, Validation Loss: 0.7164275050163269\n",
      "Epoch 70: Train Loss: 0.7018520355224609, Validation Loss: 0.7158449292182922\n",
      "Epoch 71: Train Loss: 0.6703700542449951, Validation Loss: 0.7156568169593811\n",
      "Epoch 72: Train Loss: 0.7005691885948181, Validation Loss: 0.7149342894554138\n",
      "Epoch 73: Train Loss: 0.63568514585495, Validation Loss: 0.7145446538925171\n",
      "Epoch 74: Train Loss: 0.7375911355018616, Validation Loss: 0.7135835289955139\n",
      "Epoch 75: Train Loss: 0.7200226068496705, Validation Loss: 0.7134815454483032\n",
      "Epoch 76: Train Loss: 0.712916624546051, Validation Loss: 0.7138194441795349\n",
      "Epoch 77: Train Loss: 0.6977505803108215, Validation Loss: 0.7139318585395813\n",
      "Epoch 78: Train Loss: 0.7139316320419311, Validation Loss: 0.71406090259552\n",
      "Epoch 79: Train Loss: 0.7232097387313843, Validation Loss: 0.7143340110778809\n",
      "Epoch 80: Train Loss: 0.7080644607543946, Validation Loss: 0.713983416557312\n",
      "Epoch 81: Train Loss: 0.7144117951393127, Validation Loss: 0.7139281034469604\n",
      "Epoch 82: Train Loss: 0.6926381826400757, Validation Loss: 0.714037299156189\n",
      "Epoch 83: Train Loss: 0.7066364526748657, Validation Loss: 0.7140183448791504\n",
      "Epoch 84: Train Loss: 0.7218114495277405, Validation Loss: 0.7146592736244202\n",
      "Epoch 85: Train Loss: 0.7300023078918457, Validation Loss: 0.7147115468978882\n",
      "Epoch 86: Train Loss: 0.6717591285705566, Validation Loss: 0.7152422666549683\n",
      "Epoch 87: Train Loss: 0.7078065276145935, Validation Loss: 0.7154345512390137\n",
      "Epoch 88: Train Loss: 0.6722528219223023, Validation Loss: 0.715152382850647\n",
      "Epoch 89: Train Loss: 0.6723138689994812, Validation Loss: 0.7149761915206909\n",
      "Epoch 90: Train Loss: 0.6924058556556701, Validation Loss: 0.7151126265525818\n",
      "Epoch 91: Train Loss: 0.6780447363853455, Validation Loss: 0.7155529856681824\n",
      "Epoch 92: Train Loss: 0.7177696943283081, Validation Loss: 0.715472400188446\n",
      "Epoch 93: Train Loss: 0.7556736707687378, Validation Loss: 0.7159833908081055\n",
      "Epoch 94: Train Loss: 0.7355157136917114, Validation Loss: 0.7161996960639954\n",
      "Epoch 95: Train Loss: 0.6692986369132996, Validation Loss: 0.715590238571167\n",
      "Epoch 96: Train Loss: 0.6586309790611267, Validation Loss: 0.7146652936935425\n",
      "Epoch 97: Train Loss: 0.6834804058074951, Validation Loss: 0.71487957239151\n",
      "Epoch 98: Train Loss: 0.6949318647384644, Validation Loss: 0.7143998742103577\n",
      "Epoch 99: Train Loss: 0.7262432813644409, Validation Loss: 0.7145032286643982\n",
      "Epoch 100: Train Loss: 0.7064998745918274, Validation Loss: 0.7145971059799194\n",
      "Epoch 101: Train Loss: 0.7062037706375122, Validation Loss: 0.7146236300468445\n",
      "Epoch 102: Train Loss: 0.6746112942695618, Validation Loss: 0.7153843641281128\n",
      "Epoch 103: Train Loss: 0.6712029695510864, Validation Loss: 0.7154369354248047\n",
      "Epoch 104: Train Loss: 0.7185460567474365, Validation Loss: 0.7155963778495789\n",
      "Epoch 105: Train Loss: 0.6938544392585755, Validation Loss: 0.7157235145568848\n",
      "Epoch 106: Train Loss: 0.6773485779762268, Validation Loss: 0.7158052325248718\n",
      "Epoch 107: Train Loss: 0.7112125992774964, Validation Loss: 0.7152703404426575\n",
      "Epoch 108: Train Loss: 0.6703619480133056, Validation Loss: 0.7154489755630493\n",
      "Epoch 109: Train Loss: 0.6754182696342468, Validation Loss: 0.7150101661682129\n",
      "Epoch 110: Train Loss: 0.6867392301559448, Validation Loss: 0.7156559824943542\n",
      "Epoch 111: Train Loss: 0.716418468952179, Validation Loss: 0.7165926694869995\n",
      "Epoch 112: Train Loss: 0.6691841602325439, Validation Loss: 0.716350257396698\n",
      "Epoch 113: Train Loss: 0.6637298583984375, Validation Loss: 0.7164809703826904\n",
      "Epoch 114: Train Loss: 0.6927283406257629, Validation Loss: 0.7166872620582581\n",
      "Epoch 115: Train Loss: 0.687794315814972, Validation Loss: 0.7165749669075012\n",
      "Epoch 116: Train Loss: 0.715488612651825, Validation Loss: 0.7165417671203613\n",
      "Epoch 117: Train Loss: 0.713516640663147, Validation Loss: 0.7166852355003357\n",
      "Epoch 118: Train Loss: 0.6705358743667602, Validation Loss: 0.7165834307670593\n",
      "Epoch 119: Train Loss: 0.7119109392166137, Validation Loss: 0.7162806987762451\n",
      "Epoch 120: Train Loss: 0.6941909432411194, Validation Loss: 0.7163041830062866\n",
      "Epoch 121: Train Loss: 0.7021600723266601, Validation Loss: 0.7153323888778687\n",
      "Epoch 122: Train Loss: 0.68663569688797, Validation Loss: 0.7150608897209167\n",
      "Epoch 123: Train Loss: 0.7000750660896301, Validation Loss: 0.7152189016342163\n",
      "Epoch 124: Train Loss: 0.6762272477149963, Validation Loss: 0.7153723835945129\n",
      "Epoch 125: Train Loss: 0.6766736507415771, Validation Loss: 0.7152661085128784\n",
      "Epoch 126: Train Loss: 0.688905906677246, Validation Loss: 0.7154597043991089\n",
      "Epoch 127: Train Loss: 0.6902737259864807, Validation Loss: 0.7156405448913574\n",
      "Epoch 128: Train Loss: 0.6978845715522766, Validation Loss: 0.7155072093009949\n",
      "Epoch 129: Train Loss: 0.7026666283607483, Validation Loss: 0.7160531878471375\n",
      "Epoch 130: Train Loss: 0.7089655518531799, Validation Loss: 0.7154878377914429\n",
      "Epoch 131: Train Loss: 0.7113144636154175, Validation Loss: 0.7148087620735168\n",
      "Epoch 132: Train Loss: 0.7376029253005981, Validation Loss: 0.7148220539093018\n",
      "Epoch 133: Train Loss: 0.669923722743988, Validation Loss: 0.7148406505584717\n",
      "Epoch 134: Train Loss: 0.6885225892066955, Validation Loss: 0.7149091362953186\n",
      "Epoch 135: Train Loss: 0.7211946368217468, Validation Loss: 0.714658260345459\n",
      "Epoch 136: Train Loss: 0.6537765502929688, Validation Loss: 0.7147289514541626\n",
      "Epoch 137: Train Loss: 0.6985983490943909, Validation Loss: 0.7156371474266052\n",
      "Epoch 138: Train Loss: 0.6785063147544861, Validation Loss: 0.7156488299369812\n",
      "Epoch 139: Train Loss: 0.6664829850196838, Validation Loss: 0.7152175903320312\n",
      "Epoch 140: Train Loss: 0.6959982633590698, Validation Loss: 0.7150768041610718\n",
      "Epoch 141: Train Loss: 0.6742611169815064, Validation Loss: 0.7147818207740784\n",
      "Epoch 142: Train Loss: 0.683730399608612, Validation Loss: 0.7141029238700867\n",
      "Epoch 143: Train Loss: 0.6762157201766967, Validation Loss: 0.7134616374969482\n",
      "Epoch 144: Train Loss: 0.6656049847602844, Validation Loss: 0.7139703035354614\n",
      "Epoch 145: Train Loss: 0.6853799462318421, Validation Loss: 0.7141721248626709\n",
      "Epoch 146: Train Loss: 0.736207902431488, Validation Loss: 0.7145459055900574\n",
      "Epoch 147: Train Loss: 0.6816380858421326, Validation Loss: 0.7148520946502686\n",
      "Epoch 148: Train Loss: 0.6968767523765564, Validation Loss: 0.7148066163063049\n",
      "Epoch 149: Train Loss: 0.648120665550232, Validation Loss: 0.7145320177078247\n",
      "Epoch 150: Train Loss: 0.7114438891410828, Validation Loss: 0.7144820094108582\n",
      "Epoch 151: Train Loss: 0.6651053786277771, Validation Loss: 0.7141913175582886\n",
      "Epoch 152: Train Loss: 0.6449021816253662, Validation Loss: 0.7135717272758484\n",
      "Epoch 153: Train Loss: 0.723363173007965, Validation Loss: 0.7136140465736389\n",
      "Epoch 154: Train Loss: 0.6295361757278443, Validation Loss: 0.7134678363800049\n",
      "Epoch 155: Train Loss: 0.6976407527923584, Validation Loss: 0.7138429880142212\n",
      "Epoch 156: Train Loss: 0.6752359628677368, Validation Loss: 0.7138676643371582\n",
      "Epoch 157: Train Loss: 0.7086312055587769, Validation Loss: 0.7137470245361328\n",
      "Epoch 158: Train Loss: 0.6909738302230835, Validation Loss: 0.7134732604026794\n",
      "Epoch 159: Train Loss: 0.7245006561279297, Validation Loss: 0.7135868668556213\n",
      "Epoch 160: Train Loss: 0.6543512463569641, Validation Loss: 0.7134537100791931\n",
      "Epoch 161: Train Loss: 0.6624226808547974, Validation Loss: 0.713041365146637\n",
      "Epoch 162: Train Loss: 0.6553116083145142, Validation Loss: 0.7131117582321167\n",
      "Epoch 163: Train Loss: 0.6884484887123108, Validation Loss: 0.7134131193161011\n",
      "Epoch 164: Train Loss: 0.6795800566673279, Validation Loss: 0.7133065462112427\n",
      "Epoch 165: Train Loss: 0.6722123861312866, Validation Loss: 0.7137404084205627\n",
      "Epoch 166: Train Loss: 0.724317479133606, Validation Loss: 0.7136868238449097\n",
      "Epoch 167: Train Loss: 0.6820748925209046, Validation Loss: 0.7142279148101807\n",
      "Epoch 168: Train Loss: 0.6923632502555848, Validation Loss: 0.7145334482192993\n",
      "Epoch 169: Train Loss: 0.7143594741821289, Validation Loss: 0.7143594622612\n",
      "Epoch 170: Train Loss: 0.6653680801391602, Validation Loss: 0.7136881947517395\n",
      "Epoch 171: Train Loss: 0.6553112864494324, Validation Loss: 0.713628888130188\n",
      "Epoch 172: Train Loss: 0.7009540677070618, Validation Loss: 0.7128514647483826\n",
      "Epoch 173: Train Loss: 0.6838694095611573, Validation Loss: 0.7133421897888184\n",
      "Epoch 174: Train Loss: 0.7088982105255127, Validation Loss: 0.7140980958938599\n",
      "Epoch 175: Train Loss: 0.6802181839942932, Validation Loss: 0.7142128348350525\n",
      "Epoch 176: Train Loss: 0.6635372042655945, Validation Loss: 0.7136709094047546\n",
      "Epoch 177: Train Loss: 0.6680150866508484, Validation Loss: 0.7139607071876526\n",
      "Epoch 178: Train Loss: 0.7032222151756287, Validation Loss: 0.7141587138175964\n",
      "Epoch 179: Train Loss: 0.6866401433944702, Validation Loss: 0.7136539220809937\n",
      "Epoch 180: Train Loss: 0.6664597272872925, Validation Loss: 0.7138735055923462\n",
      "Epoch 181: Train Loss: 0.6583663821220398, Validation Loss: 0.7135971188545227\n",
      "Epoch 182: Train Loss: 0.6422104001045227, Validation Loss: 0.7134391665458679\n",
      "Epoch 183: Train Loss: 0.7040823221206665, Validation Loss: 0.7137580513954163\n",
      "Epoch 184: Train Loss: 0.6825345277786254, Validation Loss: 0.7138198018074036\n",
      "Epoch 185: Train Loss: 0.7312861084938049, Validation Loss: 0.7137601971626282\n",
      "Epoch 186: Train Loss: 0.699581253528595, Validation Loss: 0.7144565582275391\n",
      "Epoch 187: Train Loss: 0.679939067363739, Validation Loss: 0.7147191166877747\n",
      "Epoch 188: Train Loss: 0.7158164262771607, Validation Loss: 0.7146486043930054\n",
      "Epoch 189: Train Loss: 0.7004868030548096, Validation Loss: 0.7143470048904419\n",
      "Epoch 190: Train Loss: 0.6734222531318664, Validation Loss: 0.7142719030380249\n",
      "Epoch 191: Train Loss: 0.6925567269325257, Validation Loss: 0.7139056921005249\n",
      "Epoch 192: Train Loss: 0.6785691857337952, Validation Loss: 0.7144281268119812\n",
      "Epoch 193: Train Loss: 0.6515029489994049, Validation Loss: 0.7150482535362244\n",
      "Epoch 194: Train Loss: 0.6656744003295898, Validation Loss: 0.7150288820266724\n",
      "Epoch 195: Train Loss: 0.6263496994972229, Validation Loss: 0.7152607440948486\n",
      "Epoch 196: Train Loss: 0.7050965547561645, Validation Loss: 0.7150157690048218\n",
      "Epoch 197: Train Loss: 0.6411734223365784, Validation Loss: 0.7145785689353943\n",
      "Epoch 198: Train Loss: 0.6676445841789246, Validation Loss: 0.7146140933036804\n",
      "Epoch 199: Train Loss: 0.6792821288108826, Validation Loss: 0.7152292728424072\n",
      "Epoch 200: Train Loss: 0.6797346234321594, Validation Loss: 0.7157706022262573\n",
      "Epoch 201: Train Loss: 0.6778073310852051, Validation Loss: 0.7150615453720093\n",
      "Epoch 202: Train Loss: 0.669881808757782, Validation Loss: 0.7150055766105652\n",
      "Epoch 203: Train Loss: 0.6877565622329712, Validation Loss: 0.7148823142051697\n",
      "Epoch 204: Train Loss: 0.6783545017242432, Validation Loss: 0.714867115020752\n",
      "Epoch 205: Train Loss: 0.6265437483787537, Validation Loss: 0.7149175405502319\n",
      "Epoch 206: Train Loss: 0.6769625902175903, Validation Loss: 0.7151260375976562\n",
      "Epoch 207: Train Loss: 0.6779917359352112, Validation Loss: 0.7154096364974976\n",
      "Epoch 208: Train Loss: 0.7031196832656861, Validation Loss: 0.7152999043464661\n",
      "Epoch 209: Train Loss: 0.6508980751037597, Validation Loss: 0.7148863673210144\n",
      "Epoch 210: Train Loss: 0.6896639943122864, Validation Loss: 0.7146764397621155\n",
      "Epoch 211: Train Loss: 0.6411135792732239, Validation Loss: 0.7143151164054871\n",
      "Epoch 212: Train Loss: 0.6689656019210816, Validation Loss: 0.7146449685096741\n",
      "Epoch 213: Train Loss: 0.6631735324859619, Validation Loss: 0.7144273519515991\n",
      "Epoch 214: Train Loss: 0.6890311121940613, Validation Loss: 0.7142033576965332\n",
      "Epoch 215: Train Loss: 0.6591158270835876, Validation Loss: 0.714259922504425\n",
      "Epoch 216: Train Loss: 0.6838180184364319, Validation Loss: 0.7140121459960938\n",
      "Epoch 217: Train Loss: 0.648798942565918, Validation Loss: 0.7140612006187439\n",
      "Epoch 218: Train Loss: 0.6692315101623535, Validation Loss: 0.713959276676178\n",
      "Epoch 219: Train Loss: 0.7002845525741577, Validation Loss: 0.7142170071601868\n",
      "Epoch 220: Train Loss: 0.6782674193382263, Validation Loss: 0.7149235606193542\n",
      "Epoch 221: Train Loss: 0.6628958344459533, Validation Loss: 0.7155464291572571\n",
      "Epoch 222: Train Loss: 0.7102718591690064, Validation Loss: 0.7164502143859863\n",
      "Epoch 223: Train Loss: 0.6622032761573792, Validation Loss: 0.7167776226997375\n",
      "Epoch 224: Train Loss: 0.6477507710456848, Validation Loss: 0.7166391015052795\n",
      "Epoch 225: Train Loss: 0.6379599094390869, Validation Loss: 0.7170157432556152\n",
      "Epoch 226: Train Loss: 0.7149466156959534, Validation Loss: 0.7167115211486816\n",
      "Epoch 227: Train Loss: 0.648680317401886, Validation Loss: 0.7167258858680725\n",
      "Epoch 228: Train Loss: 0.6259926319122314, Validation Loss: 0.7163075804710388\n",
      "Epoch 229: Train Loss: 0.6556562781333923, Validation Loss: 0.7163777947425842\n",
      "Epoch 230: Train Loss: 0.6996952891349792, Validation Loss: 0.7164219617843628\n",
      "Epoch 231: Train Loss: 0.6738968014717102, Validation Loss: 0.716665506362915\n",
      "Epoch 232: Train Loss: 0.6523093700408935, Validation Loss: 0.717778205871582\n",
      "Epoch 233: Train Loss: 0.7088924765586853, Validation Loss: 0.7176154851913452\n",
      "Epoch 234: Train Loss: 0.6541154623031616, Validation Loss: 0.718048632144928\n",
      "Epoch 235: Train Loss: 0.6855045557022095, Validation Loss: 0.7177231311798096\n",
      "Epoch 236: Train Loss: 0.6534559845924377, Validation Loss: 0.7182679176330566\n",
      "Epoch 237: Train Loss: 0.6945811748504639, Validation Loss: 0.7180837392807007\n",
      "Epoch 238: Train Loss: 0.7085167407989502, Validation Loss: 0.7186151146888733\n",
      "Epoch 239: Train Loss: 0.6549113988876343, Validation Loss: 0.7181997895240784\n",
      "Epoch 240: Train Loss: 0.6417433381080627, Validation Loss: 0.7188321352005005\n",
      "Epoch 241: Train Loss: 0.6765426874160767, Validation Loss: 0.7193522453308105\n",
      "Epoch 242: Train Loss: 0.7003011226654052, Validation Loss: 0.7185216546058655\n",
      "Epoch 243: Train Loss: 0.6880275249481201, Validation Loss: 0.71803879737854\n",
      "Epoch 244: Train Loss: 0.6718397617340088, Validation Loss: 0.7180699706077576\n",
      "Epoch 245: Train Loss: 0.6709842801094055, Validation Loss: 0.7185636758804321\n",
      "Epoch 246: Train Loss: 0.6606112003326416, Validation Loss: 0.71830815076828\n",
      "Epoch 247: Train Loss: 0.6878620505332946, Validation Loss: 0.7177819609642029\n",
      "Epoch 248: Train Loss: 0.6729650974273682, Validation Loss: 0.7174779176712036\n",
      "Epoch 249: Train Loss: 0.6702503681182861, Validation Loss: 0.7181884050369263\n",
      "Epoch 250: Train Loss: 0.700943386554718, Validation Loss: 0.7188913226127625\n",
      "Epoch 251: Train Loss: 0.7126193881034851, Validation Loss: 0.7188741564750671\n",
      "Epoch 252: Train Loss: 0.6744839072227478, Validation Loss: 0.7189350128173828\n",
      "Epoch 253: Train Loss: 0.6563247799873352, Validation Loss: 0.7196521759033203\n",
      "Epoch 254: Train Loss: 0.6891069412231445, Validation Loss: 0.7199033498764038\n",
      "Epoch 255: Train Loss: 0.6560074329376221, Validation Loss: 0.7191500067710876\n",
      "Epoch 256: Train Loss: 0.6938506484031677, Validation Loss: 0.7182765603065491\n",
      "Epoch 257: Train Loss: 0.6611767768859863, Validation Loss: 0.7184419631958008\n",
      "Epoch 258: Train Loss: 0.6650147199630737, Validation Loss: 0.7183555364608765\n",
      "Epoch 259: Train Loss: 0.6663467764854432, Validation Loss: 0.718600869178772\n",
      "Epoch 260: Train Loss: 0.6280425310134887, Validation Loss: 0.7189589738845825\n",
      "Epoch 261: Train Loss: 0.6286572575569153, Validation Loss: 0.7187672257423401\n",
      "Epoch 262: Train Loss: 0.6625325798988342, Validation Loss: 0.7187768220901489\n",
      "Epoch 263: Train Loss: 0.6150228381156921, Validation Loss: 0.7180664539337158\n",
      "Epoch 264: Train Loss: 0.6779199123382569, Validation Loss: 0.717735767364502\n",
      "Epoch 265: Train Loss: 0.6584478557109833, Validation Loss: 0.7179128527641296\n",
      "Epoch 266: Train Loss: 0.6996433973312378, Validation Loss: 0.7178744077682495\n",
      "Epoch 267: Train Loss: 0.6760903358459472, Validation Loss: 0.7177380919456482\n",
      "Epoch 268: Train Loss: 0.6384945511817932, Validation Loss: 0.717814028263092\n",
      "Epoch 269: Train Loss: 0.6393437147140503, Validation Loss: 0.7175435423851013\n",
      "Epoch 270: Train Loss: 0.6720853090286255, Validation Loss: 0.7181977033615112\n",
      "Epoch 271: Train Loss: 0.6461717128753662, Validation Loss: 0.7191362380981445\n",
      "Epoch 272: Train Loss: 0.6914557218551636, Validation Loss: 0.7199385762214661\n",
      "Epoch 273: Train Loss: 0.6383650779724122, Validation Loss: 0.7193722724914551\n",
      "Epoch 274: Train Loss: 0.6556731343269349, Validation Loss: 0.719768226146698\n",
      "Epoch 275: Train Loss: 0.6384939193725586, Validation Loss: 0.7194130420684814\n",
      "Epoch 276: Train Loss: 0.646566915512085, Validation Loss: 0.7193229794502258\n",
      "Epoch 277: Train Loss: 0.6316235303878784, Validation Loss: 0.7194991111755371\n",
      "Epoch 278: Train Loss: 0.6673815727233887, Validation Loss: 0.7197840213775635\n",
      "Epoch 279: Train Loss: 0.6951892733573913, Validation Loss: 0.7197396159172058\n",
      "Epoch 280: Train Loss: 0.6647577285766602, Validation Loss: 0.7211804389953613\n",
      "Epoch 281: Train Loss: 0.6418557167053223, Validation Loss: 0.7212634682655334\n",
      "Epoch 282: Train Loss: 0.6573543667793273, Validation Loss: 0.7216640710830688\n",
      "Epoch 283: Train Loss: 0.7313287019729614, Validation Loss: 0.722128689289093\n",
      "Epoch 284: Train Loss: 0.6711461901664734, Validation Loss: 0.7226261496543884\n",
      "Epoch 285: Train Loss: 0.6747773170471192, Validation Loss: 0.7229059338569641\n",
      "Epoch 286: Train Loss: 0.6814744472503662, Validation Loss: 0.7223041653633118\n",
      "Epoch 287: Train Loss: 0.6647443532943725, Validation Loss: 0.7222222685813904\n",
      "Epoch 288: Train Loss: 0.6643503546714783, Validation Loss: 0.7221246361732483\n",
      "Epoch 289: Train Loss: 0.6652520656585693, Validation Loss: 0.7223403453826904\n",
      "Epoch 290: Train Loss: 0.6715129494667054, Validation Loss: 0.7227055430412292\n",
      "Epoch 291: Train Loss: 0.6285403490066528, Validation Loss: 0.7242160439491272\n",
      "Epoch 292: Train Loss: 0.661880886554718, Validation Loss: 0.7248377799987793\n",
      "Epoch 293: Train Loss: 0.6604967474937439, Validation Loss: 0.7259871363639832\n",
      "Epoch 294: Train Loss: 0.6482969164848328, Validation Loss: 0.7257705330848694\n",
      "Epoch 295: Train Loss: 0.6539721250534057, Validation Loss: 0.7250558733940125\n",
      "Epoch 296: Train Loss: 0.6475153207778931, Validation Loss: 0.7253225445747375\n",
      "Epoch 297: Train Loss: 0.6817731976509094, Validation Loss: 0.7253658175468445\n",
      "Epoch 298: Train Loss: 0.671343994140625, Validation Loss: 0.7261548042297363\n",
      "Epoch 299: Train Loss: 0.6584364771842957, Validation Loss: 0.7261878848075867\n",
      "Epoch 300: Train Loss: 0.6475498080253601, Validation Loss: 0.7265517115592957\n",
      "Epoch 301: Train Loss: 0.651672899723053, Validation Loss: 0.7260611057281494\n",
      "Epoch 302: Train Loss: 0.6514869093894958, Validation Loss: 0.726325273513794\n",
      "Epoch 303: Train Loss: 0.6740808606147766, Validation Loss: 0.7263140678405762\n",
      "Epoch 304: Train Loss: 0.6461842894554138, Validation Loss: 0.7276650667190552\n",
      "Epoch 305: Train Loss: 0.6610628843307496, Validation Loss: 0.7276046872138977\n",
      "Epoch 306: Train Loss: 0.6397257804870605, Validation Loss: 0.7282047271728516\n",
      "Epoch 307: Train Loss: 0.6133313536643982, Validation Loss: 0.7281419038772583\n",
      "Epoch 308: Train Loss: 0.6713682055473328, Validation Loss: 0.7284432649612427\n",
      "Epoch 309: Train Loss: 0.6244742035865783, Validation Loss: 0.728621780872345\n",
      "Epoch 310: Train Loss: 0.6668753385543823, Validation Loss: 0.729513943195343\n",
      "Epoch 311: Train Loss: 0.6020078182220459, Validation Loss: 0.7299155592918396\n",
      "Epoch 312: Train Loss: 0.6442430138587951, Validation Loss: 0.7313805818557739\n",
      "Epoch 313: Train Loss: 0.6517844200134277, Validation Loss: 0.7319173216819763\n",
      "Epoch 314: Train Loss: 0.6466439127922058, Validation Loss: 0.7324910163879395\n",
      "Epoch 315: Train Loss: 0.682775354385376, Validation Loss: 0.7320389747619629\n",
      "Epoch 316: Train Loss: 0.666907811164856, Validation Loss: 0.7318137288093567\n",
      "Epoch 317: Train Loss: 0.6506553769111634, Validation Loss: 0.7317784428596497\n",
      "Epoch 318: Train Loss: 0.712775719165802, Validation Loss: 0.731563150882721\n",
      "Epoch 319: Train Loss: 0.6385851740837097, Validation Loss: 0.7314417958259583\n",
      "Epoch 320: Train Loss: 0.6761579155921936, Validation Loss: 0.7319679260253906\n",
      "Epoch 321: Train Loss: 0.6434889078140259, Validation Loss: 0.733117401599884\n",
      "Epoch 322: Train Loss: 0.6287911534309387, Validation Loss: 0.7329988479614258\n",
      "Epoch 323: Train Loss: 0.66890629529953, Validation Loss: 0.732910692691803\n",
      "Epoch 324: Train Loss: 0.5947382569313049, Validation Loss: 0.7333002090454102\n",
      "Epoch 325: Train Loss: 0.6037598252296448, Validation Loss: 0.7341427803039551\n",
      "Epoch 326: Train Loss: 0.6351834058761596, Validation Loss: 0.7341699004173279\n",
      "Epoch 327: Train Loss: 0.6229480981826783, Validation Loss: 0.7338806986808777\n",
      "Epoch 328: Train Loss: 0.6328442096710205, Validation Loss: 0.7343164086341858\n",
      "Epoch 329: Train Loss: 0.6262109398841857, Validation Loss: 0.7352212071418762\n",
      "Epoch 330: Train Loss: 0.6743226289749146, Validation Loss: 0.73528653383255\n",
      "Epoch 331: Train Loss: 0.6066829085350036, Validation Loss: 0.73388671875\n",
      "Epoch 332: Train Loss: 0.6487884759902954, Validation Loss: 0.7328460216522217\n",
      "Epoch 333: Train Loss: 0.6500699877738952, Validation Loss: 0.733024001121521\n",
      "Epoch 334: Train Loss: 0.6449646830558777, Validation Loss: 0.7334188222885132\n",
      "Epoch 335: Train Loss: 0.6310433387756348, Validation Loss: 0.7341102361679077\n",
      "Epoch 336: Train Loss: 0.6481709241867065, Validation Loss: 0.7353915572166443\n",
      "Epoch 337: Train Loss: 0.6658661484718322, Validation Loss: 0.7353364825248718\n",
      "Epoch 338: Train Loss: 0.6282110095024109, Validation Loss: 0.7353585362434387\n",
      "Epoch 339: Train Loss: 0.675711190700531, Validation Loss: 0.7356336712837219\n",
      "Epoch 340: Train Loss: 0.6484022736549377, Validation Loss: 0.7357692718505859\n",
      "Epoch 341: Train Loss: 0.6202023029327393, Validation Loss: 0.7362521886825562\n",
      "Epoch 342: Train Loss: 0.6200730204582214, Validation Loss: 0.7366197109222412\n",
      "Epoch 343: Train Loss: 0.6342930793762207, Validation Loss: 0.7369594573974609\n",
      "Epoch 344: Train Loss: 0.6165964603424072, Validation Loss: 0.7372934222221375\n",
      "Epoch 345: Train Loss: 0.6299009919166565, Validation Loss: 0.7360188961029053\n",
      "Epoch 346: Train Loss: 0.700854766368866, Validation Loss: 0.7357112169265747\n",
      "Epoch 347: Train Loss: 0.6617542982101441, Validation Loss: 0.7356982231140137\n",
      "Epoch 348: Train Loss: 0.6895464062690735, Validation Loss: 0.7361167073249817\n",
      "Epoch 349: Train Loss: 0.6837981462478637, Validation Loss: 0.7355177998542786\n",
      "Epoch 350: Train Loss: 0.6383206844329834, Validation Loss: 0.7358571290969849\n",
      "Epoch 351: Train Loss: 0.6550149202346802, Validation Loss: 0.7351336479187012\n",
      "Epoch 352: Train Loss: 0.6882233142852783, Validation Loss: 0.7347723841667175\n",
      "Epoch 353: Train Loss: 0.616249144077301, Validation Loss: 0.7348378896713257\n",
      "Epoch 354: Train Loss: 0.6217817664146423, Validation Loss: 0.7341805696487427\n",
      "Epoch 355: Train Loss: 0.6791650176048278, Validation Loss: 0.7346268892288208\n",
      "Epoch 356: Train Loss: 0.6749214291572571, Validation Loss: 0.7343958020210266\n",
      "Epoch 357: Train Loss: 0.6772772192955017, Validation Loss: 0.7348058223724365\n",
      "Epoch 358: Train Loss: 0.6385116696357727, Validation Loss: 0.7353070974349976\n",
      "Epoch 359: Train Loss: 0.6263699173927307, Validation Loss: 0.7355014681816101\n",
      "Epoch 360: Train Loss: 0.6038116693496705, Validation Loss: 0.736282467842102\n",
      "Epoch 361: Train Loss: 0.6288609981536866, Validation Loss: 0.7367461919784546\n",
      "Epoch 362: Train Loss: 0.6464954137802124, Validation Loss: 0.7380704879760742\n",
      "Epoch 363: Train Loss: 0.6337496995925903, Validation Loss: 0.7391079068183899\n",
      "Epoch 364: Train Loss: 0.6250534057617188, Validation Loss: 0.7389215230941772\n",
      "Epoch 365: Train Loss: 0.6526196122169494, Validation Loss: 0.7399360537528992\n",
      "Epoch 366: Train Loss: 0.6495801210403442, Validation Loss: 0.7399985194206238\n",
      "Epoch 367: Train Loss: 0.6147216320037842, Validation Loss: 0.7399364113807678\n",
      "Epoch 368: Train Loss: 0.665532898902893, Validation Loss: 0.7397802472114563\n",
      "Epoch 369: Train Loss: 0.5970475316047669, Validation Loss: 0.7397370934486389\n",
      "Epoch 370: Train Loss: 0.6672528266906739, Validation Loss: 0.740166425704956\n",
      "Epoch 371: Train Loss: 0.6728720188140869, Validation Loss: 0.7407040596008301\n",
      "Epoch 372: Train Loss: 0.6137630224227906, Validation Loss: 0.7419860363006592\n",
      "Epoch 373: Train Loss: 0.6483832478523255, Validation Loss: 0.7414577603340149\n",
      "Epoch 374: Train Loss: 0.6259707629680633, Validation Loss: 0.7404281497001648\n",
      "Epoch 375: Train Loss: 0.6261723399162292, Validation Loss: 0.7415082454681396\n",
      "Epoch 376: Train Loss: 0.6779450178146362, Validation Loss: 0.7416577339172363\n",
      "Epoch 377: Train Loss: 0.6781495213508606, Validation Loss: 0.7412428855895996\n",
      "Epoch 378: Train Loss: 0.6534682273864746, Validation Loss: 0.7413598895072937\n",
      "Epoch 379: Train Loss: 0.6441471576690674, Validation Loss: 0.7421765923500061\n",
      "Epoch 380: Train Loss: 0.6474585652351379, Validation Loss: 0.7439132332801819\n",
      "Epoch 381: Train Loss: 0.6473504424095153, Validation Loss: 0.7438231706619263\n",
      "Epoch 382: Train Loss: 0.636153495311737, Validation Loss: 0.7447822093963623\n",
      "Epoch 383: Train Loss: 0.6529913783073426, Validation Loss: 0.7433418035507202\n",
      "Epoch 384: Train Loss: 0.6490023136138916, Validation Loss: 0.7433362007141113\n",
      "Epoch 385: Train Loss: 0.5985064506530762, Validation Loss: 0.7430682182312012\n",
      "Epoch 386: Train Loss: 0.717843759059906, Validation Loss: 0.7435197830200195\n",
      "Epoch 387: Train Loss: 0.6462444305419922, Validation Loss: 0.7429111003875732\n",
      "Epoch 388: Train Loss: 0.7103679656982422, Validation Loss: 0.7432344555854797\n",
      "Epoch 389: Train Loss: 0.6764579534530639, Validation Loss: 0.743135929107666\n",
      "Epoch 390: Train Loss: 0.6537683963775635, Validation Loss: 0.7430095672607422\n",
      "Epoch 391: Train Loss: 0.6275923132896424, Validation Loss: 0.7443349957466125\n",
      "Epoch 392: Train Loss: 0.6644065618515015, Validation Loss: 0.7436919212341309\n",
      "Epoch 393: Train Loss: 0.6684475898742676, Validation Loss: 0.7445128560066223\n",
      "Epoch 394: Train Loss: 0.6653451800346375, Validation Loss: 0.7427472472190857\n",
      "Epoch 395: Train Loss: 0.6310919404029847, Validation Loss: 0.7430621981620789\n",
      "Epoch 396: Train Loss: 0.6511924028396606, Validation Loss: 0.7445703148841858\n",
      "Epoch 397: Train Loss: 0.6424642205238342, Validation Loss: 0.745519757270813\n",
      "Epoch 398: Train Loss: 0.6303225755691528, Validation Loss: 0.7458394169807434\n",
      "Epoch 399: Train Loss: 0.611502057313919, Validation Loss: 0.7465860247612\n",
      "Epoch 400: Train Loss: 0.6583394408226013, Validation Loss: 0.7467763423919678\n",
      "Epoch 401: Train Loss: 0.6182441353797913, Validation Loss: 0.7460902333259583\n",
      "Epoch 402: Train Loss: 0.6336096525192261, Validation Loss: 0.7469146251678467\n",
      "Epoch 403: Train Loss: 0.6150483965873719, Validation Loss: 0.7475869655609131\n",
      "Epoch 404: Train Loss: 0.6042913734912873, Validation Loss: 0.748079240322113\n",
      "Epoch 405: Train Loss: 0.6062627315521241, Validation Loss: 0.7497519850730896\n",
      "Epoch 406: Train Loss: 0.6255385160446167, Validation Loss: 0.7489874958992004\n",
      "Epoch 407: Train Loss: 0.6552309513092041, Validation Loss: 0.7493403553962708\n",
      "Epoch 408: Train Loss: 0.6380792737007142, Validation Loss: 0.7482015490531921\n",
      "Epoch 409: Train Loss: 0.6808706402778626, Validation Loss: 0.7480649352073669\n",
      "Epoch 410: Train Loss: 0.628491222858429, Validation Loss: 0.7482083439826965\n",
      "Epoch 411: Train Loss: 0.6590135812759399, Validation Loss: 0.7489692568778992\n",
      "Epoch 412: Train Loss: 0.6475927233695984, Validation Loss: 0.7494221925735474\n",
      "Epoch 413: Train Loss: 0.6065716326236725, Validation Loss: 0.7496484518051147\n",
      "Epoch 414: Train Loss: 0.6195695519447326, Validation Loss: 0.7478099465370178\n",
      "Epoch 415: Train Loss: 0.6762687683105468, Validation Loss: 0.7474610805511475\n",
      "Epoch 416: Train Loss: 0.6236150979995727, Validation Loss: 0.7474004626274109\n",
      "Epoch 417: Train Loss: 0.6044641733169556, Validation Loss: 0.7483949065208435\n",
      "Epoch 418: Train Loss: 0.629641592502594, Validation Loss: 0.7488892674446106\n",
      "Epoch 419: Train Loss: 0.6316936373710632, Validation Loss: 0.7487649917602539\n",
      "Epoch 420: Train Loss: 0.6345032215118408, Validation Loss: 0.7501389980316162\n",
      "Epoch 421: Train Loss: 0.6606446743011475, Validation Loss: 0.7516452670097351\n",
      "Epoch 422: Train Loss: 0.6682534337043762, Validation Loss: 0.7513110637664795\n",
      "Epoch 423: Train Loss: 0.6897400736808776, Validation Loss: 0.7521381378173828\n",
      "Epoch 424: Train Loss: 0.608481478691101, Validation Loss: 0.7540948390960693\n",
      "Epoch 425: Train Loss: 0.5823864459991455, Validation Loss: 0.7550147175788879\n",
      "Epoch 426: Train Loss: 0.6298203945159913, Validation Loss: 0.7551228404045105\n",
      "Epoch 427: Train Loss: 0.6141249537467957, Validation Loss: 0.7544706463813782\n",
      "Epoch 428: Train Loss: 0.6509756565093994, Validation Loss: 0.7537409663200378\n",
      "Epoch 429: Train Loss: 0.6815573930740356, Validation Loss: 0.7543231248855591\n",
      "Epoch 430: Train Loss: 0.5947590231895447, Validation Loss: 0.7544127702713013\n",
      "Epoch 431: Train Loss: 0.5969125866889954, Validation Loss: 0.752611517906189\n",
      "Epoch 432: Train Loss: 0.6239580750465393, Validation Loss: 0.753162682056427\n",
      "Epoch 433: Train Loss: 0.5865059316158294, Validation Loss: 0.7551454305648804\n",
      "Epoch 434: Train Loss: 0.6534349679946899, Validation Loss: 0.7548221349716187\n",
      "Epoch 435: Train Loss: 0.6592309951782227, Validation Loss: 0.7576179504394531\n",
      "Epoch 436: Train Loss: 0.6385045289993286, Validation Loss: 0.7570998668670654\n",
      "Epoch 437: Train Loss: 0.6074772655963898, Validation Loss: 0.7566898465156555\n",
      "Epoch 438: Train Loss: 0.6562654495239257, Validation Loss: 0.7565768361091614\n",
      "Epoch 439: Train Loss: 0.6197032928466797, Validation Loss: 0.755206823348999\n",
      "Epoch 440: Train Loss: 0.6346078395843506, Validation Loss: 0.755682647228241\n",
      "Epoch 441: Train Loss: 0.6486550807952881, Validation Loss: 0.7566701769828796\n",
      "Epoch 442: Train Loss: 0.6375310182571411, Validation Loss: 0.7571722865104675\n",
      "Epoch 443: Train Loss: 0.6795364499092102, Validation Loss: 0.757485568523407\n",
      "Epoch 444: Train Loss: 0.6236472964286804, Validation Loss: 0.756451427936554\n",
      "Epoch 445: Train Loss: 0.5751149833202363, Validation Loss: 0.7562068104743958\n",
      "Epoch 446: Train Loss: 0.6272945165634155, Validation Loss: 0.7564944624900818\n",
      "Epoch 447: Train Loss: 0.6334755539894104, Validation Loss: 0.7589308619499207\n",
      "Epoch 448: Train Loss: 0.6365113735198975, Validation Loss: 0.7590140700340271\n",
      "Epoch 449: Train Loss: 0.6142223000526428, Validation Loss: 0.7582113146781921\n",
      "Epoch 450: Train Loss: 0.6608238577842712, Validation Loss: 0.7572941780090332\n",
      "Epoch 451: Train Loss: 0.6053254008293152, Validation Loss: 0.7566090822219849\n",
      "Epoch 452: Train Loss: 0.638804805278778, Validation Loss: 0.7562901973724365\n",
      "Epoch 453: Train Loss: 0.6460675954818725, Validation Loss: 0.7561333775520325\n",
      "Epoch 454: Train Loss: 0.6003866910934448, Validation Loss: 0.755973219871521\n",
      "Epoch 455: Train Loss: 0.6142289638519287, Validation Loss: 0.7574372291564941\n",
      "Epoch 456: Train Loss: 0.6093861103057862, Validation Loss: 0.7583552598953247\n",
      "Epoch 457: Train Loss: 0.6124059796333313, Validation Loss: 0.7575109004974365\n",
      "Epoch 458: Train Loss: 0.608599990606308, Validation Loss: 0.7567728161811829\n",
      "Epoch 459: Train Loss: 0.6187531113624573, Validation Loss: 0.7576513290405273\n",
      "Epoch 460: Train Loss: 0.6436715722084045, Validation Loss: 0.756628155708313\n",
      "Epoch 461: Train Loss: 0.5999121189117431, Validation Loss: 0.7554469704627991\n",
      "Epoch 462: Train Loss: 0.6482515931129456, Validation Loss: 0.7563567161560059\n",
      "Epoch 463: Train Loss: 0.6224176526069641, Validation Loss: 0.7595383524894714\n",
      "Epoch 464: Train Loss: 0.5711997449398041, Validation Loss: 0.761066198348999\n",
      "Epoch 465: Train Loss: 0.628181767463684, Validation Loss: 0.7625753283500671\n",
      "Epoch 466: Train Loss: 0.5813681006431579, Validation Loss: 0.761593759059906\n",
      "Epoch 467: Train Loss: 0.6781672358512878, Validation Loss: 0.7617013454437256\n",
      "Epoch 468: Train Loss: 0.6629528760910034, Validation Loss: 0.7614191174507141\n",
      "Epoch 469: Train Loss: 0.6409684419631958, Validation Loss: 0.761924147605896\n",
      "Epoch 470: Train Loss: 0.6383516430854798, Validation Loss: 0.7629613280296326\n",
      "Epoch 471: Train Loss: 0.5998101532459259, Validation Loss: 0.7628488540649414\n",
      "Epoch 472: Train Loss: 0.658480167388916, Validation Loss: 0.7612460255622864\n",
      "Epoch 473: Train Loss: 0.5957545399665832, Validation Loss: 0.7625411152839661\n",
      "Epoch 474: Train Loss: 0.6755728125572205, Validation Loss: 0.761530876159668\n",
      "Epoch 475: Train Loss: 0.6069033741950989, Validation Loss: 0.7607551217079163\n",
      "Epoch 476: Train Loss: 0.6561796069145203, Validation Loss: 0.761695384979248\n",
      "Epoch 477: Train Loss: 0.6408810377120971, Validation Loss: 0.7606019377708435\n",
      "Epoch 478: Train Loss: 0.629079794883728, Validation Loss: 0.761274516582489\n",
      "Epoch 479: Train Loss: 0.6242042422294617, Validation Loss: 0.7624635696411133\n",
      "Epoch 480: Train Loss: 0.6639985799789428, Validation Loss: 0.7625283598899841\n",
      "Epoch 481: Train Loss: 0.6463862061500549, Validation Loss: 0.7613630294799805\n",
      "Epoch 482: Train Loss: 0.6455881834030152, Validation Loss: 0.7618794441223145\n",
      "Epoch 483: Train Loss: 0.6207226991653443, Validation Loss: 0.7632371783256531\n",
      "Epoch 484: Train Loss: 0.6554556012153625, Validation Loss: 0.7623165249824524\n",
      "Epoch 485: Train Loss: 0.5818383455276489, Validation Loss: 0.7609199285507202\n",
      "Epoch 486: Train Loss: 0.6001955151557923, Validation Loss: 0.7626846432685852\n",
      "Epoch 487: Train Loss: 0.5964691758155822, Validation Loss: 0.7632291316986084\n",
      "Epoch 488: Train Loss: 0.6364083647727966, Validation Loss: 0.7628322243690491\n",
      "Epoch 489: Train Loss: 0.6565834403038024, Validation Loss: 0.76267009973526\n",
      "Epoch 490: Train Loss: 0.6511737942695618, Validation Loss: 0.7615556716918945\n",
      "Epoch 491: Train Loss: 0.6346276521682739, Validation Loss: 0.7613627314567566\n",
      "Epoch 492: Train Loss: 0.576745355129242, Validation Loss: 0.762897253036499\n",
      "Epoch 493: Train Loss: 0.6420399904251098, Validation Loss: 0.7641830444335938\n",
      "Epoch 494: Train Loss: 0.6463619470596313, Validation Loss: 0.7640039324760437\n",
      "Epoch 495: Train Loss: 0.5907780289649963, Validation Loss: 0.7650350332260132\n",
      "Epoch 496: Train Loss: 0.6225819945335388, Validation Loss: 0.7644786834716797\n",
      "Epoch 497: Train Loss: 0.5782431721687317, Validation Loss: 0.7636170983314514\n",
      "Epoch 498: Train Loss: 0.6497425675392151, Validation Loss: 0.7641888856887817\n",
      "Epoch 499: Train Loss: 0.5771671235561371, Validation Loss: 0.7644746899604797\n",
      "Fold 4 Metrics:\n",
      "Accuracy: 0.45454545454545453, Precision: 0.5, Recall: 0.8333333333333334, F1-score: 0.625, AUC: 0.4166666666666667\n",
      "Confusion Matrix:\n",
      "[[0 5]\n",
      " [1 5]]\n",
      "Completed fold 4\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples from subject 13 to test set\n",
      "Adding 6 truth samples from subject 13 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 500)\n",
      "(132, 65, 500)\n",
      "Epoch 0: Train Loss: 0.7300016522407532, Validation Loss: 0.690843939781189\n",
      "Epoch 1: Train Loss: 0.6904536128044129, Validation Loss: 0.6909483671188354\n",
      "Epoch 2: Train Loss: 0.7227501630783081, Validation Loss: 0.6918710470199585\n",
      "Epoch 3: Train Loss: 0.7128288388252259, Validation Loss: 0.6939246654510498\n",
      "Epoch 4: Train Loss: 0.7006170511245727, Validation Loss: 0.6962160468101501\n",
      "Epoch 5: Train Loss: 0.70887770652771, Validation Loss: 0.6996586322784424\n",
      "Epoch 6: Train Loss: 0.7400280833244324, Validation Loss: 0.7013053297996521\n",
      "Epoch 7: Train Loss: 0.7140399217605591, Validation Loss: 0.702873706817627\n",
      "Epoch 8: Train Loss: 0.7022045731544495, Validation Loss: 0.7033298015594482\n",
      "Epoch 9: Train Loss: 0.7235913515090943, Validation Loss: 0.7047052383422852\n",
      "Epoch 10: Train Loss: 0.67890545129776, Validation Loss: 0.7062705755233765\n",
      "Epoch 11: Train Loss: 0.7112610697746277, Validation Loss: 0.7074224352836609\n",
      "Epoch 12: Train Loss: 0.7448021531105041, Validation Loss: 0.7084240913391113\n",
      "Epoch 13: Train Loss: 0.6840232253074646, Validation Loss: 0.7094192504882812\n",
      "Epoch 14: Train Loss: 0.7079080581665039, Validation Loss: 0.7102541327476501\n",
      "Epoch 15: Train Loss: 0.6804603219032288, Validation Loss: 0.709972083568573\n",
      "Epoch 16: Train Loss: 0.7189161300659179, Validation Loss: 0.7109307050704956\n",
      "Epoch 17: Train Loss: 0.7351908802986145, Validation Loss: 0.711280345916748\n",
      "Epoch 18: Train Loss: 0.7116717457771301, Validation Loss: 0.7111654281616211\n",
      "Epoch 19: Train Loss: 0.7297895431518555, Validation Loss: 0.7108647227287292\n",
      "Epoch 20: Train Loss: 0.6846915006637573, Validation Loss: 0.7124204635620117\n",
      "Epoch 21: Train Loss: 0.7044051051139831, Validation Loss: 0.7138909101486206\n",
      "Epoch 22: Train Loss: 0.6902410984039307, Validation Loss: 0.713824987411499\n",
      "Epoch 23: Train Loss: 0.6586134195327759, Validation Loss: 0.7145543694496155\n",
      "Epoch 24: Train Loss: 0.6808014154434204, Validation Loss: 0.7152612805366516\n",
      "Epoch 25: Train Loss: 0.7099685668945312, Validation Loss: 0.7161312103271484\n",
      "Epoch 26: Train Loss: 0.6967850923538208, Validation Loss: 0.7167238593101501\n",
      "Epoch 27: Train Loss: 0.7189697623252869, Validation Loss: 0.7167113423347473\n",
      "Epoch 28: Train Loss: 0.6982951760292053, Validation Loss: 0.7162951231002808\n",
      "Epoch 29: Train Loss: 0.7220787644386292, Validation Loss: 0.7171017527580261\n",
      "Epoch 30: Train Loss: 0.6927945137023925, Validation Loss: 0.7186241149902344\n",
      "Epoch 31: Train Loss: 0.6970202565193176, Validation Loss: 0.7207395434379578\n",
      "Epoch 32: Train Loss: 0.696725356578827, Validation Loss: 0.7216760516166687\n",
      "Epoch 33: Train Loss: 0.7343524813652038, Validation Loss: 0.7212242484092712\n",
      "Epoch 34: Train Loss: 0.7168205857276917, Validation Loss: 0.7221194505691528\n",
      "Epoch 35: Train Loss: 0.7188634157180787, Validation Loss: 0.7232263684272766\n",
      "Epoch 36: Train Loss: 0.6818456649780273, Validation Loss: 0.7223333120346069\n",
      "Epoch 37: Train Loss: 0.7044802308082581, Validation Loss: 0.7233657836914062\n",
      "Epoch 38: Train Loss: 0.7029542446136474, Validation Loss: 0.7237951159477234\n",
      "Epoch 39: Train Loss: 0.7295087695121765, Validation Loss: 0.7214187383651733\n",
      "Epoch 40: Train Loss: 0.6824820518493653, Validation Loss: 0.7227163314819336\n",
      "Epoch 41: Train Loss: 0.7278513431549072, Validation Loss: 0.723953902721405\n",
      "Epoch 42: Train Loss: 0.6973425149917603, Validation Loss: 0.7241377830505371\n",
      "Epoch 43: Train Loss: 0.6936449766159057, Validation Loss: 0.724699079990387\n",
      "Epoch 44: Train Loss: 0.680299949645996, Validation Loss: 0.7262368202209473\n",
      "Epoch 45: Train Loss: 0.7166337490081787, Validation Loss: 0.7273789644241333\n",
      "Epoch 46: Train Loss: 0.7056821703910827, Validation Loss: 0.7283328175544739\n",
      "Epoch 47: Train Loss: 0.6581877708435059, Validation Loss: 0.7268683314323425\n",
      "Epoch 48: Train Loss: 0.7361445307731629, Validation Loss: 0.7273318767547607\n",
      "Epoch 49: Train Loss: 0.6916087985038757, Validation Loss: 0.7284026741981506\n",
      "Epoch 50: Train Loss: 0.725951099395752, Validation Loss: 0.7299706935882568\n",
      "Epoch 51: Train Loss: 0.670883584022522, Validation Loss: 0.7309551239013672\n",
      "Epoch 52: Train Loss: 0.63864107131958, Validation Loss: 0.7313714623451233\n",
      "Epoch 53: Train Loss: 0.7134355664253235, Validation Loss: 0.7307857275009155\n",
      "Epoch 54: Train Loss: 0.6662929892539978, Validation Loss: 0.7312604784965515\n",
      "Epoch 55: Train Loss: 0.6767970561981201, Validation Loss: 0.7334392070770264\n",
      "Epoch 56: Train Loss: 0.6730781316757202, Validation Loss: 0.7332213521003723\n",
      "Epoch 57: Train Loss: 0.6895121693611145, Validation Loss: 0.734086811542511\n",
      "Epoch 58: Train Loss: 0.7068941116333007, Validation Loss: 0.7335185408592224\n",
      "Epoch 59: Train Loss: 0.6734760880470276, Validation Loss: 0.7337034344673157\n",
      "Epoch 60: Train Loss: 0.7280323386192322, Validation Loss: 0.7353050112724304\n",
      "Epoch 61: Train Loss: 0.6933699011802673, Validation Loss: 0.7334264516830444\n",
      "Epoch 62: Train Loss: 0.7050761580467224, Validation Loss: 0.7356851100921631\n",
      "Epoch 63: Train Loss: 0.6473028302192688, Validation Loss: 0.7379441261291504\n",
      "Epoch 64: Train Loss: 0.7311037421226502, Validation Loss: 0.7390834093093872\n",
      "Epoch 65: Train Loss: 0.6889655947685241, Validation Loss: 0.7398214340209961\n",
      "Epoch 66: Train Loss: 0.6706825494766235, Validation Loss: 0.7397284507751465\n",
      "Epoch 67: Train Loss: 0.7284658432006836, Validation Loss: 0.7396283149719238\n",
      "Epoch 68: Train Loss: 0.7210713028907776, Validation Loss: 0.739633321762085\n",
      "Epoch 69: Train Loss: 0.7125115633010864, Validation Loss: 0.7392348647117615\n",
      "Epoch 70: Train Loss: 0.7374937891960144, Validation Loss: 0.7370262145996094\n",
      "Epoch 71: Train Loss: 0.7072747945785522, Validation Loss: 0.7386231422424316\n",
      "Epoch 72: Train Loss: 0.6679619431495667, Validation Loss: 0.7385375499725342\n",
      "Epoch 73: Train Loss: 0.7244567036628723, Validation Loss: 0.7409713864326477\n",
      "Epoch 74: Train Loss: 0.672694742679596, Validation Loss: 0.7412662506103516\n",
      "Epoch 75: Train Loss: 0.7123429536819458, Validation Loss: 0.7429283261299133\n",
      "Epoch 76: Train Loss: 0.7352309107780457, Validation Loss: 0.743200957775116\n",
      "Epoch 77: Train Loss: 0.7426941394805908, Validation Loss: 0.7422216534614563\n",
      "Epoch 78: Train Loss: 0.6709417819976806, Validation Loss: 0.7404667735099792\n",
      "Epoch 79: Train Loss: 0.7463503241539001, Validation Loss: 0.7427899241447449\n",
      "Epoch 80: Train Loss: 0.7265920639038086, Validation Loss: 0.744458019733429\n",
      "Epoch 81: Train Loss: 0.7152436137199402, Validation Loss: 0.7444902062416077\n",
      "Epoch 82: Train Loss: 0.643949556350708, Validation Loss: 0.7473886013031006\n",
      "Epoch 83: Train Loss: 0.7195873737335206, Validation Loss: 0.7494828104972839\n",
      "Epoch 84: Train Loss: 0.707451331615448, Validation Loss: 0.7509202361106873\n",
      "Epoch 85: Train Loss: 0.7173234462738037, Validation Loss: 0.751868724822998\n",
      "Epoch 86: Train Loss: 0.6567011594772338, Validation Loss: 0.7513615489006042\n",
      "Epoch 87: Train Loss: 0.6313589096069336, Validation Loss: 0.7518847584724426\n",
      "Epoch 88: Train Loss: 0.6848657727241516, Validation Loss: 0.7527449727058411\n",
      "Epoch 89: Train Loss: 0.681078290939331, Validation Loss: 0.7520565986633301\n",
      "Epoch 90: Train Loss: 0.6703585982322693, Validation Loss: 0.7528447508811951\n",
      "Epoch 91: Train Loss: 0.6867931127548218, Validation Loss: 0.7551991939544678\n",
      "Epoch 92: Train Loss: 0.6718920826911926, Validation Loss: 0.7558484077453613\n",
      "Epoch 93: Train Loss: 0.6939494609832764, Validation Loss: 0.7576577663421631\n",
      "Epoch 94: Train Loss: 0.6671729683876038, Validation Loss: 0.7589133977890015\n",
      "Epoch 95: Train Loss: 0.6687378525733948, Validation Loss: 0.7594180107116699\n",
      "Epoch 96: Train Loss: 0.6734747171401978, Validation Loss: 0.7595700621604919\n",
      "Epoch 97: Train Loss: 0.6746546983718872, Validation Loss: 0.7580785751342773\n",
      "Epoch 98: Train Loss: 0.7304000496864319, Validation Loss: 0.7578819394111633\n",
      "Epoch 99: Train Loss: 0.6875069260597229, Validation Loss: 0.7582991719245911\n",
      "Epoch 100: Train Loss: 0.6766477704048157, Validation Loss: 0.7592790126800537\n",
      "Epoch 101: Train Loss: 0.6577477097511292, Validation Loss: 0.7595294117927551\n",
      "Epoch 102: Train Loss: 0.6752419471740723, Validation Loss: 0.760921061038971\n",
      "Epoch 103: Train Loss: 0.7056355834007263, Validation Loss: 0.763298749923706\n",
      "Epoch 104: Train Loss: 0.6796659350395202, Validation Loss: 0.7660701870918274\n",
      "Epoch 105: Train Loss: 0.7171333789825439, Validation Loss: 0.766093373298645\n",
      "Epoch 106: Train Loss: 0.6699630737304687, Validation Loss: 0.7665702700614929\n",
      "Epoch 107: Train Loss: 0.6866306781768798, Validation Loss: 0.7666032314300537\n",
      "Epoch 108: Train Loss: 0.7151565551757812, Validation Loss: 0.7627947926521301\n",
      "Epoch 109: Train Loss: 0.6786684513092041, Validation Loss: 0.7636876106262207\n",
      "Epoch 110: Train Loss: 0.6740259885787964, Validation Loss: 0.7652852535247803\n",
      "Epoch 111: Train Loss: 0.7271378874778748, Validation Loss: 0.7678046226501465\n",
      "Epoch 112: Train Loss: 0.6539004445075989, Validation Loss: 0.7662872076034546\n",
      "Epoch 113: Train Loss: 0.6645843505859375, Validation Loss: 0.768671989440918\n",
      "Epoch 114: Train Loss: 0.7333164572715759, Validation Loss: 0.7696095705032349\n",
      "Epoch 115: Train Loss: 0.6808488368988037, Validation Loss: 0.7715209126472473\n",
      "Epoch 116: Train Loss: 0.6650420784950256, Validation Loss: 0.772171676158905\n",
      "Epoch 117: Train Loss: 0.6798958301544189, Validation Loss: 0.775477945804596\n",
      "Epoch 118: Train Loss: 0.6772881746292114, Validation Loss: 0.7752264142036438\n",
      "Epoch 119: Train Loss: 0.6699373602867127, Validation Loss: 0.7750940322875977\n",
      "Epoch 120: Train Loss: 0.6558273553848266, Validation Loss: 0.7775864601135254\n",
      "Epoch 121: Train Loss: 0.6647557973861694, Validation Loss: 0.7798251509666443\n",
      "Epoch 122: Train Loss: 0.663102650642395, Validation Loss: 0.7820971608161926\n",
      "Epoch 123: Train Loss: 0.6992583036422729, Validation Loss: 0.7811954021453857\n",
      "Epoch 124: Train Loss: 0.6645012021064758, Validation Loss: 0.7839536666870117\n",
      "Epoch 125: Train Loss: 0.6758416175842286, Validation Loss: 0.783937394618988\n",
      "Epoch 126: Train Loss: 0.6530524849891662, Validation Loss: 0.7844870686531067\n",
      "Epoch 127: Train Loss: 0.682394540309906, Validation Loss: 0.7859299778938293\n",
      "Epoch 128: Train Loss: 0.6577249884605407, Validation Loss: 0.7857117652893066\n",
      "Epoch 129: Train Loss: 0.6701346158981323, Validation Loss: 0.785584568977356\n",
      "Epoch 130: Train Loss: 0.6479254126548767, Validation Loss: 0.7825660705566406\n",
      "Epoch 131: Train Loss: 0.6717483401298523, Validation Loss: 0.7871390581130981\n",
      "Epoch 132: Train Loss: 0.6864674329757691, Validation Loss: 0.7887683510780334\n",
      "Epoch 133: Train Loss: 0.6776085734367371, Validation Loss: 0.7896762490272522\n",
      "Epoch 134: Train Loss: 0.6598558187484741, Validation Loss: 0.793269157409668\n",
      "Epoch 135: Train Loss: 0.6565709710121155, Validation Loss: 0.794843852519989\n",
      "Epoch 136: Train Loss: 0.6319957971572876, Validation Loss: 0.7950080633163452\n",
      "Epoch 137: Train Loss: 0.6920709013938904, Validation Loss: 0.7963721752166748\n",
      "Epoch 138: Train Loss: 0.6410189390182495, Validation Loss: 0.7961027026176453\n",
      "Epoch 139: Train Loss: 0.6622037172317505, Validation Loss: 0.7961642742156982\n",
      "Epoch 140: Train Loss: 0.6941038250923157, Validation Loss: 0.8000212907791138\n",
      "Epoch 141: Train Loss: 0.6165597677230835, Validation Loss: 0.8035093545913696\n",
      "Epoch 142: Train Loss: 0.6425026178359985, Validation Loss: 0.8069507479667664\n",
      "Epoch 143: Train Loss: 0.6459899187088013, Validation Loss: 0.811078667640686\n",
      "Epoch 144: Train Loss: 0.6519631624221802, Validation Loss: 0.8120281100273132\n",
      "Epoch 145: Train Loss: 0.6897723197937011, Validation Loss: 0.8146730065345764\n",
      "Epoch 146: Train Loss: 0.7329325795173645, Validation Loss: 0.8134414553642273\n",
      "Epoch 147: Train Loss: 0.6328393697738648, Validation Loss: 0.8110852837562561\n",
      "Epoch 148: Train Loss: 0.640715754032135, Validation Loss: 0.8115928769111633\n",
      "Epoch 149: Train Loss: 0.6548664331436157, Validation Loss: 0.8101223111152649\n",
      "Epoch 150: Train Loss: 0.6794172763824463, Validation Loss: 0.8164427876472473\n",
      "Epoch 151: Train Loss: 0.6892267346382142, Validation Loss: 0.8173606395721436\n",
      "Epoch 152: Train Loss: 0.6288056135177612, Validation Loss: 0.8141679763793945\n",
      "Epoch 153: Train Loss: 0.6170675575733184, Validation Loss: 0.8166125416755676\n",
      "Epoch 154: Train Loss: 0.6942188978195191, Validation Loss: 0.8240957260131836\n",
      "Epoch 155: Train Loss: 0.6613998532295227, Validation Loss: 0.8277215361595154\n",
      "Epoch 156: Train Loss: 0.6693503618240356, Validation Loss: 0.8330449461936951\n",
      "Epoch 157: Train Loss: 0.6610086560249329, Validation Loss: 0.8352437615394592\n",
      "Epoch 158: Train Loss: 0.6281045079231262, Validation Loss: 0.8389390110969543\n",
      "Epoch 159: Train Loss: 0.6362571477890014, Validation Loss: 0.8376160860061646\n",
      "Epoch 160: Train Loss: 0.662828004360199, Validation Loss: 0.8345962166786194\n",
      "Epoch 161: Train Loss: 0.6454999685287476, Validation Loss: 0.8409116268157959\n",
      "Epoch 162: Train Loss: 0.690925395488739, Validation Loss: 0.8484665155410767\n",
      "Epoch 163: Train Loss: 0.6173991799354553, Validation Loss: 0.8545486330986023\n",
      "Epoch 164: Train Loss: 0.6397934079170227, Validation Loss: 0.8622449636459351\n",
      "Epoch 165: Train Loss: 0.6517878413200379, Validation Loss: 0.8692241907119751\n",
      "Epoch 166: Train Loss: 0.6569814920425415, Validation Loss: 0.8707433342933655\n",
      "Epoch 167: Train Loss: 0.6477415919303894, Validation Loss: 0.8705857396125793\n",
      "Epoch 168: Train Loss: 0.6734865546226502, Validation Loss: 0.8723989129066467\n",
      "Epoch 169: Train Loss: 0.6410289883613587, Validation Loss: 0.874268651008606\n",
      "Epoch 170: Train Loss: 0.6662893652915954, Validation Loss: 0.8819451928138733\n",
      "Epoch 171: Train Loss: 0.6161677598953247, Validation Loss: 0.8724203109741211\n",
      "Epoch 172: Train Loss: 0.6604965686798095, Validation Loss: 0.8796628713607788\n",
      "Epoch 173: Train Loss: 0.6692262649536133, Validation Loss: 0.8883676528930664\n",
      "Epoch 174: Train Loss: 0.6742591738700867, Validation Loss: 0.8936043977737427\n",
      "Epoch 175: Train Loss: 0.6040292263031006, Validation Loss: 0.89197838306427\n",
      "Epoch 176: Train Loss: 0.614377224445343, Validation Loss: 0.899041473865509\n",
      "Epoch 177: Train Loss: 0.7004888653755188, Validation Loss: 0.9008105397224426\n",
      "Epoch 178: Train Loss: 0.5891708254814148, Validation Loss: 0.897429883480072\n",
      "Epoch 179: Train Loss: 0.5990284562110901, Validation Loss: 0.8912948966026306\n",
      "Epoch 180: Train Loss: 0.6336997985839844, Validation Loss: 0.9007787704467773\n",
      "Epoch 181: Train Loss: 0.6342383861541748, Validation Loss: 0.9087551236152649\n",
      "Epoch 182: Train Loss: 0.6753494739532471, Validation Loss: 0.912414014339447\n",
      "Epoch 183: Train Loss: 0.6455464124679565, Validation Loss: 0.9237710237503052\n",
      "Epoch 184: Train Loss: 0.6870840549468994, Validation Loss: 0.9346334934234619\n",
      "Epoch 185: Train Loss: 0.6142148971557617, Validation Loss: 0.9328817129135132\n",
      "Epoch 186: Train Loss: 0.637528371810913, Validation Loss: 0.9276294708251953\n",
      "Epoch 187: Train Loss: 0.6534576058387757, Validation Loss: 0.9368268847465515\n",
      "Epoch 188: Train Loss: 0.6098723113536835, Validation Loss: 0.9414898157119751\n",
      "Epoch 189: Train Loss: 0.624206817150116, Validation Loss: 0.9401828646659851\n",
      "Epoch 190: Train Loss: 0.6476924657821655, Validation Loss: 0.9468799829483032\n",
      "Epoch 191: Train Loss: 0.640168559551239, Validation Loss: 0.9520496726036072\n",
      "Epoch 192: Train Loss: 0.6805956125259399, Validation Loss: 0.9556497931480408\n",
      "Epoch 193: Train Loss: 0.6413489580154419, Validation Loss: 0.9509201049804688\n",
      "Epoch 194: Train Loss: 0.5845824360847474, Validation Loss: 0.9584083557128906\n",
      "Epoch 195: Train Loss: 0.6443926692008972, Validation Loss: 0.9611696600914001\n",
      "Epoch 196: Train Loss: 0.5923707127571106, Validation Loss: 0.9651461839675903\n",
      "Epoch 197: Train Loss: 0.590391880273819, Validation Loss: 0.9768432378768921\n",
      "Epoch 198: Train Loss: 0.6388211607933044, Validation Loss: 0.9828354120254517\n",
      "Epoch 199: Train Loss: 0.6592611193656921, Validation Loss: 0.9777891635894775\n",
      "Epoch 200: Train Loss: 0.632418179512024, Validation Loss: 0.9825697541236877\n",
      "Epoch 201: Train Loss: 0.6217531085014343, Validation Loss: 0.994981050491333\n",
      "Epoch 202: Train Loss: 0.6859392881393432, Validation Loss: 0.9997047781944275\n",
      "Epoch 203: Train Loss: 0.664278781414032, Validation Loss: 1.0051825046539307\n",
      "Epoch 204: Train Loss: 0.6431753516197205, Validation Loss: 1.0054278373718262\n",
      "Epoch 205: Train Loss: 0.6134315133094788, Validation Loss: 1.001859188079834\n",
      "Epoch 206: Train Loss: 0.6119650483131409, Validation Loss: 1.005753755569458\n",
      "Epoch 207: Train Loss: 0.5936373829841614, Validation Loss: 1.018235683441162\n",
      "Epoch 208: Train Loss: 0.6416441917419433, Validation Loss: 1.0214383602142334\n",
      "Epoch 209: Train Loss: 0.6414883255958557, Validation Loss: 1.0267869234085083\n",
      "Epoch 210: Train Loss: 0.6118036866188049, Validation Loss: 1.029035210609436\n",
      "Epoch 211: Train Loss: 0.6381102442741394, Validation Loss: 1.0483342409133911\n",
      "Epoch 212: Train Loss: 0.5922611236572266, Validation Loss: 1.0527596473693848\n",
      "Epoch 213: Train Loss: 0.6459277033805847, Validation Loss: 1.0549836158752441\n",
      "Epoch 214: Train Loss: 0.6224320292472839, Validation Loss: 1.0748379230499268\n",
      "Epoch 215: Train Loss: 0.6208777785301208, Validation Loss: 1.0820146799087524\n",
      "Epoch 216: Train Loss: 0.6147751808166504, Validation Loss: 1.0800954103469849\n",
      "Epoch 217: Train Loss: 0.6270926237106323, Validation Loss: 1.0746574401855469\n",
      "Epoch 218: Train Loss: 0.5870846748352051, Validation Loss: 1.0828553438186646\n",
      "Epoch 219: Train Loss: 0.5573415875434875, Validation Loss: 1.0941731929779053\n",
      "Epoch 220: Train Loss: 0.624673855304718, Validation Loss: 1.0981966257095337\n",
      "Epoch 221: Train Loss: 0.6159493327140808, Validation Loss: 1.1143361330032349\n",
      "Epoch 222: Train Loss: 0.6054038584232331, Validation Loss: 1.1169832944869995\n",
      "Epoch 223: Train Loss: 0.6300295352935791, Validation Loss: 1.1217925548553467\n",
      "Epoch 224: Train Loss: 0.597700548171997, Validation Loss: 1.110994815826416\n",
      "Epoch 225: Train Loss: 0.5759223222732544, Validation Loss: 1.1298234462738037\n",
      "Epoch 226: Train Loss: 0.6134247422218323, Validation Loss: 1.1161211729049683\n",
      "Epoch 227: Train Loss: 0.5714428305625916, Validation Loss: 1.1356003284454346\n",
      "Epoch 228: Train Loss: 0.5431226372718811, Validation Loss: 1.1280103921890259\n",
      "Epoch 229: Train Loss: 0.6600680232048035, Validation Loss: 1.1382814645767212\n",
      "Epoch 230: Train Loss: 0.5929025292396546, Validation Loss: 1.1562587022781372\n",
      "Epoch 231: Train Loss: 0.5597230315208435, Validation Loss: 1.1724110841751099\n",
      "Epoch 232: Train Loss: 0.5908045053482056, Validation Loss: 1.172611951828003\n",
      "Epoch 233: Train Loss: 0.6562754392623902, Validation Loss: 1.1681156158447266\n",
      "Epoch 234: Train Loss: 0.6016191601753235, Validation Loss: 1.1877200603485107\n",
      "Epoch 235: Train Loss: 0.5626212596893311, Validation Loss: 1.1835566759109497\n",
      "Epoch 236: Train Loss: 0.5653515875339508, Validation Loss: 1.1936428546905518\n",
      "Epoch 237: Train Loss: 0.5643530428409577, Validation Loss: 1.208161473274231\n",
      "Epoch 238: Train Loss: 0.595294189453125, Validation Loss: 1.1987804174423218\n",
      "Epoch 239: Train Loss: 0.5405700862407684, Validation Loss: 1.2070982456207275\n",
      "Epoch 240: Train Loss: 0.5726446688175202, Validation Loss: 1.2388033866882324\n",
      "Epoch 241: Train Loss: 0.5975944995880127, Validation Loss: 1.2406210899353027\n",
      "Epoch 242: Train Loss: 0.6163170337677002, Validation Loss: 1.2519904375076294\n",
      "Epoch 243: Train Loss: 0.6290588855743409, Validation Loss: 1.2712596654891968\n",
      "Epoch 244: Train Loss: 0.5596530854701995, Validation Loss: 1.2452764511108398\n",
      "Epoch 245: Train Loss: 0.5648290038108825, Validation Loss: 1.264662742614746\n",
      "Epoch 246: Train Loss: 0.5879992008209228, Validation Loss: 1.2811944484710693\n",
      "Epoch 247: Train Loss: 0.6491490840911865, Validation Loss: 1.3009483814239502\n",
      "Epoch 248: Train Loss: 0.5712385654449463, Validation Loss: 1.3081862926483154\n",
      "Epoch 249: Train Loss: 0.5493125915527344, Validation Loss: 1.316476583480835\n",
      "Epoch 250: Train Loss: 0.6230816841125488, Validation Loss: 1.3126176595687866\n",
      "Epoch 251: Train Loss: 0.5597956240177154, Validation Loss: 1.335304856300354\n",
      "Epoch 252: Train Loss: 0.5647666335105896, Validation Loss: 1.3599724769592285\n",
      "Epoch 253: Train Loss: 0.5401444554328918, Validation Loss: 1.3815772533416748\n",
      "Epoch 254: Train Loss: 0.6345246315002442, Validation Loss: 1.38237464427948\n",
      "Epoch 255: Train Loss: 0.5768213152885437, Validation Loss: 1.3889474868774414\n",
      "Epoch 256: Train Loss: 0.5989384114742279, Validation Loss: 1.4116321802139282\n",
      "Epoch 257: Train Loss: 0.6170304775238037, Validation Loss: 1.4245387315750122\n",
      "Epoch 258: Train Loss: 0.5953201174736023, Validation Loss: 1.4465941190719604\n",
      "Epoch 259: Train Loss: 0.5500020027160645, Validation Loss: 1.4535951614379883\n",
      "Epoch 260: Train Loss: 0.5592407047748565, Validation Loss: 1.422645926475525\n",
      "Epoch 261: Train Loss: 0.5284461915493012, Validation Loss: 1.45380437374115\n",
      "Epoch 262: Train Loss: 0.6271122992038727, Validation Loss: 1.474817156791687\n",
      "Epoch 263: Train Loss: 0.5712647318840027, Validation Loss: 1.4847766160964966\n",
      "Epoch 264: Train Loss: 0.6015501141548156, Validation Loss: 1.5099081993103027\n",
      "Epoch 265: Train Loss: 0.5364953219890595, Validation Loss: 1.4999529123306274\n",
      "Epoch 266: Train Loss: 0.54886953830719, Validation Loss: 1.5352224111557007\n",
      "Epoch 267: Train Loss: 0.548158073425293, Validation Loss: 1.5371150970458984\n",
      "Epoch 268: Train Loss: 0.5465427517890931, Validation Loss: 1.5246886014938354\n",
      "Epoch 269: Train Loss: 0.49763497710227966, Validation Loss: 1.534936547279358\n",
      "Epoch 270: Train Loss: 0.6191754579544068, Validation Loss: 1.5369632244110107\n",
      "Epoch 271: Train Loss: 0.6077194094657898, Validation Loss: 1.5547003746032715\n",
      "Epoch 272: Train Loss: 0.6136297106742858, Validation Loss: 1.556589126586914\n",
      "Epoch 273: Train Loss: 0.5793438732624054, Validation Loss: 1.5531009435653687\n",
      "Epoch 274: Train Loss: 0.5411584854125977, Validation Loss: 1.53878915309906\n",
      "Epoch 275: Train Loss: 0.48964272141456605, Validation Loss: 1.540842056274414\n",
      "Epoch 276: Train Loss: 0.5176253736019134, Validation Loss: 1.5449596643447876\n",
      "Epoch 277: Train Loss: 0.5552914440631866, Validation Loss: 1.5364809036254883\n",
      "Epoch 278: Train Loss: 0.5508018255233764, Validation Loss: 1.5457476377487183\n",
      "Epoch 279: Train Loss: 0.5475230574607849, Validation Loss: 1.5423065423965454\n",
      "Epoch 280: Train Loss: 0.5124510049819946, Validation Loss: 1.563675880432129\n",
      "Epoch 281: Train Loss: 0.5142654299736023, Validation Loss: 1.5891764163970947\n",
      "Epoch 282: Train Loss: 0.5376575529575348, Validation Loss: 1.6331934928894043\n",
      "Epoch 283: Train Loss: 0.5678922295570373, Validation Loss: 1.6755183935165405\n",
      "Epoch 284: Train Loss: 0.5321354150772095, Validation Loss: 1.6948349475860596\n",
      "Epoch 285: Train Loss: 0.5359354078769684, Validation Loss: 1.6925990581512451\n",
      "Epoch 286: Train Loss: 0.5490007758140564, Validation Loss: 1.690589427947998\n",
      "Epoch 287: Train Loss: 0.535621726512909, Validation Loss: 1.6664835214614868\n",
      "Epoch 288: Train Loss: 0.5876336336135864, Validation Loss: 1.6791963577270508\n",
      "Epoch 289: Train Loss: 0.5245043158531189, Validation Loss: 1.6910494565963745\n",
      "Epoch 290: Train Loss: 0.458519184589386, Validation Loss: 1.7104179859161377\n",
      "Epoch 291: Train Loss: 0.5856808125972748, Validation Loss: 1.747496247291565\n",
      "Epoch 292: Train Loss: 0.5112675786018371, Validation Loss: 1.7804969549179077\n",
      "Epoch 293: Train Loss: 0.5690086305141449, Validation Loss: 1.801321029663086\n",
      "Epoch 294: Train Loss: 0.484638911485672, Validation Loss: 1.7546528577804565\n",
      "Epoch 295: Train Loss: 0.5152098715305329, Validation Loss: 1.7873668670654297\n",
      "Epoch 296: Train Loss: 0.5367226004600525, Validation Loss: 1.7729933261871338\n",
      "Epoch 297: Train Loss: 0.497983056306839, Validation Loss: 1.755125880241394\n",
      "Epoch 298: Train Loss: 0.5378487169742584, Validation Loss: 1.8017826080322266\n",
      "Epoch 299: Train Loss: 0.4905095398426056, Validation Loss: 1.8295314311981201\n",
      "Epoch 300: Train Loss: 0.5971950888633728, Validation Loss: 1.8309903144836426\n",
      "Epoch 301: Train Loss: 0.4752866804599762, Validation Loss: 1.8618296384811401\n",
      "Epoch 302: Train Loss: 0.46213582456111907, Validation Loss: 1.812372088432312\n",
      "Epoch 303: Train Loss: 0.5107645452022552, Validation Loss: 1.8824084997177124\n",
      "Epoch 304: Train Loss: 0.5163564920425415, Validation Loss: 1.931070327758789\n",
      "Epoch 305: Train Loss: 0.4958343505859375, Validation Loss: 1.9363445043563843\n",
      "Epoch 306: Train Loss: 0.4465436726808548, Validation Loss: 1.9305168390274048\n",
      "Epoch 307: Train Loss: 0.5100911498069763, Validation Loss: 1.9345908164978027\n",
      "Epoch 308: Train Loss: 0.46372869312763215, Validation Loss: 1.8993268013000488\n",
      "Epoch 309: Train Loss: 0.5971516311168671, Validation Loss: 1.906828761100769\n",
      "Epoch 310: Train Loss: 0.4912748396396637, Validation Loss: 1.9429036378860474\n",
      "Epoch 311: Train Loss: 0.5941001176834106, Validation Loss: 1.9334529638290405\n",
      "Epoch 312: Train Loss: 0.4887955605983734, Validation Loss: 1.9478992223739624\n",
      "Epoch 313: Train Loss: 0.4697554349899292, Validation Loss: 1.933788776397705\n",
      "Epoch 314: Train Loss: 0.5169307231903076, Validation Loss: 1.974765419960022\n",
      "Epoch 315: Train Loss: 0.5723745822906494, Validation Loss: 1.9698147773742676\n",
      "Epoch 316: Train Loss: 0.5581273436546326, Validation Loss: 1.998311161994934\n",
      "Epoch 317: Train Loss: 0.4820004224777222, Validation Loss: 2.0015463829040527\n",
      "Epoch 318: Train Loss: 0.5635366916656495, Validation Loss: 2.0157315731048584\n",
      "Epoch 319: Train Loss: 0.5612531840801239, Validation Loss: 2.0111002922058105\n",
      "Epoch 320: Train Loss: 0.5784236073493958, Validation Loss: 2.009424924850464\n",
      "Epoch 321: Train Loss: 0.47587102055549624, Validation Loss: 2.0351414680480957\n",
      "Epoch 322: Train Loss: 0.4661513090133667, Validation Loss: 2.0443406105041504\n",
      "Epoch 323: Train Loss: 0.49469868540763856, Validation Loss: 2.0716352462768555\n",
      "Epoch 324: Train Loss: 0.5309542298316956, Validation Loss: 2.081361770629883\n",
      "Epoch 325: Train Loss: 0.447629576921463, Validation Loss: 2.0973827838897705\n",
      "Epoch 326: Train Loss: 0.48713122606277465, Validation Loss: 2.092984437942505\n",
      "Epoch 327: Train Loss: 0.486980265378952, Validation Loss: 2.0838510990142822\n",
      "Epoch 328: Train Loss: 0.4114750474691391, Validation Loss: 2.0202925205230713\n",
      "Epoch 329: Train Loss: 0.6980988204479217, Validation Loss: 2.0367043018341064\n",
      "Epoch 330: Train Loss: 0.48360646367073057, Validation Loss: 2.081260919570923\n",
      "Epoch 331: Train Loss: 0.44425991773605344, Validation Loss: 2.068981409072876\n",
      "Epoch 332: Train Loss: 0.4586174190044403, Validation Loss: 2.0624589920043945\n",
      "Epoch 333: Train Loss: 0.5245679557323456, Validation Loss: 2.129197835922241\n",
      "Epoch 334: Train Loss: 0.44056965708732604, Validation Loss: 2.1478283405303955\n",
      "Epoch 335: Train Loss: 0.529775756597519, Validation Loss: 2.125166416168213\n",
      "Epoch 336: Train Loss: 0.44103364944458007, Validation Loss: 2.1260337829589844\n",
      "Epoch 337: Train Loss: 0.40957099199295044, Validation Loss: 2.1692657470703125\n",
      "Epoch 338: Train Loss: 0.422256863117218, Validation Loss: 2.1981287002563477\n",
      "Epoch 339: Train Loss: 0.4994545876979828, Validation Loss: 2.1905970573425293\n",
      "Epoch 340: Train Loss: 0.4662247896194458, Validation Loss: 2.240201711654663\n",
      "Epoch 341: Train Loss: 0.5298232018947602, Validation Loss: 2.2799124717712402\n",
      "Epoch 342: Train Loss: 0.5117936909198761, Validation Loss: 2.3214972019195557\n",
      "Epoch 343: Train Loss: 0.43897700905799864, Validation Loss: 2.355401039123535\n",
      "Epoch 344: Train Loss: 0.4270603597164154, Validation Loss: 2.3559646606445312\n",
      "Epoch 345: Train Loss: 0.4484525263309479, Validation Loss: 2.3790814876556396\n",
      "Epoch 346: Train Loss: 0.5707296252250671, Validation Loss: 2.3158061504364014\n",
      "Epoch 347: Train Loss: 0.4115394651889801, Validation Loss: 2.267488718032837\n",
      "Epoch 348: Train Loss: 0.4446412205696106, Validation Loss: 2.289503812789917\n",
      "Epoch 349: Train Loss: 0.4689010322093964, Validation Loss: 2.287853479385376\n",
      "Epoch 350: Train Loss: 0.5126367568969726, Validation Loss: 2.3420910835266113\n",
      "Epoch 351: Train Loss: 0.46806272864341736, Validation Loss: 2.361541271209717\n",
      "Epoch 352: Train Loss: 0.41293256282806395, Validation Loss: 2.399360179901123\n",
      "Epoch 353: Train Loss: 0.41531256437301634, Validation Loss: 2.405515193939209\n",
      "Epoch 354: Train Loss: 0.37277392148971555, Validation Loss: 2.367673397064209\n",
      "Epoch 355: Train Loss: 0.4131163418292999, Validation Loss: 2.4620344638824463\n",
      "Epoch 356: Train Loss: 0.46543622612953184, Validation Loss: 2.5255472660064697\n",
      "Epoch 357: Train Loss: 0.43471673130989075, Validation Loss: 2.4774489402770996\n",
      "Epoch 358: Train Loss: 0.4212058663368225, Validation Loss: 2.462130069732666\n",
      "Epoch 359: Train Loss: 0.3868670701980591, Validation Loss: 2.4663805961608887\n",
      "Epoch 360: Train Loss: 0.4717469155788422, Validation Loss: 2.539583921432495\n",
      "Epoch 361: Train Loss: 0.39824084341526034, Validation Loss: 2.5301389694213867\n",
      "Epoch 362: Train Loss: 0.4613125085830688, Validation Loss: 2.501875638961792\n",
      "Epoch 363: Train Loss: 0.360972261428833, Validation Loss: 2.5442144870758057\n",
      "Epoch 364: Train Loss: 0.4627820074558258, Validation Loss: 2.588125228881836\n",
      "Epoch 365: Train Loss: 0.39945871829986573, Validation Loss: 2.560511827468872\n",
      "Epoch 366: Train Loss: 0.4065799415111542, Validation Loss: 2.5638864040374756\n",
      "Epoch 367: Train Loss: 0.4351047337055206, Validation Loss: 2.586394786834717\n",
      "Epoch 368: Train Loss: 0.5046187758445739, Validation Loss: 2.546053886413574\n",
      "Epoch 369: Train Loss: 0.40667369961738586, Validation Loss: 2.544860363006592\n",
      "Epoch 370: Train Loss: 0.44210386276245117, Validation Loss: 2.553988456726074\n",
      "Epoch 371: Train Loss: 0.4295700192451477, Validation Loss: 2.626830816268921\n",
      "Epoch 372: Train Loss: 0.4003659009933472, Validation Loss: 2.671112060546875\n",
      "Epoch 373: Train Loss: 0.40521666407585144, Validation Loss: 2.7130980491638184\n",
      "Epoch 374: Train Loss: 0.3616733878850937, Validation Loss: 2.7035415172576904\n",
      "Epoch 375: Train Loss: 0.44978669881820676, Validation Loss: 2.6666007041931152\n",
      "Epoch 376: Train Loss: 0.37598284482955935, Validation Loss: 2.618032217025757\n",
      "Epoch 377: Train Loss: 0.48752251267433167, Validation Loss: 2.6401114463806152\n",
      "Epoch 378: Train Loss: 0.3979755759239197, Validation Loss: 2.725203275680542\n",
      "Epoch 379: Train Loss: 0.3581935167312622, Validation Loss: 2.7374465465545654\n",
      "Epoch 380: Train Loss: 0.47211363911628723, Validation Loss: 2.777251720428467\n",
      "Epoch 381: Train Loss: 0.4386570453643799, Validation Loss: 2.745077610015869\n",
      "Epoch 382: Train Loss: 0.35001503974199294, Validation Loss: 2.7587344646453857\n",
      "Epoch 383: Train Loss: 0.4030268371105194, Validation Loss: 2.842867851257324\n",
      "Epoch 384: Train Loss: 0.4150566220283508, Validation Loss: 2.8694958686828613\n",
      "Epoch 385: Train Loss: 0.39018205404281614, Validation Loss: 2.84778094291687\n",
      "Epoch 386: Train Loss: 0.5650987207889557, Validation Loss: 2.823421001434326\n",
      "Epoch 387: Train Loss: 0.4130538284778595, Validation Loss: 2.8522820472717285\n",
      "Epoch 388: Train Loss: 0.37223424911499026, Validation Loss: 2.851985454559326\n",
      "Epoch 389: Train Loss: 0.3601248383522034, Validation Loss: 2.8659613132476807\n",
      "Epoch 390: Train Loss: 0.3702415585517883, Validation Loss: 2.8685975074768066\n",
      "Epoch 391: Train Loss: 0.4201164424419403, Validation Loss: 2.8748903274536133\n",
      "Epoch 392: Train Loss: 0.417503809928894, Validation Loss: 2.984170436859131\n",
      "Epoch 393: Train Loss: 0.45210915207862856, Validation Loss: 2.930459499359131\n",
      "Epoch 394: Train Loss: 0.39560194611549376, Validation Loss: 2.9632911682128906\n",
      "Epoch 395: Train Loss: 0.4759817957878113, Validation Loss: 2.9783008098602295\n",
      "Epoch 396: Train Loss: 0.3936130106449127, Validation Loss: 2.9809234142303467\n",
      "Epoch 397: Train Loss: 0.403191339969635, Validation Loss: 3.0250561237335205\n",
      "Epoch 398: Train Loss: 0.35711333751678465, Validation Loss: 3.0096256732940674\n",
      "Epoch 399: Train Loss: 0.36492423713207245, Validation Loss: 2.9951465129852295\n",
      "Epoch 400: Train Loss: 0.3956841766834259, Validation Loss: 2.9092509746551514\n",
      "Epoch 401: Train Loss: 0.408973228931427, Validation Loss: 2.940027952194214\n",
      "Epoch 402: Train Loss: 0.37033551931381226, Validation Loss: 2.8981287479400635\n",
      "Epoch 403: Train Loss: 0.4103699862957001, Validation Loss: 2.906799077987671\n",
      "Epoch 404: Train Loss: 0.41323897838592527, Validation Loss: 2.9410674571990967\n",
      "Epoch 405: Train Loss: 0.3647069215774536, Validation Loss: 3.0398054122924805\n",
      "Epoch 406: Train Loss: 0.3312723159790039, Validation Loss: 3.0916759967803955\n",
      "Epoch 407: Train Loss: 0.49030521512031555, Validation Loss: 3.149056911468506\n",
      "Epoch 408: Train Loss: 0.38374984860420225, Validation Loss: 3.1631863117218018\n",
      "Epoch 409: Train Loss: 0.32139148414134977, Validation Loss: 3.089632272720337\n",
      "Epoch 410: Train Loss: 0.3054988205432892, Validation Loss: 3.151252508163452\n",
      "Epoch 411: Train Loss: 0.3678178608417511, Validation Loss: 3.1949713230133057\n",
      "Epoch 412: Train Loss: 0.39562034606933594, Validation Loss: 3.2780423164367676\n",
      "Epoch 413: Train Loss: 0.36710190773010254, Validation Loss: 3.306624412536621\n",
      "Epoch 414: Train Loss: 0.3572200328111649, Validation Loss: 3.3509299755096436\n",
      "Epoch 415: Train Loss: 0.3322013974189758, Validation Loss: 3.3017208576202393\n",
      "Epoch 416: Train Loss: 0.34297946095466614, Validation Loss: 3.2567176818847656\n",
      "Epoch 417: Train Loss: 0.3295067369937897, Validation Loss: 3.196638584136963\n",
      "Epoch 418: Train Loss: 0.4881262958049774, Validation Loss: 3.2375502586364746\n",
      "Epoch 419: Train Loss: 0.36060701608657836, Validation Loss: 3.316986560821533\n",
      "Epoch 420: Train Loss: 0.3610999047756195, Validation Loss: 3.207777261734009\n",
      "Epoch 421: Train Loss: 0.4025591969490051, Validation Loss: 3.226280689239502\n",
      "Epoch 422: Train Loss: 0.30510793924331664, Validation Loss: 3.2761647701263428\n",
      "Epoch 423: Train Loss: 0.3618706226348877, Validation Loss: 3.3385000228881836\n",
      "Epoch 424: Train Loss: 0.40034104585647584, Validation Loss: 3.365647554397583\n",
      "Epoch 425: Train Loss: 0.41725229024887084, Validation Loss: 3.374105930328369\n",
      "Epoch 426: Train Loss: 0.39696750044822693, Validation Loss: 3.2394988536834717\n",
      "Epoch 427: Train Loss: 0.30032499581575395, Validation Loss: 3.116196870803833\n",
      "Epoch 428: Train Loss: 0.3690688371658325, Validation Loss: 3.20168137550354\n",
      "Epoch 429: Train Loss: 0.35376628637313845, Validation Loss: 3.2667996883392334\n",
      "Epoch 430: Train Loss: 0.4199854075908661, Validation Loss: 3.3090660572052\n",
      "Epoch 431: Train Loss: 0.382500422000885, Validation Loss: 3.3890912532806396\n",
      "Epoch 432: Train Loss: 0.27773965448141097, Validation Loss: 3.3768551349639893\n",
      "Epoch 433: Train Loss: 0.5084238678216935, Validation Loss: 3.3436765670776367\n",
      "Epoch 434: Train Loss: 0.33557210564613343, Validation Loss: 3.361212730407715\n",
      "Epoch 435: Train Loss: 0.33202519416809084, Validation Loss: 3.2347326278686523\n",
      "Epoch 436: Train Loss: 0.2985278457403183, Validation Loss: 3.2636380195617676\n",
      "Epoch 437: Train Loss: 0.31082623898983003, Validation Loss: 3.266725778579712\n",
      "Epoch 438: Train Loss: 0.43469059467315674, Validation Loss: 3.4407997131347656\n",
      "Epoch 439: Train Loss: 0.39654259085655214, Validation Loss: 3.508357286453247\n",
      "Epoch 440: Train Loss: 0.546294954419136, Validation Loss: 3.389660596847534\n",
      "Epoch 441: Train Loss: 0.42814857661724093, Validation Loss: 3.4239463806152344\n",
      "Epoch 442: Train Loss: 0.33754897117614746, Validation Loss: 3.3622913360595703\n",
      "Epoch 443: Train Loss: 0.38898890614509585, Validation Loss: 3.40096116065979\n",
      "Epoch 444: Train Loss: 0.43325231075286863, Validation Loss: 3.4093871116638184\n",
      "Epoch 445: Train Loss: 0.43443413376808165, Validation Loss: 3.2933189868927\n",
      "Epoch 446: Train Loss: 0.3557808339595795, Validation Loss: 3.2786965370178223\n",
      "Epoch 447: Train Loss: 0.3121701806783676, Validation Loss: 3.3331615924835205\n",
      "Epoch 448: Train Loss: 0.4313043713569641, Validation Loss: 3.3046162128448486\n",
      "Epoch 449: Train Loss: 0.30862804055213927, Validation Loss: 3.293958902359009\n",
      "Epoch 450: Train Loss: 0.35331563353538514, Validation Loss: 3.321962833404541\n",
      "Epoch 451: Train Loss: 0.3813033878803253, Validation Loss: 3.408651113510132\n",
      "Epoch 452: Train Loss: 0.4679685026407242, Validation Loss: 3.430483818054199\n",
      "Epoch 453: Train Loss: 0.28024924993515016, Validation Loss: 3.4377667903900146\n",
      "Epoch 454: Train Loss: 0.30770526826381683, Validation Loss: 3.409644365310669\n",
      "Epoch 455: Train Loss: 0.32829957008361815, Validation Loss: 3.4665558338165283\n",
      "Epoch 456: Train Loss: 0.28795857429504396, Validation Loss: 3.486182928085327\n",
      "Epoch 457: Train Loss: 0.32834882736206056, Validation Loss: 3.5288381576538086\n",
      "Epoch 458: Train Loss: 0.3703082323074341, Validation Loss: 3.3782103061676025\n",
      "Epoch 459: Train Loss: 0.2853783667087555, Validation Loss: 3.3465709686279297\n",
      "Epoch 460: Train Loss: 0.2767337679862976, Validation Loss: 3.3876030445098877\n",
      "Epoch 461: Train Loss: 0.29387574195861815, Validation Loss: 3.4154000282287598\n",
      "Epoch 462: Train Loss: 0.2998299926519394, Validation Loss: 3.506598472595215\n",
      "Epoch 463: Train Loss: 0.287868532538414, Validation Loss: 3.581648826599121\n",
      "Epoch 464: Train Loss: 0.4817246437072754, Validation Loss: 3.571298360824585\n",
      "Epoch 465: Train Loss: 0.5252517759799957, Validation Loss: 3.619004011154175\n",
      "Epoch 466: Train Loss: 0.3133757621049881, Validation Loss: 3.5634167194366455\n",
      "Epoch 467: Train Loss: 0.3833054155111313, Validation Loss: 3.5652029514312744\n",
      "Epoch 468: Train Loss: 0.4000949949026108, Validation Loss: 3.57646107673645\n",
      "Epoch 469: Train Loss: 0.4270187795162201, Validation Loss: 3.3961386680603027\n",
      "Epoch 470: Train Loss: 0.2463800698518753, Validation Loss: 3.3868329524993896\n",
      "Epoch 471: Train Loss: 0.3337276965379715, Validation Loss: 3.358893632888794\n",
      "Epoch 472: Train Loss: 0.36730619668960574, Validation Loss: 3.3824079036712646\n",
      "Epoch 473: Train Loss: 0.2853375434875488, Validation Loss: 3.4238083362579346\n",
      "Epoch 474: Train Loss: 0.25736722648143767, Validation Loss: 3.45536732673645\n",
      "Epoch 475: Train Loss: 0.3650298535823822, Validation Loss: 3.5080413818359375\n",
      "Epoch 476: Train Loss: 0.2640138819813728, Validation Loss: 3.600146532058716\n",
      "Epoch 477: Train Loss: 0.4551559895277023, Validation Loss: 3.597511053085327\n",
      "Epoch 478: Train Loss: 0.32343913316726686, Validation Loss: 3.6133933067321777\n",
      "Epoch 479: Train Loss: 0.2944423913955688, Validation Loss: 3.576500654220581\n",
      "Epoch 480: Train Loss: 0.2929606676101685, Validation Loss: 3.6589150428771973\n",
      "Epoch 481: Train Loss: 0.2863584280014038, Validation Loss: 3.654705762863159\n",
      "Epoch 482: Train Loss: 0.358048677444458, Validation Loss: 3.669748067855835\n",
      "Epoch 483: Train Loss: 0.28163679838180544, Validation Loss: 3.6840264797210693\n",
      "Epoch 484: Train Loss: 0.38192951679229736, Validation Loss: 3.7423207759857178\n",
      "Epoch 485: Train Loss: 0.2912971168756485, Validation Loss: 3.634821891784668\n",
      "Epoch 486: Train Loss: 0.5453801155090332, Validation Loss: 3.6837410926818848\n",
      "Epoch 487: Train Loss: 0.26616987586021423, Validation Loss: 3.7022011280059814\n",
      "Epoch 488: Train Loss: 0.3970919787883759, Validation Loss: 3.7574663162231445\n",
      "Epoch 489: Train Loss: 0.4071399211883545, Validation Loss: 3.755735158920288\n",
      "Epoch 490: Train Loss: 0.24679434299468994, Validation Loss: 3.7715935707092285\n",
      "Epoch 491: Train Loss: 0.46027909219264984, Validation Loss: 3.7744462490081787\n",
      "Epoch 492: Train Loss: 0.30017435252666474, Validation Loss: 3.730858564376831\n",
      "Epoch 493: Train Loss: 0.3241615265607834, Validation Loss: 3.71285343170166\n",
      "Epoch 494: Train Loss: 0.32562007904052737, Validation Loss: 3.7035741806030273\n",
      "Epoch 495: Train Loss: 0.2604709088802338, Validation Loss: 3.7454748153686523\n",
      "Epoch 496: Train Loss: 0.2418775051832199, Validation Loss: 3.7602286338806152\n",
      "Epoch 497: Train Loss: 0.33113566040992737, Validation Loss: 3.7707626819610596\n",
      "Epoch 498: Train Loss: 0.27208163142204284, Validation Loss: 3.720734119415283\n",
      "Epoch 499: Train Loss: 0.34857117831707, Validation Loss: 3.707899332046509\n",
      "Fold 5 Metrics:\n",
      "Accuracy: 0.09090909090909091, Precision: 0.16666666666666666, Recall: 0.16666666666666666, F1-score: 0.16666666666666666, AUC: 0.08333333333333333\n",
      "Confusion Matrix:\n",
      "[[0 5]\n",
      " [5 1]]\n",
      "Completed fold 5\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples from subject 6 to test set\n",
      "Adding 6 truth samples from subject 6 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 500)\n",
      "(132, 65, 500)\n",
      "Epoch 0: Train Loss: 0.6903681039810181, Validation Loss: 0.6955950856208801\n",
      "Epoch 1: Train Loss: 0.6895606398582459, Validation Loss: 0.6971492171287537\n",
      "Epoch 2: Train Loss: 0.7054679751396179, Validation Loss: 0.699001669883728\n",
      "Epoch 3: Train Loss: 0.7195740580558777, Validation Loss: 0.701036810874939\n",
      "Epoch 4: Train Loss: 0.7178487420082093, Validation Loss: 0.7025339007377625\n",
      "Epoch 5: Train Loss: 0.7288431167602539, Validation Loss: 0.7038363218307495\n",
      "Epoch 6: Train Loss: 0.6920519351959229, Validation Loss: 0.7049226760864258\n",
      "Epoch 7: Train Loss: 0.6713405013084411, Validation Loss: 0.7060310244560242\n",
      "Epoch 8: Train Loss: 0.6727602362632752, Validation Loss: 0.706499457359314\n",
      "Epoch 9: Train Loss: 0.7404707670211792, Validation Loss: 0.7050074338912964\n",
      "Epoch 10: Train Loss: 0.7082877635955811, Validation Loss: 0.7057323455810547\n",
      "Epoch 11: Train Loss: 0.7029855608940124, Validation Loss: 0.7062411308288574\n",
      "Epoch 12: Train Loss: 0.6978748321533204, Validation Loss: 0.706172525882721\n",
      "Epoch 13: Train Loss: 0.7387256145477294, Validation Loss: 0.7061842679977417\n",
      "Epoch 14: Train Loss: 0.7060720562934876, Validation Loss: 0.7062257528305054\n",
      "Epoch 15: Train Loss: 0.7376698017120361, Validation Loss: 0.7049697637557983\n",
      "Epoch 16: Train Loss: 0.6920187115669251, Validation Loss: 0.7056540250778198\n",
      "Epoch 17: Train Loss: 0.700737190246582, Validation Loss: 0.7065353393554688\n",
      "Epoch 18: Train Loss: 0.7123315453529357, Validation Loss: 0.7062668204307556\n",
      "Epoch 19: Train Loss: 0.6941117167472839, Validation Loss: 0.7068392634391785\n",
      "Epoch 20: Train Loss: 0.7066353559494019, Validation Loss: 0.7064017057418823\n",
      "Epoch 21: Train Loss: 0.6640510320663452, Validation Loss: 0.7059605717658997\n",
      "Epoch 22: Train Loss: 0.6991198539733887, Validation Loss: 0.7056886553764343\n",
      "Epoch 23: Train Loss: 0.74620281457901, Validation Loss: 0.7057775855064392\n",
      "Epoch 24: Train Loss: 0.7198202848434448, Validation Loss: 0.7065102458000183\n",
      "Epoch 25: Train Loss: 0.7746297597885132, Validation Loss: 0.706318736076355\n",
      "Epoch 26: Train Loss: 0.6938806772232056, Validation Loss: 0.7062814235687256\n",
      "Epoch 27: Train Loss: 0.7136934161186218, Validation Loss: 0.7064739465713501\n",
      "Epoch 28: Train Loss: 0.7303381323814392, Validation Loss: 0.7067031860351562\n",
      "Epoch 29: Train Loss: 0.7055117368698121, Validation Loss: 0.7054600715637207\n",
      "Epoch 30: Train Loss: 0.7105359196662903, Validation Loss: 0.7058301568031311\n",
      "Epoch 31: Train Loss: 0.6992691397666931, Validation Loss: 0.7062527537345886\n",
      "Epoch 32: Train Loss: 0.702509343624115, Validation Loss: 0.706788957118988\n",
      "Epoch 33: Train Loss: 0.7302851796150207, Validation Loss: 0.7068139910697937\n",
      "Epoch 34: Train Loss: 0.7049565434455871, Validation Loss: 0.7064484357833862\n",
      "Epoch 35: Train Loss: 0.7792952060699463, Validation Loss: 0.7066483497619629\n",
      "Epoch 36: Train Loss: 0.6692121863365174, Validation Loss: 0.7066798210144043\n",
      "Epoch 37: Train Loss: 0.6808910250663758, Validation Loss: 0.7055342793464661\n",
      "Epoch 38: Train Loss: 0.7372381210327148, Validation Loss: 0.7062588930130005\n",
      "Epoch 39: Train Loss: 0.6855258822441102, Validation Loss: 0.7063045501708984\n",
      "Epoch 40: Train Loss: 0.697349488735199, Validation Loss: 0.7059754133224487\n",
      "Epoch 41: Train Loss: 0.7249032616615295, Validation Loss: 0.7061620950698853\n",
      "Epoch 42: Train Loss: 0.7169106245040894, Validation Loss: 0.7064338326454163\n",
      "Epoch 43: Train Loss: 0.776466715335846, Validation Loss: 0.7064300775527954\n",
      "Epoch 44: Train Loss: 0.7260266780853272, Validation Loss: 0.7059907913208008\n",
      "Epoch 45: Train Loss: 0.7139922857284546, Validation Loss: 0.7054035663604736\n",
      "Epoch 46: Train Loss: 0.7028725385665894, Validation Loss: 0.704046905040741\n",
      "Epoch 47: Train Loss: 0.718526816368103, Validation Loss: 0.7033919095993042\n",
      "Epoch 48: Train Loss: 0.7465304613113404, Validation Loss: 0.7042641639709473\n",
      "Epoch 49: Train Loss: 0.6555371403694152, Validation Loss: 0.7044839859008789\n",
      "Epoch 50: Train Loss: 0.6888825297355652, Validation Loss: 0.7046197652816772\n",
      "Epoch 51: Train Loss: 0.6985778093338013, Validation Loss: 0.705621063709259\n",
      "Epoch 52: Train Loss: 0.7177982091903686, Validation Loss: 0.7057867050170898\n",
      "Epoch 53: Train Loss: 0.6988674998283386, Validation Loss: 0.704474687576294\n",
      "Epoch 54: Train Loss: 0.7249785304069519, Validation Loss: 0.7045343518257141\n",
      "Epoch 55: Train Loss: 0.6691170573234558, Validation Loss: 0.7048572897911072\n",
      "Epoch 56: Train Loss: 0.7129857063293457, Validation Loss: 0.7040433883666992\n",
      "Epoch 57: Train Loss: 0.7414677619934082, Validation Loss: 0.703784704208374\n",
      "Epoch 58: Train Loss: 0.6877472639083863, Validation Loss: 0.7039219737052917\n",
      "Epoch 59: Train Loss: 0.6781136512756347, Validation Loss: 0.7047366499900818\n",
      "Epoch 60: Train Loss: 0.6641090631484985, Validation Loss: 0.7046388387680054\n",
      "Epoch 61: Train Loss: 0.7179232597351074, Validation Loss: 0.7039694786071777\n",
      "Epoch 62: Train Loss: 0.6867856979370117, Validation Loss: 0.7041462063789368\n",
      "Epoch 63: Train Loss: 0.7383541345596314, Validation Loss: 0.7029802799224854\n",
      "Epoch 64: Train Loss: 0.6763911485671997, Validation Loss: 0.701977550983429\n",
      "Epoch 65: Train Loss: 0.6972777128219605, Validation Loss: 0.7028352618217468\n",
      "Epoch 66: Train Loss: 0.6316695213317871, Validation Loss: 0.7038974165916443\n",
      "Epoch 67: Train Loss: 0.7340040087699891, Validation Loss: 0.7041545510292053\n",
      "Epoch 68: Train Loss: 0.6920290112495422, Validation Loss: 0.7037760019302368\n",
      "Epoch 69: Train Loss: 0.665788471698761, Validation Loss: 0.7039706707000732\n",
      "Epoch 70: Train Loss: 0.6797009706497192, Validation Loss: 0.7030037641525269\n",
      "Epoch 71: Train Loss: 0.6902655124664306, Validation Loss: 0.7023800015449524\n",
      "Epoch 72: Train Loss: 0.7271231770515442, Validation Loss: 0.7034997344017029\n",
      "Epoch 73: Train Loss: 0.687112295627594, Validation Loss: 0.7036056518554688\n",
      "Epoch 74: Train Loss: 0.6593831539154053, Validation Loss: 0.702462375164032\n",
      "Epoch 75: Train Loss: 0.753648316860199, Validation Loss: 0.7029406428337097\n",
      "Epoch 76: Train Loss: 0.6995806574821473, Validation Loss: 0.7028907537460327\n",
      "Epoch 77: Train Loss: 0.7235320568084717, Validation Loss: 0.7020108699798584\n",
      "Epoch 78: Train Loss: 0.7203540205955505, Validation Loss: 0.7025687098503113\n",
      "Epoch 79: Train Loss: 0.7172689318656922, Validation Loss: 0.70276939868927\n",
      "Epoch 80: Train Loss: 0.7376363039016723, Validation Loss: 0.7032866477966309\n",
      "Epoch 81: Train Loss: 0.7003560900688172, Validation Loss: 0.7037288546562195\n",
      "Epoch 82: Train Loss: 0.6880955576896668, Validation Loss: 0.703631579875946\n",
      "Epoch 83: Train Loss: 0.6908246517181397, Validation Loss: 0.7043293118476868\n",
      "Epoch 84: Train Loss: 0.6736289024353027, Validation Loss: 0.7031351327896118\n",
      "Epoch 85: Train Loss: 0.6650783300399781, Validation Loss: 0.70272296667099\n",
      "Epoch 86: Train Loss: 0.6721581935882568, Validation Loss: 0.7031481862068176\n",
      "Epoch 87: Train Loss: 0.7625727176666259, Validation Loss: 0.7034766674041748\n",
      "Epoch 88: Train Loss: 0.6437508940696717, Validation Loss: 0.7036275863647461\n",
      "Epoch 89: Train Loss: 0.676865303516388, Validation Loss: 0.7026619911193848\n",
      "Epoch 90: Train Loss: 0.6871020793914795, Validation Loss: 0.7034467458724976\n",
      "Epoch 91: Train Loss: 0.686017918586731, Validation Loss: 0.7037944793701172\n",
      "Epoch 92: Train Loss: 0.6932767033576965, Validation Loss: 0.7027171850204468\n",
      "Epoch 93: Train Loss: 0.6804727077484131, Validation Loss: 0.703242301940918\n",
      "Epoch 94: Train Loss: 0.7364039301872254, Validation Loss: 0.7037519812583923\n",
      "Epoch 95: Train Loss: 0.7278139114379882, Validation Loss: 0.7042800784111023\n",
      "Epoch 96: Train Loss: 0.6716939806938171, Validation Loss: 0.7037175297737122\n",
      "Epoch 97: Train Loss: 0.6462877869606019, Validation Loss: 0.7043947577476501\n",
      "Epoch 98: Train Loss: 0.6828980684280396, Validation Loss: 0.7043226957321167\n",
      "Epoch 99: Train Loss: 0.7177820920944213, Validation Loss: 0.7045747637748718\n",
      "Epoch 100: Train Loss: 0.7047156929969788, Validation Loss: 0.7040895223617554\n",
      "Epoch 101: Train Loss: 0.6718132257461548, Validation Loss: 0.7037345767021179\n",
      "Epoch 102: Train Loss: 0.6604102611541748, Validation Loss: 0.7031679153442383\n",
      "Epoch 103: Train Loss: 0.6664037823677063, Validation Loss: 0.7026312947273254\n",
      "Epoch 104: Train Loss: 0.6741072773933411, Validation Loss: 0.7026209831237793\n",
      "Epoch 105: Train Loss: 0.693504810333252, Validation Loss: 0.7029795050621033\n",
      "Epoch 106: Train Loss: 0.6678279638290405, Validation Loss: 0.702587902545929\n",
      "Epoch 107: Train Loss: 0.6863899111747742, Validation Loss: 0.7027826905250549\n",
      "Epoch 108: Train Loss: 0.7269839286804199, Validation Loss: 0.7032166123390198\n",
      "Epoch 109: Train Loss: 0.7160759210586548, Validation Loss: 0.7033098340034485\n",
      "Epoch 110: Train Loss: 0.6878169178962708, Validation Loss: 0.703612208366394\n",
      "Epoch 111: Train Loss: 0.6719876408576966, Validation Loss: 0.703077495098114\n",
      "Epoch 112: Train Loss: 0.6598035454750061, Validation Loss: 0.7028794288635254\n",
      "Epoch 113: Train Loss: 0.6815533399581909, Validation Loss: 0.7025570869445801\n",
      "Epoch 114: Train Loss: 0.733755099773407, Validation Loss: 0.70290207862854\n",
      "Epoch 115: Train Loss: 0.6548877835273743, Validation Loss: 0.7018334269523621\n",
      "Epoch 116: Train Loss: 0.6841153502464294, Validation Loss: 0.7022791504859924\n",
      "Epoch 117: Train Loss: 0.6963687181472779, Validation Loss: 0.7010595798492432\n",
      "Epoch 118: Train Loss: 0.6707710385322571, Validation Loss: 0.7017459869384766\n",
      "Epoch 119: Train Loss: 0.708396053314209, Validation Loss: 0.7020455002784729\n",
      "Epoch 120: Train Loss: 0.6510626196861267, Validation Loss: 0.7023203372955322\n",
      "Epoch 121: Train Loss: 0.6547091126441955, Validation Loss: 0.7023274302482605\n",
      "Epoch 122: Train Loss: 0.655827796459198, Validation Loss: 0.701901912689209\n",
      "Epoch 123: Train Loss: 0.6291520237922669, Validation Loss: 0.702019214630127\n",
      "Epoch 124: Train Loss: 0.6977805733680725, Validation Loss: 0.70140540599823\n",
      "Epoch 125: Train Loss: 0.6844447493553162, Validation Loss: 0.7013144493103027\n",
      "Epoch 126: Train Loss: 0.6826996803283691, Validation Loss: 0.7010143995285034\n",
      "Epoch 127: Train Loss: 0.7097891449928284, Validation Loss: 0.7011858224868774\n",
      "Epoch 128: Train Loss: 0.665231466293335, Validation Loss: 0.7005914449691772\n",
      "Epoch 129: Train Loss: 0.6918904900550842, Validation Loss: 0.7010016441345215\n",
      "Epoch 130: Train Loss: 0.6454999089241028, Validation Loss: 0.7010806202888489\n",
      "Epoch 131: Train Loss: 0.687770426273346, Validation Loss: 0.7012271881103516\n",
      "Epoch 132: Train Loss: 0.64188551902771, Validation Loss: 0.701850414276123\n",
      "Epoch 133: Train Loss: 0.6725012540817261, Validation Loss: 0.7017601728439331\n",
      "Epoch 134: Train Loss: 0.6981601595878602, Validation Loss: 0.7007418870925903\n",
      "Epoch 135: Train Loss: 0.6670958280563355, Validation Loss: 0.7013635635375977\n",
      "Epoch 136: Train Loss: 0.6699669003486634, Validation Loss: 0.7013475298881531\n",
      "Epoch 137: Train Loss: 0.6526644349098205, Validation Loss: 0.7015404105186462\n",
      "Epoch 138: Train Loss: 0.696504008769989, Validation Loss: 0.7021197080612183\n",
      "Epoch 139: Train Loss: 0.6454553008079529, Validation Loss: 0.7022497653961182\n",
      "Epoch 140: Train Loss: 0.6355984926223754, Validation Loss: 0.7008811831474304\n",
      "Epoch 141: Train Loss: 0.6888314008712768, Validation Loss: 0.7007327079772949\n",
      "Epoch 142: Train Loss: 0.6442950129508972, Validation Loss: 0.7008353471755981\n",
      "Epoch 143: Train Loss: 0.7163117527961731, Validation Loss: 0.7002050876617432\n",
      "Epoch 144: Train Loss: 0.6645914554595947, Validation Loss: 0.7001685500144958\n",
      "Epoch 145: Train Loss: 0.6965065121650695, Validation Loss: 0.7000125646591187\n",
      "Epoch 146: Train Loss: 0.657994270324707, Validation Loss: 0.700171172618866\n",
      "Epoch 147: Train Loss: 0.68564213514328, Validation Loss: 0.7009595632553101\n",
      "Epoch 148: Train Loss: 0.6520414113998413, Validation Loss: 0.7009186744689941\n",
      "Epoch 149: Train Loss: 0.6937871098518371, Validation Loss: 0.7014074325561523\n",
      "Epoch 150: Train Loss: 0.6405604481697083, Validation Loss: 0.700568675994873\n",
      "Epoch 151: Train Loss: 0.6976742506027221, Validation Loss: 0.7001910209655762\n",
      "Epoch 152: Train Loss: 0.6973058581352234, Validation Loss: 0.7000750303268433\n",
      "Epoch 153: Train Loss: 0.6894586443901062, Validation Loss: 0.700202465057373\n",
      "Epoch 154: Train Loss: 0.7227094888687133, Validation Loss: 0.7005585432052612\n",
      "Epoch 155: Train Loss: 0.7339590668678284, Validation Loss: 0.6993801593780518\n",
      "Epoch 156: Train Loss: 0.6782197713851928, Validation Loss: 0.6997953057289124\n",
      "Epoch 157: Train Loss: 0.6890803575515747, Validation Loss: 0.7003549337387085\n",
      "Epoch 158: Train Loss: 0.7123212456703186, Validation Loss: 0.7002030611038208\n",
      "Epoch 159: Train Loss: 0.6957507133483887, Validation Loss: 0.7007863521575928\n",
      "Epoch 160: Train Loss: 0.6568896770477295, Validation Loss: 0.7005345225334167\n",
      "Epoch 161: Train Loss: 0.6491564035415649, Validation Loss: 0.700936496257782\n",
      "Epoch 162: Train Loss: 0.6926560044288635, Validation Loss: 0.7004468441009521\n",
      "Epoch 163: Train Loss: 0.6829748153686523, Validation Loss: 0.6997573375701904\n",
      "Epoch 164: Train Loss: 0.734764552116394, Validation Loss: 0.6993553042411804\n",
      "Epoch 165: Train Loss: 0.6448813796043396, Validation Loss: 0.6992254257202148\n",
      "Epoch 166: Train Loss: 0.6628686189651489, Validation Loss: 0.6985563635826111\n",
      "Epoch 167: Train Loss: 0.6943689584732056, Validation Loss: 0.6985508799552917\n",
      "Epoch 168: Train Loss: 0.7069882035255433, Validation Loss: 0.6991848945617676\n",
      "Epoch 169: Train Loss: 0.6848194122314453, Validation Loss: 0.6991505026817322\n",
      "Epoch 170: Train Loss: 0.6648024082183838, Validation Loss: 0.6986817121505737\n",
      "Epoch 171: Train Loss: 0.6565388083457947, Validation Loss: 0.6985335350036621\n",
      "Epoch 172: Train Loss: 0.6700043916702271, Validation Loss: 0.699406623840332\n",
      "Epoch 173: Train Loss: 0.6557756423950195, Validation Loss: 0.6993440389633179\n",
      "Epoch 174: Train Loss: 0.5981112360954285, Validation Loss: 0.7003555297851562\n",
      "Epoch 175: Train Loss: 0.640255355834961, Validation Loss: 0.700082004070282\n",
      "Epoch 176: Train Loss: 0.6419178605079651, Validation Loss: 0.7001461982727051\n",
      "Epoch 177: Train Loss: 0.6749661087989807, Validation Loss: 0.7002923488616943\n",
      "Epoch 178: Train Loss: 0.6647696614265441, Validation Loss: 0.7006116509437561\n",
      "Epoch 179: Train Loss: 0.6286587178707123, Validation Loss: 0.7006537318229675\n",
      "Epoch 180: Train Loss: 0.6436244249343872, Validation Loss: 0.701050877571106\n",
      "Epoch 181: Train Loss: 0.6971985220909118, Validation Loss: 0.7008609771728516\n",
      "Epoch 182: Train Loss: 0.680391275882721, Validation Loss: 0.7005812525749207\n",
      "Epoch 183: Train Loss: 0.7195546269416809, Validation Loss: 0.7000937461853027\n",
      "Epoch 184: Train Loss: 0.6561531901359559, Validation Loss: 0.7010725736618042\n",
      "Epoch 185: Train Loss: 0.6394402742385864, Validation Loss: 0.7013761401176453\n",
      "Epoch 186: Train Loss: 0.6355032682418823, Validation Loss: 0.700747013092041\n",
      "Epoch 187: Train Loss: 0.6605513215065002, Validation Loss: 0.7008959054946899\n",
      "Epoch 188: Train Loss: 0.6514512777328492, Validation Loss: 0.701212465763092\n",
      "Epoch 189: Train Loss: 0.6411081790924072, Validation Loss: 0.7010340690612793\n",
      "Epoch 190: Train Loss: 0.6592596411705017, Validation Loss: 0.7007963061332703\n",
      "Epoch 191: Train Loss: 0.6603544473648071, Validation Loss: 0.7012797594070435\n",
      "Epoch 192: Train Loss: 0.6966135144233704, Validation Loss: 0.7008026838302612\n",
      "Epoch 193: Train Loss: 0.6433820366859436, Validation Loss: 0.7006728649139404\n",
      "Epoch 194: Train Loss: 0.6925776720046997, Validation Loss: 0.7003408074378967\n",
      "Epoch 195: Train Loss: 0.6318983435630798, Validation Loss: 0.7007305026054382\n",
      "Epoch 196: Train Loss: 0.6980754017829895, Validation Loss: 0.70075523853302\n",
      "Epoch 197: Train Loss: 0.7258252143859864, Validation Loss: 0.7003423571586609\n",
      "Epoch 198: Train Loss: 0.6990925788879394, Validation Loss: 0.7010111212730408\n",
      "Epoch 199: Train Loss: 0.6334841132164002, Validation Loss: 0.7001494765281677\n",
      "Epoch 200: Train Loss: 0.6617550373077392, Validation Loss: 0.7001044750213623\n",
      "Epoch 201: Train Loss: 0.6640121817588807, Validation Loss: 0.7005535364151001\n",
      "Epoch 202: Train Loss: 0.6534623384475708, Validation Loss: 0.7008520364761353\n",
      "Epoch 203: Train Loss: 0.6304646730422974, Validation Loss: 0.701509416103363\n",
      "Epoch 204: Train Loss: 0.6823921442031861, Validation Loss: 0.7023639678955078\n",
      "Epoch 205: Train Loss: 0.6616254210472107, Validation Loss: 0.7017127871513367\n",
      "Epoch 206: Train Loss: 0.6511432290077209, Validation Loss: 0.7007949948310852\n",
      "Epoch 207: Train Loss: 0.6618710637092591, Validation Loss: 0.7012878656387329\n",
      "Epoch 208: Train Loss: 0.6776436686515808, Validation Loss: 0.7009493708610535\n",
      "Epoch 209: Train Loss: 0.7029996752738953, Validation Loss: 0.7014760375022888\n",
      "Epoch 210: Train Loss: 0.6318768739700318, Validation Loss: 0.7012712955474854\n",
      "Epoch 211: Train Loss: 0.6545098066329956, Validation Loss: 0.7019925713539124\n",
      "Epoch 212: Train Loss: 0.6854851484298706, Validation Loss: 0.7020934820175171\n",
      "Epoch 213: Train Loss: 0.6693915724754333, Validation Loss: 0.7012947797775269\n",
      "Epoch 214: Train Loss: 0.650203263759613, Validation Loss: 0.7008363604545593\n",
      "Epoch 215: Train Loss: 0.641588544845581, Validation Loss: 0.7011078000068665\n",
      "Epoch 216: Train Loss: 0.6138419926166534, Validation Loss: 0.7009550929069519\n",
      "Epoch 217: Train Loss: 0.65471510887146, Validation Loss: 0.7007302045822144\n",
      "Epoch 218: Train Loss: 0.6211410641670227, Validation Loss: 0.7012609243392944\n",
      "Epoch 219: Train Loss: 0.6736879229545594, Validation Loss: 0.7009817361831665\n",
      "Epoch 220: Train Loss: 0.6637049794197083, Validation Loss: 0.7010930776596069\n",
      "Epoch 221: Train Loss: 0.6419909238815308, Validation Loss: 0.70130455493927\n",
      "Epoch 222: Train Loss: 0.6721973896026612, Validation Loss: 0.7022377252578735\n",
      "Epoch 223: Train Loss: 0.6524573802947998, Validation Loss: 0.7017830014228821\n",
      "Epoch 224: Train Loss: 0.6195623874664307, Validation Loss: 0.701363205909729\n",
      "Epoch 225: Train Loss: 0.6661442875862121, Validation Loss: 0.7015833258628845\n",
      "Epoch 226: Train Loss: 0.6903907537460328, Validation Loss: 0.7015207409858704\n",
      "Epoch 227: Train Loss: 0.6625107884407043, Validation Loss: 0.7021965384483337\n",
      "Epoch 228: Train Loss: 0.6814415931701661, Validation Loss: 0.7014249563217163\n",
      "Epoch 229: Train Loss: 0.6849677443504334, Validation Loss: 0.7020211219787598\n",
      "Epoch 230: Train Loss: 0.6462340712547302, Validation Loss: 0.7021528482437134\n",
      "Epoch 231: Train Loss: 0.6255935311317444, Validation Loss: 0.7015972137451172\n",
      "Epoch 232: Train Loss: 0.6240859627723694, Validation Loss: 0.7014044523239136\n",
      "Epoch 233: Train Loss: 0.6950051546096802, Validation Loss: 0.7014123201370239\n",
      "Epoch 234: Train Loss: 0.6603922605514526, Validation Loss: 0.701475203037262\n",
      "Epoch 235: Train Loss: 0.6709107518196106, Validation Loss: 0.7009696364402771\n",
      "Epoch 236: Train Loss: 0.6634403944015503, Validation Loss: 0.7008002996444702\n",
      "Epoch 237: Train Loss: 0.6694836974143982, Validation Loss: 0.7006826400756836\n",
      "Epoch 238: Train Loss: 0.6501237034797669, Validation Loss: 0.7008718848228455\n",
      "Epoch 239: Train Loss: 0.6722504615783691, Validation Loss: 0.701058566570282\n",
      "Epoch 240: Train Loss: 0.6224924802780152, Validation Loss: 0.7012900114059448\n",
      "Epoch 241: Train Loss: 0.6529313087463379, Validation Loss: 0.7016637325286865\n",
      "Epoch 242: Train Loss: 0.6333603978157043, Validation Loss: 0.700703501701355\n",
      "Epoch 243: Train Loss: 0.6164602279663086, Validation Loss: 0.7013111710548401\n",
      "Epoch 244: Train Loss: 0.6600131630897522, Validation Loss: 0.701574981212616\n",
      "Epoch 245: Train Loss: 0.6804078459739685, Validation Loss: 0.7005153298377991\n",
      "Epoch 246: Train Loss: 0.6378525078296662, Validation Loss: 0.7013234496116638\n",
      "Epoch 247: Train Loss: 0.6095596432685852, Validation Loss: 0.7009356617927551\n",
      "Epoch 248: Train Loss: 0.6665768623352051, Validation Loss: 0.7001157999038696\n",
      "Epoch 249: Train Loss: 0.6248587965965271, Validation Loss: 0.7000238299369812\n",
      "Epoch 250: Train Loss: 0.6718661665916443, Validation Loss: 0.7001631855964661\n",
      "Epoch 251: Train Loss: 0.6399797558784485, Validation Loss: 0.7002371549606323\n",
      "Epoch 252: Train Loss: 0.6669320106506348, Validation Loss: 0.7001376152038574\n",
      "Epoch 253: Train Loss: 0.625527310371399, Validation Loss: 0.701366126537323\n",
      "Epoch 254: Train Loss: 0.6498968601226807, Validation Loss: 0.7010148763656616\n",
      "Epoch 255: Train Loss: 0.6324464559555054, Validation Loss: 0.7001779079437256\n",
      "Epoch 256: Train Loss: 0.6276273965835572, Validation Loss: 0.6998399496078491\n",
      "Epoch 257: Train Loss: 0.7127691626548767, Validation Loss: 0.6998860836029053\n",
      "Epoch 258: Train Loss: 0.6344952464103699, Validation Loss: 0.7008914947509766\n",
      "Epoch 259: Train Loss: 0.6429611444473267, Validation Loss: 0.7009070515632629\n",
      "Epoch 260: Train Loss: 0.6233056068420411, Validation Loss: 0.699924647808075\n",
      "Epoch 261: Train Loss: 0.6335414171218872, Validation Loss: 0.700066089630127\n",
      "Epoch 262: Train Loss: 0.6433501362800598, Validation Loss: 0.6999970078468323\n",
      "Epoch 263: Train Loss: 0.6424412369728089, Validation Loss: 0.7004761099815369\n",
      "Epoch 264: Train Loss: 0.6531522512435913, Validation Loss: 0.6997544169425964\n",
      "Epoch 265: Train Loss: 0.637634003162384, Validation Loss: 0.7006127834320068\n",
      "Epoch 266: Train Loss: 0.6790047287940979, Validation Loss: 0.7007809281349182\n",
      "Epoch 267: Train Loss: 0.6379833102226258, Validation Loss: 0.7009166479110718\n",
      "Epoch 268: Train Loss: 0.6043702006340027, Validation Loss: 0.7010642886161804\n",
      "Epoch 269: Train Loss: 0.6240474343299866, Validation Loss: 0.7000033259391785\n",
      "Epoch 270: Train Loss: 0.6965478897094727, Validation Loss: 0.6998361349105835\n",
      "Epoch 271: Train Loss: 0.6648884296417237, Validation Loss: 0.7005679607391357\n",
      "Epoch 272: Train Loss: 0.6545058369636536, Validation Loss: 0.6997641324996948\n",
      "Epoch 273: Train Loss: 0.6496012687683106, Validation Loss: 0.7000044584274292\n",
      "Epoch 274: Train Loss: 0.6657530665397644, Validation Loss: 0.6985106468200684\n",
      "Epoch 275: Train Loss: 0.6125276923179627, Validation Loss: 0.698701798915863\n",
      "Epoch 276: Train Loss: 0.6099475264549256, Validation Loss: 0.6991086006164551\n",
      "Epoch 277: Train Loss: 0.6249802470207214, Validation Loss: 0.6986495852470398\n",
      "Epoch 278: Train Loss: 0.633344566822052, Validation Loss: 0.6985992193222046\n",
      "Epoch 279: Train Loss: 0.6277434349060058, Validation Loss: 0.6986586451530457\n",
      "Epoch 280: Train Loss: 0.6073213815689087, Validation Loss: 0.6990666389465332\n",
      "Epoch 281: Train Loss: 0.6175827503204345, Validation Loss: 0.6989123821258545\n",
      "Epoch 282: Train Loss: 0.6794632315635681, Validation Loss: 0.6985955238342285\n",
      "Epoch 283: Train Loss: 0.6553771138191223, Validation Loss: 0.6970212459564209\n",
      "Epoch 284: Train Loss: 0.6196163177490235, Validation Loss: 0.6957616806030273\n",
      "Epoch 285: Train Loss: 0.6258301496505737, Validation Loss: 0.6957352757453918\n",
      "Epoch 286: Train Loss: 0.6768006324768067, Validation Loss: 0.6962736248970032\n",
      "Epoch 287: Train Loss: 0.6682044744491578, Validation Loss: 0.6962476372718811\n",
      "Epoch 288: Train Loss: 0.6332027912139893, Validation Loss: 0.6956185698509216\n",
      "Epoch 289: Train Loss: 0.6222236752510071, Validation Loss: 0.696263313293457\n",
      "Epoch 290: Train Loss: 0.5809537947177887, Validation Loss: 0.696912944316864\n",
      "Epoch 291: Train Loss: 0.6795386552810669, Validation Loss: 0.6944575905799866\n",
      "Epoch 292: Train Loss: 0.6291041672229767, Validation Loss: 0.6944009065628052\n",
      "Epoch 293: Train Loss: 0.6294654488563538, Validation Loss: 0.6946591138839722\n",
      "Epoch 294: Train Loss: 0.6205470085144043, Validation Loss: 0.6929019689559937\n",
      "Epoch 295: Train Loss: 0.6391676306724549, Validation Loss: 0.6932784914970398\n",
      "Epoch 296: Train Loss: 0.6641588330268859, Validation Loss: 0.6938241720199585\n",
      "Epoch 297: Train Loss: 0.6121217131614685, Validation Loss: 0.6931074857711792\n",
      "Epoch 298: Train Loss: 0.667664110660553, Validation Loss: 0.6932030916213989\n",
      "Epoch 299: Train Loss: 0.6273221731185913, Validation Loss: 0.6941269040107727\n",
      "Epoch 300: Train Loss: 0.6353835344314576, Validation Loss: 0.6924671530723572\n",
      "Epoch 301: Train Loss: 0.6369922161102295, Validation Loss: 0.6918121576309204\n",
      "Epoch 302: Train Loss: 0.6465250730514527, Validation Loss: 0.6917251348495483\n",
      "Epoch 303: Train Loss: 0.5845248818397522, Validation Loss: 0.6924138069152832\n",
      "Epoch 304: Train Loss: 0.6036551833152771, Validation Loss: 0.6925671696662903\n",
      "Epoch 305: Train Loss: 0.6653027892112732, Validation Loss: 0.6919566988945007\n",
      "Epoch 306: Train Loss: 0.6366158843040466, Validation Loss: 0.6914193034172058\n",
      "Epoch 307: Train Loss: 0.6455667614936829, Validation Loss: 0.6907011270523071\n",
      "Epoch 308: Train Loss: 0.6457201719284058, Validation Loss: 0.6916229724884033\n",
      "Epoch 309: Train Loss: 0.6238794684410095, Validation Loss: 0.691752016544342\n",
      "Epoch 310: Train Loss: 0.6679699301719666, Validation Loss: 0.6910311579704285\n",
      "Epoch 311: Train Loss: 0.6083934426307678, Validation Loss: 0.6890068650245667\n",
      "Epoch 312: Train Loss: 0.7021404504776001, Validation Loss: 0.6871698498725891\n",
      "Epoch 313: Train Loss: 0.6045528829097748, Validation Loss: 0.6864728927612305\n",
      "Epoch 314: Train Loss: 0.6544796705245972, Validation Loss: 0.6874115467071533\n",
      "Epoch 315: Train Loss: 0.720119321346283, Validation Loss: 0.6878463625907898\n",
      "Epoch 316: Train Loss: 0.640090262889862, Validation Loss: 0.6878847479820251\n",
      "Epoch 317: Train Loss: 0.6213814854621887, Validation Loss: 0.6877520084381104\n",
      "Epoch 318: Train Loss: 0.6311885833740234, Validation Loss: 0.6873833537101746\n",
      "Epoch 319: Train Loss: 0.6090000748634339, Validation Loss: 0.6884217262268066\n",
      "Epoch 320: Train Loss: 0.6214655637741089, Validation Loss: 0.6900732517242432\n",
      "Epoch 321: Train Loss: 0.605351722240448, Validation Loss: 0.68935626745224\n",
      "Epoch 322: Train Loss: 0.641097342967987, Validation Loss: 0.6880824565887451\n",
      "Epoch 323: Train Loss: 0.6166736960411072, Validation Loss: 0.6864416599273682\n",
      "Epoch 324: Train Loss: 0.6296122908592224, Validation Loss: 0.6847199201583862\n",
      "Epoch 325: Train Loss: 0.6318259239196777, Validation Loss: 0.68699711561203\n",
      "Epoch 326: Train Loss: 0.6284440040588379, Validation Loss: 0.6866877675056458\n",
      "Epoch 327: Train Loss: 0.6074134588241578, Validation Loss: 0.6868136525154114\n",
      "Epoch 328: Train Loss: 0.6712744832038879, Validation Loss: 0.6862392425537109\n",
      "Epoch 329: Train Loss: 0.6045028448104859, Validation Loss: 0.6852117776870728\n",
      "Epoch 330: Train Loss: 0.6537420153617859, Validation Loss: 0.6845247149467468\n",
      "Epoch 331: Train Loss: 0.6035058975219727, Validation Loss: 0.6858525276184082\n",
      "Epoch 332: Train Loss: 0.5744905710220337, Validation Loss: 0.687059760093689\n",
      "Epoch 333: Train Loss: 0.6006652355194092, Validation Loss: 0.6882055997848511\n",
      "Epoch 334: Train Loss: 0.5865494132041931, Validation Loss: 0.6879140734672546\n",
      "Epoch 335: Train Loss: 0.5935815453529358, Validation Loss: 0.6880596280097961\n",
      "Epoch 336: Train Loss: 0.6439921617507934, Validation Loss: 0.6875119209289551\n",
      "Epoch 337: Train Loss: 0.6722902297973633, Validation Loss: 0.6867311000823975\n",
      "Epoch 338: Train Loss: 0.5593365550041198, Validation Loss: 0.6867244243621826\n",
      "Epoch 339: Train Loss: 0.5929257035255432, Validation Loss: 0.6884781718254089\n",
      "Epoch 340: Train Loss: 0.6330926418304443, Validation Loss: 0.688450813293457\n",
      "Epoch 341: Train Loss: 0.647745943069458, Validation Loss: 0.6853994131088257\n",
      "Epoch 342: Train Loss: 0.5947625756263732, Validation Loss: 0.6855621337890625\n",
      "Epoch 343: Train Loss: 0.597747540473938, Validation Loss: 0.6865760087966919\n",
      "Epoch 344: Train Loss: 0.5631522893905639, Validation Loss: 0.6894964575767517\n",
      "Epoch 345: Train Loss: 0.6123490452766418, Validation Loss: 0.6891179084777832\n",
      "Epoch 346: Train Loss: 0.6080767750740051, Validation Loss: 0.6876071095466614\n",
      "Epoch 347: Train Loss: 0.6284955620765686, Validation Loss: 0.687415599822998\n",
      "Epoch 348: Train Loss: 0.6493368983268738, Validation Loss: 0.6864306330680847\n",
      "Epoch 349: Train Loss: 0.6438674449920654, Validation Loss: 0.6857949495315552\n",
      "Epoch 350: Train Loss: 0.5795064806938172, Validation Loss: 0.6852003335952759\n",
      "Epoch 351: Train Loss: 0.6409622550010681, Validation Loss: 0.6849624514579773\n",
      "Epoch 352: Train Loss: 0.5861459016799927, Validation Loss: 0.6847106218338013\n",
      "Epoch 353: Train Loss: 0.5824859857559204, Validation Loss: 0.683027446269989\n",
      "Epoch 354: Train Loss: 0.5698909759521484, Validation Loss: 0.6834925413131714\n",
      "Epoch 355: Train Loss: 0.6151464343070984, Validation Loss: 0.6850878000259399\n",
      "Epoch 356: Train Loss: 0.6107006192207336, Validation Loss: 0.6848856806755066\n",
      "Epoch 357: Train Loss: 0.6170848131179809, Validation Loss: 0.6837325692176819\n",
      "Epoch 358: Train Loss: 0.5926744699478149, Validation Loss: 0.6825342178344727\n",
      "Epoch 359: Train Loss: 0.6095535278320312, Validation Loss: 0.6830791234970093\n",
      "Epoch 360: Train Loss: 0.6367772102355957, Validation Loss: 0.684473991394043\n",
      "Epoch 361: Train Loss: 0.5979784011840821, Validation Loss: 0.685408890247345\n",
      "Epoch 362: Train Loss: 0.6804030776023865, Validation Loss: 0.6854552030563354\n",
      "Epoch 363: Train Loss: 0.5962841272354126, Validation Loss: 0.6850982904434204\n",
      "Epoch 364: Train Loss: 0.5947429776191712, Validation Loss: 0.6852505207061768\n",
      "Epoch 365: Train Loss: 0.5867535710334778, Validation Loss: 0.6847252249717712\n",
      "Epoch 366: Train Loss: 0.6131141543388366, Validation Loss: 0.6827951669692993\n",
      "Epoch 367: Train Loss: 0.6370408058166503, Validation Loss: 0.6832274794578552\n",
      "Epoch 368: Train Loss: 0.5725527048110962, Validation Loss: 0.6865350604057312\n",
      "Epoch 369: Train Loss: 0.6159945368766785, Validation Loss: 0.6862726211547852\n",
      "Epoch 370: Train Loss: 0.6398904919624329, Validation Loss: 0.6847213506698608\n",
      "Epoch 371: Train Loss: 0.60728679895401, Validation Loss: 0.6846325993537903\n",
      "Epoch 372: Train Loss: 0.5706934452056884, Validation Loss: 0.6827911734580994\n",
      "Epoch 373: Train Loss: 0.6445021271705628, Validation Loss: 0.6847603917121887\n",
      "Epoch 374: Train Loss: 0.6327943801879883, Validation Loss: 0.6814456582069397\n",
      "Epoch 375: Train Loss: 0.6719962358474731, Validation Loss: 0.6822884678840637\n",
      "Epoch 376: Train Loss: 0.6062639355659485, Validation Loss: 0.685554027557373\n",
      "Epoch 377: Train Loss: 0.5708275556564331, Validation Loss: 0.6841114163398743\n",
      "Epoch 378: Train Loss: 0.605147510766983, Validation Loss: 0.6831032633781433\n",
      "Epoch 379: Train Loss: 0.59473215341568, Validation Loss: 0.681190013885498\n",
      "Epoch 380: Train Loss: 0.5839267492294311, Validation Loss: 0.6831564903259277\n",
      "Epoch 381: Train Loss: 0.5936464905738831, Validation Loss: 0.6814721822738647\n",
      "Epoch 382: Train Loss: 0.5931104779243469, Validation Loss: 0.6821855902671814\n",
      "Epoch 383: Train Loss: 0.603648328781128, Validation Loss: 0.6822328567504883\n",
      "Epoch 384: Train Loss: 0.6268754601478577, Validation Loss: 0.6801226139068604\n",
      "Epoch 385: Train Loss: 0.5989707827568054, Validation Loss: 0.6800346374511719\n",
      "Epoch 386: Train Loss: 0.6093443393707275, Validation Loss: 0.6791572570800781\n",
      "Epoch 387: Train Loss: 0.6151185274124146, Validation Loss: 0.6793974041938782\n",
      "Epoch 388: Train Loss: 0.6007487297058105, Validation Loss: 0.6801840662956238\n",
      "Epoch 389: Train Loss: 0.6427380681037903, Validation Loss: 0.6805582642555237\n",
      "Epoch 390: Train Loss: 0.5548537909984589, Validation Loss: 0.6804450750350952\n",
      "Epoch 391: Train Loss: 0.6090191841125489, Validation Loss: 0.6865561008453369\n",
      "Epoch 392: Train Loss: 0.5734164118766785, Validation Loss: 0.6818321943283081\n",
      "Epoch 393: Train Loss: 0.5966328859329224, Validation Loss: 0.6818534731864929\n",
      "Epoch 394: Train Loss: 0.578318190574646, Validation Loss: 0.6773719787597656\n",
      "Epoch 395: Train Loss: 0.6213935017585754, Validation Loss: 0.6766916513442993\n",
      "Epoch 396: Train Loss: 0.5792941331863404, Validation Loss: 0.6763414144515991\n",
      "Epoch 397: Train Loss: 0.5655641078948974, Validation Loss: 0.6754152774810791\n",
      "Epoch 398: Train Loss: 0.5905933260917664, Validation Loss: 0.6757758855819702\n",
      "Epoch 399: Train Loss: 0.5626147747039795, Validation Loss: 0.6766232848167419\n",
      "Epoch 400: Train Loss: 0.6153228282928467, Validation Loss: 0.6742386817932129\n",
      "Epoch 401: Train Loss: 0.5875888586044311, Validation Loss: 0.6737590432167053\n",
      "Epoch 402: Train Loss: 0.5756184935569764, Validation Loss: 0.6727125644683838\n",
      "Epoch 403: Train Loss: 0.5729540467262269, Validation Loss: 0.6715896129608154\n",
      "Epoch 404: Train Loss: 0.5355522274971009, Validation Loss: 0.673022449016571\n",
      "Epoch 405: Train Loss: 0.6047841072082519, Validation Loss: 0.6723126769065857\n",
      "Epoch 406: Train Loss: 0.5800981163978577, Validation Loss: 0.6782002449035645\n",
      "Epoch 407: Train Loss: 0.5647433638572693, Validation Loss: 0.6759479641914368\n",
      "Epoch 408: Train Loss: 0.5935882210731507, Validation Loss: 0.6772425770759583\n",
      "Epoch 409: Train Loss: 0.5517072081565857, Validation Loss: 0.6758520603179932\n",
      "Epoch 410: Train Loss: 0.6028855502605438, Validation Loss: 0.671096682548523\n",
      "Epoch 411: Train Loss: 0.5855612993240357, Validation Loss: 0.6721881031990051\n",
      "Epoch 412: Train Loss: 0.5905092716217041, Validation Loss: 0.670699954032898\n",
      "Epoch 413: Train Loss: 0.5381292581558228, Validation Loss: 0.6683982610702515\n",
      "Epoch 414: Train Loss: 0.5321703672409057, Validation Loss: 0.6687794923782349\n",
      "Epoch 415: Train Loss: 0.6062303066253663, Validation Loss: 0.6692827939987183\n",
      "Epoch 416: Train Loss: 0.62065190076828, Validation Loss: 0.6677072644233704\n",
      "Epoch 417: Train Loss: 0.5776867032051086, Validation Loss: 0.6666292548179626\n",
      "Epoch 418: Train Loss: 0.5816158294677735, Validation Loss: 0.6663486361503601\n",
      "Epoch 419: Train Loss: 0.5991126418113708, Validation Loss: 0.667471706867218\n",
      "Epoch 420: Train Loss: 0.5348861515522003, Validation Loss: 0.6668730974197388\n",
      "Epoch 421: Train Loss: 0.551441079378128, Validation Loss: 0.665462076663971\n",
      "Epoch 422: Train Loss: 0.5691905081272125, Validation Loss: 0.6644039154052734\n",
      "Epoch 423: Train Loss: 0.5546569168567658, Validation Loss: 0.6670551896095276\n",
      "Epoch 424: Train Loss: 0.6306121468544006, Validation Loss: 0.6655107140541077\n",
      "Epoch 425: Train Loss: 0.5620957434177398, Validation Loss: 0.6642320156097412\n",
      "Epoch 426: Train Loss: 0.576749038696289, Validation Loss: 0.6628621220588684\n",
      "Epoch 427: Train Loss: 0.5607666611671448, Validation Loss: 0.6616045832633972\n",
      "Epoch 428: Train Loss: 0.5956510186195374, Validation Loss: 0.6686424016952515\n",
      "Epoch 429: Train Loss: 0.5493396759033203, Validation Loss: 0.6663024425506592\n",
      "Epoch 430: Train Loss: 0.5464929521083832, Validation Loss: 0.663812518119812\n",
      "Epoch 431: Train Loss: 0.5611256241798401, Validation Loss: 0.6622505187988281\n",
      "Epoch 432: Train Loss: 0.5616758286952972, Validation Loss: 0.6644517183303833\n",
      "Epoch 433: Train Loss: 0.5470653414726258, Validation Loss: 0.6620516777038574\n",
      "Epoch 434: Train Loss: 0.5658650517463684, Validation Loss: 0.6625508666038513\n",
      "Epoch 435: Train Loss: 0.5684900045394897, Validation Loss: 0.6644654870033264\n",
      "Epoch 436: Train Loss: 0.5769493222236634, Validation Loss: 0.6645568013191223\n",
      "Epoch 437: Train Loss: 0.6369184255599976, Validation Loss: 0.6620418429374695\n",
      "Epoch 438: Train Loss: 0.5367556810379028, Validation Loss: 0.6593049764633179\n",
      "Epoch 439: Train Loss: 0.5681503295898438, Validation Loss: 0.6598760485649109\n",
      "Epoch 440: Train Loss: 0.61051105260849, Validation Loss: 0.6589792966842651\n",
      "Epoch 441: Train Loss: 0.5694052219390869, Validation Loss: 0.656766414642334\n",
      "Epoch 442: Train Loss: 0.5601577043533326, Validation Loss: 0.6597627997398376\n",
      "Epoch 443: Train Loss: 0.5989439368247986, Validation Loss: 0.6609071493148804\n",
      "Epoch 444: Train Loss: 0.5185077548027038, Validation Loss: 0.6622809171676636\n",
      "Epoch 445: Train Loss: 0.5611528217792511, Validation Loss: 0.6626604199409485\n",
      "Epoch 446: Train Loss: 0.5083687603473663, Validation Loss: 0.6611858606338501\n",
      "Epoch 447: Train Loss: 0.5510892033576965, Validation Loss: 0.6592710018157959\n",
      "Epoch 448: Train Loss: 0.5157436072826386, Validation Loss: 0.6599520444869995\n",
      "Epoch 449: Train Loss: 0.5810788035392761, Validation Loss: 0.6619934439659119\n",
      "Epoch 450: Train Loss: 0.5740473866462708, Validation Loss: 0.660094141960144\n",
      "Epoch 451: Train Loss: 0.5475935399532318, Validation Loss: 0.6633396148681641\n",
      "Epoch 452: Train Loss: 0.5139181792736054, Validation Loss: 0.6610673666000366\n",
      "Epoch 453: Train Loss: 0.5074381232261658, Validation Loss: 0.6586052775382996\n",
      "Epoch 454: Train Loss: 0.497346556186676, Validation Loss: 0.6540819406509399\n",
      "Epoch 455: Train Loss: 0.570755934715271, Validation Loss: 0.6522949934005737\n",
      "Epoch 456: Train Loss: 0.5782665014266968, Validation Loss: 0.6508361101150513\n",
      "Epoch 457: Train Loss: 0.5334824562072754, Validation Loss: 0.6513907313346863\n",
      "Epoch 458: Train Loss: 0.5390035808086395, Validation Loss: 0.6522423028945923\n",
      "Epoch 459: Train Loss: 0.5309258818626403, Validation Loss: 0.6513698101043701\n",
      "Epoch 460: Train Loss: 0.5836942434310913, Validation Loss: 0.649462103843689\n",
      "Epoch 461: Train Loss: 0.5631705522537231, Validation Loss: 0.6486693024635315\n",
      "Epoch 462: Train Loss: 0.5912201404571533, Validation Loss: 0.6492376327514648\n",
      "Epoch 463: Train Loss: 0.5160546004772186, Validation Loss: 0.6464542150497437\n",
      "Epoch 464: Train Loss: 0.5331855416297913, Validation Loss: 0.6503205299377441\n",
      "Epoch 465: Train Loss: 0.527648103237152, Validation Loss: 0.647533118724823\n",
      "Epoch 466: Train Loss: 0.5992762625217438, Validation Loss: 0.6495291590690613\n",
      "Epoch 467: Train Loss: 0.5133596539497376, Validation Loss: 0.6496602892875671\n",
      "Epoch 468: Train Loss: 0.5035695016384125, Validation Loss: 0.6489363312721252\n",
      "Epoch 469: Train Loss: 0.5324067771434784, Validation Loss: 0.6458582282066345\n",
      "Epoch 470: Train Loss: 0.4903204619884491, Validation Loss: 0.6438977718353271\n",
      "Epoch 471: Train Loss: 0.5067969620227813, Validation Loss: 0.6455121040344238\n",
      "Epoch 472: Train Loss: 0.5340476334095001, Validation Loss: 0.6456279754638672\n",
      "Epoch 473: Train Loss: 0.47635736465454104, Validation Loss: 0.6407413482666016\n",
      "Epoch 474: Train Loss: 0.5258530080318451, Validation Loss: 0.6432735323905945\n",
      "Epoch 475: Train Loss: 0.5144097983837128, Validation Loss: 0.6438195705413818\n",
      "Epoch 476: Train Loss: 0.5978330194950103, Validation Loss: 0.6417155861854553\n",
      "Epoch 477: Train Loss: 0.5491094827651978, Validation Loss: 0.6448578238487244\n",
      "Epoch 478: Train Loss: 0.5045185148715973, Validation Loss: 0.6459954977035522\n",
      "Epoch 479: Train Loss: 0.5548762798309326, Validation Loss: 0.6464831233024597\n",
      "Epoch 480: Train Loss: 0.5297657728195191, Validation Loss: 0.6438418626785278\n",
      "Epoch 481: Train Loss: 0.5078977525234223, Validation Loss: 0.6425434947013855\n",
      "Epoch 482: Train Loss: 0.5457836568355561, Validation Loss: 0.6389737725257874\n",
      "Epoch 483: Train Loss: 0.5349290549755097, Validation Loss: 0.6370267868041992\n",
      "Epoch 484: Train Loss: 0.5893712759017944, Validation Loss: 0.6378108859062195\n",
      "Epoch 485: Train Loss: 0.49951126575469973, Validation Loss: 0.6367988586425781\n",
      "Epoch 486: Train Loss: 0.46232831478118896, Validation Loss: 0.6342688202857971\n",
      "Epoch 487: Train Loss: 0.5418343007564544, Validation Loss: 0.6344536542892456\n",
      "Epoch 488: Train Loss: 0.5390572190284729, Validation Loss: 0.6343809962272644\n",
      "Epoch 489: Train Loss: 0.49604611992836, Validation Loss: 0.6358367800712585\n",
      "Epoch 490: Train Loss: 0.5064487159252167, Validation Loss: 0.6340613961219788\n",
      "Epoch 491: Train Loss: 0.4900044560432434, Validation Loss: 0.6338728666305542\n",
      "Epoch 492: Train Loss: 0.5355607569217682, Validation Loss: 0.6311550736427307\n",
      "Epoch 493: Train Loss: 0.5253465175628662, Validation Loss: 0.6307703256607056\n",
      "Epoch 494: Train Loss: 0.5208081066608429, Validation Loss: 0.632195770740509\n",
      "Epoch 495: Train Loss: 0.5108565330505371, Validation Loss: 0.6291648149490356\n",
      "Epoch 496: Train Loss: 0.5327171564102173, Validation Loss: 0.6303480267524719\n",
      "Epoch 497: Train Loss: 0.5202683091163636, Validation Loss: 0.6293052434921265\n",
      "Epoch 498: Train Loss: 0.5000209927558898, Validation Loss: 0.6258547306060791\n",
      "Epoch 499: Train Loss: 0.49593034386634827, Validation Loss: 0.6302868723869324\n",
      "Fold 6 Metrics:\n",
      "Accuracy: 0.7272727272727273, Precision: 0.7142857142857143, Recall: 0.8333333333333334, F1-score: 0.7692307692307693, AUC: 0.7166666666666668\n",
      "Confusion Matrix:\n",
      "[[3 2]\n",
      " [1 5]]\n",
      "Completed fold 6\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples from subject 12 to test set\n",
      "Adding 6 truth samples from subject 12 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 500)\n",
      "(132, 65, 500)\n",
      "Epoch 0: Train Loss: 0.6975245952606202, Validation Loss: 0.6941764950752258\n",
      "Epoch 1: Train Loss: 0.7489255905151367, Validation Loss: 0.6947569251060486\n",
      "Epoch 2: Train Loss: 0.7415215373039246, Validation Loss: 0.6957435607910156\n",
      "Epoch 3: Train Loss: 0.6972206711769104, Validation Loss: 0.6966924667358398\n",
      "Epoch 4: Train Loss: 0.6962822318077088, Validation Loss: 0.6976447105407715\n",
      "Epoch 5: Train Loss: 0.7767362594604492, Validation Loss: 0.6978154182434082\n",
      "Epoch 6: Train Loss: 0.6804900765419006, Validation Loss: 0.6980843544006348\n",
      "Epoch 7: Train Loss: 0.8030287742614746, Validation Loss: 0.6986642479896545\n",
      "Epoch 8: Train Loss: 0.7305464029312134, Validation Loss: 0.6991011500358582\n",
      "Epoch 9: Train Loss: 0.772778856754303, Validation Loss: 0.6989313364028931\n",
      "Epoch 10: Train Loss: 0.69504873752594, Validation Loss: 0.6991416811943054\n",
      "Epoch 11: Train Loss: 0.6725714802742004, Validation Loss: 0.7001319527626038\n",
      "Epoch 12: Train Loss: 0.7108792901039124, Validation Loss: 0.700627863407135\n",
      "Epoch 13: Train Loss: 0.7318331122398376, Validation Loss: 0.7009782791137695\n",
      "Epoch 14: Train Loss: 0.6832282781600952, Validation Loss: 0.7011880874633789\n",
      "Epoch 15: Train Loss: 0.7188477039337158, Validation Loss: 0.7013322114944458\n",
      "Epoch 16: Train Loss: 0.6692832946777344, Validation Loss: 0.7011208534240723\n",
      "Epoch 17: Train Loss: 0.6777127265930176, Validation Loss: 0.7012647390365601\n",
      "Epoch 18: Train Loss: 0.6936726331710815, Validation Loss: 0.7009716033935547\n",
      "Epoch 19: Train Loss: 0.6762865066528321, Validation Loss: 0.7007683515548706\n",
      "Epoch 20: Train Loss: 0.7202270269393921, Validation Loss: 0.701394259929657\n",
      "Epoch 21: Train Loss: 0.7378740191459656, Validation Loss: 0.7013033628463745\n",
      "Epoch 22: Train Loss: 0.7077675461769104, Validation Loss: 0.7018441557884216\n",
      "Epoch 23: Train Loss: 0.6700024247169495, Validation Loss: 0.7022675275802612\n",
      "Epoch 24: Train Loss: 0.6783102869987487, Validation Loss: 0.7027077674865723\n",
      "Epoch 25: Train Loss: 0.7072673082351685, Validation Loss: 0.7022040486335754\n",
      "Epoch 26: Train Loss: 0.7115239858627319, Validation Loss: 0.7025302648544312\n",
      "Epoch 27: Train Loss: 0.6944963574409485, Validation Loss: 0.7024669051170349\n",
      "Epoch 28: Train Loss: 0.7478776454925538, Validation Loss: 0.7025731801986694\n",
      "Epoch 29: Train Loss: 0.6954886674880981, Validation Loss: 0.7029643654823303\n",
      "Epoch 30: Train Loss: 0.7029772520065307, Validation Loss: 0.7031641006469727\n",
      "Epoch 31: Train Loss: 0.698174774646759, Validation Loss: 0.7031826376914978\n",
      "Epoch 32: Train Loss: 0.715579891204834, Validation Loss: 0.7029480934143066\n",
      "Epoch 33: Train Loss: 0.643294757604599, Validation Loss: 0.7029848098754883\n",
      "Epoch 34: Train Loss: 0.70604008436203, Validation Loss: 0.7034923434257507\n",
      "Epoch 35: Train Loss: 0.721161675453186, Validation Loss: 0.7037373185157776\n",
      "Epoch 36: Train Loss: 0.7261505603790284, Validation Loss: 0.7038914561271667\n",
      "Epoch 37: Train Loss: 0.7211759924888611, Validation Loss: 0.7033693194389343\n",
      "Epoch 38: Train Loss: 0.6891746997833252, Validation Loss: 0.7040026783943176\n",
      "Epoch 39: Train Loss: 0.693211829662323, Validation Loss: 0.7041139006614685\n",
      "Epoch 40: Train Loss: 0.7258424758911133, Validation Loss: 0.7039870023727417\n",
      "Epoch 41: Train Loss: 0.7002954602241516, Validation Loss: 0.7039358615875244\n",
      "Epoch 42: Train Loss: 0.7004442334175109, Validation Loss: 0.7041041254997253\n",
      "Epoch 43: Train Loss: 0.6707677006721496, Validation Loss: 0.7046594619750977\n",
      "Epoch 44: Train Loss: 0.7365158677101136, Validation Loss: 0.7049784064292908\n",
      "Epoch 45: Train Loss: 0.690382444858551, Validation Loss: 0.705625057220459\n",
      "Epoch 46: Train Loss: 0.6839729905128479, Validation Loss: 0.7049922347068787\n",
      "Epoch 47: Train Loss: 0.6929737091064453, Validation Loss: 0.7058154940605164\n",
      "Epoch 48: Train Loss: 0.6504696130752563, Validation Loss: 0.7056271433830261\n",
      "Epoch 49: Train Loss: 0.6969841361045838, Validation Loss: 0.7053828239440918\n",
      "Epoch 50: Train Loss: 0.7387656331062317, Validation Loss: 0.7062820196151733\n",
      "Epoch 51: Train Loss: 0.7211265802383423, Validation Loss: 0.7059081196784973\n",
      "Epoch 52: Train Loss: 0.7283703446388244, Validation Loss: 0.7068661451339722\n",
      "Epoch 53: Train Loss: 0.6468189597129822, Validation Loss: 0.7066283226013184\n",
      "Epoch 54: Train Loss: 0.6877890944480896, Validation Loss: 0.7067418694496155\n",
      "Epoch 55: Train Loss: 0.7091159462928772, Validation Loss: 0.7067987322807312\n",
      "Epoch 56: Train Loss: 0.7389602661132812, Validation Loss: 0.7072173953056335\n",
      "Epoch 57: Train Loss: 0.6541061401367188, Validation Loss: 0.7068521976470947\n",
      "Epoch 58: Train Loss: 0.6348905205726624, Validation Loss: 0.7072963118553162\n",
      "Epoch 59: Train Loss: 0.6920923948287964, Validation Loss: 0.7064990401268005\n",
      "Epoch 60: Train Loss: 0.6520029664039612, Validation Loss: 0.7068931460380554\n",
      "Epoch 61: Train Loss: 0.7229187488555908, Validation Loss: 0.707129180431366\n",
      "Epoch 62: Train Loss: 0.7353619813919068, Validation Loss: 0.7079811692237854\n",
      "Epoch 63: Train Loss: 0.7018333792686462, Validation Loss: 0.707216203212738\n",
      "Epoch 64: Train Loss: 0.6662884712219238, Validation Loss: 0.7072854042053223\n",
      "Epoch 65: Train Loss: 0.6582661032676697, Validation Loss: 0.7079610824584961\n",
      "Epoch 66: Train Loss: 0.6670435190200805, Validation Loss: 0.7078160047531128\n",
      "Epoch 67: Train Loss: 0.6908666372299195, Validation Loss: 0.707758903503418\n",
      "Epoch 68: Train Loss: 0.7118939518928528, Validation Loss: 0.7071760296821594\n",
      "Epoch 69: Train Loss: 0.747310996055603, Validation Loss: 0.7071677446365356\n",
      "Epoch 70: Train Loss: 0.6377666115760803, Validation Loss: 0.7076315879821777\n",
      "Epoch 71: Train Loss: 0.721434211730957, Validation Loss: 0.7079722881317139\n",
      "Epoch 72: Train Loss: 0.6554422020912171, Validation Loss: 0.7086207270622253\n",
      "Epoch 73: Train Loss: 0.6933045506477356, Validation Loss: 0.7088499069213867\n",
      "Epoch 74: Train Loss: 0.7307447195053101, Validation Loss: 0.7088165879249573\n",
      "Epoch 75: Train Loss: 0.7751899480819702, Validation Loss: 0.7093559503555298\n",
      "Epoch 76: Train Loss: 0.7029500484466553, Validation Loss: 0.7093513607978821\n",
      "Epoch 77: Train Loss: 0.6913592100143433, Validation Loss: 0.7091064453125\n",
      "Epoch 78: Train Loss: 0.7103363990783691, Validation Loss: 0.7092183828353882\n",
      "Epoch 79: Train Loss: 0.65398188829422, Validation Loss: 0.708579957485199\n",
      "Epoch 80: Train Loss: 0.6495090007781983, Validation Loss: 0.70860755443573\n",
      "Epoch 81: Train Loss: 0.6328015923500061, Validation Loss: 0.7089343667030334\n",
      "Epoch 82: Train Loss: 0.7223793983459472, Validation Loss: 0.7088404297828674\n",
      "Epoch 83: Train Loss: 0.7140493512153625, Validation Loss: 0.7088254690170288\n",
      "Epoch 84: Train Loss: 0.6358744025230407, Validation Loss: 0.7094956040382385\n",
      "Epoch 85: Train Loss: 0.6707178711891174, Validation Loss: 0.7085090279579163\n",
      "Epoch 86: Train Loss: 0.6979575157165527, Validation Loss: 0.7081756591796875\n",
      "Epoch 87: Train Loss: 0.6987737655639649, Validation Loss: 0.7093027830123901\n",
      "Epoch 88: Train Loss: 0.6721870183944703, Validation Loss: 0.7098149061203003\n",
      "Epoch 89: Train Loss: 0.710008418560028, Validation Loss: 0.709568202495575\n",
      "Epoch 90: Train Loss: 0.7524311065673828, Validation Loss: 0.7090862989425659\n",
      "Epoch 91: Train Loss: 0.7099588990211487, Validation Loss: 0.7095162272453308\n",
      "Epoch 92: Train Loss: 0.6841097116470337, Validation Loss: 0.7099243402481079\n",
      "Epoch 93: Train Loss: 0.6342997908592224, Validation Loss: 0.7098989486694336\n",
      "Epoch 94: Train Loss: 0.646506154537201, Validation Loss: 0.7103295922279358\n",
      "Epoch 95: Train Loss: 0.7064367651939392, Validation Loss: 0.7099426984786987\n",
      "Epoch 96: Train Loss: 0.7146830201148987, Validation Loss: 0.7086130380630493\n",
      "Epoch 97: Train Loss: 0.6880540132522583, Validation Loss: 0.7088200449943542\n",
      "Epoch 98: Train Loss: 0.6359222888946533, Validation Loss: 0.7085422277450562\n",
      "Epoch 99: Train Loss: 0.7070230364799499, Validation Loss: 0.7097138166427612\n",
      "Epoch 100: Train Loss: 0.651816725730896, Validation Loss: 0.7107863426208496\n",
      "Epoch 101: Train Loss: 0.7017752528190613, Validation Loss: 0.7111707925796509\n",
      "Epoch 102: Train Loss: 0.7094288825988769, Validation Loss: 0.7109411954879761\n",
      "Epoch 103: Train Loss: 0.6826947093009949, Validation Loss: 0.710798978805542\n",
      "Epoch 104: Train Loss: 0.6569705605506897, Validation Loss: 0.7108525633811951\n",
      "Epoch 105: Train Loss: 0.7063684940338135, Validation Loss: 0.7114654183387756\n",
      "Epoch 106: Train Loss: 0.6608274579048157, Validation Loss: 0.7124985456466675\n",
      "Epoch 107: Train Loss: 0.7190274596214294, Validation Loss: 0.7129641175270081\n",
      "Epoch 108: Train Loss: 0.6762153029441833, Validation Loss: 0.713114857673645\n",
      "Epoch 109: Train Loss: 0.6442070603370667, Validation Loss: 0.7124137878417969\n",
      "Epoch 110: Train Loss: 0.6426281690597534, Validation Loss: 0.7134717702865601\n",
      "Epoch 111: Train Loss: 0.7289641737937927, Validation Loss: 0.7138605117797852\n",
      "Epoch 112: Train Loss: 0.7104716181755066, Validation Loss: 0.7142056822776794\n",
      "Epoch 113: Train Loss: 0.6827098965644837, Validation Loss: 0.7142554521560669\n",
      "Epoch 114: Train Loss: 0.6603405714035034, Validation Loss: 0.71434086561203\n",
      "Epoch 115: Train Loss: 0.7480576515197754, Validation Loss: 0.7154736518859863\n",
      "Epoch 116: Train Loss: 0.6695524692535401, Validation Loss: 0.7148380279541016\n",
      "Epoch 117: Train Loss: 0.6992186427116394, Validation Loss: 0.7146646976470947\n",
      "Epoch 118: Train Loss: 0.6790614604949952, Validation Loss: 0.713811993598938\n",
      "Epoch 119: Train Loss: 0.6873868346214295, Validation Loss: 0.7149403691291809\n",
      "Epoch 120: Train Loss: 0.68619624376297, Validation Loss: 0.715347945690155\n",
      "Epoch 121: Train Loss: 0.6976624846458435, Validation Loss: 0.715310275554657\n",
      "Epoch 122: Train Loss: 0.7092858672142028, Validation Loss: 0.7161045074462891\n",
      "Epoch 123: Train Loss: 0.7096878290176392, Validation Loss: 0.7167975902557373\n",
      "Epoch 124: Train Loss: 0.6600778341293335, Validation Loss: 0.7163934707641602\n",
      "Epoch 125: Train Loss: 0.7139759063720703, Validation Loss: 0.7161433100700378\n",
      "Epoch 126: Train Loss: 0.6489819526672364, Validation Loss: 0.7158055305480957\n",
      "Epoch 127: Train Loss: 0.627204304933548, Validation Loss: 0.7156916260719299\n",
      "Epoch 128: Train Loss: 0.6744392871856689, Validation Loss: 0.716324508190155\n",
      "Epoch 129: Train Loss: 0.6079448759555817, Validation Loss: 0.7166801691055298\n",
      "Epoch 130: Train Loss: 0.6453409790992737, Validation Loss: 0.7164557576179504\n",
      "Epoch 131: Train Loss: 0.6916397452354431, Validation Loss: 0.7172918319702148\n",
      "Epoch 132: Train Loss: 0.6464239716529846, Validation Loss: 0.7175406813621521\n",
      "Epoch 133: Train Loss: 0.7076816558837891, Validation Loss: 0.7176110148429871\n",
      "Epoch 134: Train Loss: 0.6920959949493408, Validation Loss: 0.7187653183937073\n",
      "Epoch 135: Train Loss: 0.6532007217407226, Validation Loss: 0.7189533710479736\n",
      "Epoch 136: Train Loss: 0.6481470823287964, Validation Loss: 0.7184785008430481\n",
      "Epoch 137: Train Loss: 0.7167775511741639, Validation Loss: 0.7185697555541992\n",
      "Epoch 138: Train Loss: 0.6234622597694397, Validation Loss: 0.7165535688400269\n",
      "Epoch 139: Train Loss: 0.694433057308197, Validation Loss: 0.7174453735351562\n",
      "Epoch 140: Train Loss: 0.6763565421104432, Validation Loss: 0.7169226408004761\n",
      "Epoch 141: Train Loss: 0.6559790968894958, Validation Loss: 0.7179116010665894\n",
      "Epoch 142: Train Loss: 0.6662818193435669, Validation Loss: 0.718308687210083\n",
      "Epoch 143: Train Loss: 0.6286873936653137, Validation Loss: 0.7178576588630676\n",
      "Epoch 144: Train Loss: 0.6333279132843017, Validation Loss: 0.7184292674064636\n",
      "Epoch 145: Train Loss: 0.6285218477249146, Validation Loss: 0.7191967368125916\n",
      "Epoch 146: Train Loss: 0.6437110900878906, Validation Loss: 0.7192041873931885\n",
      "Epoch 147: Train Loss: 0.6221102356910706, Validation Loss: 0.7199957966804504\n",
      "Epoch 148: Train Loss: 0.6943814992904663, Validation Loss: 0.7197554111480713\n",
      "Epoch 149: Train Loss: 0.6521481394767761, Validation Loss: 0.7193836569786072\n",
      "Epoch 150: Train Loss: 0.6647186756134034, Validation Loss: 0.7182025909423828\n",
      "Epoch 151: Train Loss: 0.6830971956253051, Validation Loss: 0.7195177674293518\n",
      "Epoch 152: Train Loss: 0.6495030999183655, Validation Loss: 0.7195072174072266\n",
      "Epoch 153: Train Loss: 0.7084519028663635, Validation Loss: 0.72038334608078\n",
      "Epoch 154: Train Loss: 0.6792246341705322, Validation Loss: 0.7212428450584412\n",
      "Epoch 155: Train Loss: 0.6632740020751953, Validation Loss: 0.7219563722610474\n",
      "Epoch 156: Train Loss: 0.6990747928619385, Validation Loss: 0.7211015224456787\n",
      "Epoch 157: Train Loss: 0.7035616755485534, Validation Loss: 0.720324695110321\n",
      "Epoch 158: Train Loss: 0.6419236421585083, Validation Loss: 0.7207706570625305\n",
      "Epoch 159: Train Loss: 0.6074679851531982, Validation Loss: 0.722018301486969\n",
      "Epoch 160: Train Loss: 0.7173435926437378, Validation Loss: 0.7233315706253052\n",
      "Epoch 161: Train Loss: 0.6458654642105103, Validation Loss: 0.7236953377723694\n",
      "Epoch 162: Train Loss: 0.675454294681549, Validation Loss: 0.7234071493148804\n",
      "Epoch 163: Train Loss: 0.6427313804626464, Validation Loss: 0.7238942384719849\n",
      "Epoch 164: Train Loss: 0.66113201379776, Validation Loss: 0.7242337465286255\n",
      "Epoch 165: Train Loss: 0.6312686681747437, Validation Loss: 0.7239863276481628\n",
      "Epoch 166: Train Loss: 0.6779240250587464, Validation Loss: 0.7245238423347473\n",
      "Epoch 167: Train Loss: 0.6430527687072753, Validation Loss: 0.724115252494812\n",
      "Epoch 168: Train Loss: 0.6620453834533692, Validation Loss: 0.7245308756828308\n",
      "Epoch 169: Train Loss: 0.6530070304870605, Validation Loss: 0.7246434092521667\n",
      "Epoch 170: Train Loss: 0.6692098617553711, Validation Loss: 0.7243010997772217\n",
      "Epoch 171: Train Loss: 0.6308104753494262, Validation Loss: 0.7258568406105042\n",
      "Epoch 172: Train Loss: 0.6653395533561707, Validation Loss: 0.7272040247917175\n",
      "Epoch 173: Train Loss: 0.6095729231834411, Validation Loss: 0.7287357449531555\n",
      "Epoch 174: Train Loss: 0.6368332624435424, Validation Loss: 0.7277730703353882\n",
      "Epoch 175: Train Loss: 0.6461617827415467, Validation Loss: 0.7296810150146484\n",
      "Epoch 176: Train Loss: 0.6489315390586853, Validation Loss: 0.7309647798538208\n",
      "Epoch 177: Train Loss: 0.6444347977638245, Validation Loss: 0.7307820320129395\n",
      "Epoch 178: Train Loss: 0.5931995689868927, Validation Loss: 0.7317981719970703\n",
      "Epoch 179: Train Loss: 0.6729890942573548, Validation Loss: 0.7321668267250061\n",
      "Epoch 180: Train Loss: 0.6810327887535095, Validation Loss: 0.7321746945381165\n",
      "Epoch 181: Train Loss: 0.6667374253273011, Validation Loss: 0.7332911491394043\n",
      "Epoch 182: Train Loss: 0.6811774015426636, Validation Loss: 0.7338652610778809\n",
      "Epoch 183: Train Loss: 0.624122041463852, Validation Loss: 0.7317891120910645\n",
      "Epoch 184: Train Loss: 0.6603465437889099, Validation Loss: 0.7331464886665344\n",
      "Epoch 185: Train Loss: 0.6581946730613708, Validation Loss: 0.7340583801269531\n",
      "Epoch 186: Train Loss: 0.6294008493423462, Validation Loss: 0.735771656036377\n",
      "Epoch 187: Train Loss: 0.6305708765983582, Validation Loss: 0.7345431447029114\n",
      "Epoch 188: Train Loss: 0.6489621996879578, Validation Loss: 0.7359815239906311\n",
      "Epoch 189: Train Loss: 0.6251795649528503, Validation Loss: 0.7342758178710938\n",
      "Epoch 190: Train Loss: 0.6445322394371032, Validation Loss: 0.7349730730056763\n",
      "Epoch 191: Train Loss: 0.6337549567222596, Validation Loss: 0.7368513941764832\n",
      "Epoch 192: Train Loss: 0.6260260224342347, Validation Loss: 0.7399886846542358\n",
      "Epoch 193: Train Loss: 0.653144896030426, Validation Loss: 0.7403945922851562\n",
      "Epoch 194: Train Loss: 0.646711790561676, Validation Loss: 0.7395515441894531\n",
      "Epoch 195: Train Loss: 0.6424660921096802, Validation Loss: 0.7404386401176453\n",
      "Epoch 196: Train Loss: 0.6809903979301453, Validation Loss: 0.741407573223114\n",
      "Epoch 197: Train Loss: 0.6684612512588501, Validation Loss: 0.7400581240653992\n",
      "Epoch 198: Train Loss: 0.6312854051589966, Validation Loss: 0.740989089012146\n",
      "Epoch 199: Train Loss: 0.6093378663063049, Validation Loss: 0.7411425709724426\n",
      "Epoch 200: Train Loss: 0.6190467655658722, Validation Loss: 0.7422515153884888\n",
      "Epoch 201: Train Loss: 0.6651581645011901, Validation Loss: 0.7449728846549988\n",
      "Epoch 202: Train Loss: 0.631689977645874, Validation Loss: 0.7479531764984131\n",
      "Epoch 203: Train Loss: 0.6026338994503021, Validation Loss: 0.7472318410873413\n",
      "Epoch 204: Train Loss: 0.6269820094108581, Validation Loss: 0.7478354573249817\n",
      "Epoch 205: Train Loss: 0.7288466572761536, Validation Loss: 0.7471062541007996\n",
      "Epoch 206: Train Loss: 0.6454920530319214, Validation Loss: 0.749627411365509\n",
      "Epoch 207: Train Loss: 0.6521139979362488, Validation Loss: 0.7487059831619263\n",
      "Epoch 208: Train Loss: 0.6314006328582764, Validation Loss: 0.7501916289329529\n",
      "Epoch 209: Train Loss: 0.6904555320739746, Validation Loss: 0.7504764199256897\n",
      "Epoch 210: Train Loss: 0.7195737481117248, Validation Loss: 0.7519793510437012\n",
      "Epoch 211: Train Loss: 0.6773026704788208, Validation Loss: 0.7543099522590637\n",
      "Epoch 212: Train Loss: 0.6356576323509217, Validation Loss: 0.7528433799743652\n",
      "Epoch 213: Train Loss: 0.6627604603767395, Validation Loss: 0.754047155380249\n",
      "Epoch 214: Train Loss: 0.6206429123878479, Validation Loss: 0.7571017742156982\n",
      "Epoch 215: Train Loss: 0.6109416127204895, Validation Loss: 0.755829930305481\n",
      "Epoch 216: Train Loss: 0.6209260106086731, Validation Loss: 0.758202314376831\n",
      "Epoch 217: Train Loss: 0.6832154154777527, Validation Loss: 0.7567336559295654\n",
      "Epoch 218: Train Loss: 0.6576890110969543, Validation Loss: 0.7599411010742188\n",
      "Epoch 219: Train Loss: 0.6399069666862488, Validation Loss: 0.7597790956497192\n",
      "Epoch 220: Train Loss: 0.6303787708282471, Validation Loss: 0.7632592916488647\n",
      "Epoch 221: Train Loss: 0.6369359016418457, Validation Loss: 0.7642220854759216\n",
      "Epoch 222: Train Loss: 0.6144739866256714, Validation Loss: 0.7657251358032227\n",
      "Epoch 223: Train Loss: 0.6115966200828552, Validation Loss: 0.769859254360199\n",
      "Epoch 224: Train Loss: 0.6124579548835755, Validation Loss: 0.7705486416816711\n",
      "Epoch 225: Train Loss: 0.6333023905754089, Validation Loss: 0.7741742730140686\n",
      "Epoch 226: Train Loss: 0.6374485611915588, Validation Loss: 0.7770543098449707\n",
      "Epoch 227: Train Loss: 0.5870189189910888, Validation Loss: 0.779114305973053\n",
      "Epoch 228: Train Loss: 0.5897526025772095, Validation Loss: 0.7811339497566223\n",
      "Epoch 229: Train Loss: 0.6925740361213684, Validation Loss: 0.7787679433822632\n",
      "Epoch 230: Train Loss: 0.630906879901886, Validation Loss: 0.78427654504776\n",
      "Epoch 231: Train Loss: 0.5908038675785064, Validation Loss: 0.7845304012298584\n",
      "Epoch 232: Train Loss: 0.6101189374923706, Validation Loss: 0.7874035835266113\n",
      "Epoch 233: Train Loss: 0.6438939213752747, Validation Loss: 0.7921579480171204\n",
      "Epoch 234: Train Loss: 0.580588412284851, Validation Loss: 0.798201858997345\n",
      "Epoch 235: Train Loss: 0.6006662011146545, Validation Loss: 0.8011781573295593\n",
      "Epoch 236: Train Loss: 0.6033252716064453, Validation Loss: 0.805220365524292\n",
      "Epoch 237: Train Loss: 0.6142556190490722, Validation Loss: 0.8065438866615295\n",
      "Epoch 238: Train Loss: 0.5702172756195069, Validation Loss: 0.8097511529922485\n",
      "Epoch 239: Train Loss: 0.5924092531204224, Validation Loss: 0.8112834095954895\n",
      "Epoch 240: Train Loss: 0.6033333659172058, Validation Loss: 0.8224691152572632\n",
      "Epoch 241: Train Loss: 0.595002281665802, Validation Loss: 0.8308172225952148\n",
      "Epoch 242: Train Loss: 0.5677749752998352, Validation Loss: 0.837709903717041\n",
      "Epoch 243: Train Loss: 0.5469760537147522, Validation Loss: 0.843432605266571\n",
      "Epoch 244: Train Loss: 0.5873242855072022, Validation Loss: 0.8589386343955994\n",
      "Epoch 245: Train Loss: 0.5664316534996032, Validation Loss: 0.8667781949043274\n",
      "Epoch 246: Train Loss: 0.6569822072982788, Validation Loss: 0.8773402571678162\n",
      "Epoch 247: Train Loss: 0.5747006475925446, Validation Loss: 0.8852338790893555\n",
      "Epoch 248: Train Loss: 0.5356170237064362, Validation Loss: 0.8774013519287109\n",
      "Epoch 249: Train Loss: 0.5605881810188293, Validation Loss: 0.8833339810371399\n",
      "Epoch 250: Train Loss: 0.6032393217086792, Validation Loss: 0.900399923324585\n",
      "Epoch 251: Train Loss: 0.6016815185546875, Validation Loss: 0.926156222820282\n",
      "Epoch 252: Train Loss: 0.6163385033607482, Validation Loss: 0.9461416602134705\n",
      "Epoch 253: Train Loss: 0.6020004451274872, Validation Loss: 0.9698448777198792\n",
      "Epoch 254: Train Loss: 0.5510144054889679, Validation Loss: 0.9773174524307251\n",
      "Epoch 255: Train Loss: 0.5730889320373536, Validation Loss: 0.9877250790596008\n",
      "Epoch 256: Train Loss: 0.5035586297512055, Validation Loss: 0.9898582696914673\n",
      "Epoch 257: Train Loss: 0.5422176837921142, Validation Loss: 1.00570809841156\n",
      "Epoch 258: Train Loss: 0.5504297912120819, Validation Loss: 1.011881947517395\n",
      "Epoch 259: Train Loss: 0.6006831288337707, Validation Loss: 1.0187758207321167\n",
      "Epoch 260: Train Loss: 0.5613381445407868, Validation Loss: 1.0340393781661987\n",
      "Epoch 261: Train Loss: 0.5520572304725647, Validation Loss: 1.0500497817993164\n",
      "Epoch 262: Train Loss: 0.583555543422699, Validation Loss: 1.081809163093567\n",
      "Epoch 263: Train Loss: 0.5622410356998444, Validation Loss: 1.1018143892288208\n",
      "Epoch 264: Train Loss: 0.6051129698753357, Validation Loss: 1.1049621105194092\n",
      "Epoch 265: Train Loss: 0.5153753876686096, Validation Loss: 1.12748122215271\n",
      "Epoch 266: Train Loss: 0.5145402908325195, Validation Loss: 1.1376489400863647\n",
      "Epoch 267: Train Loss: 0.6086428821086883, Validation Loss: 1.1474356651306152\n",
      "Epoch 268: Train Loss: 0.5883607685565948, Validation Loss: 1.1502625942230225\n",
      "Epoch 269: Train Loss: 0.5662334203720093, Validation Loss: 1.154266119003296\n",
      "Epoch 270: Train Loss: 0.4930253028869629, Validation Loss: 1.1690163612365723\n",
      "Epoch 271: Train Loss: 0.5138205885887146, Validation Loss: 1.1919500827789307\n",
      "Epoch 272: Train Loss: 0.5135126113891602, Validation Loss: 1.2065824270248413\n",
      "Epoch 273: Train Loss: 0.5428353786468506, Validation Loss: 1.2372920513153076\n",
      "Epoch 274: Train Loss: 0.5929768085479736, Validation Loss: 1.2502521276474\n",
      "Epoch 275: Train Loss: 0.5787107169628143, Validation Loss: 1.2742005586624146\n",
      "Epoch 276: Train Loss: 0.5406718134880066, Validation Loss: 1.2798097133636475\n",
      "Epoch 277: Train Loss: 0.5708906531333924, Validation Loss: 1.2909743785858154\n",
      "Epoch 278: Train Loss: 0.4975138187408447, Validation Loss: 1.2907145023345947\n",
      "Epoch 279: Train Loss: 0.4699592888355255, Validation Loss: 1.2744606733322144\n",
      "Epoch 280: Train Loss: 0.5455293178558349, Validation Loss: 1.3113760948181152\n",
      "Epoch 281: Train Loss: 0.5644419372081757, Validation Loss: 1.3274058103561401\n",
      "Epoch 282: Train Loss: 0.6552135527133942, Validation Loss: 1.3245974779129028\n",
      "Epoch 283: Train Loss: 0.6377813458442688, Validation Loss: 1.3259778022766113\n",
      "Epoch 284: Train Loss: 0.5212177097797394, Validation Loss: 1.3316339254379272\n",
      "Epoch 285: Train Loss: 0.595632541179657, Validation Loss: 1.3387010097503662\n",
      "Epoch 286: Train Loss: 0.5254250347614289, Validation Loss: 1.336450457572937\n",
      "Epoch 287: Train Loss: 0.5038661181926727, Validation Loss: 1.3448286056518555\n",
      "Epoch 288: Train Loss: 0.490593695640564, Validation Loss: 1.360040307044983\n",
      "Epoch 289: Train Loss: 0.6205605149269104, Validation Loss: 1.3698331117630005\n",
      "Epoch 290: Train Loss: 0.48568045496940615, Validation Loss: 1.3677879571914673\n",
      "Epoch 291: Train Loss: 0.4966773152351379, Validation Loss: 1.3744136095046997\n",
      "Epoch 292: Train Loss: 0.5884936690330506, Validation Loss: 1.4042328596115112\n",
      "Epoch 293: Train Loss: 0.48232264518737794, Validation Loss: 1.4008464813232422\n",
      "Epoch 294: Train Loss: 0.5529720544815063, Validation Loss: 1.3944687843322754\n",
      "Epoch 295: Train Loss: 0.5186801493167877, Validation Loss: 1.3978263139724731\n",
      "Epoch 296: Train Loss: 0.5826851308345795, Validation Loss: 1.4058905839920044\n",
      "Epoch 297: Train Loss: 0.511750853061676, Validation Loss: 1.431809902191162\n",
      "Epoch 298: Train Loss: 0.5109302759170532, Validation Loss: 1.4438735246658325\n",
      "Epoch 299: Train Loss: 0.54728764295578, Validation Loss: 1.4605083465576172\n",
      "Epoch 300: Train Loss: 0.5035809755325318, Validation Loss: 1.4615256786346436\n",
      "Epoch 301: Train Loss: 0.49395559430122377, Validation Loss: 1.4650534391403198\n",
      "Epoch 302: Train Loss: 0.5079185366630554, Validation Loss: 1.5128118991851807\n",
      "Epoch 303: Train Loss: 0.5633117735385895, Validation Loss: 1.551235556602478\n",
      "Epoch 304: Train Loss: 0.5574350416660309, Validation Loss: 1.5683414936065674\n",
      "Epoch 305: Train Loss: 0.517534339427948, Validation Loss: 1.5780537128448486\n",
      "Epoch 306: Train Loss: 0.5523133099079132, Validation Loss: 1.5963960886001587\n",
      "Epoch 307: Train Loss: 0.5631347239017487, Validation Loss: 1.5945525169372559\n",
      "Epoch 308: Train Loss: 0.5130938112735748, Validation Loss: 1.579662561416626\n",
      "Epoch 309: Train Loss: 0.481801038980484, Validation Loss: 1.591853380203247\n",
      "Epoch 310: Train Loss: 0.5600537717342376, Validation Loss: 1.5863147974014282\n",
      "Epoch 311: Train Loss: 0.5486952006816864, Validation Loss: 1.6206079721450806\n",
      "Epoch 312: Train Loss: 0.5073901295661927, Validation Loss: 1.6111030578613281\n",
      "Epoch 313: Train Loss: 0.5415207028388977, Validation Loss: 1.6101042032241821\n",
      "Epoch 314: Train Loss: 0.5255099773406983, Validation Loss: 1.5995126962661743\n",
      "Epoch 315: Train Loss: 0.5235452055931091, Validation Loss: 1.6065456867218018\n",
      "Epoch 316: Train Loss: 0.48648509979248045, Validation Loss: 1.6155169010162354\n",
      "Epoch 317: Train Loss: 0.4933070719242096, Validation Loss: 1.6266711950302124\n",
      "Epoch 318: Train Loss: 0.5100956916809082, Validation Loss: 1.642599105834961\n",
      "Epoch 319: Train Loss: 0.5563771188259125, Validation Loss: 1.6600031852722168\n",
      "Epoch 320: Train Loss: 0.6342682123184205, Validation Loss: 1.6818758249282837\n",
      "Epoch 321: Train Loss: 0.4996442854404449, Validation Loss: 1.6987632513046265\n",
      "Epoch 322: Train Loss: 0.509074205160141, Validation Loss: 1.7017487287521362\n",
      "Epoch 323: Train Loss: 0.4757061243057251, Validation Loss: 1.699917197227478\n",
      "Epoch 324: Train Loss: 0.4716278433799744, Validation Loss: 1.7271019220352173\n",
      "Epoch 325: Train Loss: 0.5874148190021515, Validation Loss: 1.7527403831481934\n",
      "Epoch 326: Train Loss: 0.4754614531993866, Validation Loss: 1.768101453781128\n",
      "Epoch 327: Train Loss: 0.4725642740726471, Validation Loss: 1.7722290754318237\n",
      "Epoch 328: Train Loss: 0.43070446252822875, Validation Loss: 1.7489601373672485\n",
      "Epoch 329: Train Loss: 0.47104724645614626, Validation Loss: 1.7207481861114502\n",
      "Epoch 330: Train Loss: 0.4594683825969696, Validation Loss: 1.7724424600601196\n",
      "Epoch 331: Train Loss: 0.46416213512420657, Validation Loss: 1.769059419631958\n",
      "Epoch 332: Train Loss: 0.4171631097793579, Validation Loss: 1.7971094846725464\n",
      "Epoch 333: Train Loss: 0.4470330536365509, Validation Loss: 1.8410941362380981\n",
      "Epoch 334: Train Loss: 0.4930184781551361, Validation Loss: 1.8740147352218628\n",
      "Epoch 335: Train Loss: 0.5235063135623932, Validation Loss: 1.9008225202560425\n",
      "Epoch 336: Train Loss: 0.5350443840026855, Validation Loss: 1.8886672258377075\n",
      "Epoch 337: Train Loss: 0.45692922472953795, Validation Loss: 1.916780948638916\n",
      "Epoch 338: Train Loss: 0.46265179514884947, Validation Loss: 1.9114357233047485\n",
      "Epoch 339: Train Loss: 0.48337858319282534, Validation Loss: 1.908164143562317\n",
      "Epoch 340: Train Loss: 0.395734840631485, Validation Loss: 1.9507790803909302\n",
      "Epoch 341: Train Loss: 0.4934064447879791, Validation Loss: 1.944654941558838\n",
      "Epoch 342: Train Loss: 0.4264474093914032, Validation Loss: 1.9237759113311768\n",
      "Epoch 343: Train Loss: 0.45112845301628113, Validation Loss: 1.9744083881378174\n",
      "Epoch 344: Train Loss: 0.5280681371688842, Validation Loss: 2.0035054683685303\n",
      "Epoch 345: Train Loss: 0.4307871997356415, Validation Loss: 2.03879976272583\n",
      "Epoch 346: Train Loss: 0.42387730479240415, Validation Loss: 2.0388669967651367\n",
      "Epoch 347: Train Loss: 0.4589927077293396, Validation Loss: 2.006676435470581\n",
      "Epoch 348: Train Loss: 0.5012243807315826, Validation Loss: 2.0284268856048584\n",
      "Epoch 349: Train Loss: 0.5127906680107117, Validation Loss: 2.0278913974761963\n",
      "Epoch 350: Train Loss: 0.5050931751728058, Validation Loss: 2.0516321659088135\n",
      "Epoch 351: Train Loss: 0.4930774748325348, Validation Loss: 2.0425984859466553\n",
      "Epoch 352: Train Loss: 0.46523738503456114, Validation Loss: 2.043657064437866\n",
      "Epoch 353: Train Loss: 0.4496751487255096, Validation Loss: 2.048464298248291\n",
      "Epoch 354: Train Loss: 0.4956315100193024, Validation Loss: 2.0543363094329834\n",
      "Epoch 355: Train Loss: 0.4011426031589508, Validation Loss: 2.04487681388855\n",
      "Epoch 356: Train Loss: 0.5444443166255951, Validation Loss: 2.081423759460449\n",
      "Epoch 357: Train Loss: 0.48428005576133726, Validation Loss: 2.0846049785614014\n",
      "Epoch 358: Train Loss: 0.44876848459243773, Validation Loss: 2.0634477138519287\n",
      "Epoch 359: Train Loss: 0.5715562462806701, Validation Loss: 2.0658230781555176\n",
      "Epoch 360: Train Loss: 0.443972235918045, Validation Loss: 2.0771353244781494\n",
      "Epoch 361: Train Loss: 0.4203121542930603, Validation Loss: 2.0875027179718018\n",
      "Epoch 362: Train Loss: 0.476041316986084, Validation Loss: 2.12540864944458\n",
      "Epoch 363: Train Loss: 0.41363502144813535, Validation Loss: 2.1570725440979004\n",
      "Epoch 364: Train Loss: 0.4568026661872864, Validation Loss: 2.122845411300659\n",
      "Epoch 365: Train Loss: 0.46046860218048097, Validation Loss: 2.183345317840576\n",
      "Epoch 366: Train Loss: 0.47133889198303225, Validation Loss: 2.22231125831604\n",
      "Epoch 367: Train Loss: 0.4269200086593628, Validation Loss: 2.248704433441162\n",
      "Epoch 368: Train Loss: 0.42407692670822145, Validation Loss: 2.2389075756073\n",
      "Epoch 369: Train Loss: 0.46638903617858884, Validation Loss: 2.267484188079834\n",
      "Epoch 370: Train Loss: 0.4692209780216217, Validation Loss: 2.2638232707977295\n",
      "Epoch 371: Train Loss: 0.42689846754074096, Validation Loss: 2.2746686935424805\n",
      "Epoch 372: Train Loss: 0.5706172287464142, Validation Loss: 2.3396501541137695\n",
      "Epoch 373: Train Loss: 0.4256810247898102, Validation Loss: 2.34657883644104\n",
      "Epoch 374: Train Loss: 0.4702256977558136, Validation Loss: 2.4021975994110107\n",
      "Epoch 375: Train Loss: 0.5415931463241577, Validation Loss: 2.371851682662964\n",
      "Epoch 376: Train Loss: 0.4288740515708923, Validation Loss: 2.381159782409668\n",
      "Epoch 377: Train Loss: 0.425306099653244, Validation Loss: 2.3383920192718506\n",
      "Epoch 378: Train Loss: 0.4155794560909271, Validation Loss: 2.2811529636383057\n",
      "Epoch 379: Train Loss: 0.4448789060115814, Validation Loss: 2.321683406829834\n",
      "Epoch 380: Train Loss: 0.4097521483898163, Validation Loss: 2.3421168327331543\n",
      "Epoch 381: Train Loss: 0.4242366135120392, Validation Loss: 2.4721012115478516\n",
      "Epoch 382: Train Loss: 0.41429410576820375, Validation Loss: 2.4674670696258545\n",
      "Epoch 383: Train Loss: 0.36058768928050994, Validation Loss: 2.4828310012817383\n",
      "Epoch 384: Train Loss: 0.5208000838756561, Validation Loss: 2.5299649238586426\n",
      "Epoch 385: Train Loss: 0.393726772069931, Validation Loss: 2.5511367321014404\n",
      "Epoch 386: Train Loss: 0.3923346221446991, Validation Loss: 2.5947463512420654\n",
      "Epoch 387: Train Loss: 0.39376204609870913, Validation Loss: 2.6577069759368896\n",
      "Epoch 388: Train Loss: 0.4178271234035492, Validation Loss: 2.678331136703491\n",
      "Epoch 389: Train Loss: 0.39104737639427184, Validation Loss: 2.67244815826416\n",
      "Epoch 390: Train Loss: 0.3487211525440216, Validation Loss: 2.687842845916748\n",
      "Epoch 391: Train Loss: 0.47993622422218324, Validation Loss: 2.770473003387451\n",
      "Epoch 392: Train Loss: 0.5352635204792022, Validation Loss: 2.7679994106292725\n",
      "Epoch 393: Train Loss: 0.43919349312782285, Validation Loss: 2.780190944671631\n",
      "Epoch 394: Train Loss: 0.5865513145923614, Validation Loss: 2.8297176361083984\n",
      "Epoch 395: Train Loss: 0.4266120374202728, Validation Loss: 2.811208724975586\n",
      "Epoch 396: Train Loss: 0.41685439348220826, Validation Loss: 2.7958288192749023\n",
      "Epoch 397: Train Loss: 0.3740155935287476, Validation Loss: 2.842332363128662\n",
      "Epoch 398: Train Loss: 0.555942279100418, Validation Loss: 2.8444862365722656\n",
      "Epoch 399: Train Loss: 0.4297961711883545, Validation Loss: 2.7982523441314697\n",
      "Epoch 400: Train Loss: 0.3448844015598297, Validation Loss: 2.8238086700439453\n",
      "Epoch 401: Train Loss: 0.3737203776836395, Validation Loss: 2.867830991744995\n",
      "Epoch 402: Train Loss: 0.3353159576654434, Validation Loss: 2.855985403060913\n",
      "Epoch 403: Train Loss: 0.3581217974424362, Validation Loss: 2.937506914138794\n",
      "Epoch 404: Train Loss: 0.36120472848415375, Validation Loss: 2.992804765701294\n",
      "Epoch 405: Train Loss: 0.3477760970592499, Validation Loss: 3.0370495319366455\n",
      "Epoch 406: Train Loss: 0.5354264974594116, Validation Loss: 3.138950824737549\n",
      "Epoch 407: Train Loss: 0.33671417236328127, Validation Loss: 3.065089464187622\n",
      "Epoch 408: Train Loss: 0.43729167580604555, Validation Loss: 3.096773862838745\n",
      "Epoch 409: Train Loss: 0.41554892659187315, Validation Loss: 3.0806050300598145\n",
      "Epoch 410: Train Loss: 0.37510313391685485, Validation Loss: 3.139566659927368\n",
      "Epoch 411: Train Loss: 0.45042216777801514, Validation Loss: 3.162585973739624\n",
      "Epoch 412: Train Loss: 0.3831942558288574, Validation Loss: 3.2019736766815186\n",
      "Epoch 413: Train Loss: 0.37684375047683716, Validation Loss: 3.1706390380859375\n",
      "Epoch 414: Train Loss: 0.34199696183204653, Validation Loss: 3.1528546810150146\n",
      "Epoch 415: Train Loss: 0.36192556023597716, Validation Loss: 3.147362232208252\n",
      "Epoch 416: Train Loss: 0.3056795567274094, Validation Loss: 3.208789110183716\n",
      "Epoch 417: Train Loss: 0.3533492207527161, Validation Loss: 3.2210447788238525\n",
      "Epoch 418: Train Loss: 0.35171844959259035, Validation Loss: 3.315455675125122\n",
      "Epoch 419: Train Loss: 0.3852343499660492, Validation Loss: 3.330096483230591\n",
      "Epoch 420: Train Loss: 0.3593696355819702, Validation Loss: 3.3802688121795654\n",
      "Epoch 421: Train Loss: 0.33533649444580077, Validation Loss: 3.460360527038574\n",
      "Epoch 422: Train Loss: 0.32829062938690184, Validation Loss: 3.471500873565674\n",
      "Epoch 423: Train Loss: 0.29769734740257264, Validation Loss: 3.534125566482544\n",
      "Epoch 424: Train Loss: 0.3123680159449577, Validation Loss: 3.615490674972534\n",
      "Epoch 425: Train Loss: 0.34058902263641355, Validation Loss: 3.6786117553710938\n",
      "Epoch 426: Train Loss: 0.3806396543979645, Validation Loss: 3.8049962520599365\n",
      "Epoch 427: Train Loss: 0.46000043153762815, Validation Loss: 3.831320285797119\n",
      "Epoch 428: Train Loss: 0.3400158077478409, Validation Loss: 3.7872259616851807\n",
      "Epoch 429: Train Loss: 0.35703860521316527, Validation Loss: 3.8515493869781494\n",
      "Epoch 430: Train Loss: 0.27224316596984866, Validation Loss: 3.84944748878479\n",
      "Epoch 431: Train Loss: 0.34739712774753573, Validation Loss: 3.9124016761779785\n",
      "Epoch 432: Train Loss: 0.3975078403949738, Validation Loss: 3.8840138912200928\n",
      "Epoch 433: Train Loss: 0.3324661374092102, Validation Loss: 3.9640138149261475\n",
      "Epoch 434: Train Loss: 0.3487500548362732, Validation Loss: 4.049294471740723\n",
      "Epoch 435: Train Loss: 0.28835371136665344, Validation Loss: 4.063148021697998\n",
      "Epoch 436: Train Loss: 0.35548346042633056, Validation Loss: 3.987886667251587\n",
      "Epoch 437: Train Loss: 0.3002265602350235, Validation Loss: 3.955226182937622\n",
      "Epoch 438: Train Loss: 0.4034193158149719, Validation Loss: 3.967806816101074\n",
      "Epoch 439: Train Loss: 0.33761513233184814, Validation Loss: 3.87546706199646\n",
      "Epoch 440: Train Loss: 0.4201540559530258, Validation Loss: 3.8693976402282715\n",
      "Epoch 441: Train Loss: 0.4201710820198059, Validation Loss: 4.015718460083008\n",
      "Epoch 442: Train Loss: 0.2909007787704468, Validation Loss: 4.077253341674805\n",
      "Epoch 443: Train Loss: 0.31004700660705564, Validation Loss: 4.005337715148926\n",
      "Epoch 444: Train Loss: 0.28069871962070464, Validation Loss: 4.201443195343018\n",
      "Epoch 445: Train Loss: 0.31503310799598694, Validation Loss: 4.391305446624756\n",
      "Epoch 446: Train Loss: 0.24584514796733856, Validation Loss: 4.298002243041992\n",
      "Epoch 447: Train Loss: 0.2547177404165268, Validation Loss: 4.403431415557861\n",
      "Epoch 448: Train Loss: 0.311752912402153, Validation Loss: 4.305804252624512\n",
      "Epoch 449: Train Loss: 0.33520983159542084, Validation Loss: 4.33570671081543\n",
      "Epoch 450: Train Loss: 0.3158250033855438, Validation Loss: 4.413759231567383\n",
      "Epoch 451: Train Loss: 0.4259689748287201, Validation Loss: 4.473372936248779\n",
      "Epoch 452: Train Loss: 0.29127953946590424, Validation Loss: 4.5355939865112305\n",
      "Epoch 453: Train Loss: 0.32367490231990814, Validation Loss: 4.61827278137207\n",
      "Epoch 454: Train Loss: 0.3119392514228821, Validation Loss: 4.701274394989014\n",
      "Epoch 455: Train Loss: 0.29896703362464905, Validation Loss: 4.7026567459106445\n",
      "Epoch 456: Train Loss: 0.30849566459655764, Validation Loss: 4.644895553588867\n",
      "Epoch 457: Train Loss: 0.34279603958129884, Validation Loss: 4.6596598625183105\n",
      "Epoch 458: Train Loss: 0.2582806199789047, Validation Loss: 4.602387428283691\n",
      "Epoch 459: Train Loss: 0.26782217621803284, Validation Loss: 4.681895732879639\n",
      "Epoch 460: Train Loss: 0.38039950728416444, Validation Loss: 4.7402119636535645\n",
      "Epoch 461: Train Loss: 0.48275295495986936, Validation Loss: 4.915808200836182\n",
      "Epoch 462: Train Loss: 0.40747973024845124, Validation Loss: 5.000321865081787\n",
      "Epoch 463: Train Loss: 0.39579840898513796, Validation Loss: 4.8921661376953125\n",
      "Epoch 464: Train Loss: 0.250688037276268, Validation Loss: 4.845295429229736\n",
      "Epoch 465: Train Loss: 0.3083031475543976, Validation Loss: 4.903475761413574\n",
      "Epoch 466: Train Loss: 0.40015847980976105, Validation Loss: 4.861834526062012\n",
      "Epoch 467: Train Loss: 0.2838497072458267, Validation Loss: 4.846463680267334\n",
      "Epoch 468: Train Loss: 0.33302966952323915, Validation Loss: 4.980402946472168\n",
      "Epoch 469: Train Loss: 0.42096690833568573, Validation Loss: 4.939579486846924\n",
      "Epoch 470: Train Loss: 0.33769223392009734, Validation Loss: 4.917198657989502\n",
      "Epoch 471: Train Loss: 0.2347379982471466, Validation Loss: 4.913775444030762\n",
      "Epoch 472: Train Loss: 0.37027607858181, Validation Loss: 5.019659042358398\n",
      "Epoch 473: Train Loss: 0.29396666288375856, Validation Loss: 5.0006890296936035\n",
      "Epoch 474: Train Loss: 0.34086030423641206, Validation Loss: 5.113181114196777\n",
      "Epoch 475: Train Loss: 0.24456734955310822, Validation Loss: 5.091010570526123\n",
      "Epoch 476: Train Loss: 0.4111988127231598, Validation Loss: 5.009125232696533\n",
      "Epoch 477: Train Loss: 0.2678883343935013, Validation Loss: 4.9741339683532715\n",
      "Epoch 478: Train Loss: 0.2744864046573639, Validation Loss: 4.95955753326416\n",
      "Epoch 479: Train Loss: 0.22514996975660323, Validation Loss: 4.91253662109375\n",
      "Epoch 480: Train Loss: 0.46592576503753663, Validation Loss: 5.048650741577148\n",
      "Epoch 481: Train Loss: 0.22444119602441787, Validation Loss: 5.150147438049316\n",
      "Epoch 482: Train Loss: 0.3223473370075226, Validation Loss: 5.149524211883545\n",
      "Epoch 483: Train Loss: 0.21542447209358215, Validation Loss: 5.297332286834717\n",
      "Epoch 484: Train Loss: 0.383137109875679, Validation Loss: 5.451794147491455\n",
      "Epoch 485: Train Loss: 0.2577104985713959, Validation Loss: 5.353806495666504\n",
      "Epoch 486: Train Loss: 0.303692689538002, Validation Loss: 5.269441604614258\n",
      "Epoch 487: Train Loss: 0.249656143784523, Validation Loss: 5.321025848388672\n",
      "Epoch 488: Train Loss: 0.23130748867988588, Validation Loss: 5.361745834350586\n",
      "Epoch 489: Train Loss: 0.3458432048559189, Validation Loss: 5.485726356506348\n",
      "Epoch 490: Train Loss: 0.2096613749861717, Validation Loss: 5.378762722015381\n",
      "Epoch 491: Train Loss: 0.28754193186759947, Validation Loss: 5.332579612731934\n",
      "Epoch 492: Train Loss: 0.27380097210407256, Validation Loss: 5.2970123291015625\n",
      "Epoch 493: Train Loss: 0.27429859042167665, Validation Loss: 5.380137920379639\n",
      "Epoch 494: Train Loss: 0.20876652151346206, Validation Loss: 5.475839138031006\n",
      "Epoch 495: Train Loss: 0.2532210022211075, Validation Loss: 5.398374080657959\n",
      "Epoch 496: Train Loss: 0.31441123187541964, Validation Loss: 5.504647254943848\n",
      "Epoch 497: Train Loss: 0.29636339843273163, Validation Loss: 5.49334716796875\n",
      "Epoch 498: Train Loss: 0.23485129177570344, Validation Loss: 5.606899261474609\n",
      "Epoch 499: Train Loss: 0.2907356172800064, Validation Loss: 5.586496829986572\n",
      "Fold 7 Metrics:\n",
      "Accuracy: 0.0, Precision: 0.0, Recall: 0.0, F1-score: 0.0, AUC: 0.0\n",
      "Confusion Matrix:\n",
      "[[0 5]\n",
      " [6 0]]\n",
      "Completed fold 7\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples from subject 9 to test set\n",
      "Adding 6 truth samples from subject 9 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 500)\n",
      "(132, 65, 500)\n",
      "Epoch 0: Train Loss: 0.7151019692420959, Validation Loss: 0.6977481245994568\n",
      "Epoch 1: Train Loss: 0.698361051082611, Validation Loss: 0.7000482082366943\n",
      "Epoch 2: Train Loss: 0.7224803686141967, Validation Loss: 0.7017711400985718\n",
      "Epoch 3: Train Loss: 0.7112530589103698, Validation Loss: 0.7034006714820862\n",
      "Epoch 4: Train Loss: 0.7398123025894165, Validation Loss: 0.7049276828765869\n",
      "Epoch 5: Train Loss: 0.7051122307777404, Validation Loss: 0.7058477997779846\n",
      "Epoch 6: Train Loss: 0.7551597952842712, Validation Loss: 0.7062017321586609\n",
      "Epoch 7: Train Loss: 0.7770009517669678, Validation Loss: 0.7067356109619141\n",
      "Epoch 8: Train Loss: 0.6766327977180481, Validation Loss: 0.7075114250183105\n",
      "Epoch 9: Train Loss: 0.7112584471702575, Validation Loss: 0.7073348760604858\n",
      "Epoch 10: Train Loss: 0.7271015644073486, Validation Loss: 0.7078298330307007\n",
      "Epoch 11: Train Loss: 0.7284586071968079, Validation Loss: 0.7073889970779419\n",
      "Epoch 12: Train Loss: 0.6880231499671936, Validation Loss: 0.7070615887641907\n",
      "Epoch 13: Train Loss: 0.7809530615806579, Validation Loss: 0.7071590423583984\n",
      "Epoch 14: Train Loss: 0.7029428124427796, Validation Loss: 0.7073839902877808\n",
      "Epoch 15: Train Loss: 0.6692070007324219, Validation Loss: 0.7062324285507202\n",
      "Epoch 16: Train Loss: 0.7363958477973938, Validation Loss: 0.7055929899215698\n",
      "Epoch 17: Train Loss: 0.6790564298629761, Validation Loss: 0.7066472172737122\n",
      "Epoch 18: Train Loss: 0.763767683506012, Validation Loss: 0.7067798376083374\n",
      "Epoch 19: Train Loss: 0.7203575611114502, Validation Loss: 0.7067775726318359\n",
      "Epoch 20: Train Loss: 0.7779657125473023, Validation Loss: 0.706479012966156\n",
      "Epoch 21: Train Loss: 0.6922393798828125, Validation Loss: 0.7064128518104553\n",
      "Epoch 22: Train Loss: 0.6861440777778626, Validation Loss: 0.7070105671882629\n",
      "Epoch 23: Train Loss: 0.71844562292099, Validation Loss: 0.7070919871330261\n",
      "Epoch 24: Train Loss: 0.712203299999237, Validation Loss: 0.7068524360656738\n",
      "Epoch 25: Train Loss: 0.6784887433052063, Validation Loss: 0.7066187262535095\n",
      "Epoch 26: Train Loss: 0.765202808380127, Validation Loss: 0.7061187028884888\n",
      "Epoch 27: Train Loss: 0.7229023218154907, Validation Loss: 0.7067369818687439\n",
      "Epoch 28: Train Loss: 0.7096880078315735, Validation Loss: 0.7063859701156616\n",
      "Epoch 29: Train Loss: 0.7425841689109802, Validation Loss: 0.7066888809204102\n",
      "Epoch 30: Train Loss: 0.7780555605888366, Validation Loss: 0.7059668302536011\n",
      "Epoch 31: Train Loss: 0.715373182296753, Validation Loss: 0.7047534584999084\n",
      "Epoch 32: Train Loss: 0.7521072149276733, Validation Loss: 0.7052684426307678\n",
      "Epoch 33: Train Loss: 0.7228341937065125, Validation Loss: 0.7054111361503601\n",
      "Epoch 34: Train Loss: 0.7165743231773376, Validation Loss: 0.7041915655136108\n",
      "Epoch 35: Train Loss: 0.710524570941925, Validation Loss: 0.704765260219574\n",
      "Epoch 36: Train Loss: 0.7214867234230041, Validation Loss: 0.7057825326919556\n",
      "Epoch 37: Train Loss: 0.7569331288337707, Validation Loss: 0.7042990326881409\n",
      "Epoch 38: Train Loss: 0.6597227454185486, Validation Loss: 0.7045763731002808\n",
      "Epoch 39: Train Loss: 0.7199600458145141, Validation Loss: 0.7048144340515137\n",
      "Epoch 40: Train Loss: 0.696696400642395, Validation Loss: 0.7048020958900452\n",
      "Epoch 41: Train Loss: 0.7348766207695008, Validation Loss: 0.7045825719833374\n",
      "Epoch 42: Train Loss: 0.6692164301872253, Validation Loss: 0.7044332027435303\n",
      "Epoch 43: Train Loss: 0.7522363185882568, Validation Loss: 0.7045677900314331\n",
      "Epoch 44: Train Loss: 0.7189194083213806, Validation Loss: 0.7043024897575378\n",
      "Epoch 45: Train Loss: 0.7398736834526062, Validation Loss: 0.7043137550354004\n",
      "Epoch 46: Train Loss: 0.6863680005073547, Validation Loss: 0.7041871547698975\n",
      "Epoch 47: Train Loss: 0.6802204966545105, Validation Loss: 0.7040053009986877\n",
      "Epoch 48: Train Loss: 0.7191193222999572, Validation Loss: 0.7029823660850525\n",
      "Epoch 49: Train Loss: 0.7530823588371277, Validation Loss: 0.7035673260688782\n",
      "Epoch 50: Train Loss: 0.7404885411262512, Validation Loss: 0.7031717300415039\n",
      "Epoch 51: Train Loss: 0.7077979445457458, Validation Loss: 0.703490674495697\n",
      "Epoch 52: Train Loss: 0.6917536616325378, Validation Loss: 0.7039780616760254\n",
      "Epoch 53: Train Loss: 0.7522634625434875, Validation Loss: 0.7038695812225342\n",
      "Epoch 54: Train Loss: 0.6806791067123413, Validation Loss: 0.7032088041305542\n",
      "Epoch 55: Train Loss: 0.6978867888450623, Validation Loss: 0.7030749320983887\n",
      "Epoch 56: Train Loss: 0.679079270362854, Validation Loss: 0.7029684782028198\n",
      "Epoch 57: Train Loss: 0.6896170735359192, Validation Loss: 0.7031170725822449\n",
      "Epoch 58: Train Loss: 0.7462255597114563, Validation Loss: 0.702906608581543\n",
      "Epoch 59: Train Loss: 0.6850327014923095, Validation Loss: 0.7027711272239685\n",
      "Epoch 60: Train Loss: 0.7619442820549012, Validation Loss: 0.7028053402900696\n",
      "Epoch 61: Train Loss: 0.7099937438964844, Validation Loss: 0.702502429485321\n",
      "Epoch 62: Train Loss: 0.7092808008193969, Validation Loss: 0.7017472386360168\n",
      "Epoch 63: Train Loss: 0.7083693861961364, Validation Loss: 0.7022135853767395\n",
      "Epoch 64: Train Loss: 0.7491265177726746, Validation Loss: 0.7024096846580505\n",
      "Epoch 65: Train Loss: 0.7194751620292663, Validation Loss: 0.7026359438896179\n",
      "Epoch 66: Train Loss: 0.7476868391036987, Validation Loss: 0.7018390893936157\n",
      "Epoch 67: Train Loss: 0.6884494543075561, Validation Loss: 0.7024459838867188\n",
      "Epoch 68: Train Loss: 0.7359983921051025, Validation Loss: 0.7029679417610168\n",
      "Epoch 69: Train Loss: 0.6901967406272889, Validation Loss: 0.7022563815116882\n",
      "Epoch 70: Train Loss: 0.7149085640907288, Validation Loss: 0.7023895978927612\n",
      "Epoch 71: Train Loss: 0.6898352265357971, Validation Loss: 0.7027706503868103\n",
      "Epoch 72: Train Loss: 0.7202805042266845, Validation Loss: 0.7028782963752747\n",
      "Epoch 73: Train Loss: 0.6835387945175171, Validation Loss: 0.702995777130127\n",
      "Epoch 74: Train Loss: 0.6753389596939087, Validation Loss: 0.7032625675201416\n",
      "Epoch 75: Train Loss: 0.750647783279419, Validation Loss: 0.703875720500946\n",
      "Epoch 76: Train Loss: 0.6871721625328064, Validation Loss: 0.7036095857620239\n",
      "Epoch 77: Train Loss: 0.7037749409675598, Validation Loss: 0.7038171887397766\n",
      "Epoch 78: Train Loss: 0.6929498434066772, Validation Loss: 0.7034536600112915\n",
      "Epoch 79: Train Loss: 0.7154708743095398, Validation Loss: 0.702725887298584\n",
      "Epoch 80: Train Loss: 0.7475761890411377, Validation Loss: 0.7024017572402954\n",
      "Epoch 81: Train Loss: 0.689060652256012, Validation Loss: 0.7023444175720215\n",
      "Epoch 82: Train Loss: 0.685303270816803, Validation Loss: 0.7021915316581726\n",
      "Epoch 83: Train Loss: 0.6712352752685546, Validation Loss: 0.7022606730461121\n",
      "Epoch 84: Train Loss: 0.7060407400131226, Validation Loss: 0.7022607326507568\n",
      "Epoch 85: Train Loss: 0.6997742056846619, Validation Loss: 0.7024191617965698\n",
      "Epoch 86: Train Loss: 0.6545703172683716, Validation Loss: 0.7027071714401245\n",
      "Epoch 87: Train Loss: 0.7087283730506897, Validation Loss: 0.7028394341468811\n",
      "Epoch 88: Train Loss: 0.7059704065322876, Validation Loss: 0.7028886675834656\n",
      "Epoch 89: Train Loss: 0.704653012752533, Validation Loss: 0.7018513679504395\n",
      "Epoch 90: Train Loss: 0.6849563002586365, Validation Loss: 0.7016339302062988\n",
      "Epoch 91: Train Loss: 0.6614132642745971, Validation Loss: 0.7012864947319031\n",
      "Epoch 92: Train Loss: 0.6901485800743103, Validation Loss: 0.7007719278335571\n",
      "Epoch 93: Train Loss: 0.7163449287414551, Validation Loss: 0.7009852528572083\n",
      "Epoch 94: Train Loss: 0.7132257580757141, Validation Loss: 0.7012140154838562\n",
      "Epoch 95: Train Loss: 0.6635162770748139, Validation Loss: 0.7014099359512329\n",
      "Epoch 96: Train Loss: 0.6722207188606262, Validation Loss: 0.7017859816551208\n",
      "Epoch 97: Train Loss: 0.6574649453163147, Validation Loss: 0.7006628513336182\n",
      "Epoch 98: Train Loss: 0.7051971435546875, Validation Loss: 0.7008527517318726\n",
      "Epoch 99: Train Loss: 0.6760136723518372, Validation Loss: 0.7010408043861389\n",
      "Epoch 100: Train Loss: 0.6707168102264405, Validation Loss: 0.701338529586792\n",
      "Epoch 101: Train Loss: 0.6847844600677491, Validation Loss: 0.7016687393188477\n",
      "Epoch 102: Train Loss: 0.670124065876007, Validation Loss: 0.7009867429733276\n",
      "Epoch 103: Train Loss: 0.6983267426490783, Validation Loss: 0.7008407711982727\n",
      "Epoch 104: Train Loss: 0.7028263092041016, Validation Loss: 0.700476348400116\n",
      "Epoch 105: Train Loss: 0.6878498196601868, Validation Loss: 0.7002676129341125\n",
      "Epoch 106: Train Loss: 0.6692156672477723, Validation Loss: 0.69989013671875\n",
      "Epoch 107: Train Loss: 0.7010030269622802, Validation Loss: 0.7002790570259094\n",
      "Epoch 108: Train Loss: 0.7023923635482788, Validation Loss: 0.7005234956741333\n",
      "Epoch 109: Train Loss: 0.6729442238807678, Validation Loss: 0.7004276514053345\n",
      "Epoch 110: Train Loss: 0.7040345072746277, Validation Loss: 0.6987807154655457\n",
      "Epoch 111: Train Loss: 0.7098841428756714, Validation Loss: 0.6986902952194214\n",
      "Epoch 112: Train Loss: 0.6753749847412109, Validation Loss: 0.699002742767334\n",
      "Epoch 113: Train Loss: 0.664867103099823, Validation Loss: 0.6988427639007568\n",
      "Epoch 114: Train Loss: 0.697742509841919, Validation Loss: 0.6986650824546814\n",
      "Epoch 115: Train Loss: 0.7265421986579895, Validation Loss: 0.6984862089157104\n",
      "Epoch 116: Train Loss: 0.700146496295929, Validation Loss: 0.6986704468727112\n",
      "Epoch 117: Train Loss: 0.6747190237045289, Validation Loss: 0.6986812949180603\n",
      "Epoch 118: Train Loss: 0.6776145458221435, Validation Loss: 0.6980516314506531\n",
      "Epoch 119: Train Loss: 0.6632407426834106, Validation Loss: 0.6975347995758057\n",
      "Epoch 120: Train Loss: 0.6633283495903015, Validation Loss: 0.6976580023765564\n",
      "Epoch 121: Train Loss: 0.7207780838012695, Validation Loss: 0.6967940330505371\n",
      "Epoch 122: Train Loss: 0.6821455359458923, Validation Loss: 0.696728527545929\n",
      "Epoch 123: Train Loss: 0.6612240076065063, Validation Loss: 0.6964284777641296\n",
      "Epoch 124: Train Loss: 0.6687885880470276, Validation Loss: 0.6967390775680542\n",
      "Epoch 125: Train Loss: 0.6906017541885376, Validation Loss: 0.6964232921600342\n",
      "Epoch 126: Train Loss: 0.6501554489135742, Validation Loss: 0.6964821815490723\n",
      "Epoch 127: Train Loss: 0.6609124898910522, Validation Loss: 0.6965826749801636\n",
      "Epoch 128: Train Loss: 0.6962070345878602, Validation Loss: 0.6962671279907227\n",
      "Epoch 129: Train Loss: 0.6533736705780029, Validation Loss: 0.6962762475013733\n",
      "Epoch 130: Train Loss: 0.6982020735740662, Validation Loss: 0.6963849067687988\n",
      "Epoch 131: Train Loss: 0.6662947297096252, Validation Loss: 0.6961972713470459\n",
      "Epoch 132: Train Loss: 0.6566925883293152, Validation Loss: 0.695690929889679\n",
      "Epoch 133: Train Loss: 0.7077911138534546, Validation Loss: 0.6953387260437012\n",
      "Epoch 134: Train Loss: 0.7033859729766846, Validation Loss: 0.6957370042800903\n",
      "Epoch 135: Train Loss: 0.6591774582862854, Validation Loss: 0.695465087890625\n",
      "Epoch 136: Train Loss: 0.6542814612388611, Validation Loss: 0.6960020661354065\n",
      "Epoch 137: Train Loss: 0.6612865328788757, Validation Loss: 0.6960288882255554\n",
      "Epoch 138: Train Loss: 0.7131835460662842, Validation Loss: 0.6955175995826721\n",
      "Epoch 139: Train Loss: 0.6875734686851501, Validation Loss: 0.6947735548019409\n",
      "Epoch 140: Train Loss: 0.6788378596305847, Validation Loss: 0.6941999197006226\n",
      "Epoch 141: Train Loss: 0.6544809818267823, Validation Loss: 0.6940237879753113\n",
      "Epoch 142: Train Loss: 0.7175869941711426, Validation Loss: 0.6935625672340393\n",
      "Epoch 143: Train Loss: 0.659687626361847, Validation Loss: 0.6932200193405151\n",
      "Epoch 144: Train Loss: 0.6715673327445983, Validation Loss: 0.6929289102554321\n",
      "Epoch 145: Train Loss: 0.6797860145568848, Validation Loss: 0.6928456425666809\n",
      "Epoch 146: Train Loss: 0.7518916487693786, Validation Loss: 0.6925447583198547\n",
      "Epoch 147: Train Loss: 0.7011207461357116, Validation Loss: 0.6919140815734863\n",
      "Epoch 148: Train Loss: 0.674314308166504, Validation Loss: 0.6925415992736816\n",
      "Epoch 149: Train Loss: 0.6934280633926392, Validation Loss: 0.69206702709198\n",
      "Epoch 150: Train Loss: 0.6980845689773559, Validation Loss: 0.6918801665306091\n",
      "Epoch 151: Train Loss: 0.7481942415237427, Validation Loss: 0.6919166445732117\n",
      "Epoch 152: Train Loss: 0.7003013610839843, Validation Loss: 0.691694438457489\n",
      "Epoch 153: Train Loss: 0.7156940817832946, Validation Loss: 0.6913377046585083\n",
      "Epoch 154: Train Loss: 0.7337106704711914, Validation Loss: 0.6912800669670105\n",
      "Epoch 155: Train Loss: 0.6841193318367005, Validation Loss: 0.6909671425819397\n",
      "Epoch 156: Train Loss: 0.6736819982528687, Validation Loss: 0.6909467577934265\n",
      "Epoch 157: Train Loss: 0.6861792206764221, Validation Loss: 0.6909529566764832\n",
      "Epoch 158: Train Loss: 0.6736581087112427, Validation Loss: 0.6909934878349304\n",
      "Epoch 159: Train Loss: 0.6535848736763, Validation Loss: 0.6909594535827637\n",
      "Epoch 160: Train Loss: 0.7097110152244568, Validation Loss: 0.6904639601707458\n",
      "Epoch 161: Train Loss: 0.6905338168144226, Validation Loss: 0.6906973719596863\n",
      "Epoch 162: Train Loss: 0.7025604367256164, Validation Loss: 0.6905171275138855\n",
      "Epoch 163: Train Loss: 0.6865918040275574, Validation Loss: 0.6902115941047668\n",
      "Epoch 164: Train Loss: 0.6868607401847839, Validation Loss: 0.6906225085258484\n",
      "Epoch 165: Train Loss: 0.6971516132354736, Validation Loss: 0.6905648708343506\n",
      "Epoch 166: Train Loss: 0.6864489436149597, Validation Loss: 0.6905903816223145\n",
      "Epoch 167: Train Loss: 0.6871044516563416, Validation Loss: 0.6905168294906616\n",
      "Epoch 168: Train Loss: 0.6910792350769043, Validation Loss: 0.6902402639389038\n",
      "Epoch 169: Train Loss: 0.7074225902557373, Validation Loss: 0.6902561187744141\n",
      "Epoch 170: Train Loss: 0.6729333162307739, Validation Loss: 0.689990222454071\n",
      "Epoch 171: Train Loss: 0.6996672034263611, Validation Loss: 0.6898272037506104\n",
      "Epoch 172: Train Loss: 0.739827561378479, Validation Loss: 0.6899766325950623\n",
      "Epoch 173: Train Loss: 0.7046370506286621, Validation Loss: 0.6897479295730591\n",
      "Epoch 174: Train Loss: 0.6796781659126282, Validation Loss: 0.689448893070221\n",
      "Epoch 175: Train Loss: 0.6526185512542725, Validation Loss: 0.6894000172615051\n",
      "Epoch 176: Train Loss: 0.6121923983097076, Validation Loss: 0.6890949010848999\n",
      "Epoch 177: Train Loss: 0.6893468141555786, Validation Loss: 0.6890146136283875\n",
      "Epoch 178: Train Loss: 0.6968543648719787, Validation Loss: 0.6890697479248047\n",
      "Epoch 179: Train Loss: 0.6571817398071289, Validation Loss: 0.6890770196914673\n",
      "Epoch 180: Train Loss: 0.67233647108078, Validation Loss: 0.6883783340454102\n",
      "Epoch 181: Train Loss: 0.6414963245391846, Validation Loss: 0.6885863542556763\n",
      "Epoch 182: Train Loss: 0.6799363493919373, Validation Loss: 0.6883404850959778\n",
      "Epoch 183: Train Loss: 0.672078549861908, Validation Loss: 0.6879034042358398\n",
      "Epoch 184: Train Loss: 0.7068798065185546, Validation Loss: 0.6880151629447937\n",
      "Epoch 185: Train Loss: 0.6404735445976257, Validation Loss: 0.6879744529724121\n",
      "Epoch 186: Train Loss: 0.6548494458198547, Validation Loss: 0.6880480647087097\n",
      "Epoch 187: Train Loss: 0.6297656178474427, Validation Loss: 0.688115119934082\n",
      "Epoch 188: Train Loss: 0.678222393989563, Validation Loss: 0.6879404783248901\n",
      "Epoch 189: Train Loss: 0.6785968542098999, Validation Loss: 0.6875810623168945\n",
      "Epoch 190: Train Loss: 0.703027892112732, Validation Loss: 0.6874358654022217\n",
      "Epoch 191: Train Loss: 0.6424813389778137, Validation Loss: 0.6870047450065613\n",
      "Epoch 192: Train Loss: 0.7029492616653442, Validation Loss: 0.6868604421615601\n",
      "Epoch 193: Train Loss: 0.6898990392684936, Validation Loss: 0.6863380670547485\n",
      "Epoch 194: Train Loss: 0.6647879958152771, Validation Loss: 0.686096727848053\n",
      "Epoch 195: Train Loss: 0.6406416773796082, Validation Loss: 0.6856789588928223\n",
      "Epoch 196: Train Loss: 0.6927040934562683, Validation Loss: 0.6853635311126709\n",
      "Epoch 197: Train Loss: 0.6334088385105133, Validation Loss: 0.685734748840332\n",
      "Epoch 198: Train Loss: 0.6342617511749268, Validation Loss: 0.6854898929595947\n",
      "Epoch 199: Train Loss: 0.6561792135238648, Validation Loss: 0.6853834986686707\n",
      "Epoch 200: Train Loss: 0.6332707285881043, Validation Loss: 0.6844409704208374\n",
      "Epoch 201: Train Loss: 0.6446204900741577, Validation Loss: 0.6843380928039551\n",
      "Epoch 202: Train Loss: 0.6261921405792237, Validation Loss: 0.6847228407859802\n",
      "Epoch 203: Train Loss: 0.7115772128105163, Validation Loss: 0.6841617822647095\n",
      "Epoch 204: Train Loss: 0.6804649829864502, Validation Loss: 0.6836921572685242\n",
      "Epoch 205: Train Loss: 0.6456253170967102, Validation Loss: 0.6835898756980896\n",
      "Epoch 206: Train Loss: 0.7011428475379944, Validation Loss: 0.6834477782249451\n",
      "Epoch 207: Train Loss: 0.6677053809165955, Validation Loss: 0.6831257939338684\n",
      "Epoch 208: Train Loss: 0.6670397520065308, Validation Loss: 0.6828521490097046\n",
      "Epoch 209: Train Loss: 0.6559263706207276, Validation Loss: 0.6826295256614685\n",
      "Epoch 210: Train Loss: 0.6687731146812439, Validation Loss: 0.6816182732582092\n",
      "Epoch 211: Train Loss: 0.6870215773582459, Validation Loss: 0.6808063387870789\n",
      "Epoch 212: Train Loss: 0.6506048798561096, Validation Loss: 0.6809247136116028\n",
      "Epoch 213: Train Loss: 0.6662438035011291, Validation Loss: 0.6797579526901245\n",
      "Epoch 214: Train Loss: 0.668589198589325, Validation Loss: 0.6795970797538757\n",
      "Epoch 215: Train Loss: 0.6964850902557373, Validation Loss: 0.6789020299911499\n",
      "Epoch 216: Train Loss: 0.6725469589233398, Validation Loss: 0.67897629737854\n",
      "Epoch 217: Train Loss: 0.643598449230194, Validation Loss: 0.6786346435546875\n",
      "Epoch 218: Train Loss: 0.7068399786949158, Validation Loss: 0.6785737872123718\n",
      "Epoch 219: Train Loss: 0.6774204254150391, Validation Loss: 0.6787668466567993\n",
      "Epoch 220: Train Loss: 0.6614816427230835, Validation Loss: 0.6784998178482056\n",
      "Epoch 221: Train Loss: 0.682148551940918, Validation Loss: 0.678365170955658\n",
      "Epoch 222: Train Loss: 0.655968165397644, Validation Loss: 0.6777356863021851\n",
      "Epoch 223: Train Loss: 0.6391510605812073, Validation Loss: 0.6772649884223938\n",
      "Epoch 224: Train Loss: 0.6830097556114196, Validation Loss: 0.6765214800834656\n",
      "Epoch 225: Train Loss: 0.6505636930465698, Validation Loss: 0.6761755347251892\n",
      "Epoch 226: Train Loss: 0.6681625723838807, Validation Loss: 0.675289511680603\n",
      "Epoch 227: Train Loss: 0.6620395302772522, Validation Loss: 0.6748813390731812\n",
      "Epoch 228: Train Loss: 0.6417942643165588, Validation Loss: 0.6752800941467285\n",
      "Epoch 229: Train Loss: 0.6315074801445008, Validation Loss: 0.6751655340194702\n",
      "Epoch 230: Train Loss: 0.6597187638282775, Validation Loss: 0.6740378141403198\n",
      "Epoch 231: Train Loss: 0.6851695775985718, Validation Loss: 0.6730804443359375\n",
      "Epoch 232: Train Loss: 0.7032195687294006, Validation Loss: 0.6722251772880554\n",
      "Epoch 233: Train Loss: 0.644929850101471, Validation Loss: 0.6708850264549255\n",
      "Epoch 234: Train Loss: 0.617893636226654, Validation Loss: 0.6703216433525085\n",
      "Epoch 235: Train Loss: 0.6531544208526612, Validation Loss: 0.6700036525726318\n",
      "Epoch 236: Train Loss: 0.6837298989295959, Validation Loss: 0.6693081855773926\n",
      "Epoch 237: Train Loss: 0.6608563899993897, Validation Loss: 0.6707822680473328\n",
      "Epoch 238: Train Loss: 0.6549107074737549, Validation Loss: 0.6694357395172119\n",
      "Epoch 239: Train Loss: 0.6899096369743347, Validation Loss: 0.6699033379554749\n",
      "Epoch 240: Train Loss: 0.6822301149368286, Validation Loss: 0.6685164570808411\n",
      "Epoch 241: Train Loss: 0.6484146595001221, Validation Loss: 0.6678248643875122\n",
      "Epoch 242: Train Loss: 0.6345117092132568, Validation Loss: 0.6682136654853821\n",
      "Epoch 243: Train Loss: 0.6638669848442078, Validation Loss: 0.6667689085006714\n",
      "Epoch 244: Train Loss: 0.6431771397590638, Validation Loss: 0.6657505035400391\n",
      "Epoch 245: Train Loss: 0.6296751856803894, Validation Loss: 0.665212094783783\n",
      "Epoch 246: Train Loss: 0.6324902653694153, Validation Loss: 0.6662825345993042\n",
      "Epoch 247: Train Loss: 0.6579577565193176, Validation Loss: 0.6653153300285339\n",
      "Epoch 248: Train Loss: 0.6665756583213807, Validation Loss: 0.6649231910705566\n",
      "Epoch 249: Train Loss: 0.6550033569335938, Validation Loss: 0.6640105843544006\n",
      "Epoch 250: Train Loss: 0.6659732818603515, Validation Loss: 0.6623300313949585\n",
      "Epoch 251: Train Loss: 0.6612297773361206, Validation Loss: 0.6615952849388123\n",
      "Epoch 252: Train Loss: 0.654528820514679, Validation Loss: 0.6605896949768066\n",
      "Epoch 253: Train Loss: 0.6066248953342438, Validation Loss: 0.6595516204833984\n",
      "Epoch 254: Train Loss: 0.6512293338775634, Validation Loss: 0.6572645902633667\n",
      "Epoch 255: Train Loss: 0.6884638905525208, Validation Loss: 0.656197726726532\n",
      "Epoch 256: Train Loss: 0.6587038636207581, Validation Loss: 0.655344545841217\n",
      "Epoch 257: Train Loss: 0.6584824204444886, Validation Loss: 0.6552002429962158\n",
      "Epoch 258: Train Loss: 0.6628355622291565, Validation Loss: 0.6550490856170654\n",
      "Epoch 259: Train Loss: 0.6692079305648804, Validation Loss: 0.6548486948013306\n",
      "Epoch 260: Train Loss: 0.7082330107688903, Validation Loss: 0.6526651978492737\n",
      "Epoch 261: Train Loss: 0.6434540152549744, Validation Loss: 0.651748538017273\n",
      "Epoch 262: Train Loss: 0.6495097279548645, Validation Loss: 0.6509142518043518\n",
      "Epoch 263: Train Loss: 0.6563994288444519, Validation Loss: 0.649703860282898\n",
      "Epoch 264: Train Loss: 0.6545100092887879, Validation Loss: 0.6479848623275757\n",
      "Epoch 265: Train Loss: 0.6252565622329712, Validation Loss: 0.646264910697937\n",
      "Epoch 266: Train Loss: 0.6459883213043213, Validation Loss: 0.6445695161819458\n",
      "Epoch 267: Train Loss: 0.6678892970085144, Validation Loss: 0.6443004608154297\n",
      "Epoch 268: Train Loss: 0.650638735294342, Validation Loss: 0.6441981792449951\n",
      "Epoch 269: Train Loss: 0.6062705278396606, Validation Loss: 0.6439638733863831\n",
      "Epoch 270: Train Loss: 0.6173152089118957, Validation Loss: 0.6413447856903076\n",
      "Epoch 271: Train Loss: 0.63746018409729, Validation Loss: 0.6401569247245789\n",
      "Epoch 272: Train Loss: 0.6456498861312866, Validation Loss: 0.6366704702377319\n",
      "Epoch 273: Train Loss: 0.6252745389938354, Validation Loss: 0.6329594850540161\n",
      "Epoch 274: Train Loss: 0.6417165517807006, Validation Loss: 0.6331681609153748\n",
      "Epoch 275: Train Loss: 0.6347665786743164, Validation Loss: 0.629801332950592\n",
      "Epoch 276: Train Loss: 0.671773624420166, Validation Loss: 0.6285592913627625\n",
      "Epoch 277: Train Loss: 0.6594490051269531, Validation Loss: 0.6273759603500366\n",
      "Epoch 278: Train Loss: 0.6416621327400207, Validation Loss: 0.6270648837089539\n",
      "Epoch 279: Train Loss: 0.6664591789245605, Validation Loss: 0.6275981664657593\n",
      "Epoch 280: Train Loss: 0.6199473023414612, Validation Loss: 0.6241030097007751\n",
      "Epoch 281: Train Loss: 0.6328750133514405, Validation Loss: 0.6224175095558167\n",
      "Epoch 282: Train Loss: 0.6763657450675964, Validation Loss: 0.6165037751197815\n",
      "Epoch 283: Train Loss: 0.6164297103881836, Validation Loss: 0.6132960915565491\n",
      "Epoch 284: Train Loss: 0.6769264698028564, Validation Loss: 0.6117493510246277\n",
      "Epoch 285: Train Loss: 0.6457260847091675, Validation Loss: 0.6098196506500244\n",
      "Epoch 286: Train Loss: 0.6514394760131836, Validation Loss: 0.6086885333061218\n",
      "Epoch 287: Train Loss: 0.7185442566871643, Validation Loss: 0.6077293157577515\n",
      "Epoch 288: Train Loss: 0.6688204169273376, Validation Loss: 0.6077988147735596\n",
      "Epoch 289: Train Loss: 0.6633731722831726, Validation Loss: 0.6075258255004883\n",
      "Epoch 290: Train Loss: 0.6526440739631653, Validation Loss: 0.6054194569587708\n",
      "Epoch 291: Train Loss: 0.6519046902656556, Validation Loss: 0.6033055186271667\n",
      "Epoch 292: Train Loss: 0.6209610581398011, Validation Loss: 0.5973714590072632\n",
      "Epoch 293: Train Loss: 0.627599835395813, Validation Loss: 0.5939532518386841\n",
      "Epoch 294: Train Loss: 0.5946427702903747, Validation Loss: 0.5890681743621826\n",
      "Epoch 295: Train Loss: 0.6076018035411834, Validation Loss: 0.5854408740997314\n",
      "Epoch 296: Train Loss: 0.6222355484962463, Validation Loss: 0.5882726311683655\n",
      "Epoch 297: Train Loss: 0.6631088614463806, Validation Loss: 0.5850942134857178\n",
      "Epoch 298: Train Loss: 0.6227217316627502, Validation Loss: 0.5830989480018616\n",
      "Epoch 299: Train Loss: 0.6229823589324951, Validation Loss: 0.5827013850212097\n",
      "Epoch 300: Train Loss: 0.6627768874168396, Validation Loss: 0.5776919722557068\n",
      "Epoch 301: Train Loss: 0.6371650695800781, Validation Loss: 0.574678897857666\n",
      "Epoch 302: Train Loss: 0.6093648672103882, Validation Loss: 0.5700533390045166\n",
      "Epoch 303: Train Loss: 0.5976485788822175, Validation Loss: 0.5651191473007202\n",
      "Epoch 304: Train Loss: 0.6290986895561218, Validation Loss: 0.5623001456260681\n",
      "Epoch 305: Train Loss: 0.6238866209983825, Validation Loss: 0.5604420900344849\n",
      "Epoch 306: Train Loss: 0.6429752588272095, Validation Loss: 0.5545709133148193\n",
      "Epoch 307: Train Loss: 0.6487253189086915, Validation Loss: 0.553088903427124\n",
      "Epoch 308: Train Loss: 0.6442741632461548, Validation Loss: 0.5547625422477722\n",
      "Epoch 309: Train Loss: 0.6502581238746643, Validation Loss: 0.5521071553230286\n",
      "Epoch 310: Train Loss: 0.6194220542907715, Validation Loss: 0.5455763339996338\n",
      "Epoch 311: Train Loss: 0.5579634726047515, Validation Loss: 0.5412278175354004\n",
      "Epoch 312: Train Loss: 0.5923471450805664, Validation Loss: 0.5341150760650635\n",
      "Epoch 313: Train Loss: 0.6443636298179627, Validation Loss: 0.519862711429596\n",
      "Epoch 314: Train Loss: 0.5877742409706116, Validation Loss: 0.5098879337310791\n",
      "Epoch 315: Train Loss: 0.6292762994766236, Validation Loss: 0.5054709315299988\n",
      "Epoch 316: Train Loss: 0.6109475374221802, Validation Loss: 0.4996524155139923\n",
      "Epoch 317: Train Loss: 0.640746808052063, Validation Loss: 0.49843528866767883\n",
      "Epoch 318: Train Loss: 0.6066139101982116, Validation Loss: 0.5015571117401123\n",
      "Epoch 319: Train Loss: 0.6010150790214539, Validation Loss: 0.4998818635940552\n",
      "Epoch 320: Train Loss: 0.6097040176391602, Validation Loss: 0.49473077058792114\n",
      "Epoch 321: Train Loss: 0.6357224345207214, Validation Loss: 0.49021193385124207\n",
      "Epoch 322: Train Loss: 0.6258239388465882, Validation Loss: 0.4864959418773651\n",
      "Epoch 323: Train Loss: 0.5843531250953674, Validation Loss: 0.479983925819397\n",
      "Epoch 324: Train Loss: 0.6379741549491882, Validation Loss: 0.47733351588249207\n",
      "Epoch 325: Train Loss: 0.612805986404419, Validation Loss: 0.4698217213153839\n",
      "Epoch 326: Train Loss: 0.6659753561019898, Validation Loss: 0.46370190382003784\n",
      "Epoch 327: Train Loss: 0.5862390995025635, Validation Loss: 0.4619090259075165\n",
      "Epoch 328: Train Loss: 0.6442604541778565, Validation Loss: 0.45903265476226807\n",
      "Epoch 329: Train Loss: 0.604429030418396, Validation Loss: 0.46282958984375\n",
      "Epoch 330: Train Loss: 0.6064789772033692, Validation Loss: 0.4625934362411499\n",
      "Epoch 331: Train Loss: 0.6165472149848938, Validation Loss: 0.4549752175807953\n",
      "Epoch 332: Train Loss: 0.6682979226112366, Validation Loss: 0.447261780500412\n",
      "Epoch 333: Train Loss: 0.5755563020706177, Validation Loss: 0.4420809745788574\n",
      "Epoch 334: Train Loss: 0.6077922701835632, Validation Loss: 0.43773823976516724\n",
      "Epoch 335: Train Loss: 0.6182162880897522, Validation Loss: 0.4320610761642456\n",
      "Epoch 336: Train Loss: 0.6381862163543701, Validation Loss: 0.42979851365089417\n",
      "Epoch 337: Train Loss: 0.579404205083847, Validation Loss: 0.43498438596725464\n",
      "Epoch 338: Train Loss: 0.6032078266143799, Validation Loss: 0.4333433210849762\n",
      "Epoch 339: Train Loss: 0.6043736934661865, Validation Loss: 0.4410666227340698\n",
      "Epoch 340: Train Loss: 0.610807204246521, Validation Loss: 0.4399740397930145\n",
      "Epoch 341: Train Loss: 0.6217863082885742, Validation Loss: 0.43996456265449524\n",
      "Epoch 342: Train Loss: 0.6143038153648377, Validation Loss: 0.43822699785232544\n",
      "Epoch 343: Train Loss: 0.5824247479438782, Validation Loss: 0.4419209659099579\n",
      "Epoch 344: Train Loss: 0.6339850187301636, Validation Loss: 0.4370693266391754\n",
      "Epoch 345: Train Loss: 0.632962453365326, Validation Loss: 0.4309145510196686\n",
      "Epoch 346: Train Loss: 0.6647323966026306, Validation Loss: 0.42693907022476196\n",
      "Epoch 347: Train Loss: 0.5686938047409058, Validation Loss: 0.43160247802734375\n",
      "Epoch 348: Train Loss: 0.6198300957679749, Validation Loss: 0.42969733476638794\n",
      "Epoch 349: Train Loss: 0.5921924352645874, Validation Loss: 0.42695966362953186\n",
      "Epoch 350: Train Loss: 0.5966304063796997, Validation Loss: 0.4207206666469574\n",
      "Epoch 351: Train Loss: 0.6207650184631348, Validation Loss: 0.417764276266098\n",
      "Epoch 352: Train Loss: 0.5721903443336487, Validation Loss: 0.4110926687717438\n",
      "Epoch 353: Train Loss: 0.6259052753448486, Validation Loss: 0.41124171018600464\n",
      "Epoch 354: Train Loss: 0.6488767147064209, Validation Loss: 0.41132399439811707\n",
      "Epoch 355: Train Loss: 0.5817245721817017, Validation Loss: 0.4218518137931824\n",
      "Epoch 356: Train Loss: 0.6082221746444703, Validation Loss: 0.4298161268234253\n",
      "Epoch 357: Train Loss: 0.5760542750358582, Validation Loss: 0.42477867007255554\n",
      "Epoch 358: Train Loss: 0.5549212098121643, Validation Loss: 0.42309680581092834\n",
      "Epoch 359: Train Loss: 0.6045138955116272, Validation Loss: 0.4186304807662964\n",
      "Epoch 360: Train Loss: 0.536537891626358, Validation Loss: 0.41356655955314636\n",
      "Epoch 361: Train Loss: 0.5715576171875, Validation Loss: 0.4117088317871094\n",
      "Epoch 362: Train Loss: 0.6329295635223389, Validation Loss: 0.4016389846801758\n",
      "Epoch 363: Train Loss: 0.5903369903564453, Validation Loss: 0.3980541229248047\n",
      "Epoch 364: Train Loss: 0.5470439910888671, Validation Loss: 0.38832640647888184\n",
      "Epoch 365: Train Loss: 0.5882607340812683, Validation Loss: 0.38290977478027344\n",
      "Epoch 366: Train Loss: 0.5429526567459106, Validation Loss: 0.38414502143859863\n",
      "Epoch 367: Train Loss: 0.5964883446693421, Validation Loss: 0.38051220774650574\n",
      "Epoch 368: Train Loss: 0.6175007820129395, Validation Loss: 0.3745640814304352\n",
      "Epoch 369: Train Loss: 0.5602526128292084, Validation Loss: 0.37023600935935974\n",
      "Epoch 370: Train Loss: 0.567151015996933, Validation Loss: 0.3633381128311157\n",
      "Epoch 371: Train Loss: 0.5574889719486237, Validation Loss: 0.35392510890960693\n",
      "Epoch 372: Train Loss: 0.5419892370700836, Validation Loss: 0.3532071113586426\n",
      "Epoch 373: Train Loss: 0.5779053688049316, Validation Loss: 0.3444363474845886\n",
      "Epoch 374: Train Loss: 0.5372613787651062, Validation Loss: 0.3437318503856659\n",
      "Epoch 375: Train Loss: 0.5577661275863648, Validation Loss: 0.34039628505706787\n",
      "Epoch 376: Train Loss: 0.6002786755561829, Validation Loss: 0.3379151225090027\n",
      "Epoch 377: Train Loss: 0.6414867043495178, Validation Loss: 0.3374670743942261\n",
      "Epoch 378: Train Loss: 0.5652097582817077, Validation Loss: 0.3391401171684265\n",
      "Epoch 379: Train Loss: 0.5792332053184509, Validation Loss: 0.3400138318538666\n",
      "Epoch 380: Train Loss: 0.5620524764060975, Validation Loss: 0.3411535620689392\n",
      "Epoch 381: Train Loss: 0.6145861446857452, Validation Loss: 0.3336317539215088\n",
      "Epoch 382: Train Loss: 0.5435667335987091, Validation Loss: 0.334897518157959\n",
      "Epoch 383: Train Loss: 0.5458244681358337, Validation Loss: 0.3352367579936981\n",
      "Epoch 384: Train Loss: 0.5487882375717164, Validation Loss: 0.3343031406402588\n",
      "Epoch 385: Train Loss: 0.5175008594989776, Validation Loss: 0.3292171061038971\n",
      "Epoch 386: Train Loss: 0.5315491616725921, Validation Loss: 0.3217715620994568\n",
      "Epoch 387: Train Loss: 0.5298016786575317, Validation Loss: 0.324776291847229\n",
      "Epoch 388: Train Loss: 0.5717869162559509, Validation Loss: 0.3222813904285431\n",
      "Epoch 389: Train Loss: 0.6021523237228393, Validation Loss: 0.3242007791996002\n",
      "Epoch 390: Train Loss: 0.4899808704853058, Validation Loss: 0.32125651836395264\n",
      "Epoch 391: Train Loss: 0.5139723300933838, Validation Loss: 0.32002174854278564\n",
      "Epoch 392: Train Loss: 0.6063490390777588, Validation Loss: 0.3268231749534607\n",
      "Epoch 393: Train Loss: 0.5307239413261413, Validation Loss: 0.3229515850543976\n",
      "Epoch 394: Train Loss: 0.5502460122108459, Validation Loss: 0.3209787607192993\n",
      "Epoch 395: Train Loss: 0.6254943490028382, Validation Loss: 0.3157845437526703\n",
      "Epoch 396: Train Loss: 0.5967940390110016, Validation Loss: 0.31534361839294434\n",
      "Epoch 397: Train Loss: 0.5993455648422241, Validation Loss: 0.31793394684791565\n",
      "Epoch 398: Train Loss: 0.5899936556816101, Validation Loss: 0.3244610130786896\n",
      "Epoch 399: Train Loss: 0.5889616847038269, Validation Loss: 0.3212897479534149\n",
      "Epoch 400: Train Loss: 0.5788587272167206, Validation Loss: 0.3195834159851074\n",
      "Epoch 401: Train Loss: 0.5565260052680969, Validation Loss: 0.3146452307701111\n",
      "Epoch 402: Train Loss: 0.6206209659576416, Validation Loss: 0.31488120555877686\n",
      "Epoch 403: Train Loss: 0.538983291387558, Validation Loss: 0.3148846924304962\n",
      "Epoch 404: Train Loss: 0.510875117778778, Validation Loss: 0.3114226162433624\n",
      "Epoch 405: Train Loss: 0.5235826730728149, Validation Loss: 0.3166045546531677\n",
      "Epoch 406: Train Loss: 0.5207253754138946, Validation Loss: 0.3172443211078644\n",
      "Epoch 407: Train Loss: 0.5569413900375366, Validation Loss: 0.3198508024215698\n",
      "Epoch 408: Train Loss: 0.5463984608650208, Validation Loss: 0.3105468153953552\n",
      "Epoch 409: Train Loss: 0.552667248249054, Validation Loss: 0.31183964014053345\n",
      "Epoch 410: Train Loss: 0.5831122159957886, Validation Loss: 0.30803436040878296\n",
      "Epoch 411: Train Loss: 0.5121144533157349, Validation Loss: 0.31440427899360657\n",
      "Epoch 412: Train Loss: 0.6402337729930878, Validation Loss: 0.31723880767822266\n",
      "Epoch 413: Train Loss: 0.5991946935653687, Validation Loss: 0.32448306679725647\n",
      "Epoch 414: Train Loss: 0.5395746111869812, Validation Loss: 0.33043766021728516\n",
      "Epoch 415: Train Loss: 0.5547183632850647, Validation Loss: 0.33248385787010193\n",
      "Epoch 416: Train Loss: 0.4988503098487854, Validation Loss: 0.32608988881111145\n",
      "Epoch 417: Train Loss: 0.5616655111312866, Validation Loss: 0.3213251829147339\n",
      "Epoch 418: Train Loss: 0.5369505286216736, Validation Loss: 0.320617139339447\n",
      "Epoch 419: Train Loss: 0.5723961591720581, Validation Loss: 0.3160012662410736\n",
      "Epoch 420: Train Loss: 0.5396604359149932, Validation Loss: 0.31770285964012146\n",
      "Epoch 421: Train Loss: 0.4645288735628128, Validation Loss: 0.29770132899284363\n",
      "Epoch 422: Train Loss: 0.5411806464195251, Validation Loss: 0.28908395767211914\n",
      "Epoch 423: Train Loss: 0.5494058132171631, Validation Loss: 0.2778977155685425\n",
      "Epoch 424: Train Loss: 0.5213487863540649, Validation Loss: 0.2861030697822571\n",
      "Epoch 425: Train Loss: 0.5058942675590515, Validation Loss: 0.2858535051345825\n",
      "Epoch 426: Train Loss: 0.4403496325016022, Validation Loss: 0.28177082538604736\n",
      "Epoch 427: Train Loss: 0.5112148880958557, Validation Loss: 0.28979504108428955\n",
      "Epoch 428: Train Loss: 0.58303102850914, Validation Loss: 0.27957019209861755\n",
      "Epoch 429: Train Loss: 0.5712242424488068, Validation Loss: 0.2739546597003937\n",
      "Epoch 430: Train Loss: 0.5492020845413208, Validation Loss: 0.27428409457206726\n",
      "Epoch 431: Train Loss: 0.4805168151855469, Validation Loss: 0.2711828052997589\n",
      "Epoch 432: Train Loss: 0.5122438371181488, Validation Loss: 0.27013885974884033\n",
      "Epoch 433: Train Loss: 0.4847773313522339, Validation Loss: 0.26760464906692505\n",
      "Epoch 434: Train Loss: 0.47770447134971616, Validation Loss: 0.2728077173233032\n",
      "Epoch 435: Train Loss: 0.5050786435604095, Validation Loss: 0.27491697669029236\n",
      "Epoch 436: Train Loss: 0.5360259473323822, Validation Loss: 0.26584985852241516\n",
      "Epoch 437: Train Loss: 0.5072444617748261, Validation Loss: 0.2597655951976776\n",
      "Epoch 438: Train Loss: 0.5211011886596679, Validation Loss: 0.27201566100120544\n",
      "Epoch 439: Train Loss: 0.5066140234470368, Validation Loss: 0.2701399028301239\n",
      "Epoch 440: Train Loss: 0.4594491541385651, Validation Loss: 0.26617395877838135\n",
      "Epoch 441: Train Loss: 0.4952096164226532, Validation Loss: 0.26434823870658875\n",
      "Epoch 442: Train Loss: 0.4680962860584259, Validation Loss: 0.25969982147216797\n",
      "Epoch 443: Train Loss: 0.5497138023376464, Validation Loss: 0.25138935446739197\n",
      "Epoch 444: Train Loss: 0.5055339753627777, Validation Loss: 0.25303637981414795\n",
      "Epoch 445: Train Loss: 0.4792726278305054, Validation Loss: 0.25139138102531433\n",
      "Epoch 446: Train Loss: 0.4578494757413864, Validation Loss: 0.24739442765712738\n",
      "Epoch 447: Train Loss: 0.5112621366977692, Validation Loss: 0.2473836988210678\n",
      "Epoch 448: Train Loss: 0.5162919402122498, Validation Loss: 0.25159552693367004\n",
      "Epoch 449: Train Loss: 0.5338496148586274, Validation Loss: 0.2494700402021408\n",
      "Epoch 450: Train Loss: 0.4832404315471649, Validation Loss: 0.24840740859508514\n",
      "Epoch 451: Train Loss: 0.48075629472732545, Validation Loss: 0.24693502485752106\n",
      "Epoch 452: Train Loss: 0.5450137972831726, Validation Loss: 0.2406810224056244\n",
      "Epoch 453: Train Loss: 0.4650743782520294, Validation Loss: 0.23841695487499237\n",
      "Epoch 454: Train Loss: 0.5305723309516907, Validation Loss: 0.23911605775356293\n",
      "Epoch 455: Train Loss: 0.515400630235672, Validation Loss: 0.24445809423923492\n",
      "Epoch 456: Train Loss: 0.4771786093711853, Validation Loss: 0.23240002989768982\n",
      "Epoch 457: Train Loss: 0.45072769522666933, Validation Loss: 0.2311103343963623\n",
      "Epoch 458: Train Loss: 0.5110817372798919, Validation Loss: 0.2466721534729004\n",
      "Epoch 459: Train Loss: 0.505491292476654, Validation Loss: 0.2380301058292389\n",
      "Epoch 460: Train Loss: 0.5335979759693146, Validation Loss: 0.2475300431251526\n",
      "Epoch 461: Train Loss: 0.48576163649559023, Validation Loss: 0.2517507076263428\n",
      "Epoch 462: Train Loss: 0.45777348279953, Validation Loss: 0.24314123392105103\n",
      "Epoch 463: Train Loss: 0.43961669206619264, Validation Loss: 0.2353501319885254\n",
      "Epoch 464: Train Loss: 0.4945178210735321, Validation Loss: 0.23516005277633667\n",
      "Epoch 465: Train Loss: 0.5433792173862457, Validation Loss: 0.24173502624034882\n",
      "Epoch 466: Train Loss: 0.4730191767215729, Validation Loss: 0.23697274923324585\n",
      "Epoch 467: Train Loss: 0.4721100628376007, Validation Loss: 0.22982639074325562\n",
      "Epoch 468: Train Loss: 0.4854004800319672, Validation Loss: 0.23251238465309143\n",
      "Epoch 469: Train Loss: 0.5527585029602051, Validation Loss: 0.22327621281147003\n",
      "Epoch 470: Train Loss: 0.4693059027194977, Validation Loss: 0.2227877974510193\n",
      "Epoch 471: Train Loss: 0.48943339586257933, Validation Loss: 0.21982410550117493\n",
      "Epoch 472: Train Loss: 0.47168527245521547, Validation Loss: 0.22153165936470032\n",
      "Epoch 473: Train Loss: 0.5355508744716644, Validation Loss: 0.22043880820274353\n",
      "Epoch 474: Train Loss: 0.546780526638031, Validation Loss: 0.21549586951732635\n",
      "Epoch 475: Train Loss: 0.4709886133670807, Validation Loss: 0.21715329587459564\n",
      "Epoch 476: Train Loss: 0.5002613544464112, Validation Loss: 0.2222931832075119\n",
      "Epoch 477: Train Loss: 0.4825435221195221, Validation Loss: 0.22659415006637573\n",
      "Epoch 478: Train Loss: 0.42179792523384096, Validation Loss: 0.22499476373195648\n",
      "Epoch 479: Train Loss: 0.4658400177955627, Validation Loss: 0.22963617742061615\n",
      "Epoch 480: Train Loss: 0.4888797402381897, Validation Loss: 0.23531141877174377\n",
      "Epoch 481: Train Loss: 0.4653383374214172, Validation Loss: 0.2191007435321808\n",
      "Epoch 482: Train Loss: 0.4951609313488007, Validation Loss: 0.23067492246627808\n",
      "Epoch 483: Train Loss: 0.4837500393390656, Validation Loss: 0.2221878618001938\n",
      "Epoch 484: Train Loss: 0.4785930097103119, Validation Loss: 0.22193045914173126\n",
      "Epoch 485: Train Loss: 0.4836245059967041, Validation Loss: 0.2262209802865982\n",
      "Epoch 486: Train Loss: 0.46996610164642333, Validation Loss: 0.2209596186876297\n",
      "Epoch 487: Train Loss: 0.45371496081352236, Validation Loss: 0.23393572866916656\n",
      "Epoch 488: Train Loss: 0.4379680037498474, Validation Loss: 0.22765015065670013\n",
      "Epoch 489: Train Loss: 0.42479555010795594, Validation Loss: 0.21598751842975616\n",
      "Epoch 490: Train Loss: 0.5625705599784852, Validation Loss: 0.20450443029403687\n",
      "Epoch 491: Train Loss: 0.46123974323272704, Validation Loss: 0.21404996514320374\n",
      "Epoch 492: Train Loss: 0.45818973779678346, Validation Loss: 0.22144640982151031\n",
      "Epoch 493: Train Loss: 0.42645988464355467, Validation Loss: 0.22408707439899445\n",
      "Epoch 494: Train Loss: 0.43324395418167116, Validation Loss: 0.22497005760669708\n",
      "Epoch 495: Train Loss: 0.3852153718471527, Validation Loss: 0.21932600438594818\n",
      "Epoch 496: Train Loss: 0.4559681236743927, Validation Loss: 0.20815999805927277\n",
      "Epoch 497: Train Loss: 0.44748027324676515, Validation Loss: 0.21363863348960876\n",
      "Epoch 498: Train Loss: 0.5834623336791992, Validation Loss: 0.19826343655586243\n",
      "Epoch 499: Train Loss: 0.5438238441944122, Validation Loss: 0.19998550415039062\n",
      "Fold 8 Metrics:\n",
      "Accuracy: 1.0, Precision: 1.0, Recall: 1.0, F1-score: 1.0, AUC: 1.0\n",
      "Confusion Matrix:\n",
      "[[5 0]\n",
      " [0 6]]\n",
      "Completed fold 8\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples from subject 4 to test set\n",
      "Adding 6 truth samples from subject 4 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 500)\n",
      "(132, 65, 500)\n",
      "Epoch 0: Train Loss: 0.7120487451553345, Validation Loss: 0.698549211025238\n",
      "Epoch 1: Train Loss: 0.7036814212799072, Validation Loss: 0.7009595632553101\n",
      "Epoch 2: Train Loss: 0.712272298336029, Validation Loss: 0.703768253326416\n",
      "Epoch 3: Train Loss: 0.7100215554237366, Validation Loss: 0.706108570098877\n",
      "Epoch 4: Train Loss: 0.6835095763206482, Validation Loss: 0.7085610628128052\n",
      "Epoch 5: Train Loss: 0.723616623878479, Validation Loss: 0.7097840309143066\n",
      "Epoch 6: Train Loss: 0.6973469495773316, Validation Loss: 0.7116523385047913\n",
      "Epoch 7: Train Loss: 0.6556480169296265, Validation Loss: 0.712364673614502\n",
      "Epoch 8: Train Loss: 0.6942353248596191, Validation Loss: 0.7126513719558716\n",
      "Epoch 9: Train Loss: 0.7117880463600159, Validation Loss: 0.7134639620780945\n",
      "Epoch 10: Train Loss: 0.7103342056274414, Validation Loss: 0.7130887508392334\n",
      "Epoch 11: Train Loss: 0.7428013563156128, Validation Loss: 0.7131105661392212\n",
      "Epoch 12: Train Loss: 0.7398100733757019, Validation Loss: 0.7128483653068542\n",
      "Epoch 13: Train Loss: 0.7118008971214295, Validation Loss: 0.7125453948974609\n",
      "Epoch 14: Train Loss: 0.6902226686477662, Validation Loss: 0.7130742073059082\n",
      "Epoch 15: Train Loss: 0.7075776696205139, Validation Loss: 0.7129499912261963\n",
      "Epoch 16: Train Loss: 0.6722181916236878, Validation Loss: 0.712103545665741\n",
      "Epoch 17: Train Loss: 0.6981112241744996, Validation Loss: 0.710957944393158\n",
      "Epoch 18: Train Loss: 0.6599569857120514, Validation Loss: 0.7118659615516663\n",
      "Epoch 19: Train Loss: 0.7262974739074707, Validation Loss: 0.7113393545150757\n",
      "Epoch 20: Train Loss: 0.7097260236740113, Validation Loss: 0.7126897573471069\n",
      "Epoch 21: Train Loss: 0.6983888506889343, Validation Loss: 0.7125750780105591\n",
      "Epoch 22: Train Loss: 0.7211711645126343, Validation Loss: 0.7125381231307983\n",
      "Epoch 23: Train Loss: 0.660295283794403, Validation Loss: 0.7128546237945557\n",
      "Epoch 24: Train Loss: 0.6896888613700867, Validation Loss: 0.7114308476448059\n",
      "Epoch 25: Train Loss: 0.6985467791557312, Validation Loss: 0.7118924260139465\n",
      "Epoch 26: Train Loss: 0.6699053764343261, Validation Loss: 0.711465060710907\n",
      "Epoch 27: Train Loss: 0.7505003094673157, Validation Loss: 0.7121554613113403\n",
      "Epoch 28: Train Loss: 0.6679442167282105, Validation Loss: 0.7128480672836304\n",
      "Epoch 29: Train Loss: 0.7189088344573975, Validation Loss: 0.7115846276283264\n",
      "Epoch 30: Train Loss: 0.6739346146583557, Validation Loss: 0.7126106023788452\n",
      "Epoch 31: Train Loss: 0.6741896390914917, Validation Loss: 0.7125604748725891\n",
      "Epoch 32: Train Loss: 0.7442713499069213, Validation Loss: 0.713371992111206\n",
      "Epoch 33: Train Loss: 0.6971763014793396, Validation Loss: 0.7137511968612671\n",
      "Epoch 34: Train Loss: 0.7416910529136658, Validation Loss: 0.7137633562088013\n",
      "Epoch 35: Train Loss: 0.6805280327796936, Validation Loss: 0.7124254107475281\n",
      "Epoch 36: Train Loss: 0.7131833672523499, Validation Loss: 0.7133764028549194\n",
      "Epoch 37: Train Loss: 0.7018107175827026, Validation Loss: 0.7128362655639648\n",
      "Epoch 38: Train Loss: 0.6844567656517029, Validation Loss: 0.713214099407196\n",
      "Epoch 39: Train Loss: 0.6768509864807128, Validation Loss: 0.7136756777763367\n",
      "Epoch 40: Train Loss: 0.6786756992340088, Validation Loss: 0.7134494781494141\n",
      "Epoch 41: Train Loss: 0.7002189874649047, Validation Loss: 0.7131814956665039\n",
      "Epoch 42: Train Loss: 0.638597559928894, Validation Loss: 0.7130765914916992\n",
      "Epoch 43: Train Loss: 0.6495943665504456, Validation Loss: 0.7140973806381226\n",
      "Epoch 44: Train Loss: 0.696469497680664, Validation Loss: 0.7146947383880615\n",
      "Epoch 45: Train Loss: 0.681952714920044, Validation Loss: 0.7148841619491577\n",
      "Epoch 46: Train Loss: 0.729556679725647, Validation Loss: 0.7148976922035217\n",
      "Epoch 47: Train Loss: 0.7144241571426392, Validation Loss: 0.715179979801178\n",
      "Epoch 48: Train Loss: 0.6921225190162659, Validation Loss: 0.7152417898178101\n",
      "Epoch 49: Train Loss: 0.6950210452079773, Validation Loss: 0.7152873873710632\n",
      "Epoch 50: Train Loss: 0.7059854626655578, Validation Loss: 0.7148807048797607\n",
      "Epoch 51: Train Loss: 0.6259804606437683, Validation Loss: 0.7149729132652283\n",
      "Epoch 52: Train Loss: 0.659926176071167, Validation Loss: 0.71475750207901\n",
      "Epoch 53: Train Loss: 0.7251736998558045, Validation Loss: 0.715269923210144\n",
      "Epoch 54: Train Loss: 0.7139251232147217, Validation Loss: 0.7151536345481873\n",
      "Epoch 55: Train Loss: 0.7004890203475952, Validation Loss: 0.7148767113685608\n",
      "Epoch 56: Train Loss: 0.6794942617416382, Validation Loss: 0.7152682542800903\n",
      "Epoch 57: Train Loss: 0.700555169582367, Validation Loss: 0.7151975035667419\n",
      "Epoch 58: Train Loss: 0.6963632583618165, Validation Loss: 0.7157553434371948\n",
      "Epoch 59: Train Loss: 0.6995117664337158, Validation Loss: 0.7157549858093262\n",
      "Epoch 60: Train Loss: 0.6504505634307861, Validation Loss: 0.7162289023399353\n",
      "Epoch 61: Train Loss: 0.6884640455245972, Validation Loss: 0.7166009545326233\n",
      "Epoch 62: Train Loss: 0.706593644618988, Validation Loss: 0.7166442275047302\n",
      "Epoch 63: Train Loss: 0.6921539306640625, Validation Loss: 0.7163156867027283\n",
      "Epoch 64: Train Loss: 0.7091496706008911, Validation Loss: 0.7167760729789734\n",
      "Epoch 65: Train Loss: 0.7434093475341796, Validation Loss: 0.7161651253700256\n",
      "Epoch 66: Train Loss: 0.686831259727478, Validation Loss: 0.7156995534896851\n",
      "Epoch 67: Train Loss: 0.6723771572113038, Validation Loss: 0.7163095474243164\n",
      "Epoch 68: Train Loss: 0.7068512916564942, Validation Loss: 0.7160292863845825\n",
      "Epoch 69: Train Loss: 0.682602071762085, Validation Loss: 0.7160694003105164\n",
      "Epoch 70: Train Loss: 0.6394289493560791, Validation Loss: 0.7162755131721497\n",
      "Epoch 71: Train Loss: 0.6757862091064453, Validation Loss: 0.7144123911857605\n",
      "Epoch 72: Train Loss: 0.6838102698326111, Validation Loss: 0.7141212224960327\n",
      "Epoch 73: Train Loss: 0.657081937789917, Validation Loss: 0.7155436277389526\n",
      "Epoch 74: Train Loss: 0.6466379046440125, Validation Loss: 0.7169909477233887\n",
      "Epoch 75: Train Loss: 0.6858451962471008, Validation Loss: 0.7176856994628906\n",
      "Epoch 76: Train Loss: 0.6883575081825256, Validation Loss: 0.7179479598999023\n",
      "Epoch 77: Train Loss: 0.7042178273200989, Validation Loss: 0.717902660369873\n",
      "Epoch 78: Train Loss: 0.7203512907028198, Validation Loss: 0.7179528474807739\n",
      "Epoch 79: Train Loss: 0.6731544256210327, Validation Loss: 0.7181478142738342\n",
      "Epoch 80: Train Loss: 0.6971567392349243, Validation Loss: 0.7181991934776306\n",
      "Epoch 81: Train Loss: 0.6906783938407898, Validation Loss: 0.7186902761459351\n",
      "Epoch 82: Train Loss: 0.6770449876785278, Validation Loss: 0.7184538245201111\n",
      "Epoch 83: Train Loss: 0.7014638543128967, Validation Loss: 0.7183032035827637\n",
      "Epoch 84: Train Loss: 0.6990252137184143, Validation Loss: 0.7184221148490906\n",
      "Epoch 85: Train Loss: 0.6767041206359863, Validation Loss: 0.7182769179344177\n",
      "Epoch 86: Train Loss: 0.6774470925331115, Validation Loss: 0.7185325622558594\n",
      "Epoch 87: Train Loss: 0.6872063636779785, Validation Loss: 0.7173066139221191\n",
      "Epoch 88: Train Loss: 0.7028373003005981, Validation Loss: 0.718268096446991\n",
      "Epoch 89: Train Loss: 0.7065049290657044, Validation Loss: 0.7166741490364075\n",
      "Epoch 90: Train Loss: 0.6379209876060485, Validation Loss: 0.7185319662094116\n",
      "Epoch 91: Train Loss: 0.6978749871253968, Validation Loss: 0.7190869450569153\n",
      "Epoch 92: Train Loss: 0.6204077720642089, Validation Loss: 0.7176016569137573\n",
      "Epoch 93: Train Loss: 0.6792142152786255, Validation Loss: 0.7182573676109314\n",
      "Epoch 94: Train Loss: 0.6985092520713806, Validation Loss: 0.7178837656974792\n",
      "Epoch 95: Train Loss: 0.7199094533920288, Validation Loss: 0.7185203433036804\n",
      "Epoch 96: Train Loss: 0.677358865737915, Validation Loss: 0.718454122543335\n",
      "Epoch 97: Train Loss: 0.609247088432312, Validation Loss: 0.719072163105011\n",
      "Epoch 98: Train Loss: 0.6980268836021424, Validation Loss: 0.7196668386459351\n",
      "Epoch 99: Train Loss: 0.7011272549629212, Validation Loss: 0.7201829552650452\n",
      "Epoch 100: Train Loss: 0.6931232333183288, Validation Loss: 0.7200368046760559\n",
      "Epoch 101: Train Loss: 0.6604697346687317, Validation Loss: 0.7196545600891113\n",
      "Epoch 102: Train Loss: 0.6745092391967773, Validation Loss: 0.7198359370231628\n",
      "Epoch 103: Train Loss: 0.689007830619812, Validation Loss: 0.720088541507721\n",
      "Epoch 104: Train Loss: 0.6847791552543641, Validation Loss: 0.7209323048591614\n",
      "Epoch 105: Train Loss: 0.6228914797306061, Validation Loss: 0.7212026715278625\n",
      "Epoch 106: Train Loss: 0.6997581362724304, Validation Loss: 0.7196537256240845\n",
      "Epoch 107: Train Loss: 0.6566324949264526, Validation Loss: 0.7190933227539062\n",
      "Epoch 108: Train Loss: 0.6764366388320923, Validation Loss: 0.7202532291412354\n",
      "Epoch 109: Train Loss: 0.7132794737815857, Validation Loss: 0.7205513715744019\n",
      "Epoch 110: Train Loss: 0.6807745099067688, Validation Loss: 0.7210052609443665\n",
      "Epoch 111: Train Loss: 0.6707170248031616, Validation Loss: 0.7206618785858154\n",
      "Epoch 112: Train Loss: 0.6519324302673339, Validation Loss: 0.7213073968887329\n",
      "Epoch 113: Train Loss: 0.6643593072891235, Validation Loss: 0.721217155456543\n",
      "Epoch 114: Train Loss: 0.6819860458374023, Validation Loss: 0.7200466394424438\n",
      "Epoch 115: Train Loss: 0.6959133982658386, Validation Loss: 0.7203906178474426\n",
      "Epoch 116: Train Loss: 0.6644488453865052, Validation Loss: 0.7213645577430725\n",
      "Epoch 117: Train Loss: 0.6532534003257752, Validation Loss: 0.7213032245635986\n",
      "Epoch 118: Train Loss: 0.650692617893219, Validation Loss: 0.721233069896698\n",
      "Epoch 119: Train Loss: 0.6575099945068359, Validation Loss: 0.7220877408981323\n",
      "Epoch 120: Train Loss: 0.7000816941261292, Validation Loss: 0.7225052118301392\n",
      "Epoch 121: Train Loss: 0.7068768858909606, Validation Loss: 0.7230111956596375\n",
      "Epoch 122: Train Loss: 0.6525015950202941, Validation Loss: 0.7225704193115234\n",
      "Epoch 123: Train Loss: 0.6978402495384216, Validation Loss: 0.7221753001213074\n",
      "Epoch 124: Train Loss: 0.7031345248222352, Validation Loss: 0.7217493057250977\n",
      "Epoch 125: Train Loss: 0.6599195003509521, Validation Loss: 0.7226834893226624\n",
      "Epoch 126: Train Loss: 0.69748854637146, Validation Loss: 0.7208710312843323\n",
      "Epoch 127: Train Loss: 0.6495483875274658, Validation Loss: 0.7212715148925781\n",
      "Epoch 128: Train Loss: 0.6476853728294373, Validation Loss: 0.7222406268119812\n",
      "Epoch 129: Train Loss: 0.6806851744651794, Validation Loss: 0.7202994227409363\n",
      "Epoch 130: Train Loss: 0.6523871302604676, Validation Loss: 0.721851646900177\n",
      "Epoch 131: Train Loss: 0.6667271375656127, Validation Loss: 0.7232262492179871\n",
      "Epoch 132: Train Loss: 0.6666975140571594, Validation Loss: 0.7232028841972351\n",
      "Epoch 133: Train Loss: 0.668146800994873, Validation Loss: 0.7232730984687805\n",
      "Epoch 134: Train Loss: 0.6572229862213135, Validation Loss: 0.7241694331169128\n",
      "Epoch 135: Train Loss: 0.68504478931427, Validation Loss: 0.7234547734260559\n",
      "Epoch 136: Train Loss: 0.640247642993927, Validation Loss: 0.7228566408157349\n",
      "Epoch 137: Train Loss: 0.6608279585838318, Validation Loss: 0.7239488959312439\n",
      "Epoch 138: Train Loss: 0.6989687800407409, Validation Loss: 0.7243574261665344\n",
      "Epoch 139: Train Loss: 0.6379810333251953, Validation Loss: 0.7229416966438293\n",
      "Epoch 140: Train Loss: 0.6387226641178131, Validation Loss: 0.7217742800712585\n",
      "Epoch 141: Train Loss: 0.6807962894439697, Validation Loss: 0.7233724594116211\n",
      "Epoch 142: Train Loss: 0.6725473761558532, Validation Loss: 0.7239232659339905\n",
      "Epoch 143: Train Loss: 0.6506006121635437, Validation Loss: 0.7248086929321289\n",
      "Epoch 144: Train Loss: 0.6350010395050049, Validation Loss: 0.722858726978302\n",
      "Epoch 145: Train Loss: 0.6654256582260132, Validation Loss: 0.7242575287818909\n",
      "Epoch 146: Train Loss: 0.6666905879974365, Validation Loss: 0.724862813949585\n",
      "Epoch 147: Train Loss: 0.640120005607605, Validation Loss: 0.7261033654212952\n",
      "Epoch 148: Train Loss: 0.6432062387466431, Validation Loss: 0.7267382740974426\n",
      "Epoch 149: Train Loss: 0.6687453627586365, Validation Loss: 0.7253691554069519\n",
      "Epoch 150: Train Loss: 0.6220493674278259, Validation Loss: 0.7244743704795837\n",
      "Epoch 151: Train Loss: 0.6179235339164734, Validation Loss: 0.7255032062530518\n",
      "Epoch 152: Train Loss: 0.6639318704605103, Validation Loss: 0.7276261448860168\n",
      "Epoch 153: Train Loss: 0.6435805678367614, Validation Loss: 0.7281758189201355\n",
      "Epoch 154: Train Loss: 0.6588302850723267, Validation Loss: 0.7298628687858582\n",
      "Epoch 155: Train Loss: 0.6397611439228058, Validation Loss: 0.7298176884651184\n",
      "Epoch 156: Train Loss: 0.6844304203987122, Validation Loss: 0.7270668148994446\n",
      "Epoch 157: Train Loss: 0.676041042804718, Validation Loss: 0.7271844744682312\n",
      "Epoch 158: Train Loss: 0.6951681017875672, Validation Loss: 0.72772216796875\n",
      "Epoch 159: Train Loss: 0.6235538840293884, Validation Loss: 0.7279853820800781\n",
      "Epoch 160: Train Loss: 0.6542508244514466, Validation Loss: 0.7289921641349792\n",
      "Epoch 161: Train Loss: 0.6186416506767273, Validation Loss: 0.7279253005981445\n",
      "Epoch 162: Train Loss: 0.6472940087318421, Validation Loss: 0.7281977534294128\n",
      "Epoch 163: Train Loss: 0.6112728118896484, Validation Loss: 0.7289538383483887\n",
      "Epoch 164: Train Loss: 0.6496049404144287, Validation Loss: 0.7300809025764465\n",
      "Epoch 165: Train Loss: 0.6214027881622315, Validation Loss: 0.7296657562255859\n",
      "Epoch 166: Train Loss: 0.6752137780189514, Validation Loss: 0.7305102348327637\n",
      "Epoch 167: Train Loss: 0.6513310194015502, Validation Loss: 0.7301547527313232\n",
      "Epoch 168: Train Loss: 0.6611311197280884, Validation Loss: 0.7309030294418335\n",
      "Epoch 169: Train Loss: 0.6615433692932129, Validation Loss: 0.7321121096611023\n",
      "Epoch 170: Train Loss: 0.6777691125869751, Validation Loss: 0.7324802279472351\n",
      "Epoch 171: Train Loss: 0.6114713311195373, Validation Loss: 0.7329937815666199\n",
      "Epoch 172: Train Loss: 0.6621988534927368, Validation Loss: 0.7333303093910217\n",
      "Epoch 173: Train Loss: 0.6483444333076477, Validation Loss: 0.7328723669052124\n",
      "Epoch 174: Train Loss: 0.6915756225585937, Validation Loss: 0.7339387536048889\n",
      "Epoch 175: Train Loss: 0.6554792165756226, Validation Loss: 0.7342135310173035\n",
      "Epoch 176: Train Loss: 0.6175472736358643, Validation Loss: 0.7334578633308411\n",
      "Epoch 177: Train Loss: 0.6183217406272888, Validation Loss: 0.7337548136711121\n",
      "Epoch 178: Train Loss: 0.6728837132453919, Validation Loss: 0.731904149055481\n",
      "Epoch 179: Train Loss: 0.6268127858638763, Validation Loss: 0.7296426296234131\n",
      "Epoch 180: Train Loss: 0.6377463459968566, Validation Loss: 0.7313537001609802\n",
      "Epoch 181: Train Loss: 0.7215336918830871, Validation Loss: 0.7328750491142273\n",
      "Epoch 182: Train Loss: 0.6145248651504517, Validation Loss: 0.7323464751243591\n",
      "Epoch 183: Train Loss: 0.6990532517433167, Validation Loss: 0.7351664900779724\n",
      "Epoch 184: Train Loss: 0.6403363108634949, Validation Loss: 0.7367629408836365\n",
      "Epoch 185: Train Loss: 0.7030935049057007, Validation Loss: 0.7379769682884216\n",
      "Epoch 186: Train Loss: 0.6871162533760071, Validation Loss: 0.739236056804657\n",
      "Epoch 187: Train Loss: 0.6429291725158691, Validation Loss: 0.7386295199394226\n",
      "Epoch 188: Train Loss: 0.6419076085090637, Validation Loss: 0.7390023469924927\n",
      "Epoch 189: Train Loss: 0.6357949256896973, Validation Loss: 0.7374925017356873\n",
      "Epoch 190: Train Loss: 0.6620402932167053, Validation Loss: 0.739039421081543\n",
      "Epoch 191: Train Loss: 0.6627115607261658, Validation Loss: 0.7399192452430725\n",
      "Epoch 192: Train Loss: 0.6137428164482117, Validation Loss: 0.7402124404907227\n",
      "Epoch 193: Train Loss: 0.6640689611434937, Validation Loss: 0.7402858734130859\n",
      "Epoch 194: Train Loss: 0.6619526624679566, Validation Loss: 0.7384719848632812\n",
      "Epoch 195: Train Loss: 0.6479568481445312, Validation Loss: 0.7374299764633179\n",
      "Epoch 196: Train Loss: 0.676092267036438, Validation Loss: 0.7394585609436035\n",
      "Epoch 197: Train Loss: 0.6515265226364135, Validation Loss: 0.7401034235954285\n",
      "Epoch 198: Train Loss: 0.6326515078544617, Validation Loss: 0.7412403225898743\n",
      "Epoch 199: Train Loss: 0.6224434077739716, Validation Loss: 0.7410478591918945\n",
      "Epoch 200: Train Loss: 0.6223073124885559, Validation Loss: 0.7412458062171936\n",
      "Epoch 201: Train Loss: 0.6272202253341674, Validation Loss: 0.7389457821846008\n",
      "Epoch 202: Train Loss: 0.6547990679740906, Validation Loss: 0.7411075234413147\n",
      "Epoch 203: Train Loss: 0.658225154876709, Validation Loss: 0.7423321604728699\n",
      "Epoch 204: Train Loss: 0.6572967052459717, Validation Loss: 0.743205726146698\n",
      "Epoch 205: Train Loss: 0.6372125267982482, Validation Loss: 0.7439311146736145\n",
      "Epoch 206: Train Loss: 0.6907320141792297, Validation Loss: 0.7450536489486694\n",
      "Epoch 207: Train Loss: 0.6669537663459778, Validation Loss: 0.7460882067680359\n",
      "Epoch 208: Train Loss: 0.6370859622955323, Validation Loss: 0.7445287108421326\n",
      "Epoch 209: Train Loss: 0.712867534160614, Validation Loss: 0.745911717414856\n",
      "Epoch 210: Train Loss: 0.6067584156990051, Validation Loss: 0.7440677285194397\n",
      "Epoch 211: Train Loss: 0.6725237846374512, Validation Loss: 0.7466145753860474\n",
      "Epoch 212: Train Loss: 0.6896537184715271, Validation Loss: 0.7474163174629211\n",
      "Epoch 213: Train Loss: 0.6542818903923034, Validation Loss: 0.7491129040718079\n",
      "Epoch 214: Train Loss: 0.6670127868652344, Validation Loss: 0.7484442591667175\n",
      "Epoch 215: Train Loss: 0.6850128054618836, Validation Loss: 0.7478129267692566\n",
      "Epoch 216: Train Loss: 0.6501170873641968, Validation Loss: 0.746782124042511\n",
      "Epoch 217: Train Loss: 0.6751566410064698, Validation Loss: 0.7467076182365417\n",
      "Epoch 218: Train Loss: 0.608478331565857, Validation Loss: 0.7444646954536438\n",
      "Epoch 219: Train Loss: 0.6952452421188354, Validation Loss: 0.7468196153640747\n",
      "Epoch 220: Train Loss: 0.6496127128601075, Validation Loss: 0.7484688758850098\n",
      "Epoch 221: Train Loss: 0.6665222764015197, Validation Loss: 0.7490813732147217\n",
      "Epoch 222: Train Loss: 0.6297864556312561, Validation Loss: 0.7512202262878418\n",
      "Epoch 223: Train Loss: 0.612163758277893, Validation Loss: 0.7481919527053833\n",
      "Epoch 224: Train Loss: 0.6676311254501343, Validation Loss: 0.7453945279121399\n",
      "Epoch 225: Train Loss: 0.6456883788108826, Validation Loss: 0.7474489212036133\n",
      "Epoch 226: Train Loss: 0.6276856422424316, Validation Loss: 0.7485665678977966\n",
      "Epoch 227: Train Loss: 0.6437258124351501, Validation Loss: 0.7494743466377258\n",
      "Epoch 228: Train Loss: 0.6305410861968994, Validation Loss: 0.7473616600036621\n",
      "Epoch 229: Train Loss: 0.6582270979881286, Validation Loss: 0.7503198385238647\n",
      "Epoch 230: Train Loss: 0.645127534866333, Validation Loss: 0.7504944801330566\n",
      "Epoch 231: Train Loss: 0.6241684556007385, Validation Loss: 0.7520879507064819\n",
      "Epoch 232: Train Loss: 0.6644898056983948, Validation Loss: 0.7510527968406677\n",
      "Epoch 233: Train Loss: 0.6523218870162963, Validation Loss: 0.7514185905456543\n",
      "Epoch 234: Train Loss: 0.672416377067566, Validation Loss: 0.7526480555534363\n",
      "Epoch 235: Train Loss: 0.6403867959976196, Validation Loss: 0.752358615398407\n",
      "Epoch 236: Train Loss: 0.7187703967094421, Validation Loss: 0.7528805732727051\n",
      "Epoch 237: Train Loss: 0.6821205377578735, Validation Loss: 0.7532918453216553\n",
      "Epoch 238: Train Loss: 0.6687015652656555, Validation Loss: 0.7509576678276062\n",
      "Epoch 239: Train Loss: 0.5984595715999603, Validation Loss: 0.7511988878250122\n",
      "Epoch 240: Train Loss: 0.657550036907196, Validation Loss: 0.7524582743644714\n",
      "Epoch 241: Train Loss: 0.6001920878887177, Validation Loss: 0.7526059150695801\n",
      "Epoch 242: Train Loss: 0.6446940302848816, Validation Loss: 0.7540130019187927\n",
      "Epoch 243: Train Loss: 0.6263707160949707, Validation Loss: 0.7555806636810303\n",
      "Epoch 244: Train Loss: 0.6583750128746033, Validation Loss: 0.7566365599632263\n",
      "Epoch 245: Train Loss: 0.6473242282867432, Validation Loss: 0.7549640536308289\n",
      "Epoch 246: Train Loss: 0.5937783777713775, Validation Loss: 0.7549504637718201\n",
      "Epoch 247: Train Loss: 0.6692941904067993, Validation Loss: 0.7547721862792969\n",
      "Epoch 248: Train Loss: 0.6521291613578797, Validation Loss: 0.7548639178276062\n",
      "Epoch 249: Train Loss: 0.6146729111671447, Validation Loss: 0.7552375793457031\n",
      "Epoch 250: Train Loss: 0.658634626865387, Validation Loss: 0.7562003135681152\n",
      "Epoch 251: Train Loss: 0.6023853957653046, Validation Loss: 0.7566560506820679\n",
      "Epoch 252: Train Loss: 0.640518069267273, Validation Loss: 0.7534518241882324\n",
      "Epoch 253: Train Loss: 0.6719066262245178, Validation Loss: 0.7554011940956116\n",
      "Epoch 254: Train Loss: 0.6795979022979737, Validation Loss: 0.756363570690155\n",
      "Epoch 255: Train Loss: 0.6456506133079529, Validation Loss: 0.7566402554512024\n",
      "Epoch 256: Train Loss: 0.6247132658958435, Validation Loss: 0.7584611773490906\n",
      "Epoch 257: Train Loss: 0.6074343681335449, Validation Loss: 0.7601650357246399\n",
      "Epoch 258: Train Loss: 0.5931370317935943, Validation Loss: 0.7579182982444763\n",
      "Epoch 259: Train Loss: 0.6193124890327454, Validation Loss: 0.7587264776229858\n",
      "Epoch 260: Train Loss: 0.6762139439582825, Validation Loss: 0.7592020034790039\n",
      "Epoch 261: Train Loss: 0.6595266819000244, Validation Loss: 0.758732795715332\n",
      "Epoch 262: Train Loss: 0.6425999760627746, Validation Loss: 0.759330689907074\n",
      "Epoch 263: Train Loss: 0.6620781540870666, Validation Loss: 0.7610675692558289\n",
      "Epoch 264: Train Loss: 0.6111592054367065, Validation Loss: 0.760887086391449\n",
      "Epoch 265: Train Loss: 0.6487998247146607, Validation Loss: 0.7615824937820435\n",
      "Epoch 266: Train Loss: 0.6068585276603699, Validation Loss: 0.7619878053665161\n",
      "Epoch 267: Train Loss: 0.6220558166503907, Validation Loss: 0.7578946352005005\n",
      "Epoch 268: Train Loss: 0.6808510780334472, Validation Loss: 0.7608588933944702\n",
      "Epoch 269: Train Loss: 0.6414934158325195, Validation Loss: 0.7617149353027344\n",
      "Epoch 270: Train Loss: 0.6105218052864074, Validation Loss: 0.762437641620636\n",
      "Epoch 271: Train Loss: 0.6266237974166871, Validation Loss: 0.7634682655334473\n",
      "Epoch 272: Train Loss: 0.6093375563621521, Validation Loss: 0.7650139927864075\n",
      "Epoch 273: Train Loss: 0.6344952464103699, Validation Loss: 0.7622092366218567\n",
      "Epoch 274: Train Loss: 0.6509248614311218, Validation Loss: 0.763904869556427\n",
      "Epoch 275: Train Loss: 0.6185837745666504, Validation Loss: 0.7605880498886108\n",
      "Epoch 276: Train Loss: 0.6897497773170471, Validation Loss: 0.7633754014968872\n",
      "Epoch 277: Train Loss: 0.6431841731071473, Validation Loss: 0.7660422921180725\n",
      "Epoch 278: Train Loss: 0.6072463870048523, Validation Loss: 0.7647053599357605\n",
      "Epoch 279: Train Loss: 0.6798213362693787, Validation Loss: 0.7668265700340271\n",
      "Epoch 280: Train Loss: 0.6497603535652161, Validation Loss: 0.7691679000854492\n",
      "Epoch 281: Train Loss: 0.6988234400749207, Validation Loss: 0.7681521773338318\n",
      "Epoch 282: Train Loss: 0.6455239534378052, Validation Loss: 0.7686613202095032\n",
      "Epoch 283: Train Loss: 0.6333388447761535, Validation Loss: 0.7690795063972473\n",
      "Epoch 284: Train Loss: 0.6370683312416077, Validation Loss: 0.7648477554321289\n",
      "Epoch 285: Train Loss: 0.6430442214012146, Validation Loss: 0.7663106322288513\n",
      "Epoch 286: Train Loss: 0.6141350746154786, Validation Loss: 0.7637236714363098\n",
      "Epoch 287: Train Loss: 0.6551369667053223, Validation Loss: 0.7666398882865906\n",
      "Epoch 288: Train Loss: 0.6879758238792419, Validation Loss: 0.7682062983512878\n",
      "Epoch 289: Train Loss: 0.6436022758483887, Validation Loss: 0.7690268754959106\n",
      "Epoch 290: Train Loss: 0.6872902631759643, Validation Loss: 0.7691543698310852\n",
      "Epoch 291: Train Loss: 0.6205102801322937, Validation Loss: 0.7699348330497742\n",
      "Epoch 292: Train Loss: 0.6376367926597595, Validation Loss: 0.7676021456718445\n",
      "Epoch 293: Train Loss: 0.6442932248115539, Validation Loss: 0.770304262638092\n",
      "Epoch 294: Train Loss: 0.6354942917823792, Validation Loss: 0.7711023092269897\n",
      "Epoch 295: Train Loss: 0.6609954714775086, Validation Loss: 0.7727059721946716\n",
      "Epoch 296: Train Loss: 0.6855393886566162, Validation Loss: 0.7734540700912476\n",
      "Epoch 297: Train Loss: 0.6299331784248352, Validation Loss: 0.7728338241577148\n",
      "Epoch 298: Train Loss: 0.6435298562049866, Validation Loss: 0.7733930349349976\n",
      "Epoch 299: Train Loss: 0.6141005992889405, Validation Loss: 0.7742893099784851\n",
      "Epoch 300: Train Loss: 0.6173945903778076, Validation Loss: 0.7735106348991394\n",
      "Epoch 301: Train Loss: 0.6274929642677307, Validation Loss: 0.7713496088981628\n",
      "Epoch 302: Train Loss: 0.6422582030296325, Validation Loss: 0.7737036943435669\n",
      "Epoch 303: Train Loss: 0.643623697757721, Validation Loss: 0.7734084725379944\n",
      "Epoch 304: Train Loss: 0.6387091875076294, Validation Loss: 0.7731800079345703\n",
      "Epoch 305: Train Loss: 0.639579689502716, Validation Loss: 0.7734957933425903\n",
      "Epoch 306: Train Loss: 0.631155526638031, Validation Loss: 0.7764140963554382\n",
      "Epoch 307: Train Loss: 0.6343193411827087, Validation Loss: 0.7776698470115662\n",
      "Epoch 308: Train Loss: 0.6747496962547302, Validation Loss: 0.7786647081375122\n",
      "Epoch 309: Train Loss: 0.6210664868354797, Validation Loss: 0.776970624923706\n",
      "Epoch 310: Train Loss: 0.603974026441574, Validation Loss: 0.7772597670555115\n",
      "Epoch 311: Train Loss: 0.6130825400352478, Validation Loss: 0.7776534557342529\n",
      "Epoch 312: Train Loss: 0.598216426372528, Validation Loss: 0.7811021208763123\n",
      "Epoch 313: Train Loss: 0.6208553075790405, Validation Loss: 0.7833873629570007\n",
      "Epoch 314: Train Loss: 0.6315163731575012, Validation Loss: 0.7829875349998474\n",
      "Epoch 315: Train Loss: 0.5891622960567474, Validation Loss: 0.7848849892616272\n",
      "Epoch 316: Train Loss: 0.6224527001380921, Validation Loss: 0.7786137461662292\n",
      "Epoch 317: Train Loss: 0.6427216172218323, Validation Loss: 0.7823270559310913\n",
      "Epoch 318: Train Loss: 0.5749638915061951, Validation Loss: 0.7827577590942383\n",
      "Epoch 319: Train Loss: 0.6410691857337951, Validation Loss: 0.784691333770752\n",
      "Epoch 320: Train Loss: 0.6231263756752015, Validation Loss: 0.7817493081092834\n",
      "Epoch 321: Train Loss: 0.6660923957824707, Validation Loss: 0.7857545018196106\n",
      "Epoch 322: Train Loss: 0.5770543038845062, Validation Loss: 0.7868955731391907\n",
      "Epoch 323: Train Loss: 0.6624031901359558, Validation Loss: 0.7888259887695312\n",
      "Epoch 324: Train Loss: 0.6211709260940552, Validation Loss: 0.7891152501106262\n",
      "Epoch 325: Train Loss: 0.6162433385848999, Validation Loss: 0.7887563109397888\n",
      "Epoch 326: Train Loss: 0.6316653966903687, Validation Loss: 0.786883533000946\n",
      "Epoch 327: Train Loss: 0.6152586996555328, Validation Loss: 0.7887183427810669\n",
      "Epoch 328: Train Loss: 0.563423627614975, Validation Loss: 0.7897351980209351\n",
      "Epoch 329: Train Loss: 0.5813471615314484, Validation Loss: 0.7912051677703857\n",
      "Epoch 330: Train Loss: 0.6333893537521362, Validation Loss: 0.7907657623291016\n",
      "Epoch 331: Train Loss: 0.6118261337280273, Validation Loss: 0.7936701774597168\n",
      "Epoch 332: Train Loss: 0.5909258008003235, Validation Loss: 0.7923264503479004\n",
      "Epoch 333: Train Loss: 0.5693222165107727, Validation Loss: 0.7928506135940552\n",
      "Epoch 334: Train Loss: 0.631684136390686, Validation Loss: 0.7959537506103516\n",
      "Epoch 335: Train Loss: 0.5994248151779175, Validation Loss: 0.7964482307434082\n",
      "Epoch 336: Train Loss: 0.6480993866920471, Validation Loss: 0.7914915084838867\n",
      "Epoch 337: Train Loss: 0.6736174106597901, Validation Loss: 0.7966338992118835\n",
      "Epoch 338: Train Loss: 0.6473840713500977, Validation Loss: 0.7986550331115723\n",
      "Epoch 339: Train Loss: 0.6124644875526428, Validation Loss: 0.8002333641052246\n",
      "Epoch 340: Train Loss: 0.6544465541839599, Validation Loss: 0.8047342300415039\n",
      "Epoch 341: Train Loss: 0.5922799229621887, Validation Loss: 0.807344913482666\n",
      "Epoch 342: Train Loss: 0.6229711651802063, Validation Loss: 0.8049575090408325\n",
      "Epoch 343: Train Loss: 0.6326576471328735, Validation Loss: 0.8078933954238892\n",
      "Epoch 344: Train Loss: 0.6431228995323182, Validation Loss: 0.8093356490135193\n",
      "Epoch 345: Train Loss: 0.5892651438713074, Validation Loss: 0.8075437545776367\n",
      "Epoch 346: Train Loss: 0.6646390914916992, Validation Loss: 0.8116977214813232\n",
      "Epoch 347: Train Loss: 0.6210860729217529, Validation Loss: 0.8089767694473267\n",
      "Epoch 348: Train Loss: 0.6321986556053162, Validation Loss: 0.8085460662841797\n",
      "Epoch 349: Train Loss: 0.6417817950248719, Validation Loss: 0.8045583963394165\n",
      "Epoch 350: Train Loss: 0.6468476414680481, Validation Loss: 0.800022304058075\n",
      "Epoch 351: Train Loss: 0.6060705304145813, Validation Loss: 0.805313229560852\n",
      "Epoch 352: Train Loss: 0.6153362274169922, Validation Loss: 0.8016260266304016\n",
      "Epoch 353: Train Loss: 0.6223356008529664, Validation Loss: 0.8099923133850098\n",
      "Epoch 354: Train Loss: 0.6621239900588989, Validation Loss: 0.813206672668457\n",
      "Epoch 355: Train Loss: 0.6210269689559936, Validation Loss: 0.8175511956214905\n",
      "Epoch 356: Train Loss: 0.5843629896640777, Validation Loss: 0.8148959279060364\n",
      "Epoch 357: Train Loss: 0.6147423624992371, Validation Loss: 0.8113107681274414\n",
      "Epoch 358: Train Loss: 0.6329132080078125, Validation Loss: 0.8154842257499695\n",
      "Epoch 359: Train Loss: 0.6771037101745605, Validation Loss: 0.8215394616127014\n",
      "Epoch 360: Train Loss: 0.565291303396225, Validation Loss: 0.8281400799751282\n",
      "Epoch 361: Train Loss: 0.6115358710289002, Validation Loss: 0.829416036605835\n",
      "Epoch 362: Train Loss: 0.6590488672256469, Validation Loss: 0.8322365880012512\n",
      "Epoch 363: Train Loss: 0.5780032396316528, Validation Loss: 0.8364372253417969\n",
      "Epoch 364: Train Loss: 0.6487695455551148, Validation Loss: 0.8411513566970825\n",
      "Epoch 365: Train Loss: 0.6068316340446472, Validation Loss: 0.8295004963874817\n",
      "Epoch 366: Train Loss: 0.5343794286251068, Validation Loss: 0.8281927108764648\n",
      "Epoch 367: Train Loss: 0.5908280730247497, Validation Loss: 0.8243697881698608\n",
      "Epoch 368: Train Loss: 0.6135446667671204, Validation Loss: 0.8286355137825012\n",
      "Epoch 369: Train Loss: 0.6009225130081177, Validation Loss: 0.8359113931655884\n",
      "Epoch 370: Train Loss: 0.618749737739563, Validation Loss: 0.8384672403335571\n",
      "Epoch 371: Train Loss: 0.5810252368450165, Validation Loss: 0.8426545262336731\n",
      "Epoch 372: Train Loss: 0.6062727928161621, Validation Loss: 0.8442264199256897\n",
      "Epoch 373: Train Loss: 0.5795963048934937, Validation Loss: 0.8503135442733765\n",
      "Epoch 374: Train Loss: 0.6458636522293091, Validation Loss: 0.8550626635551453\n",
      "Epoch 375: Train Loss: 0.6257092833518982, Validation Loss: 0.8523618578910828\n",
      "Epoch 376: Train Loss: 0.5928911507129669, Validation Loss: 0.840390145778656\n",
      "Epoch 377: Train Loss: 0.6469779491424561, Validation Loss: 0.8453208804130554\n",
      "Epoch 378: Train Loss: 0.5743156790733337, Validation Loss: 0.844179093837738\n",
      "Epoch 379: Train Loss: 0.6005204200744629, Validation Loss: 0.8512023687362671\n",
      "Epoch 380: Train Loss: 0.5929114580154419, Validation Loss: 0.857593297958374\n",
      "Epoch 381: Train Loss: 0.6138882756233215, Validation Loss: 0.8656215071678162\n",
      "Epoch 382: Train Loss: 0.5697841286659241, Validation Loss: 0.8674224615097046\n",
      "Epoch 383: Train Loss: 0.5925833821296692, Validation Loss: 0.8678829073905945\n",
      "Epoch 384: Train Loss: 0.5584575653076171, Validation Loss: 0.8613439798355103\n",
      "Epoch 385: Train Loss: 0.5605738937854767, Validation Loss: 0.8703451156616211\n",
      "Epoch 386: Train Loss: 0.6232036113739013, Validation Loss: 0.879067599773407\n",
      "Epoch 387: Train Loss: 0.6380322575569153, Validation Loss: 0.879669189453125\n",
      "Epoch 388: Train Loss: 0.5828787446022033, Validation Loss: 0.8842920064926147\n",
      "Epoch 389: Train Loss: 0.5971152782440186, Validation Loss: 0.8917837142944336\n",
      "Epoch 390: Train Loss: 0.6614302515983581, Validation Loss: 0.8996507525444031\n",
      "Epoch 391: Train Loss: 0.6002614140510559, Validation Loss: 0.8839578032493591\n",
      "Epoch 392: Train Loss: 0.5781703293323517, Validation Loss: 0.8908921480178833\n",
      "Epoch 393: Train Loss: 0.5680844485759735, Validation Loss: 0.8974270224571228\n",
      "Epoch 394: Train Loss: 0.6002218961715698, Validation Loss: 0.9057890176773071\n",
      "Epoch 395: Train Loss: 0.6410493731498719, Validation Loss: 0.9066954851150513\n",
      "Epoch 396: Train Loss: 0.6214154005050659, Validation Loss: 0.9115650653839111\n",
      "Epoch 397: Train Loss: 0.5928988933563233, Validation Loss: 0.9067832231521606\n",
      "Epoch 398: Train Loss: 0.6181485414505005, Validation Loss: 0.9028491973876953\n",
      "Epoch 399: Train Loss: 0.5808804631233215, Validation Loss: 0.9023681282997131\n",
      "Epoch 400: Train Loss: 0.589710021018982, Validation Loss: 0.91717928647995\n",
      "Epoch 401: Train Loss: 0.5696649312973022, Validation Loss: 0.9252681732177734\n",
      "Epoch 402: Train Loss: 0.586260974407196, Validation Loss: 0.929379403591156\n",
      "Epoch 403: Train Loss: 0.6412169218063355, Validation Loss: 0.9265447854995728\n",
      "Epoch 404: Train Loss: 0.5841516494750977, Validation Loss: 0.9203923940658569\n",
      "Epoch 405: Train Loss: 0.5672900915145874, Validation Loss: 0.9278386831283569\n",
      "Epoch 406: Train Loss: 0.6143604993820191, Validation Loss: 0.9331191778182983\n",
      "Epoch 407: Train Loss: 0.5406056344509125, Validation Loss: 0.9370746612548828\n",
      "Epoch 408: Train Loss: 0.5997261047363281, Validation Loss: 0.921874463558197\n",
      "Epoch 409: Train Loss: 0.6206022739410401, Validation Loss: 0.9357826709747314\n",
      "Epoch 410: Train Loss: 0.5903791069984436, Validation Loss: 0.9557146430015564\n",
      "Epoch 411: Train Loss: 0.6452414870262146, Validation Loss: 0.9605296850204468\n",
      "Epoch 412: Train Loss: 0.5325819373130798, Validation Loss: 0.9396954774856567\n",
      "Epoch 413: Train Loss: 0.6052254676818848, Validation Loss: 0.9520643353462219\n",
      "Epoch 414: Train Loss: 0.5663162708282471, Validation Loss: 0.9643839001655579\n",
      "Epoch 415: Train Loss: 0.5425217926502228, Validation Loss: 0.9688388705253601\n",
      "Epoch 416: Train Loss: 0.5335443317890167, Validation Loss: 0.9705690145492554\n",
      "Epoch 417: Train Loss: 0.5377411901950836, Validation Loss: 0.9492200613021851\n",
      "Epoch 418: Train Loss: 0.5976167321205139, Validation Loss: 0.9292959570884705\n",
      "Epoch 419: Train Loss: 0.5203613042831421, Validation Loss: 0.9315876364707947\n",
      "Epoch 420: Train Loss: 0.5685794472694397, Validation Loss: 0.9588050246238708\n",
      "Epoch 421: Train Loss: 0.6632706642150878, Validation Loss: 0.9879822134971619\n",
      "Epoch 422: Train Loss: 0.6295060515403748, Validation Loss: 0.9935832619667053\n",
      "Epoch 423: Train Loss: 0.560649573802948, Validation Loss: 1.0188159942626953\n",
      "Epoch 424: Train Loss: 0.5715784668922425, Validation Loss: 1.0434249639511108\n",
      "Epoch 425: Train Loss: 0.5726222813129425, Validation Loss: 1.0434062480926514\n",
      "Epoch 426: Train Loss: 0.530443811416626, Validation Loss: 1.0219699144363403\n",
      "Epoch 427: Train Loss: 0.5834218382835388, Validation Loss: 1.0426520109176636\n",
      "Epoch 428: Train Loss: 0.5920252442359925, Validation Loss: 1.029961347579956\n",
      "Epoch 429: Train Loss: 0.5841548919677735, Validation Loss: 1.0504075288772583\n",
      "Epoch 430: Train Loss: 0.6047097682952881, Validation Loss: 1.0629903078079224\n",
      "Epoch 431: Train Loss: 0.585375326871872, Validation Loss: 1.0940043926239014\n",
      "Epoch 432: Train Loss: 0.5920988321304321, Validation Loss: 1.1148945093154907\n",
      "Epoch 433: Train Loss: 0.6295476675033569, Validation Loss: 1.0861949920654297\n",
      "Epoch 434: Train Loss: 0.5700060963630676, Validation Loss: 1.1113488674163818\n",
      "Epoch 435: Train Loss: 0.5882107853889466, Validation Loss: 1.156433343887329\n",
      "Epoch 436: Train Loss: 0.5862339615821839, Validation Loss: 1.176180362701416\n",
      "Epoch 437: Train Loss: 0.5387438058853149, Validation Loss: 1.1853018999099731\n",
      "Epoch 438: Train Loss: 0.5604665398597717, Validation Loss: 1.1917505264282227\n",
      "Epoch 439: Train Loss: 0.5549010634422302, Validation Loss: 1.2060245275497437\n",
      "Epoch 440: Train Loss: 0.5377634763717651, Validation Loss: 1.2007561922073364\n",
      "Epoch 441: Train Loss: 0.6028467118740082, Validation Loss: 1.2599420547485352\n",
      "Epoch 442: Train Loss: 0.5497323215007782, Validation Loss: 1.270898699760437\n",
      "Epoch 443: Train Loss: 0.5613520562648773, Validation Loss: 1.2912031412124634\n",
      "Epoch 444: Train Loss: 0.6026000022888184, Validation Loss: 1.3011988401412964\n",
      "Epoch 445: Train Loss: 0.5432102501392364, Validation Loss: 1.3381351232528687\n",
      "Epoch 446: Train Loss: 0.5454035103321075, Validation Loss: 1.385369062423706\n",
      "Epoch 447: Train Loss: 0.6003049612045288, Validation Loss: 1.3841731548309326\n",
      "Epoch 448: Train Loss: 0.6364101767539978, Validation Loss: 1.4066107273101807\n",
      "Epoch 449: Train Loss: 0.640581750869751, Validation Loss: 1.4434512853622437\n",
      "Epoch 450: Train Loss: 0.5804742753505707, Validation Loss: 1.4630085229873657\n",
      "Epoch 451: Train Loss: 0.5575725078582764, Validation Loss: 1.417832374572754\n",
      "Epoch 452: Train Loss: 0.6032458066940307, Validation Loss: 1.4790583848953247\n",
      "Epoch 453: Train Loss: 0.6155877351760864, Validation Loss: 1.467940330505371\n",
      "Epoch 454: Train Loss: 0.5538277268409729, Validation Loss: 1.4863605499267578\n",
      "Epoch 455: Train Loss: 0.535884290933609, Validation Loss: 1.5124855041503906\n",
      "Epoch 456: Train Loss: 0.6348278403282166, Validation Loss: 1.5077544450759888\n",
      "Epoch 457: Train Loss: 0.5726805210113526, Validation Loss: 1.480676293373108\n",
      "Epoch 458: Train Loss: 0.5258770227432251, Validation Loss: 1.4879575967788696\n",
      "Epoch 459: Train Loss: 0.5663601398468018, Validation Loss: 1.4197434186935425\n",
      "Epoch 460: Train Loss: 0.61345294713974, Validation Loss: 1.4710826873779297\n",
      "Epoch 461: Train Loss: 0.5664627552032471, Validation Loss: 1.471143364906311\n",
      "Epoch 462: Train Loss: 0.5242281973361969, Validation Loss: 1.4969120025634766\n",
      "Epoch 463: Train Loss: 0.5011526644229889, Validation Loss: 1.4612172842025757\n",
      "Epoch 464: Train Loss: 0.6399124443531037, Validation Loss: 1.5266916751861572\n",
      "Epoch 465: Train Loss: 0.6210761070251465, Validation Loss: 1.5579124689102173\n",
      "Epoch 466: Train Loss: 0.5359965920448303, Validation Loss: 1.5719270706176758\n",
      "Epoch 467: Train Loss: 0.5882559418678284, Validation Loss: 1.5536222457885742\n",
      "Epoch 468: Train Loss: 0.6234232902526855, Validation Loss: 1.541398525238037\n",
      "Epoch 469: Train Loss: 0.515103942155838, Validation Loss: 1.5365302562713623\n",
      "Epoch 470: Train Loss: 0.5164787292480468, Validation Loss: 1.5546735525131226\n",
      "Epoch 471: Train Loss: 0.5607187628746033, Validation Loss: 1.5472911596298218\n",
      "Epoch 472: Train Loss: 0.5512840509414673, Validation Loss: 1.6192814111709595\n",
      "Epoch 473: Train Loss: 0.5590315461158752, Validation Loss: 1.6713457107543945\n",
      "Epoch 474: Train Loss: 0.5716784954071045, Validation Loss: 1.7189180850982666\n",
      "Epoch 475: Train Loss: 0.5026732921600342, Validation Loss: 1.7431923151016235\n",
      "Epoch 476: Train Loss: 0.5824751734733582, Validation Loss: 1.8078621625900269\n",
      "Epoch 477: Train Loss: 0.5269023954868317, Validation Loss: 1.814826488494873\n",
      "Epoch 478: Train Loss: 0.5153841078281403, Validation Loss: 1.7478923797607422\n",
      "Epoch 479: Train Loss: 0.5526098489761353, Validation Loss: 1.7834258079528809\n",
      "Epoch 480: Train Loss: 0.5192853331565856, Validation Loss: 1.807002305984497\n",
      "Epoch 481: Train Loss: 0.5698685824871064, Validation Loss: 1.8737614154815674\n",
      "Epoch 482: Train Loss: 0.5536597728729248, Validation Loss: 1.849747896194458\n",
      "Epoch 483: Train Loss: 0.5700850427150727, Validation Loss: 1.7965372800827026\n",
      "Epoch 484: Train Loss: 0.5264104127883911, Validation Loss: 1.7651230096817017\n",
      "Epoch 485: Train Loss: 0.5561594784259796, Validation Loss: 1.7490869760513306\n",
      "Epoch 486: Train Loss: 0.5828092157840729, Validation Loss: 1.7628909349441528\n",
      "Epoch 487: Train Loss: 0.5013294219970703, Validation Loss: 1.6625255346298218\n",
      "Epoch 488: Train Loss: 0.521335756778717, Validation Loss: 1.6429473161697388\n",
      "Epoch 489: Train Loss: 0.5191348433494568, Validation Loss: 1.6772791147232056\n",
      "Epoch 490: Train Loss: 0.5694867432117462, Validation Loss: 1.6991599798202515\n",
      "Epoch 491: Train Loss: 0.5981807231903076, Validation Loss: 1.7339396476745605\n",
      "Epoch 492: Train Loss: 0.5431998610496521, Validation Loss: 1.7189158201217651\n",
      "Epoch 493: Train Loss: 0.521531629562378, Validation Loss: 1.7416319847106934\n",
      "Epoch 494: Train Loss: 0.541158276796341, Validation Loss: 1.7409186363220215\n",
      "Epoch 495: Train Loss: 0.5039489805698395, Validation Loss: 1.7460869550704956\n",
      "Epoch 496: Train Loss: 0.49809626340866087, Validation Loss: 1.742568850517273\n",
      "Epoch 497: Train Loss: 0.6859612941741944, Validation Loss: 1.7870705127716064\n",
      "Epoch 498: Train Loss: 0.611860477924347, Validation Loss: 1.750067114830017\n",
      "Epoch 499: Train Loss: 0.5098793804645538, Validation Loss: 1.7567780017852783\n",
      "Fold 9 Metrics:\n",
      "Accuracy: 0.0, Precision: 0.0, Recall: 0.0, F1-score: 0.0, AUC: 0.0\n",
      "Confusion Matrix:\n",
      "[[0 5]\n",
      " [6 0]]\n",
      "Completed fold 9\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples from subject 5 to test set\n",
      "Adding 6 truth samples from subject 5 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 500)\n",
      "(132, 65, 500)\n",
      "Epoch 0: Train Loss: 0.6474539577960968, Validation Loss: 0.6930530667304993\n",
      "Epoch 1: Train Loss: 0.6847897648811341, Validation Loss: 0.692267894744873\n",
      "Epoch 2: Train Loss: 0.720720899105072, Validation Loss: 0.6910779476165771\n",
      "Epoch 3: Train Loss: 0.722309947013855, Validation Loss: 0.6896669268608093\n",
      "Epoch 4: Train Loss: 0.7119561314582825, Validation Loss: 0.688447117805481\n",
      "Epoch 5: Train Loss: 0.7911859393119812, Validation Loss: 0.6872512698173523\n",
      "Epoch 6: Train Loss: 0.7550643444061279, Validation Loss: 0.6865425109863281\n",
      "Epoch 7: Train Loss: 0.6666687965393067, Validation Loss: 0.6858336329460144\n",
      "Epoch 8: Train Loss: 0.7304372072219849, Validation Loss: 0.6854180097579956\n",
      "Epoch 9: Train Loss: 0.6678560137748718, Validation Loss: 0.6849621534347534\n",
      "Epoch 10: Train Loss: 0.6635264039039612, Validation Loss: 0.6844714879989624\n",
      "Epoch 11: Train Loss: 0.6864988327026367, Validation Loss: 0.68437659740448\n",
      "Epoch 12: Train Loss: 0.6822960019111634, Validation Loss: 0.684243381023407\n",
      "Epoch 13: Train Loss: 0.6985612154006958, Validation Loss: 0.6841180920600891\n",
      "Epoch 14: Train Loss: 0.732988691329956, Validation Loss: 0.6846329569816589\n",
      "Epoch 15: Train Loss: 0.707611906528473, Validation Loss: 0.6842866539955139\n",
      "Epoch 16: Train Loss: 0.6869237899780274, Validation Loss: 0.684025764465332\n",
      "Epoch 17: Train Loss: 0.7419694900512696, Validation Loss: 0.6848182082176208\n",
      "Epoch 18: Train Loss: 0.6780099034309387, Validation Loss: 0.6844148635864258\n",
      "Epoch 19: Train Loss: 0.7097923398017884, Validation Loss: 0.6840536594390869\n",
      "Epoch 20: Train Loss: 0.7014299035072327, Validation Loss: 0.684563934803009\n",
      "Epoch 21: Train Loss: 0.6817401647567749, Validation Loss: 0.6849613189697266\n",
      "Epoch 22: Train Loss: 0.6725711107254029, Validation Loss: 0.684704065322876\n",
      "Epoch 23: Train Loss: 0.7134583473205567, Validation Loss: 0.6843633055686951\n",
      "Epoch 24: Train Loss: 0.6953358054161072, Validation Loss: 0.6844794154167175\n",
      "Epoch 25: Train Loss: 0.6861280083656311, Validation Loss: 0.6842893958091736\n",
      "Epoch 26: Train Loss: 0.6961250543594361, Validation Loss: 0.6844010949134827\n",
      "Epoch 27: Train Loss: 0.66353999376297, Validation Loss: 0.6842702627182007\n",
      "Epoch 28: Train Loss: 0.6998970746994019, Validation Loss: 0.684073805809021\n",
      "Epoch 29: Train Loss: 0.7004462003707885, Validation Loss: 0.6843223571777344\n",
      "Epoch 30: Train Loss: 0.6836427569389343, Validation Loss: 0.6839558482170105\n",
      "Epoch 31: Train Loss: 0.6753496885299682, Validation Loss: 0.6839009523391724\n",
      "Epoch 32: Train Loss: 0.7003750324249267, Validation Loss: 0.6836854219436646\n",
      "Epoch 33: Train Loss: 0.6594477772712708, Validation Loss: 0.6835641860961914\n",
      "Epoch 34: Train Loss: 0.6809852838516235, Validation Loss: 0.6839392185211182\n",
      "Epoch 35: Train Loss: 0.6957188844680786, Validation Loss: 0.6840073466300964\n",
      "Epoch 36: Train Loss: 0.7031673550605774, Validation Loss: 0.6846003532409668\n",
      "Epoch 37: Train Loss: 0.7116769194602967, Validation Loss: 0.684260904788971\n",
      "Epoch 38: Train Loss: 0.644084119796753, Validation Loss: 0.6838565468788147\n",
      "Epoch 39: Train Loss: 0.6943246603012085, Validation Loss: 0.6837638020515442\n",
      "Epoch 40: Train Loss: 0.7079456448554993, Validation Loss: 0.6839233040809631\n",
      "Epoch 41: Train Loss: 0.6678330898284912, Validation Loss: 0.684165894985199\n",
      "Epoch 42: Train Loss: 0.6719486236572265, Validation Loss: 0.6851910948753357\n",
      "Epoch 43: Train Loss: 0.7161637067794799, Validation Loss: 0.6848651766777039\n",
      "Epoch 44: Train Loss: 0.6659173727035522, Validation Loss: 0.6845647096633911\n",
      "Epoch 45: Train Loss: 0.6717280149459839, Validation Loss: 0.684751033782959\n",
      "Epoch 46: Train Loss: 0.6531448245048523, Validation Loss: 0.6849874258041382\n",
      "Epoch 47: Train Loss: 0.6795699715614318, Validation Loss: 0.6844133138656616\n",
      "Epoch 48: Train Loss: 0.7202450633049011, Validation Loss: 0.6845418214797974\n",
      "Epoch 49: Train Loss: 0.7294266581535339, Validation Loss: 0.684177041053772\n",
      "Epoch 50: Train Loss: 0.7222919344902039, Validation Loss: 0.6839781403541565\n",
      "Epoch 51: Train Loss: 0.6887841582298279, Validation Loss: 0.6847331523895264\n",
      "Epoch 52: Train Loss: 0.6739736199378967, Validation Loss: 0.684522271156311\n",
      "Epoch 53: Train Loss: 0.7267616271972657, Validation Loss: 0.6843583583831787\n",
      "Epoch 54: Train Loss: 0.7088127374649048, Validation Loss: 0.6845895051956177\n",
      "Epoch 55: Train Loss: 0.7298205852508545, Validation Loss: 0.6843444108963013\n",
      "Epoch 56: Train Loss: 0.7512372732162476, Validation Loss: 0.6843355894088745\n",
      "Epoch 57: Train Loss: 0.6662569761276245, Validation Loss: 0.6843045949935913\n",
      "Epoch 58: Train Loss: 0.7100209712982177, Validation Loss: 0.6841307282447815\n",
      "Epoch 59: Train Loss: 0.6663564562797546, Validation Loss: 0.6849521398544312\n",
      "Epoch 60: Train Loss: 0.7612722158432007, Validation Loss: 0.6844841837882996\n",
      "Epoch 61: Train Loss: 0.6997474789619446, Validation Loss: 0.6844543218612671\n",
      "Epoch 62: Train Loss: 0.670894455909729, Validation Loss: 0.68479984998703\n",
      "Epoch 63: Train Loss: 0.7001490592956543, Validation Loss: 0.6846218109130859\n",
      "Epoch 64: Train Loss: 0.6791470408439636, Validation Loss: 0.6840974688529968\n",
      "Epoch 65: Train Loss: 0.6885634541511536, Validation Loss: 0.6840980052947998\n",
      "Epoch 66: Train Loss: 0.6541972875595092, Validation Loss: 0.6841128468513489\n",
      "Epoch 67: Train Loss: 0.7093981742858887, Validation Loss: 0.6840541958808899\n",
      "Epoch 68: Train Loss: 0.7114661455154419, Validation Loss: 0.6845188736915588\n",
      "Epoch 69: Train Loss: 0.6633719682693482, Validation Loss: 0.6844963431358337\n",
      "Epoch 70: Train Loss: 0.6813196778297425, Validation Loss: 0.6845919489860535\n",
      "Epoch 71: Train Loss: 0.7085702538490295, Validation Loss: 0.6844544410705566\n",
      "Epoch 72: Train Loss: 0.6950136542320251, Validation Loss: 0.6843951344490051\n",
      "Epoch 73: Train Loss: 0.6978307127952575, Validation Loss: 0.6843091249465942\n",
      "Epoch 74: Train Loss: 0.7210553407669067, Validation Loss: 0.6843163967132568\n",
      "Epoch 75: Train Loss: 0.7142319679260254, Validation Loss: 0.6841915845870972\n",
      "Epoch 76: Train Loss: 0.6908457517623902, Validation Loss: 0.6839723587036133\n",
      "Epoch 77: Train Loss: 0.735240125656128, Validation Loss: 0.6840059161186218\n",
      "Epoch 78: Train Loss: 0.66975177526474, Validation Loss: 0.6843015551567078\n",
      "Epoch 79: Train Loss: 0.6693244457244873, Validation Loss: 0.684525191783905\n",
      "Epoch 80: Train Loss: 0.6821155667304992, Validation Loss: 0.6841884851455688\n",
      "Epoch 81: Train Loss: 0.6656669735908508, Validation Loss: 0.6845386028289795\n",
      "Epoch 82: Train Loss: 0.6745352268218994, Validation Loss: 0.6849727630615234\n",
      "Epoch 83: Train Loss: 0.7093140006065368, Validation Loss: 0.6845444440841675\n",
      "Epoch 84: Train Loss: 0.6656352639198303, Validation Loss: 0.6842202544212341\n",
      "Epoch 85: Train Loss: 0.6929332613945007, Validation Loss: 0.684270441532135\n",
      "Epoch 86: Train Loss: 0.7016425728797913, Validation Loss: 0.6842367649078369\n",
      "Epoch 87: Train Loss: 0.6815613865852356, Validation Loss: 0.6848006248474121\n",
      "Epoch 88: Train Loss: 0.6857392072677613, Validation Loss: 0.6847684383392334\n",
      "Epoch 89: Train Loss: 0.6604188501834869, Validation Loss: 0.6849257946014404\n",
      "Epoch 90: Train Loss: 0.6752034068107605, Validation Loss: 0.6846091151237488\n",
      "Epoch 91: Train Loss: 0.7028468489646912, Validation Loss: 0.6843764185905457\n",
      "Epoch 92: Train Loss: 0.6470439791679382, Validation Loss: 0.6840353012084961\n",
      "Epoch 93: Train Loss: 0.7118742346763611, Validation Loss: 0.6841287612915039\n",
      "Epoch 94: Train Loss: 0.6972267985343933, Validation Loss: 0.6842327117919922\n",
      "Epoch 95: Train Loss: 0.6531022667884827, Validation Loss: 0.6846384406089783\n",
      "Epoch 96: Train Loss: 0.6799167513847351, Validation Loss: 0.684429407119751\n",
      "Epoch 97: Train Loss: 0.6875349521636963, Validation Loss: 0.6840399503707886\n",
      "Epoch 98: Train Loss: 0.7986552000045777, Validation Loss: 0.6842948794364929\n",
      "Epoch 99: Train Loss: 0.672433602809906, Validation Loss: 0.6839620471000671\n",
      "Epoch 100: Train Loss: 0.6993954300880432, Validation Loss: 0.6841087341308594\n",
      "Epoch 101: Train Loss: 0.6799525141716003, Validation Loss: 0.6837652921676636\n",
      "Epoch 102: Train Loss: 0.7051307559013367, Validation Loss: 0.6839706301689148\n",
      "Epoch 103: Train Loss: 0.7035994410514832, Validation Loss: 0.6836866736412048\n",
      "Epoch 104: Train Loss: 0.7480708360671997, Validation Loss: 0.6844295263290405\n",
      "Epoch 105: Train Loss: 0.6570465207099915, Validation Loss: 0.6846730709075928\n",
      "Epoch 106: Train Loss: 0.6404518127441406, Validation Loss: 0.6846774220466614\n",
      "Epoch 107: Train Loss: 0.6671661257743835, Validation Loss: 0.684087872505188\n",
      "Epoch 108: Train Loss: 0.7132181882858276, Validation Loss: 0.6841919422149658\n",
      "Epoch 109: Train Loss: 0.6566133379936219, Validation Loss: 0.6840829849243164\n",
      "Epoch 110: Train Loss: 0.6452912211418151, Validation Loss: 0.6838244795799255\n",
      "Epoch 111: Train Loss: 0.7408388972282409, Validation Loss: 0.6834045052528381\n",
      "Epoch 112: Train Loss: 0.7525519967079163, Validation Loss: 0.6831344366073608\n",
      "Epoch 113: Train Loss: 0.684493112564087, Validation Loss: 0.6830090880393982\n",
      "Epoch 114: Train Loss: 0.6912557721138001, Validation Loss: 0.6825671195983887\n",
      "Epoch 115: Train Loss: 0.6771179795265198, Validation Loss: 0.6823012232780457\n",
      "Epoch 116: Train Loss: 0.6767944455146789, Validation Loss: 0.6823903322219849\n",
      "Epoch 117: Train Loss: 0.6717345476150512, Validation Loss: 0.6824503540992737\n",
      "Epoch 118: Train Loss: 0.671074903011322, Validation Loss: 0.6831611394882202\n",
      "Epoch 119: Train Loss: 0.6961227536201477, Validation Loss: 0.6827007532119751\n",
      "Epoch 120: Train Loss: 0.6821504831314087, Validation Loss: 0.6828194856643677\n",
      "Epoch 121: Train Loss: 0.6938641309738159, Validation Loss: 0.6820915341377258\n",
      "Epoch 122: Train Loss: 0.6576421141624451, Validation Loss: 0.6831499934196472\n",
      "Epoch 123: Train Loss: 0.6552625298500061, Validation Loss: 0.6821965575218201\n",
      "Epoch 124: Train Loss: 0.6658102989196777, Validation Loss: 0.6816384792327881\n",
      "Epoch 125: Train Loss: 0.6818121671676636, Validation Loss: 0.6814042925834656\n",
      "Epoch 126: Train Loss: 0.6517457246780396, Validation Loss: 0.6813795566558838\n",
      "Epoch 127: Train Loss: 0.6698024153709412, Validation Loss: 0.6819769144058228\n",
      "Epoch 128: Train Loss: 0.6876988649368286, Validation Loss: 0.6816375851631165\n",
      "Epoch 129: Train Loss: 0.7035186052322387, Validation Loss: 0.6821786165237427\n",
      "Epoch 130: Train Loss: 0.6765085220336914, Validation Loss: 0.6819073557853699\n",
      "Epoch 131: Train Loss: 0.6784131765365601, Validation Loss: 0.6815477013587952\n",
      "Epoch 132: Train Loss: 0.7177056074142456, Validation Loss: 0.6820245981216431\n",
      "Epoch 133: Train Loss: 0.6746821403503418, Validation Loss: 0.6823481321334839\n",
      "Epoch 134: Train Loss: 0.6642404675483704, Validation Loss: 0.6828411817550659\n",
      "Epoch 135: Train Loss: 0.6850739359855652, Validation Loss: 0.6827232241630554\n",
      "Epoch 136: Train Loss: 0.6760133981704712, Validation Loss: 0.6825671195983887\n",
      "Epoch 137: Train Loss: 0.6775246024131775, Validation Loss: 0.6825469136238098\n",
      "Epoch 138: Train Loss: 0.6917893648147583, Validation Loss: 0.6827000379562378\n",
      "Epoch 139: Train Loss: 0.728209376335144, Validation Loss: 0.6836742758750916\n",
      "Epoch 140: Train Loss: 0.6726251602172851, Validation Loss: 0.6830245852470398\n",
      "Epoch 141: Train Loss: 0.683012056350708, Validation Loss: 0.6827801465988159\n",
      "Epoch 142: Train Loss: 0.6606882095336915, Validation Loss: 0.6821475028991699\n",
      "Epoch 143: Train Loss: 0.6879155278205872, Validation Loss: 0.6817197203636169\n",
      "Epoch 144: Train Loss: 0.6902984976768494, Validation Loss: 0.6815375089645386\n",
      "Epoch 145: Train Loss: 0.6583482980728149, Validation Loss: 0.6817554831504822\n",
      "Epoch 146: Train Loss: 0.6563272714614868, Validation Loss: 0.6817209720611572\n",
      "Epoch 147: Train Loss: 0.6784367203712464, Validation Loss: 0.6825598478317261\n",
      "Epoch 148: Train Loss: 0.6214730143547058, Validation Loss: 0.6820884346961975\n",
      "Epoch 149: Train Loss: 0.6466665029525757, Validation Loss: 0.6819442510604858\n",
      "Epoch 150: Train Loss: 0.6784233093261719, Validation Loss: 0.6816847920417786\n",
      "Epoch 151: Train Loss: 0.6292671680450439, Validation Loss: 0.6822499632835388\n",
      "Epoch 152: Train Loss: 0.6609165906906128, Validation Loss: 0.6837910413742065\n",
      "Epoch 153: Train Loss: 0.6647060513496399, Validation Loss: 0.6830636262893677\n",
      "Epoch 154: Train Loss: 0.6671097159385682, Validation Loss: 0.6818196773529053\n",
      "Epoch 155: Train Loss: 0.6679410457611084, Validation Loss: 0.6814596652984619\n",
      "Epoch 156: Train Loss: 0.6613669991493225, Validation Loss: 0.6813905835151672\n",
      "Epoch 157: Train Loss: 0.6634008169174195, Validation Loss: 0.6815350651741028\n",
      "Epoch 158: Train Loss: 0.6983436703681946, Validation Loss: 0.6813071966171265\n",
      "Epoch 159: Train Loss: 0.66966153383255, Validation Loss: 0.6810578107833862\n",
      "Epoch 160: Train Loss: 0.6890282750129699, Validation Loss: 0.6809142231941223\n",
      "Epoch 161: Train Loss: 0.6734787940979003, Validation Loss: 0.6806809306144714\n",
      "Epoch 162: Train Loss: 0.6868525624275208, Validation Loss: 0.680778443813324\n",
      "Epoch 163: Train Loss: 0.6553539872169495, Validation Loss: 0.6803811192512512\n",
      "Epoch 164: Train Loss: 0.6454106450080872, Validation Loss: 0.679893434047699\n",
      "Epoch 165: Train Loss: 0.6767258286476135, Validation Loss: 0.6808504462242126\n",
      "Epoch 166: Train Loss: 0.679999828338623, Validation Loss: 0.6798893809318542\n",
      "Epoch 167: Train Loss: 0.676433265209198, Validation Loss: 0.6796233057975769\n",
      "Epoch 168: Train Loss: 0.6727624654769897, Validation Loss: 0.6797770261764526\n",
      "Epoch 169: Train Loss: 0.6775132179260254, Validation Loss: 0.6799039840698242\n",
      "Epoch 170: Train Loss: 0.6523144721984864, Validation Loss: 0.6805115938186646\n",
      "Epoch 171: Train Loss: 0.7005701661109924, Validation Loss: 0.6794562339782715\n",
      "Epoch 172: Train Loss: 0.6658770918846131, Validation Loss: 0.6793534159660339\n",
      "Epoch 173: Train Loss: 0.6791619181632995, Validation Loss: 0.6788374185562134\n",
      "Epoch 174: Train Loss: 0.6839828848838806, Validation Loss: 0.6789801120758057\n",
      "Epoch 175: Train Loss: 0.7064594388008117, Validation Loss: 0.6787343621253967\n",
      "Epoch 176: Train Loss: 0.6324322938919067, Validation Loss: 0.6810609102249146\n",
      "Epoch 177: Train Loss: 0.7288429021835328, Validation Loss: 0.680083155632019\n",
      "Epoch 178: Train Loss: 0.7070869326591491, Validation Loss: 0.6792833805084229\n",
      "Epoch 179: Train Loss: 0.6595390796661377, Validation Loss: 0.6801832914352417\n",
      "Epoch 180: Train Loss: 0.6767088651657105, Validation Loss: 0.6798744201660156\n",
      "Epoch 181: Train Loss: 0.6194434404373169, Validation Loss: 0.6796491742134094\n",
      "Epoch 182: Train Loss: 0.6800178050994873, Validation Loss: 0.6791645884513855\n",
      "Epoch 183: Train Loss: 0.6826052069664001, Validation Loss: 0.6787927746772766\n",
      "Epoch 184: Train Loss: 0.6718395471572876, Validation Loss: 0.6786362528800964\n",
      "Epoch 185: Train Loss: 0.648710823059082, Validation Loss: 0.6795424222946167\n",
      "Epoch 186: Train Loss: 0.6759132742881775, Validation Loss: 0.6797005534172058\n",
      "Epoch 187: Train Loss: 0.6742980480194092, Validation Loss: 0.679385781288147\n",
      "Epoch 188: Train Loss: 0.6450279831886292, Validation Loss: 0.6788098216056824\n",
      "Epoch 189: Train Loss: 0.6679559350013733, Validation Loss: 0.6788780093193054\n",
      "Epoch 190: Train Loss: 0.6508284449577332, Validation Loss: 0.68011474609375\n",
      "Epoch 191: Train Loss: 0.6738684415817261, Validation Loss: 0.680636465549469\n",
      "Epoch 192: Train Loss: 0.6445080280303955, Validation Loss: 0.6805729866027832\n",
      "Epoch 193: Train Loss: 0.6646217107772827, Validation Loss: 0.6798006892204285\n",
      "Epoch 194: Train Loss: 0.6740915298461914, Validation Loss: 0.6793574690818787\n",
      "Epoch 195: Train Loss: 0.6706492304801941, Validation Loss: 0.6789649128913879\n",
      "Epoch 196: Train Loss: 0.6596685409545898, Validation Loss: 0.6785009503364563\n",
      "Epoch 197: Train Loss: 0.6943712949752807, Validation Loss: 0.678185760974884\n",
      "Epoch 198: Train Loss: 0.656845211982727, Validation Loss: 0.6776485443115234\n",
      "Epoch 199: Train Loss: 0.6260844707489014, Validation Loss: 0.6785000562667847\n",
      "Epoch 200: Train Loss: 0.6650018930435181, Validation Loss: 0.6783658266067505\n",
      "Epoch 201: Train Loss: 0.709153151512146, Validation Loss: 0.6781417727470398\n",
      "Epoch 202: Train Loss: 0.6604664325714111, Validation Loss: 0.6781104207038879\n",
      "Epoch 203: Train Loss: 0.6247618019580841, Validation Loss: 0.6794736385345459\n",
      "Epoch 204: Train Loss: 0.6699578046798706, Validation Loss: 0.6789441704750061\n",
      "Epoch 205: Train Loss: 0.6579627990722656, Validation Loss: 0.6793240308761597\n",
      "Epoch 206: Train Loss: 0.626330041885376, Validation Loss: 0.6788141131401062\n",
      "Epoch 207: Train Loss: 0.6633734226226806, Validation Loss: 0.6780610084533691\n",
      "Epoch 208: Train Loss: 0.6596176624298096, Validation Loss: 0.6780324578285217\n",
      "Epoch 209: Train Loss: 0.6643945693969726, Validation Loss: 0.6777653694152832\n",
      "Epoch 210: Train Loss: 0.669256865978241, Validation Loss: 0.677876353263855\n",
      "Epoch 211: Train Loss: 0.6421700239181518, Validation Loss: 0.6776734590530396\n",
      "Epoch 212: Train Loss: 0.6285655736923218, Validation Loss: 0.6788140535354614\n",
      "Epoch 213: Train Loss: 0.6314244270324707, Validation Loss: 0.6780298948287964\n",
      "Epoch 214: Train Loss: 0.6190847039222718, Validation Loss: 0.677932620048523\n",
      "Epoch 215: Train Loss: 0.6477700233459472, Validation Loss: 0.6792759895324707\n",
      "Epoch 216: Train Loss: 0.7123119592666626, Validation Loss: 0.6780264377593994\n",
      "Epoch 217: Train Loss: 0.6663016319274903, Validation Loss: 0.6779662370681763\n",
      "Epoch 218: Train Loss: 0.6590855240821838, Validation Loss: 0.6779085993766785\n",
      "Epoch 219: Train Loss: 0.6569186925888062, Validation Loss: 0.6774998307228088\n",
      "Epoch 220: Train Loss: 0.6301365196704865, Validation Loss: 0.6773578524589539\n",
      "Epoch 221: Train Loss: 0.6601152539253234, Validation Loss: 0.6776896715164185\n",
      "Epoch 222: Train Loss: 0.6727659106254578, Validation Loss: 0.6764369010925293\n",
      "Epoch 223: Train Loss: 0.6575170516967773, Validation Loss: 0.6767718195915222\n",
      "Epoch 224: Train Loss: 0.6457401037216186, Validation Loss: 0.6771775484085083\n",
      "Epoch 225: Train Loss: 0.6896730661392212, Validation Loss: 0.6766513586044312\n",
      "Epoch 226: Train Loss: 0.6255042791366577, Validation Loss: 0.6763104200363159\n",
      "Epoch 227: Train Loss: 0.6839456200599671, Validation Loss: 0.676422119140625\n",
      "Epoch 228: Train Loss: 0.6724398612976075, Validation Loss: 0.6766395568847656\n",
      "Epoch 229: Train Loss: 0.6281516909599304, Validation Loss: 0.6765244007110596\n",
      "Epoch 230: Train Loss: 0.6856263875961304, Validation Loss: 0.6757822036743164\n",
      "Epoch 231: Train Loss: 0.64620521068573, Validation Loss: 0.6756632924079895\n",
      "Epoch 232: Train Loss: 0.684513795375824, Validation Loss: 0.6756715774536133\n",
      "Epoch 233: Train Loss: 0.6821963310241699, Validation Loss: 0.6756025552749634\n",
      "Epoch 234: Train Loss: 0.671128511428833, Validation Loss: 0.6752151846885681\n",
      "Epoch 235: Train Loss: 0.6479527473449707, Validation Loss: 0.6773477792739868\n",
      "Epoch 236: Train Loss: 0.6790194749832154, Validation Loss: 0.6761412024497986\n",
      "Epoch 237: Train Loss: 0.6353793025016785, Validation Loss: 0.6755008697509766\n",
      "Epoch 238: Train Loss: 0.6759378790855408, Validation Loss: 0.676453173160553\n",
      "Epoch 239: Train Loss: 0.6520425319671631, Validation Loss: 0.6768325567245483\n",
      "Epoch 240: Train Loss: 0.6711834907531739, Validation Loss: 0.6764838099479675\n",
      "Epoch 241: Train Loss: 0.655689024925232, Validation Loss: 0.6757694482803345\n",
      "Epoch 242: Train Loss: 0.6390760898590088, Validation Loss: 0.6765268445014954\n",
      "Epoch 243: Train Loss: 0.6678067922592164, Validation Loss: 0.6756097674369812\n",
      "Epoch 244: Train Loss: 0.6480368494987487, Validation Loss: 0.6762641072273254\n",
      "Epoch 245: Train Loss: 0.6614045381546021, Validation Loss: 0.6753138303756714\n",
      "Epoch 246: Train Loss: 0.6734835028648376, Validation Loss: 0.675411581993103\n",
      "Epoch 247: Train Loss: 0.6101998090744019, Validation Loss: 0.6753374338150024\n",
      "Epoch 248: Train Loss: 0.639653205871582, Validation Loss: 0.674310564994812\n",
      "Epoch 249: Train Loss: 0.6663108825683594, Validation Loss: 0.6743558049201965\n",
      "Epoch 250: Train Loss: 0.6439483046531678, Validation Loss: 0.6748606562614441\n",
      "Epoch 251: Train Loss: 0.6641201853752137, Validation Loss: 0.6743970513343811\n",
      "Epoch 252: Train Loss: 0.6027255296707154, Validation Loss: 0.6749756336212158\n",
      "Epoch 253: Train Loss: 0.6629290342330932, Validation Loss: 0.6741089820861816\n",
      "Epoch 254: Train Loss: 0.6515481472015381, Validation Loss: 0.6734265089035034\n",
      "Epoch 255: Train Loss: 0.6774574041366577, Validation Loss: 0.672880232334137\n",
      "Epoch 256: Train Loss: 0.6393255591392517, Validation Loss: 0.6727719306945801\n",
      "Epoch 257: Train Loss: 0.6172222137451172, Validation Loss: 0.6718658804893494\n",
      "Epoch 258: Train Loss: 0.6735068678855896, Validation Loss: 0.6724545955657959\n",
      "Epoch 259: Train Loss: 0.6482268333435058, Validation Loss: 0.6728914380073547\n",
      "Epoch 260: Train Loss: 0.6524852871894836, Validation Loss: 0.6725494265556335\n",
      "Epoch 261: Train Loss: 0.6819600701332093, Validation Loss: 0.6714527606964111\n",
      "Epoch 262: Train Loss: 0.6659894585609436, Validation Loss: 0.6714304685592651\n",
      "Epoch 263: Train Loss: 0.6962028503417969, Validation Loss: 0.670521080493927\n",
      "Epoch 264: Train Loss: 0.6818547964096069, Validation Loss: 0.6703052520751953\n",
      "Epoch 265: Train Loss: 0.6134115934371949, Validation Loss: 0.6715338826179504\n",
      "Epoch 266: Train Loss: 0.676858413219452, Validation Loss: 0.6701306700706482\n",
      "Epoch 267: Train Loss: 0.6109962463378906, Validation Loss: 0.6701630353927612\n",
      "Epoch 268: Train Loss: 0.6664989233016968, Validation Loss: 0.6705942749977112\n",
      "Epoch 269: Train Loss: 0.6405088543891907, Validation Loss: 0.6701911687850952\n",
      "Epoch 270: Train Loss: 0.6188684105873108, Validation Loss: 0.6693991422653198\n",
      "Epoch 271: Train Loss: 0.6491520762443542, Validation Loss: 0.668393075466156\n",
      "Epoch 272: Train Loss: 0.6555399298667908, Validation Loss: 0.6683363914489746\n",
      "Epoch 273: Train Loss: 0.6482322216033936, Validation Loss: 0.6678014397621155\n",
      "Epoch 274: Train Loss: 0.6655787587165832, Validation Loss: 0.6669228076934814\n",
      "Epoch 275: Train Loss: 0.6596712589263916, Validation Loss: 0.6670670509338379\n",
      "Epoch 276: Train Loss: 0.6397026777267456, Validation Loss: 0.6669370532035828\n",
      "Epoch 277: Train Loss: 0.654922616481781, Validation Loss: 0.6665453910827637\n",
      "Epoch 278: Train Loss: 0.6369379162788391, Validation Loss: 0.6662985682487488\n",
      "Epoch 279: Train Loss: 0.623506498336792, Validation Loss: 0.6654548645019531\n",
      "Epoch 280: Train Loss: 0.6310275673866272, Validation Loss: 0.6656469106674194\n",
      "Epoch 281: Train Loss: 0.6142556428909302, Validation Loss: 0.6641194820404053\n",
      "Epoch 282: Train Loss: 0.6944098830223083, Validation Loss: 0.6642895936965942\n",
      "Epoch 283: Train Loss: 0.7068518996238708, Validation Loss: 0.6638519167900085\n",
      "Epoch 284: Train Loss: 0.6335744619369507, Validation Loss: 0.6634526252746582\n",
      "Epoch 285: Train Loss: 0.6773235321044921, Validation Loss: 0.6638131737709045\n",
      "Epoch 286: Train Loss: 0.6672296047210693, Validation Loss: 0.6644629836082458\n",
      "Epoch 287: Train Loss: 0.6432896614074707, Validation Loss: 0.6623433828353882\n",
      "Epoch 288: Train Loss: 0.6461519837379456, Validation Loss: 0.6612019538879395\n",
      "Epoch 289: Train Loss: 0.6296110630035401, Validation Loss: 0.6603390574455261\n",
      "Epoch 290: Train Loss: 0.6415391802787781, Validation Loss: 0.6613451242446899\n",
      "Epoch 291: Train Loss: 0.6695848107337952, Validation Loss: 0.6607733964920044\n",
      "Epoch 292: Train Loss: 0.6408482670783997, Validation Loss: 0.6606438159942627\n",
      "Epoch 293: Train Loss: 0.5745891749858856, Validation Loss: 0.6601529121398926\n",
      "Epoch 294: Train Loss: 0.6524432897567749, Validation Loss: 0.6625734567642212\n",
      "Epoch 295: Train Loss: 0.6070767998695373, Validation Loss: 0.6597334742546082\n",
      "Epoch 296: Train Loss: 0.6517394542694092, Validation Loss: 0.6599006652832031\n",
      "Epoch 297: Train Loss: 0.6483443021774292, Validation Loss: 0.6575360298156738\n",
      "Epoch 298: Train Loss: 0.6328237056732178, Validation Loss: 0.6598133444786072\n",
      "Epoch 299: Train Loss: 0.6541330218315125, Validation Loss: 0.6595038175582886\n",
      "Epoch 300: Train Loss: 0.6191844940185547, Validation Loss: 0.6581997871398926\n",
      "Epoch 301: Train Loss: 0.6060478806495666, Validation Loss: 0.6574519872665405\n",
      "Epoch 302: Train Loss: 0.5790225744247437, Validation Loss: 0.6568578481674194\n",
      "Epoch 303: Train Loss: 0.6229281663894654, Validation Loss: 0.6555365920066833\n",
      "Epoch 304: Train Loss: 0.6503984093666076, Validation Loss: 0.6539715528488159\n",
      "Epoch 305: Train Loss: 0.6409068703651428, Validation Loss: 0.6524217128753662\n",
      "Epoch 306: Train Loss: 0.574050122499466, Validation Loss: 0.652815043926239\n",
      "Epoch 307: Train Loss: 0.6554422736167907, Validation Loss: 0.6530913710594177\n",
      "Epoch 308: Train Loss: 0.6410947799682617, Validation Loss: 0.6499852538108826\n",
      "Epoch 309: Train Loss: 0.6268531560897828, Validation Loss: 0.6477913856506348\n",
      "Epoch 310: Train Loss: 0.6108098864555359, Validation Loss: 0.6496976613998413\n",
      "Epoch 311: Train Loss: 0.5828787922859192, Validation Loss: 0.6499755382537842\n",
      "Epoch 312: Train Loss: 0.6308688998222352, Validation Loss: 0.6467467546463013\n",
      "Epoch 313: Train Loss: 0.5914614081382752, Validation Loss: 0.6473381519317627\n",
      "Epoch 314: Train Loss: 0.6059224367141723, Validation Loss: 0.6494139432907104\n",
      "Epoch 315: Train Loss: 0.6195125102996826, Validation Loss: 0.6460410952568054\n",
      "Epoch 316: Train Loss: 0.6085598349571228, Validation Loss: 0.6416164040565491\n",
      "Epoch 317: Train Loss: 0.6202948212623596, Validation Loss: 0.6419055461883545\n",
      "Epoch 318: Train Loss: 0.6013876438140869, Validation Loss: 0.643930196762085\n",
      "Epoch 319: Train Loss: 0.6317212104797363, Validation Loss: 0.6412145495414734\n",
      "Epoch 320: Train Loss: 0.6330872893333435, Validation Loss: 0.6421045064926147\n",
      "Epoch 321: Train Loss: 0.6178215980529785, Validation Loss: 0.6452323198318481\n",
      "Epoch 322: Train Loss: 0.6473995327949524, Validation Loss: 0.6382274627685547\n",
      "Epoch 323: Train Loss: 0.5978288412094116, Validation Loss: 0.6397640109062195\n",
      "Epoch 324: Train Loss: 0.5836731553077698, Validation Loss: 0.6388044953346252\n",
      "Epoch 325: Train Loss: 0.6060453295707703, Validation Loss: 0.6346237063407898\n",
      "Epoch 326: Train Loss: 0.6620532751083374, Validation Loss: 0.6350215673446655\n",
      "Epoch 327: Train Loss: 0.6059850454330444, Validation Loss: 0.633212149143219\n",
      "Epoch 328: Train Loss: 0.6346420049667358, Validation Loss: 0.63397216796875\n",
      "Epoch 329: Train Loss: 0.6105621218681335, Validation Loss: 0.6312050819396973\n",
      "Epoch 330: Train Loss: 0.6027432799339294, Validation Loss: 0.6292975544929504\n",
      "Epoch 331: Train Loss: 0.5842603921890259, Validation Loss: 0.627689778804779\n",
      "Epoch 332: Train Loss: 0.5777780175209045, Validation Loss: 0.6255293488502502\n",
      "Epoch 333: Train Loss: 0.5514250755310058, Validation Loss: 0.6221875548362732\n",
      "Epoch 334: Train Loss: 0.5815677404403686, Validation Loss: 0.6202060580253601\n",
      "Epoch 335: Train Loss: 0.5953103184700013, Validation Loss: 0.6191617846488953\n",
      "Epoch 336: Train Loss: 0.5949194312095643, Validation Loss: 0.6154817938804626\n",
      "Epoch 337: Train Loss: 0.6377247929573059, Validation Loss: 0.612387478351593\n",
      "Epoch 338: Train Loss: 0.6182081222534179, Validation Loss: 0.6100543737411499\n",
      "Epoch 339: Train Loss: 0.6216054320335388, Validation Loss: 0.609988272190094\n",
      "Epoch 340: Train Loss: 0.550306248664856, Validation Loss: 0.6094384789466858\n",
      "Epoch 341: Train Loss: 0.5724912524223328, Validation Loss: 0.6043065786361694\n",
      "Epoch 342: Train Loss: 0.5433102071285247, Validation Loss: 0.5978619456291199\n",
      "Epoch 343: Train Loss: 0.6109209895133972, Validation Loss: 0.6004262566566467\n",
      "Epoch 344: Train Loss: 0.5909327745437623, Validation Loss: 0.6026687622070312\n",
      "Epoch 345: Train Loss: 0.6062711000442504, Validation Loss: 0.5984855890274048\n",
      "Epoch 346: Train Loss: 0.5488232672214508, Validation Loss: 0.5969637632369995\n",
      "Epoch 347: Train Loss: 0.5273257851600647, Validation Loss: 0.6001598238945007\n",
      "Epoch 348: Train Loss: 0.6145016551017761, Validation Loss: 0.5994135141372681\n",
      "Epoch 349: Train Loss: 0.6008445858955384, Validation Loss: 0.6001755595207214\n",
      "Epoch 350: Train Loss: 0.5499245643615722, Validation Loss: 0.6028755307197571\n",
      "Epoch 351: Train Loss: 0.631497859954834, Validation Loss: 0.5925649404525757\n",
      "Epoch 352: Train Loss: 0.5695703029632568, Validation Loss: 0.5958071351051331\n",
      "Epoch 353: Train Loss: 0.6001134991645813, Validation Loss: 0.5885921120643616\n",
      "Epoch 354: Train Loss: 0.5692787051200867, Validation Loss: 0.5840184688568115\n",
      "Epoch 355: Train Loss: 0.5437625169754028, Validation Loss: 0.5805708169937134\n",
      "Epoch 356: Train Loss: 0.5979378461837769, Validation Loss: 0.5783032178878784\n",
      "Epoch 357: Train Loss: 0.5871499538421631, Validation Loss: 0.5798467993736267\n",
      "Epoch 358: Train Loss: 0.5419667005538941, Validation Loss: 0.5755287408828735\n",
      "Epoch 359: Train Loss: 0.5382808387279511, Validation Loss: 0.5713775753974915\n",
      "Epoch 360: Train Loss: 0.551084715127945, Validation Loss: 0.5752256512641907\n",
      "Epoch 361: Train Loss: 0.545409083366394, Validation Loss: 0.5778559446334839\n",
      "Epoch 362: Train Loss: 0.6265187323093414, Validation Loss: 0.5809524059295654\n",
      "Epoch 363: Train Loss: 0.5808627963066101, Validation Loss: 0.5895940065383911\n",
      "Epoch 364: Train Loss: 0.5714674472808838, Validation Loss: 0.5915582180023193\n",
      "Epoch 365: Train Loss: 0.5402662992477417, Validation Loss: 0.588560163974762\n",
      "Epoch 366: Train Loss: 0.5645357012748718, Validation Loss: 0.5820627212524414\n",
      "Epoch 367: Train Loss: 0.56299307346344, Validation Loss: 0.5757869482040405\n",
      "Epoch 368: Train Loss: 0.55171138048172, Validation Loss: 0.5738352537155151\n",
      "Epoch 369: Train Loss: 0.5645380973815918, Validation Loss: 0.5710792541503906\n",
      "Epoch 370: Train Loss: 0.5192231416702271, Validation Loss: 0.5682340860366821\n",
      "Epoch 371: Train Loss: 0.6112302005290985, Validation Loss: 0.559939444065094\n",
      "Epoch 372: Train Loss: 0.45953774750232695, Validation Loss: 0.5505671501159668\n",
      "Epoch 373: Train Loss: 0.5117701470851899, Validation Loss: 0.5522067546844482\n",
      "Epoch 374: Train Loss: 0.5173446595668793, Validation Loss: 0.5518389344215393\n",
      "Epoch 375: Train Loss: 0.5182317197322845, Validation Loss: 0.5542250275611877\n",
      "Epoch 376: Train Loss: 0.5123667180538177, Validation Loss: 0.5526965856552124\n",
      "Epoch 377: Train Loss: 0.4695089101791382, Validation Loss: 0.5505502820014954\n",
      "Epoch 378: Train Loss: 0.47594935297966, Validation Loss: 0.5569371581077576\n",
      "Epoch 379: Train Loss: 0.5055742740631104, Validation Loss: 0.5542767643928528\n",
      "Epoch 380: Train Loss: 0.5220236480236053, Validation Loss: 0.5482766628265381\n",
      "Epoch 381: Train Loss: 0.491126275062561, Validation Loss: 0.5498074293136597\n",
      "Epoch 382: Train Loss: 0.5276617527008056, Validation Loss: 0.5508062839508057\n",
      "Epoch 383: Train Loss: 0.537467360496521, Validation Loss: 0.5407538414001465\n",
      "Epoch 384: Train Loss: 0.517728990316391, Validation Loss: 0.5411465764045715\n",
      "Epoch 385: Train Loss: 0.4827010929584503, Validation Loss: 0.5408172607421875\n",
      "Epoch 386: Train Loss: 0.47580512166023253, Validation Loss: 0.5401401519775391\n",
      "Epoch 387: Train Loss: 0.4303793847560883, Validation Loss: 0.5319000482559204\n",
      "Epoch 388: Train Loss: 0.47732807993888854, Validation Loss: 0.5335515737533569\n",
      "Epoch 389: Train Loss: 0.5860006272792816, Validation Loss: 0.5393598079681396\n",
      "Epoch 390: Train Loss: 0.5144149422645569, Validation Loss: 0.5381618142127991\n",
      "Epoch 391: Train Loss: 0.4906578779220581, Validation Loss: 0.5353114604949951\n",
      "Epoch 392: Train Loss: 0.4893329441547394, Validation Loss: 0.5499207973480225\n",
      "Epoch 393: Train Loss: 0.427673214673996, Validation Loss: 0.5417191982269287\n",
      "Epoch 394: Train Loss: 0.5873282372951507, Validation Loss: 0.5194852352142334\n",
      "Epoch 395: Train Loss: 0.4872440814971924, Validation Loss: 0.5297498106956482\n",
      "Epoch 396: Train Loss: 0.5904726684093475, Validation Loss: 0.5274707078933716\n",
      "Epoch 397: Train Loss: 0.5206323564052582, Validation Loss: 0.5266609787940979\n",
      "Epoch 398: Train Loss: 0.5050524771213531, Validation Loss: 0.5270266532897949\n",
      "Epoch 399: Train Loss: 0.46710544228553774, Validation Loss: 0.5404539704322815\n",
      "Epoch 400: Train Loss: 0.53221355676651, Validation Loss: 0.5567406415939331\n",
      "Epoch 401: Train Loss: 0.4225579500198364, Validation Loss: 0.5493374466896057\n",
      "Epoch 402: Train Loss: 0.4687322676181793, Validation Loss: 0.5402244329452515\n",
      "Epoch 403: Train Loss: 0.5017679870128632, Validation Loss: 0.529486894607544\n",
      "Epoch 404: Train Loss: 0.47411786317825316, Validation Loss: 0.5335109829902649\n",
      "Epoch 405: Train Loss: 0.4820448100566864, Validation Loss: 0.5401570796966553\n",
      "Epoch 406: Train Loss: 0.5500810205936432, Validation Loss: 0.5284807085990906\n",
      "Epoch 407: Train Loss: 0.43568626046180725, Validation Loss: 0.5423488616943359\n",
      "Epoch 408: Train Loss: 0.49068763852119446, Validation Loss: 0.5316888689994812\n",
      "Epoch 409: Train Loss: 0.43353103995323183, Validation Loss: 0.5281305909156799\n",
      "Epoch 410: Train Loss: 0.4622225403785706, Validation Loss: 0.5285249352455139\n",
      "Epoch 411: Train Loss: 0.4722802460193634, Validation Loss: 0.5503580570220947\n",
      "Epoch 412: Train Loss: 0.4362832486629486, Validation Loss: 0.5307285785675049\n",
      "Epoch 413: Train Loss: 0.4688841223716736, Validation Loss: 0.518804132938385\n",
      "Epoch 414: Train Loss: 0.40260223150253294, Validation Loss: 0.5309627056121826\n",
      "Epoch 415: Train Loss: 0.4857476890087128, Validation Loss: 0.5355600118637085\n",
      "Epoch 416: Train Loss: 0.4949382424354553, Validation Loss: 0.5296348929405212\n",
      "Epoch 417: Train Loss: 0.4550701558589935, Validation Loss: 0.543059229850769\n",
      "Epoch 418: Train Loss: 0.4732917666435242, Validation Loss: 0.5414113998413086\n",
      "Epoch 419: Train Loss: 0.42680038809776305, Validation Loss: 0.5511456727981567\n",
      "Epoch 420: Train Loss: 0.4238128960132599, Validation Loss: 0.5679975152015686\n",
      "Epoch 421: Train Loss: 0.40295621156692507, Validation Loss: 0.5602496862411499\n",
      "Epoch 422: Train Loss: 0.37571353912353517, Validation Loss: 0.5618813037872314\n",
      "Epoch 423: Train Loss: 0.37003490030765535, Validation Loss: 0.5515080094337463\n",
      "Epoch 424: Train Loss: 0.4024483323097229, Validation Loss: 0.5444445013999939\n",
      "Epoch 425: Train Loss: 0.4496951699256897, Validation Loss: 0.5338993072509766\n",
      "Epoch 426: Train Loss: 0.361637482047081, Validation Loss: 0.5469412803649902\n",
      "Epoch 427: Train Loss: 0.42285919189453125, Validation Loss: 0.5572243332862854\n",
      "Epoch 428: Train Loss: 0.4210697650909424, Validation Loss: 0.5429664254188538\n",
      "Epoch 429: Train Loss: 0.39369423389434816, Validation Loss: 0.5386191606521606\n",
      "Epoch 430: Train Loss: 0.37780995965003966, Validation Loss: 0.5475087761878967\n",
      "Epoch 431: Train Loss: 0.42801970839500425, Validation Loss: 0.5814797878265381\n",
      "Epoch 432: Train Loss: 0.4290854513645172, Validation Loss: 0.5375269651412964\n",
      "Epoch 433: Train Loss: 0.534623658657074, Validation Loss: 0.4918409585952759\n",
      "Epoch 434: Train Loss: 0.38100639581680296, Validation Loss: 0.5035431385040283\n",
      "Epoch 435: Train Loss: 0.3686233878135681, Validation Loss: 0.48834970593452454\n",
      "Epoch 436: Train Loss: 0.4155085325241089, Validation Loss: 0.5309512615203857\n",
      "Epoch 437: Train Loss: 0.43025046586990356, Validation Loss: 0.5499263405799866\n",
      "Epoch 438: Train Loss: 0.39962918162345884, Validation Loss: 0.5602781772613525\n",
      "Epoch 439: Train Loss: 0.34926128685474395, Validation Loss: 0.5516326427459717\n",
      "Epoch 440: Train Loss: 0.38748505115509035, Validation Loss: 0.5422008633613586\n",
      "Epoch 441: Train Loss: 0.38159781098365786, Validation Loss: 0.5423155426979065\n",
      "Epoch 442: Train Loss: 0.3958898961544037, Validation Loss: 0.5262753963470459\n",
      "Epoch 443: Train Loss: 0.3788978695869446, Validation Loss: 0.5105652213096619\n",
      "Epoch 444: Train Loss: 0.4126433849334717, Validation Loss: 0.5244715213775635\n",
      "Epoch 445: Train Loss: 0.374108213186264, Validation Loss: 0.544340193271637\n",
      "Epoch 446: Train Loss: 0.3124619573354721, Validation Loss: 0.577465295791626\n",
      "Epoch 447: Train Loss: 0.32405672371387484, Validation Loss: 0.5340052843093872\n",
      "Epoch 448: Train Loss: 0.39101840257644654, Validation Loss: 0.5623192191123962\n",
      "Epoch 449: Train Loss: 0.3875894010066986, Validation Loss: 0.55320805311203\n",
      "Epoch 450: Train Loss: 0.3126403480768204, Validation Loss: 0.5352475643157959\n",
      "Epoch 451: Train Loss: 0.3389486253261566, Validation Loss: 0.5615149140357971\n",
      "Epoch 452: Train Loss: 0.2831446185708046, Validation Loss: 0.54034823179245\n",
      "Epoch 453: Train Loss: 0.31544819176197053, Validation Loss: 0.5511371493339539\n",
      "Epoch 454: Train Loss: 0.5040134131908417, Validation Loss: 0.5329108238220215\n",
      "Epoch 455: Train Loss: 0.3629014015197754, Validation Loss: 0.5157992243766785\n",
      "Epoch 456: Train Loss: 0.3379682540893555, Validation Loss: 0.5278468728065491\n",
      "Epoch 457: Train Loss: 0.41669997572898865, Validation Loss: 0.5489786863327026\n",
      "Epoch 458: Train Loss: 0.3477177232503891, Validation Loss: 0.5566942691802979\n",
      "Epoch 459: Train Loss: 0.31951109766960145, Validation Loss: 0.5720653533935547\n",
      "Epoch 460: Train Loss: 0.34786139726638793, Validation Loss: 0.5835967659950256\n",
      "Epoch 461: Train Loss: 0.34325672388076783, Validation Loss: 0.5676217079162598\n",
      "Epoch 462: Train Loss: 0.364643520116806, Validation Loss: 0.5859220027923584\n",
      "Epoch 463: Train Loss: 0.33129035234451293, Validation Loss: 0.5976012945175171\n",
      "Epoch 464: Train Loss: 0.4595819354057312, Validation Loss: 0.5441076755523682\n",
      "Epoch 465: Train Loss: 0.31223363280296323, Validation Loss: 0.5480343103408813\n",
      "Epoch 466: Train Loss: 0.3324388325214386, Validation Loss: 0.5582120418548584\n",
      "Epoch 467: Train Loss: 0.3225250571966171, Validation Loss: 0.5734950304031372\n",
      "Epoch 468: Train Loss: 0.29344744980335236, Validation Loss: 0.5743502378463745\n",
      "Epoch 469: Train Loss: 0.28806215673685076, Validation Loss: 0.5824007987976074\n",
      "Epoch 470: Train Loss: 0.3119079887866974, Validation Loss: 0.5835023522377014\n",
      "Epoch 471: Train Loss: 0.36168951988220216, Validation Loss: 0.5214829444885254\n",
      "Epoch 472: Train Loss: 0.3232915222644806, Validation Loss: 0.5007577538490295\n",
      "Epoch 473: Train Loss: 0.3486554563045502, Validation Loss: 0.548071563243866\n",
      "Epoch 474: Train Loss: 0.3021283596754074, Validation Loss: 0.5263438820838928\n",
      "Epoch 475: Train Loss: 0.3503373622894287, Validation Loss: 0.5431309938430786\n",
      "Epoch 476: Train Loss: 0.29338863790035247, Validation Loss: 0.531491219997406\n",
      "Epoch 477: Train Loss: 0.36208980679512026, Validation Loss: 0.5254542827606201\n",
      "Epoch 478: Train Loss: 0.37951275408267976, Validation Loss: 0.5821019411087036\n",
      "Epoch 479: Train Loss: 0.3057130664587021, Validation Loss: 0.5614581108093262\n",
      "Epoch 480: Train Loss: 0.3787631422281265, Validation Loss: 0.587205171585083\n",
      "Epoch 481: Train Loss: 0.32178595662117004, Validation Loss: 0.5621694326400757\n",
      "Epoch 482: Train Loss: 0.29069764018058775, Validation Loss: 0.5508197546005249\n",
      "Epoch 483: Train Loss: 0.32057085931301116, Validation Loss: 0.5967748761177063\n",
      "Epoch 484: Train Loss: 0.31852779984474183, Validation Loss: 0.5748928189277649\n",
      "Epoch 485: Train Loss: 0.2891068607568741, Validation Loss: 0.5526871681213379\n",
      "Epoch 486: Train Loss: 0.30199686586856844, Validation Loss: 0.5411378145217896\n",
      "Epoch 487: Train Loss: 0.29646913409233094, Validation Loss: 0.5691570043563843\n",
      "Epoch 488: Train Loss: 0.30637496113777163, Validation Loss: 0.5969737768173218\n",
      "Epoch 489: Train Loss: 0.3863970160484314, Validation Loss: 0.6218700408935547\n",
      "Epoch 490: Train Loss: 0.4095154345035553, Validation Loss: 0.6154313683509827\n",
      "Epoch 491: Train Loss: 0.3431752324104309, Validation Loss: 0.6097272038459778\n",
      "Epoch 492: Train Loss: 0.3931728690862656, Validation Loss: 0.5697989463806152\n",
      "Epoch 493: Train Loss: 0.40094916224479676, Validation Loss: 0.5932390689849854\n",
      "Epoch 494: Train Loss: 0.26174153536558153, Validation Loss: 0.6171861290931702\n",
      "Epoch 495: Train Loss: 0.34620791077613833, Validation Loss: 0.6184440851211548\n",
      "Epoch 496: Train Loss: 0.3032566964626312, Validation Loss: 0.5780068039894104\n",
      "Epoch 497: Train Loss: 0.3271044850349426, Validation Loss: 0.5936644077301025\n",
      "Epoch 498: Train Loss: 0.4508295476436615, Validation Loss: 0.6443391442298889\n",
      "Epoch 499: Train Loss: 0.33738868236541747, Validation Loss: 0.6440686583518982\n",
      "Fold 10 Metrics:\n",
      "Accuracy: 0.7272727272727273, Precision: 0.6666666666666666, Recall: 1.0, F1-score: 0.8, AUC: 0.7\n",
      "Confusion Matrix:\n",
      "[[2 3]\n",
      " [0 6]]\n",
      "Completed fold 10\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples from subject 1 to test set\n",
      "Adding 6 truth samples from subject 1 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 500)\n",
      "(132, 65, 500)\n",
      "Epoch 0: Train Loss: 0.6834750533103943, Validation Loss: 0.691802442073822\n",
      "Epoch 1: Train Loss: 0.756290328502655, Validation Loss: 0.6916979551315308\n",
      "Epoch 2: Train Loss: 0.7234997987747193, Validation Loss: 0.6911158561706543\n",
      "Epoch 3: Train Loss: 0.6966365575790405, Validation Loss: 0.6908918023109436\n",
      "Epoch 4: Train Loss: 0.7012156963348388, Validation Loss: 0.6899745464324951\n",
      "Epoch 5: Train Loss: 0.7141172409057617, Validation Loss: 0.6891619563102722\n",
      "Epoch 6: Train Loss: 0.7311466813087464, Validation Loss: 0.689398467540741\n",
      "Epoch 7: Train Loss: 0.7299292087554932, Validation Loss: 0.6888713836669922\n",
      "Epoch 8: Train Loss: 0.718549382686615, Validation Loss: 0.6904404759407043\n",
      "Epoch 9: Train Loss: 0.7327446460723877, Validation Loss: 0.6904972195625305\n",
      "Epoch 10: Train Loss: 0.7254888653755188, Validation Loss: 0.6911205649375916\n",
      "Epoch 11: Train Loss: 0.7074496269226074, Validation Loss: 0.6901946067810059\n",
      "Epoch 12: Train Loss: 0.7199391841888427, Validation Loss: 0.6911994814872742\n",
      "Epoch 13: Train Loss: 0.698971438407898, Validation Loss: 0.6909529566764832\n",
      "Epoch 14: Train Loss: 0.7806730628013611, Validation Loss: 0.6928281188011169\n",
      "Epoch 15: Train Loss: 0.7269411683082581, Validation Loss: 0.6932572722434998\n",
      "Epoch 16: Train Loss: 0.713289487361908, Validation Loss: 0.6936074495315552\n",
      "Epoch 17: Train Loss: 0.6896622061729432, Validation Loss: 0.6945079565048218\n",
      "Epoch 18: Train Loss: 0.6867439270019531, Validation Loss: 0.6947801113128662\n",
      "Epoch 19: Train Loss: 0.6828258037567139, Validation Loss: 0.6942967772483826\n",
      "Epoch 20: Train Loss: 0.7285829305648803, Validation Loss: 0.6952894926071167\n",
      "Epoch 21: Train Loss: 0.66993408203125, Validation Loss: 0.6965505480766296\n",
      "Epoch 22: Train Loss: 0.6883539438247681, Validation Loss: 0.6963537335395813\n",
      "Epoch 23: Train Loss: 0.7182863116264343, Validation Loss: 0.6964728236198425\n",
      "Epoch 24: Train Loss: 0.6845411419868469, Validation Loss: 0.6957055330276489\n",
      "Epoch 25: Train Loss: 0.698772931098938, Validation Loss: 0.6940447092056274\n",
      "Epoch 26: Train Loss: 0.6852978706359864, Validation Loss: 0.6939442157745361\n",
      "Epoch 27: Train Loss: 0.6894386410713196, Validation Loss: 0.6935591697692871\n",
      "Epoch 28: Train Loss: 0.6829585671424866, Validation Loss: 0.6927085518836975\n",
      "Epoch 29: Train Loss: 0.6727598071098327, Validation Loss: 0.6925998330116272\n",
      "Epoch 30: Train Loss: 0.6864707112312317, Validation Loss: 0.6910413503646851\n",
      "Epoch 31: Train Loss: 0.6803379774093627, Validation Loss: 0.6920267939567566\n",
      "Epoch 32: Train Loss: 0.6993367791175842, Validation Loss: 0.693218469619751\n",
      "Epoch 33: Train Loss: 0.7147172808647155, Validation Loss: 0.6946185827255249\n",
      "Epoch 34: Train Loss: 0.7042628049850463, Validation Loss: 0.696574866771698\n",
      "Epoch 35: Train Loss: 0.7164453268051147, Validation Loss: 0.696780264377594\n",
      "Epoch 36: Train Loss: 0.7058660387992859, Validation Loss: 0.6972667574882507\n",
      "Epoch 37: Train Loss: 0.6878229022026062, Validation Loss: 0.6979947090148926\n",
      "Epoch 38: Train Loss: 0.7371051073074341, Validation Loss: 0.6980894804000854\n",
      "Epoch 39: Train Loss: 0.7051913261413574, Validation Loss: 0.698878288269043\n",
      "Epoch 40: Train Loss: 0.6902218461036682, Validation Loss: 0.6987678408622742\n",
      "Epoch 41: Train Loss: 0.7198215842247009, Validation Loss: 0.6983269453048706\n",
      "Epoch 42: Train Loss: 0.7250300288200379, Validation Loss: 0.6985200643539429\n",
      "Epoch 43: Train Loss: 0.6918736696243286, Validation Loss: 0.6992117762565613\n",
      "Epoch 44: Train Loss: 0.683954119682312, Validation Loss: 0.6985207796096802\n",
      "Epoch 45: Train Loss: 0.6920061826705932, Validation Loss: 0.6993083357810974\n",
      "Epoch 46: Train Loss: 0.7015286922454834, Validation Loss: 0.700886607170105\n",
      "Epoch 47: Train Loss: 0.6897794246673584, Validation Loss: 0.7011739611625671\n",
      "Epoch 48: Train Loss: 0.6972258687019348, Validation Loss: 0.7013952136039734\n",
      "Epoch 49: Train Loss: 0.6812253713607788, Validation Loss: 0.7021374702453613\n",
      "Epoch 50: Train Loss: 0.6706709027290344, Validation Loss: 0.7031041383743286\n",
      "Epoch 51: Train Loss: 0.6984669685363769, Validation Loss: 0.7050433158874512\n",
      "Epoch 52: Train Loss: 0.7127481818199157, Validation Loss: 0.7054893970489502\n",
      "Epoch 53: Train Loss: 0.6991242647171021, Validation Loss: 0.7065827250480652\n",
      "Epoch 54: Train Loss: 0.6916263461112976, Validation Loss: 0.7076118588447571\n",
      "Epoch 55: Train Loss: 0.6686630606651306, Validation Loss: 0.7080284953117371\n",
      "Epoch 56: Train Loss: 0.6979664325714111, Validation Loss: 0.7096389532089233\n",
      "Epoch 57: Train Loss: 0.6958957314491272, Validation Loss: 0.7107881903648376\n",
      "Epoch 58: Train Loss: 0.7158545970916748, Validation Loss: 0.7101812362670898\n",
      "Epoch 59: Train Loss: 0.6669320344924927, Validation Loss: 0.7105422019958496\n",
      "Epoch 60: Train Loss: 0.6764066338539123, Validation Loss: 0.7082234025001526\n",
      "Epoch 61: Train Loss: 0.7273467421531677, Validation Loss: 0.708901584148407\n",
      "Epoch 62: Train Loss: 0.686554753780365, Validation Loss: 0.7105560898780823\n",
      "Epoch 63: Train Loss: 0.7101305007934571, Validation Loss: 0.7116795182228088\n",
      "Epoch 64: Train Loss: 0.7003337621688843, Validation Loss: 0.7122384309768677\n",
      "Epoch 65: Train Loss: 0.6882794499397278, Validation Loss: 0.7127455472946167\n",
      "Epoch 66: Train Loss: 0.7197381734848023, Validation Loss: 0.7125182747840881\n",
      "Epoch 67: Train Loss: 0.7260014176368713, Validation Loss: 0.711794376373291\n",
      "Epoch 68: Train Loss: 0.71719970703125, Validation Loss: 0.7115456461906433\n",
      "Epoch 69: Train Loss: 0.6896883249282837, Validation Loss: 0.7110297083854675\n",
      "Epoch 70: Train Loss: 0.6927042722702026, Validation Loss: 0.7104465365409851\n",
      "Epoch 71: Train Loss: 0.6970143795013428, Validation Loss: 0.7094796895980835\n",
      "Epoch 72: Train Loss: 0.6804652094841004, Validation Loss: 0.708373486995697\n",
      "Epoch 73: Train Loss: 0.6558034896850586, Validation Loss: 0.7074187397956848\n",
      "Epoch 74: Train Loss: 0.6731051445007324, Validation Loss: 0.7070837020874023\n",
      "Epoch 75: Train Loss: 0.7002526998519898, Validation Loss: 0.7080681920051575\n",
      "Epoch 76: Train Loss: 0.6748804092407227, Validation Loss: 0.7075947523117065\n",
      "Epoch 77: Train Loss: 0.698393714427948, Validation Loss: 0.7084811329841614\n",
      "Epoch 78: Train Loss: 0.7210009694099426, Validation Loss: 0.708791971206665\n",
      "Epoch 79: Train Loss: 0.6630841851234436, Validation Loss: 0.7077827453613281\n",
      "Epoch 80: Train Loss: 0.7506506323814393, Validation Loss: 0.7086080312728882\n",
      "Epoch 81: Train Loss: 0.6858509659767151, Validation Loss: 0.7074871063232422\n",
      "Epoch 82: Train Loss: 0.6989780306816101, Validation Loss: 0.7061087489128113\n",
      "Epoch 83: Train Loss: 0.7015451788902283, Validation Loss: 0.7077096104621887\n",
      "Epoch 84: Train Loss: 0.7194193124771118, Validation Loss: 0.7074409127235413\n",
      "Epoch 85: Train Loss: 0.7047362089157104, Validation Loss: 0.7082867622375488\n",
      "Epoch 86: Train Loss: 0.70431569814682, Validation Loss: 0.7071658372879028\n",
      "Epoch 87: Train Loss: 0.6987475633621216, Validation Loss: 0.7076787948608398\n",
      "Epoch 88: Train Loss: 0.7269848704338073, Validation Loss: 0.7081046104431152\n",
      "Epoch 89: Train Loss: 0.7077751755714417, Validation Loss: 0.7079662084579468\n",
      "Epoch 90: Train Loss: 0.6750128746032715, Validation Loss: 0.7069002985954285\n",
      "Epoch 91: Train Loss: 0.7086279392242432, Validation Loss: 0.7061630487442017\n",
      "Epoch 92: Train Loss: 0.6659832119941711, Validation Loss: 0.7063848376274109\n",
      "Epoch 93: Train Loss: 0.664943790435791, Validation Loss: 0.7075380086898804\n",
      "Epoch 94: Train Loss: 0.7031810045242309, Validation Loss: 0.7087227702140808\n",
      "Epoch 95: Train Loss: 0.7369229793548584, Validation Loss: 0.7079748511314392\n",
      "Epoch 96: Train Loss: 0.6458004832267761, Validation Loss: 0.7067474126815796\n",
      "Epoch 97: Train Loss: 0.7138541221618653, Validation Loss: 0.706702709197998\n",
      "Epoch 98: Train Loss: 0.744858181476593, Validation Loss: 0.7067570686340332\n",
      "Epoch 99: Train Loss: 0.6925611138343811, Validation Loss: 0.7057356834411621\n",
      "Epoch 100: Train Loss: 0.6654060482978821, Validation Loss: 0.7064059376716614\n",
      "Epoch 101: Train Loss: 0.6930147051811218, Validation Loss: 0.7078285217285156\n",
      "Epoch 102: Train Loss: 0.6821527004241943, Validation Loss: 0.7069635391235352\n",
      "Epoch 103: Train Loss: 0.7172515273094178, Validation Loss: 0.7068701386451721\n",
      "Epoch 104: Train Loss: 0.6935751795768738, Validation Loss: 0.7059309482574463\n",
      "Epoch 105: Train Loss: 0.7033226609230041, Validation Loss: 0.7058811187744141\n",
      "Epoch 106: Train Loss: 0.6330139756202697, Validation Loss: 0.7069153785705566\n",
      "Epoch 107: Train Loss: 0.6567099213600158, Validation Loss: 0.7069076299667358\n",
      "Epoch 108: Train Loss: 0.7043027877807617, Validation Loss: 0.7062777280807495\n",
      "Epoch 109: Train Loss: 0.7065656661987305, Validation Loss: 0.7061589956283569\n",
      "Epoch 110: Train Loss: 0.7230250000953674, Validation Loss: 0.7053049802780151\n",
      "Epoch 111: Train Loss: 0.6946513652801514, Validation Loss: 0.7045547962188721\n",
      "Epoch 112: Train Loss: 0.6903806567192078, Validation Loss: 0.7037368416786194\n",
      "Epoch 113: Train Loss: 0.7169825553894043, Validation Loss: 0.7029319405555725\n",
      "Epoch 114: Train Loss: 0.6444735169410706, Validation Loss: 0.7032508254051208\n",
      "Epoch 115: Train Loss: 0.6432338953018188, Validation Loss: 0.7025797367095947\n",
      "Epoch 116: Train Loss: 0.658466386795044, Validation Loss: 0.7048518061637878\n",
      "Epoch 117: Train Loss: 0.6915135622024536, Validation Loss: 0.7050763964653015\n",
      "Epoch 118: Train Loss: 0.6764286279678344, Validation Loss: 0.7052863240242004\n",
      "Epoch 119: Train Loss: 0.6371572852134705, Validation Loss: 0.7047647833824158\n",
      "Epoch 120: Train Loss: 0.6807778716087342, Validation Loss: 0.7043508887290955\n",
      "Epoch 121: Train Loss: 0.6782822251319885, Validation Loss: 0.7037105560302734\n",
      "Epoch 122: Train Loss: 0.6380531311035156, Validation Loss: 0.702970564365387\n",
      "Epoch 123: Train Loss: 0.7287080526351929, Validation Loss: 0.705314576625824\n",
      "Epoch 124: Train Loss: 0.6952873826026916, Validation Loss: 0.7066792845726013\n",
      "Epoch 125: Train Loss: 0.6906591415405273, Validation Loss: 0.7072197794914246\n",
      "Epoch 126: Train Loss: 0.7015406847000122, Validation Loss: 0.7077885866165161\n",
      "Epoch 127: Train Loss: 0.6741912007331848, Validation Loss: 0.7067227959632874\n",
      "Epoch 128: Train Loss: 0.6803755640983582, Validation Loss: 0.7074815630912781\n",
      "Epoch 129: Train Loss: 0.7010090231895447, Validation Loss: 0.70639568567276\n",
      "Epoch 130: Train Loss: 0.7154756307601928, Validation Loss: 0.7072280645370483\n",
      "Epoch 131: Train Loss: 0.6862650156021118, Validation Loss: 0.7058797478675842\n",
      "Epoch 132: Train Loss: 0.71153324842453, Validation Loss: 0.7055233716964722\n",
      "Epoch 133: Train Loss: 0.7125105023384094, Validation Loss: 0.7049302458763123\n",
      "Epoch 134: Train Loss: 0.6501847505569458, Validation Loss: 0.7059240341186523\n",
      "Epoch 135: Train Loss: 0.6799578785896301, Validation Loss: 0.7047640681266785\n",
      "Epoch 136: Train Loss: 0.6535132646560669, Validation Loss: 0.7051995396614075\n",
      "Epoch 137: Train Loss: 0.6249397277832032, Validation Loss: 0.7055859565734863\n",
      "Epoch 138: Train Loss: 0.6411482810974121, Validation Loss: 0.7051504850387573\n",
      "Epoch 139: Train Loss: 0.6602014303207397, Validation Loss: 0.7063389420509338\n",
      "Epoch 140: Train Loss: 0.6558405518531799, Validation Loss: 0.7089074850082397\n",
      "Epoch 141: Train Loss: 0.694665539264679, Validation Loss: 0.7097854018211365\n",
      "Epoch 142: Train Loss: 0.727472472190857, Validation Loss: 0.7106331586837769\n",
      "Epoch 143: Train Loss: 0.6615187406539917, Validation Loss: 0.7130736112594604\n",
      "Epoch 144: Train Loss: 0.681603753566742, Validation Loss: 0.7127802968025208\n",
      "Epoch 145: Train Loss: 0.6600645422935486, Validation Loss: 0.7108238339424133\n",
      "Epoch 146: Train Loss: 0.6845807433128357, Validation Loss: 0.7092108726501465\n",
      "Epoch 147: Train Loss: 0.6925873041152955, Validation Loss: 0.7093515396118164\n",
      "Epoch 148: Train Loss: 0.6850328803062439, Validation Loss: 0.7097179293632507\n",
      "Epoch 149: Train Loss: 0.6793362379074097, Validation Loss: 0.7101103663444519\n",
      "Epoch 150: Train Loss: 0.7042843341827393, Validation Loss: 0.7111573219299316\n",
      "Epoch 151: Train Loss: 0.6833557844161987, Validation Loss: 0.7120920419692993\n",
      "Epoch 152: Train Loss: 0.6707512140274048, Validation Loss: 0.7147790789604187\n",
      "Epoch 153: Train Loss: 0.7020297884941101, Validation Loss: 0.714413046836853\n",
      "Epoch 154: Train Loss: 0.6830220222473145, Validation Loss: 0.7133864164352417\n",
      "Epoch 155: Train Loss: 0.6910187125205993, Validation Loss: 0.7134445905685425\n",
      "Epoch 156: Train Loss: 0.7076595902442933, Validation Loss: 0.71512371301651\n",
      "Epoch 157: Train Loss: 0.6802000403404236, Validation Loss: 0.7159325480461121\n",
      "Epoch 158: Train Loss: 0.6594615936279297, Validation Loss: 0.7164654731750488\n",
      "Epoch 159: Train Loss: 0.6628806471824646, Validation Loss: 0.7171185612678528\n",
      "Epoch 160: Train Loss: 0.7307620286941529, Validation Loss: 0.7144988775253296\n",
      "Epoch 161: Train Loss: 0.7035127997398376, Validation Loss: 0.7132037878036499\n",
      "Epoch 162: Train Loss: 0.6712135314941406, Validation Loss: 0.7133715152740479\n",
      "Epoch 163: Train Loss: 0.7034564733505249, Validation Loss: 0.7137453556060791\n",
      "Epoch 164: Train Loss: 0.6603280901908875, Validation Loss: 0.7146194577217102\n",
      "Epoch 165: Train Loss: 0.6942503452301025, Validation Loss: 0.7158867120742798\n",
      "Epoch 166: Train Loss: 0.6765970110893249, Validation Loss: 0.7150696516036987\n",
      "Epoch 167: Train Loss: 0.6771306037902832, Validation Loss: 0.7146437764167786\n",
      "Epoch 168: Train Loss: 0.693285071849823, Validation Loss: 0.7150452136993408\n",
      "Epoch 169: Train Loss: 0.6974871397018433, Validation Loss: 0.7142617702484131\n",
      "Epoch 170: Train Loss: 0.6770580649375916, Validation Loss: 0.714073657989502\n",
      "Epoch 171: Train Loss: 0.6947173476219177, Validation Loss: 0.7139333486557007\n",
      "Epoch 172: Train Loss: 0.6718733906745911, Validation Loss: 0.71564120054245\n",
      "Epoch 173: Train Loss: 0.6952521800994873, Validation Loss: 0.7152014970779419\n",
      "Epoch 174: Train Loss: 0.6742884039878845, Validation Loss: 0.7149518728256226\n",
      "Epoch 175: Train Loss: 0.6844075560569763, Validation Loss: 0.7152661681175232\n",
      "Epoch 176: Train Loss: 0.6900431513786316, Validation Loss: 0.7152695655822754\n",
      "Epoch 177: Train Loss: 0.6735256433486938, Validation Loss: 0.7137455344200134\n",
      "Epoch 178: Train Loss: 0.6707585334777832, Validation Loss: 0.7136307954788208\n",
      "Epoch 179: Train Loss: 0.7048084259033203, Validation Loss: 0.7152355313301086\n",
      "Epoch 180: Train Loss: 0.6588430523872375, Validation Loss: 0.7153693437576294\n",
      "Epoch 181: Train Loss: 0.688821530342102, Validation Loss: 0.7150365710258484\n",
      "Epoch 182: Train Loss: 0.6411536335945129, Validation Loss: 0.7150830626487732\n",
      "Epoch 183: Train Loss: 0.7082530736923218, Validation Loss: 0.7157055735588074\n",
      "Epoch 184: Train Loss: 0.6950421810150147, Validation Loss: 0.7164593935012817\n",
      "Epoch 185: Train Loss: 0.6492219805717468, Validation Loss: 0.7156142592430115\n",
      "Epoch 186: Train Loss: 0.6398674488067627, Validation Loss: 0.717185914516449\n",
      "Epoch 187: Train Loss: 0.673200249671936, Validation Loss: 0.7175505757331848\n",
      "Epoch 188: Train Loss: 0.6858796834945678, Validation Loss: 0.7171528339385986\n",
      "Epoch 189: Train Loss: 0.6169266879558564, Validation Loss: 0.7170156240463257\n",
      "Epoch 190: Train Loss: 0.6757947921752929, Validation Loss: 0.7175624370574951\n",
      "Epoch 191: Train Loss: 0.636939525604248, Validation Loss: 0.7165464162826538\n",
      "Epoch 192: Train Loss: 0.6389549136161804, Validation Loss: 0.7155355215072632\n",
      "Epoch 193: Train Loss: 0.6547895193099975, Validation Loss: 0.7168021202087402\n",
      "Epoch 194: Train Loss: 0.7066333055496216, Validation Loss: 0.7177213430404663\n",
      "Epoch 195: Train Loss: 0.6762048721313476, Validation Loss: 0.7175736427307129\n",
      "Epoch 196: Train Loss: 0.6587407350540161, Validation Loss: 0.7173298597335815\n",
      "Epoch 197: Train Loss: 0.6576244950294494, Validation Loss: 0.7174298763275146\n",
      "Epoch 198: Train Loss: 0.6734909296035767, Validation Loss: 0.717242956161499\n",
      "Epoch 199: Train Loss: 0.6703783869743347, Validation Loss: 0.7161296606063843\n",
      "Epoch 200: Train Loss: 0.6885114550590515, Validation Loss: 0.7166197896003723\n",
      "Epoch 201: Train Loss: 0.6717437744140625, Validation Loss: 0.7162030339241028\n",
      "Epoch 202: Train Loss: 0.6905558705329895, Validation Loss: 0.7164897322654724\n",
      "Epoch 203: Train Loss: 0.6827871799468994, Validation Loss: 0.7157027125358582\n",
      "Epoch 204: Train Loss: 0.6394267916679383, Validation Loss: 0.7148975729942322\n",
      "Epoch 205: Train Loss: 0.6971591353416443, Validation Loss: 0.713705837726593\n",
      "Epoch 206: Train Loss: 0.6693796634674072, Validation Loss: 0.7137461304664612\n",
      "Epoch 207: Train Loss: 0.6327999114990235, Validation Loss: 0.7145190238952637\n",
      "Epoch 208: Train Loss: 0.6501066446304321, Validation Loss: 0.7133080363273621\n",
      "Epoch 209: Train Loss: 0.6627074480056763, Validation Loss: 0.7131085991859436\n",
      "Epoch 210: Train Loss: 0.6552781820297241, Validation Loss: 0.7138058543205261\n",
      "Epoch 211: Train Loss: 0.6819344639778138, Validation Loss: 0.7147973775863647\n",
      "Epoch 212: Train Loss: 0.677799642086029, Validation Loss: 0.7161775231361389\n",
      "Epoch 213: Train Loss: 0.6720963835716247, Validation Loss: 0.7167558073997498\n",
      "Epoch 214: Train Loss: 0.7004412651062012, Validation Loss: 0.7172336578369141\n",
      "Epoch 215: Train Loss: 0.6351522922515869, Validation Loss: 0.7156715989112854\n",
      "Epoch 216: Train Loss: 0.663757061958313, Validation Loss: 0.7155738472938538\n",
      "Epoch 217: Train Loss: 0.6662983775138855, Validation Loss: 0.7152511477470398\n",
      "Epoch 218: Train Loss: 0.6679493069648743, Validation Loss: 0.7150207757949829\n",
      "Epoch 219: Train Loss: 0.6686521530151367, Validation Loss: 0.7152463793754578\n",
      "Epoch 220: Train Loss: 0.6412279844284058, Validation Loss: 0.7170769572257996\n",
      "Epoch 221: Train Loss: 0.657151174545288, Validation Loss: 0.7173474431037903\n",
      "Epoch 222: Train Loss: 0.6344843745231629, Validation Loss: 0.7153427004814148\n",
      "Epoch 223: Train Loss: 0.6896099925041199, Validation Loss: 0.7145408391952515\n",
      "Epoch 224: Train Loss: 0.6491385340690613, Validation Loss: 0.7155526876449585\n",
      "Epoch 225: Train Loss: 0.64393310546875, Validation Loss: 0.7153120636940002\n",
      "Epoch 226: Train Loss: 0.70033038854599, Validation Loss: 0.7152895927429199\n",
      "Epoch 227: Train Loss: 0.7155901551246643, Validation Loss: 0.7175359129905701\n",
      "Epoch 228: Train Loss: 0.6215414762496948, Validation Loss: 0.7169400453567505\n",
      "Epoch 229: Train Loss: 0.6586310386657714, Validation Loss: 0.7156974673271179\n",
      "Epoch 230: Train Loss: 0.6696974754333496, Validation Loss: 0.7179795503616333\n",
      "Epoch 231: Train Loss: 0.63635733127594, Validation Loss: 0.7196038365364075\n",
      "Epoch 232: Train Loss: 0.6772769331932068, Validation Loss: 0.7201568484306335\n",
      "Epoch 233: Train Loss: 0.6497166991233826, Validation Loss: 0.7195754051208496\n",
      "Epoch 234: Train Loss: 0.6479637265205384, Validation Loss: 0.7188082337379456\n",
      "Epoch 235: Train Loss: 0.6540179014205932, Validation Loss: 0.7171440720558167\n",
      "Epoch 236: Train Loss: 0.658855402469635, Validation Loss: 0.7169547080993652\n",
      "Epoch 237: Train Loss: 0.6580589294433594, Validation Loss: 0.7173429727554321\n",
      "Epoch 238: Train Loss: 0.6749215722084045, Validation Loss: 0.7165845632553101\n",
      "Epoch 239: Train Loss: 0.6362547039985657, Validation Loss: 0.7159065008163452\n",
      "Epoch 240: Train Loss: 0.6670045852661133, Validation Loss: 0.719572901725769\n",
      "Epoch 241: Train Loss: 0.6706674933433533, Validation Loss: 0.7212650179862976\n",
      "Epoch 242: Train Loss: 0.6267760038375855, Validation Loss: 0.7211482524871826\n",
      "Epoch 243: Train Loss: 0.6651507019996643, Validation Loss: 0.7191140055656433\n",
      "Epoch 244: Train Loss: 0.6639400482177734, Validation Loss: 0.7177859544754028\n",
      "Epoch 245: Train Loss: 0.6672149181365967, Validation Loss: 0.7167952060699463\n",
      "Epoch 246: Train Loss: 0.6537853240966797, Validation Loss: 0.7167473435401917\n",
      "Epoch 247: Train Loss: 0.6358078956604004, Validation Loss: 0.7165817022323608\n",
      "Epoch 248: Train Loss: 0.6424600958824158, Validation Loss: 0.7179253697395325\n",
      "Epoch 249: Train Loss: 0.6895387291908264, Validation Loss: 0.7172781825065613\n",
      "Epoch 250: Train Loss: 0.6523468732833863, Validation Loss: 0.7193247675895691\n",
      "Epoch 251: Train Loss: 0.6789513945579528, Validation Loss: 0.7201951146125793\n",
      "Epoch 252: Train Loss: 0.6359791994094849, Validation Loss: 0.7214465141296387\n",
      "Epoch 253: Train Loss: 0.5933203995227814, Validation Loss: 0.7253313064575195\n",
      "Epoch 254: Train Loss: 0.656367015838623, Validation Loss: 0.7234603762626648\n",
      "Epoch 255: Train Loss: 0.6443452000617981, Validation Loss: 0.7241978645324707\n",
      "Epoch 256: Train Loss: 0.5958837509155274, Validation Loss: 0.7254186868667603\n",
      "Epoch 257: Train Loss: 0.6647541284561157, Validation Loss: 0.726528525352478\n",
      "Epoch 258: Train Loss: 0.6459299921989441, Validation Loss: 0.7265740036964417\n",
      "Epoch 259: Train Loss: 0.631829309463501, Validation Loss: 0.727237343788147\n",
      "Epoch 260: Train Loss: 0.6293845057487488, Validation Loss: 0.7255942225456238\n",
      "Epoch 261: Train Loss: 0.6420639038085938, Validation Loss: 0.7244051098823547\n",
      "Epoch 262: Train Loss: 0.6451060771942139, Validation Loss: 0.7249228358268738\n",
      "Epoch 263: Train Loss: 0.6278894901275635, Validation Loss: 0.7232640385627747\n",
      "Epoch 264: Train Loss: 0.6918630242347718, Validation Loss: 0.7236052751541138\n",
      "Epoch 265: Train Loss: 0.7153727531433105, Validation Loss: 0.7226189970970154\n",
      "Epoch 266: Train Loss: 0.6480432152748108, Validation Loss: 0.7230282425880432\n",
      "Epoch 267: Train Loss: 0.6553283929824829, Validation Loss: 0.7237514853477478\n",
      "Epoch 268: Train Loss: 0.6697806596755982, Validation Loss: 0.7233797907829285\n",
      "Epoch 269: Train Loss: 0.6412541270256042, Validation Loss: 0.7237378358840942\n",
      "Epoch 270: Train Loss: 0.6669769883155823, Validation Loss: 0.7236436009407043\n",
      "Epoch 271: Train Loss: 0.6259562015533447, Validation Loss: 0.7247354388237\n",
      "Epoch 272: Train Loss: 0.6421010255813598, Validation Loss: 0.7241265773773193\n",
      "Epoch 273: Train Loss: 0.6383477210998535, Validation Loss: 0.7239007353782654\n",
      "Epoch 274: Train Loss: 0.63530113697052, Validation Loss: 0.7222774624824524\n",
      "Epoch 275: Train Loss: 0.6900573015213013, Validation Loss: 0.7248472571372986\n",
      "Epoch 276: Train Loss: 0.6402455687522888, Validation Loss: 0.7253032922744751\n",
      "Epoch 277: Train Loss: 0.6397738814353943, Validation Loss: 0.7244243025779724\n",
      "Epoch 278: Train Loss: 0.6177103519439697, Validation Loss: 0.7267990112304688\n",
      "Epoch 279: Train Loss: 0.6555548310279846, Validation Loss: 0.7247722744941711\n",
      "Epoch 280: Train Loss: 0.6544438600540161, Validation Loss: 0.7250725626945496\n",
      "Epoch 281: Train Loss: 0.6082603216171265, Validation Loss: 0.7261055111885071\n",
      "Epoch 282: Train Loss: 0.6602165222167968, Validation Loss: 0.7250601649284363\n",
      "Epoch 283: Train Loss: 0.6707775950431824, Validation Loss: 0.7248926162719727\n",
      "Epoch 284: Train Loss: 0.6858252048492431, Validation Loss: 0.7257257699966431\n",
      "Epoch 285: Train Loss: 0.6711994886398316, Validation Loss: 0.7265903949737549\n",
      "Epoch 286: Train Loss: 0.6249696135520935, Validation Loss: 0.7294204235076904\n",
      "Epoch 287: Train Loss: 0.660775363445282, Validation Loss: 0.728839099407196\n",
      "Epoch 288: Train Loss: 0.6561290144920349, Validation Loss: 0.7289897203445435\n",
      "Epoch 289: Train Loss: 0.6573264360427856, Validation Loss: 0.7302021980285645\n",
      "Epoch 290: Train Loss: 0.6553856730461121, Validation Loss: 0.7313979864120483\n",
      "Epoch 291: Train Loss: 0.632646107673645, Validation Loss: 0.7332631945610046\n",
      "Epoch 292: Train Loss: 0.6298519134521484, Validation Loss: 0.7360594868659973\n",
      "Epoch 293: Train Loss: 0.634291160106659, Validation Loss: 0.7358859181404114\n",
      "Epoch 294: Train Loss: 0.6748946309089661, Validation Loss: 0.7368245720863342\n",
      "Epoch 295: Train Loss: 0.6639098167419434, Validation Loss: 0.734703004360199\n",
      "Epoch 296: Train Loss: 0.6840705156326294, Validation Loss: 0.7368928790092468\n",
      "Epoch 297: Train Loss: 0.6384710311889649, Validation Loss: 0.7379586100578308\n",
      "Epoch 298: Train Loss: 0.6491031169891357, Validation Loss: 0.7380048036575317\n",
      "Epoch 299: Train Loss: 0.6631961703300476, Validation Loss: 0.7352748513221741\n",
      "Epoch 300: Train Loss: 0.6891012072563172, Validation Loss: 0.736943244934082\n",
      "Epoch 301: Train Loss: 0.63169367313385, Validation Loss: 0.7373692989349365\n",
      "Epoch 302: Train Loss: 0.6806713104248047, Validation Loss: 0.7362895011901855\n",
      "Epoch 303: Train Loss: 0.6105097532272339, Validation Loss: 0.7416877746582031\n",
      "Epoch 304: Train Loss: 0.6509653806686402, Validation Loss: 0.7419435381889343\n",
      "Epoch 305: Train Loss: 0.6990922808647155, Validation Loss: 0.7418824434280396\n",
      "Epoch 306: Train Loss: 0.6808064579963684, Validation Loss: 0.7406733632087708\n",
      "Epoch 307: Train Loss: 0.6236290454864502, Validation Loss: 0.742091715335846\n",
      "Epoch 308: Train Loss: 0.6780378460884094, Validation Loss: 0.7412720918655396\n",
      "Epoch 309: Train Loss: 0.6507177472114563, Validation Loss: 0.7417847514152527\n",
      "Epoch 310: Train Loss: 0.638538908958435, Validation Loss: 0.7423047423362732\n",
      "Epoch 311: Train Loss: 0.6546410918235779, Validation Loss: 0.7421004176139832\n",
      "Epoch 312: Train Loss: 0.6093538045883179, Validation Loss: 0.7411816716194153\n",
      "Epoch 313: Train Loss: 0.638271939754486, Validation Loss: 0.741665244102478\n",
      "Epoch 314: Train Loss: 0.6317904353141784, Validation Loss: 0.7400938868522644\n",
      "Epoch 315: Train Loss: 0.6561456084251404, Validation Loss: 0.7386236190795898\n",
      "Epoch 316: Train Loss: 0.5953300178050995, Validation Loss: 0.7389840483665466\n",
      "Epoch 317: Train Loss: 0.6684249401092529, Validation Loss: 0.7412757873535156\n",
      "Epoch 318: Train Loss: 0.6414650678634644, Validation Loss: 0.7410227656364441\n",
      "Epoch 319: Train Loss: 0.6852140784263611, Validation Loss: 0.7399739623069763\n",
      "Epoch 320: Train Loss: 0.590007996559143, Validation Loss: 0.7388612031936646\n",
      "Epoch 321: Train Loss: 0.6395502924919129, Validation Loss: 0.7349549531936646\n",
      "Epoch 322: Train Loss: 0.6761075019836426, Validation Loss: 0.7348281741142273\n",
      "Epoch 323: Train Loss: 0.6595420598983764, Validation Loss: 0.7369784712791443\n",
      "Epoch 324: Train Loss: 0.6533547520637513, Validation Loss: 0.7403978109359741\n",
      "Epoch 325: Train Loss: 0.6423459887504578, Validation Loss: 0.7399250864982605\n",
      "Epoch 326: Train Loss: 0.6311163544654846, Validation Loss: 0.7388116717338562\n",
      "Epoch 327: Train Loss: 0.63606116771698, Validation Loss: 0.7363845705986023\n",
      "Epoch 328: Train Loss: 0.6754152417182923, Validation Loss: 0.7358106374740601\n",
      "Epoch 329: Train Loss: 0.6575673937797546, Validation Loss: 0.7352296113967896\n",
      "Epoch 330: Train Loss: 0.6545029401779174, Validation Loss: 0.7368982434272766\n",
      "Epoch 331: Train Loss: 0.5983300507068634, Validation Loss: 0.7355198860168457\n",
      "Epoch 332: Train Loss: 0.5918620526790619, Validation Loss: 0.7398740649223328\n",
      "Epoch 333: Train Loss: 0.641273045539856, Validation Loss: 0.7404791712760925\n",
      "Epoch 334: Train Loss: 0.6189679384231568, Validation Loss: 0.74175626039505\n",
      "Epoch 335: Train Loss: 0.6281193971633912, Validation Loss: 0.7406116127967834\n",
      "Epoch 336: Train Loss: 0.6471941113471985, Validation Loss: 0.7404815554618835\n",
      "Epoch 337: Train Loss: 0.6409298777580261, Validation Loss: 0.7417672276496887\n",
      "Epoch 338: Train Loss: 0.629638385772705, Validation Loss: 0.7407210469245911\n",
      "Epoch 339: Train Loss: 0.6363314151763916, Validation Loss: 0.7392581105232239\n",
      "Epoch 340: Train Loss: 0.6428017497062684, Validation Loss: 0.7410197257995605\n",
      "Epoch 341: Train Loss: 0.619793438911438, Validation Loss: 0.7407146096229553\n",
      "Epoch 342: Train Loss: 0.6299938082695007, Validation Loss: 0.7435650825500488\n",
      "Epoch 343: Train Loss: 0.6286387085914612, Validation Loss: 0.7435988783836365\n",
      "Epoch 344: Train Loss: 0.6886289834976196, Validation Loss: 0.7426072955131531\n",
      "Epoch 345: Train Loss: 0.6679389953613282, Validation Loss: 0.7470197677612305\n",
      "Epoch 346: Train Loss: 0.6160750150680542, Validation Loss: 0.7433525919914246\n",
      "Epoch 347: Train Loss: 0.626065444946289, Validation Loss: 0.7459695339202881\n",
      "Epoch 348: Train Loss: 0.6434174299240112, Validation Loss: 0.7451283931732178\n",
      "Epoch 349: Train Loss: 0.6457567334175109, Validation Loss: 0.7449294328689575\n",
      "Epoch 350: Train Loss: 0.6091382265090942, Validation Loss: 0.7465846538543701\n",
      "Epoch 351: Train Loss: 0.6347730875015258, Validation Loss: 0.7465506792068481\n",
      "Epoch 352: Train Loss: 0.6616626739501953, Validation Loss: 0.7440794110298157\n",
      "Epoch 353: Train Loss: 0.6398601770401001, Validation Loss: 0.7457098960876465\n",
      "Epoch 354: Train Loss: 0.6589123368263244, Validation Loss: 0.7452065348625183\n",
      "Epoch 355: Train Loss: 0.6352925062179565, Validation Loss: 0.7434126734733582\n",
      "Epoch 356: Train Loss: 0.6041544556617737, Validation Loss: 0.7457578778266907\n",
      "Epoch 357: Train Loss: 0.6862733364105225, Validation Loss: 0.741598904132843\n",
      "Epoch 358: Train Loss: 0.6386518239974975, Validation Loss: 0.7417729496955872\n",
      "Epoch 359: Train Loss: 0.692279863357544, Validation Loss: 0.7442936897277832\n",
      "Epoch 360: Train Loss: 0.6197886109352112, Validation Loss: 0.7433838844299316\n",
      "Epoch 361: Train Loss: 0.6185675144195557, Validation Loss: 0.7404958009719849\n",
      "Epoch 362: Train Loss: 0.6348801851272583, Validation Loss: 0.7401041984558105\n",
      "Epoch 363: Train Loss: 0.6288321375846863, Validation Loss: 0.7382845878601074\n",
      "Epoch 364: Train Loss: 0.6289223313331604, Validation Loss: 0.7378878593444824\n",
      "Epoch 365: Train Loss: 0.60044846534729, Validation Loss: 0.7387518882751465\n",
      "Epoch 366: Train Loss: 0.6346658587455749, Validation Loss: 0.7384921908378601\n",
      "Epoch 367: Train Loss: 0.6003552317619324, Validation Loss: 0.7379867434501648\n",
      "Epoch 368: Train Loss: 0.5837242364883423, Validation Loss: 0.7409188747406006\n",
      "Epoch 369: Train Loss: 0.6243319988250733, Validation Loss: 0.7399279475212097\n",
      "Epoch 370: Train Loss: 0.6377447485923767, Validation Loss: 0.7427959442138672\n",
      "Epoch 371: Train Loss: 0.6746930599212646, Validation Loss: 0.740083634853363\n",
      "Epoch 372: Train Loss: 0.6274093627929688, Validation Loss: 0.741319477558136\n",
      "Epoch 373: Train Loss: 0.6479596138000489, Validation Loss: 0.7436243891716003\n",
      "Epoch 374: Train Loss: 0.6549497604370117, Validation Loss: 0.7467275261878967\n",
      "Epoch 375: Train Loss: 0.6019848346710205, Validation Loss: 0.7472291588783264\n",
      "Epoch 376: Train Loss: 0.6347801566123963, Validation Loss: 0.7509734034538269\n",
      "Epoch 377: Train Loss: 0.6685861110687256, Validation Loss: 0.7496791481971741\n",
      "Epoch 378: Train Loss: 0.6734804511070251, Validation Loss: 0.749614417552948\n",
      "Epoch 379: Train Loss: 0.6440537691116333, Validation Loss: 0.7470223307609558\n",
      "Epoch 380: Train Loss: 0.6105169415473938, Validation Loss: 0.7488299608230591\n",
      "Epoch 381: Train Loss: 0.6250816702842712, Validation Loss: 0.7503552436828613\n",
      "Epoch 382: Train Loss: 0.5981659531593323, Validation Loss: 0.7481275200843811\n",
      "Epoch 383: Train Loss: 0.6030501842498779, Validation Loss: 0.7489278316497803\n",
      "Epoch 384: Train Loss: 0.6604044914245606, Validation Loss: 0.7478219866752625\n",
      "Epoch 385: Train Loss: 0.6121670842170716, Validation Loss: 0.7500452995300293\n",
      "Epoch 386: Train Loss: 0.6559653043746948, Validation Loss: 0.74885094165802\n",
      "Epoch 387: Train Loss: 0.6241939187049865, Validation Loss: 0.7505118250846863\n",
      "Epoch 388: Train Loss: 0.6309936165809631, Validation Loss: 0.7462232708930969\n",
      "Epoch 389: Train Loss: 0.6034251809120178, Validation Loss: 0.7485907673835754\n",
      "Epoch 390: Train Loss: 0.6078426718711853, Validation Loss: 0.753242015838623\n",
      "Epoch 391: Train Loss: 0.5867226362228394, Validation Loss: 0.7501249313354492\n",
      "Epoch 392: Train Loss: 0.6561699032783508, Validation Loss: 0.7516541481018066\n",
      "Epoch 393: Train Loss: 0.6714955329895019, Validation Loss: 0.7493682503700256\n",
      "Epoch 394: Train Loss: 0.6387438654899598, Validation Loss: 0.7488812804222107\n",
      "Epoch 395: Train Loss: 0.6396515130996704, Validation Loss: 0.7533502578735352\n",
      "Epoch 396: Train Loss: 0.5911715030670166, Validation Loss: 0.7532155513763428\n",
      "Epoch 397: Train Loss: 0.6120883345603942, Validation Loss: 0.7532275319099426\n",
      "Epoch 398: Train Loss: 0.6181002616882324, Validation Loss: 0.756190836429596\n",
      "Epoch 399: Train Loss: 0.6484158635139465, Validation Loss: 0.7561530470848083\n",
      "Epoch 400: Train Loss: 0.586710947751999, Validation Loss: 0.7624589800834656\n",
      "Epoch 401: Train Loss: 0.6490358471870422, Validation Loss: 0.7576676607131958\n",
      "Epoch 402: Train Loss: 0.6199946761131286, Validation Loss: 0.7610541582107544\n",
      "Epoch 403: Train Loss: 0.6549184560775757, Validation Loss: 0.7603424191474915\n",
      "Epoch 404: Train Loss: 0.6399722695350647, Validation Loss: 0.7596849203109741\n",
      "Epoch 405: Train Loss: 0.6053666710853577, Validation Loss: 0.7624743580818176\n",
      "Epoch 406: Train Loss: 0.6855024814605712, Validation Loss: 0.7603826522827148\n",
      "Epoch 407: Train Loss: 0.5824188232421875, Validation Loss: 0.7618228197097778\n",
      "Epoch 408: Train Loss: 0.6179198503494263, Validation Loss: 0.7598101496696472\n",
      "Epoch 409: Train Loss: 0.6259368181228637, Validation Loss: 0.7642306685447693\n",
      "Epoch 410: Train Loss: 0.6373923897743226, Validation Loss: 0.7629586458206177\n",
      "Epoch 411: Train Loss: 0.6719995737075806, Validation Loss: 0.7636117935180664\n",
      "Epoch 412: Train Loss: 0.6295231223106384, Validation Loss: 0.7643906474113464\n",
      "Epoch 413: Train Loss: 0.64093519449234, Validation Loss: 0.7657367587089539\n",
      "Epoch 414: Train Loss: 0.6069158911705017, Validation Loss: 0.77396559715271\n",
      "Epoch 415: Train Loss: 0.6501279830932617, Validation Loss: 0.7713662981987\n",
      "Epoch 416: Train Loss: 0.6168884754180908, Validation Loss: 0.7744523286819458\n",
      "Epoch 417: Train Loss: 0.5643057763576508, Validation Loss: 0.7743857502937317\n",
      "Epoch 418: Train Loss: 0.6120350360870361, Validation Loss: 0.7767333984375\n",
      "Epoch 419: Train Loss: 0.5837556481361389, Validation Loss: 0.7759513258934021\n",
      "Epoch 420: Train Loss: 0.5984205365180969, Validation Loss: 0.7800366282463074\n",
      "Epoch 421: Train Loss: 0.6085450172424316, Validation Loss: 0.776801586151123\n",
      "Epoch 422: Train Loss: 0.6256772756576539, Validation Loss: 0.7776432037353516\n",
      "Epoch 423: Train Loss: 0.6282116532325744, Validation Loss: 0.7755734920501709\n",
      "Epoch 424: Train Loss: 0.6026863217353821, Validation Loss: 0.775820255279541\n",
      "Epoch 425: Train Loss: 0.5954346895217896, Validation Loss: 0.7736785411834717\n",
      "Epoch 426: Train Loss: 0.5900412559509277, Validation Loss: 0.7772156596183777\n",
      "Epoch 427: Train Loss: 0.6464560508728028, Validation Loss: 0.7781858444213867\n",
      "Epoch 428: Train Loss: 0.6052101135253907, Validation Loss: 0.7760295271873474\n",
      "Epoch 429: Train Loss: 0.5891834259033203, Validation Loss: 0.772622287273407\n",
      "Epoch 430: Train Loss: 0.5950045585632324, Validation Loss: 0.7741355895996094\n",
      "Epoch 431: Train Loss: 0.5917791962623596, Validation Loss: 0.7754189372062683\n",
      "Epoch 432: Train Loss: 0.6001177906990052, Validation Loss: 0.7730235457420349\n",
      "Epoch 433: Train Loss: 0.6083914875984192, Validation Loss: 0.7780773043632507\n",
      "Epoch 434: Train Loss: 0.5814386129379272, Validation Loss: 0.7755528688430786\n",
      "Epoch 435: Train Loss: 0.5888184785842896, Validation Loss: 0.778278112411499\n",
      "Epoch 436: Train Loss: 0.6147472500801087, Validation Loss: 0.784920871257782\n",
      "Epoch 437: Train Loss: 0.6097485780715942, Validation Loss: 0.7820254564285278\n",
      "Epoch 438: Train Loss: 0.6081332087516784, Validation Loss: 0.7822729349136353\n",
      "Epoch 439: Train Loss: 0.5591248035430908, Validation Loss: 0.7822362780570984\n",
      "Epoch 440: Train Loss: 0.5739902853965759, Validation Loss: 0.7854436635971069\n",
      "Epoch 441: Train Loss: 0.5685145258903503, Validation Loss: 0.7804942727088928\n",
      "Epoch 442: Train Loss: 0.6244933366775512, Validation Loss: 0.7767343521118164\n",
      "Epoch 443: Train Loss: 0.5965756297111511, Validation Loss: 0.7771523594856262\n",
      "Epoch 444: Train Loss: 0.6352010726928711, Validation Loss: 0.7784436941146851\n",
      "Epoch 445: Train Loss: 0.6122126698493957, Validation Loss: 0.7831895351409912\n",
      "Epoch 446: Train Loss: 0.556591796875, Validation Loss: 0.7828591465950012\n",
      "Epoch 447: Train Loss: 0.5891353607177734, Validation Loss: 0.7845767140388489\n",
      "Epoch 448: Train Loss: 0.6612838864326477, Validation Loss: 0.7867207527160645\n",
      "Epoch 449: Train Loss: 0.5953065395355225, Validation Loss: 0.7862395644187927\n",
      "Epoch 450: Train Loss: 0.628983986377716, Validation Loss: 0.7807615399360657\n",
      "Epoch 451: Train Loss: 0.540517520904541, Validation Loss: 0.7863603830337524\n",
      "Epoch 452: Train Loss: 0.6240770101547242, Validation Loss: 0.7859689593315125\n",
      "Epoch 453: Train Loss: 0.5789638519287109, Validation Loss: 0.7830405831336975\n",
      "Epoch 454: Train Loss: 0.5458540558815003, Validation Loss: 0.7799913287162781\n",
      "Epoch 455: Train Loss: 0.6020845890045166, Validation Loss: 0.7875774502754211\n",
      "Epoch 456: Train Loss: 0.5643148422241211, Validation Loss: 0.7908113598823547\n",
      "Epoch 457: Train Loss: 0.6104146122932435, Validation Loss: 0.7879319190979004\n",
      "Epoch 458: Train Loss: 0.5576438307762146, Validation Loss: 0.7917817234992981\n",
      "Epoch 459: Train Loss: 0.6071833491325378, Validation Loss: 0.7939828634262085\n",
      "Epoch 460: Train Loss: 0.6321818470954895, Validation Loss: 0.7949254512786865\n",
      "Epoch 461: Train Loss: 0.5785413026809693, Validation Loss: 0.7979574203491211\n",
      "Epoch 462: Train Loss: 0.5851363658905029, Validation Loss: 0.7980030179023743\n",
      "Epoch 463: Train Loss: 0.5525374233722686, Validation Loss: 0.799572765827179\n",
      "Epoch 464: Train Loss: 0.6439989328384399, Validation Loss: 0.8026143908500671\n",
      "Epoch 465: Train Loss: 0.6229702234268188, Validation Loss: 0.7998172640800476\n",
      "Epoch 466: Train Loss: 0.5616999626159668, Validation Loss: 0.803565502166748\n",
      "Epoch 467: Train Loss: 0.6126150250434875, Validation Loss: 0.7998703122138977\n",
      "Epoch 468: Train Loss: 0.6217741966247559, Validation Loss: 0.7983760833740234\n",
      "Epoch 469: Train Loss: 0.5822433233261108, Validation Loss: 0.8012356758117676\n",
      "Epoch 470: Train Loss: 0.5673234462738037, Validation Loss: 0.8133994340896606\n",
      "Epoch 471: Train Loss: 0.5719510197639466, Validation Loss: 0.8128579258918762\n",
      "Epoch 472: Train Loss: 0.6466760039329529, Validation Loss: 0.8144046664237976\n",
      "Epoch 473: Train Loss: 0.5442434310913086, Validation Loss: 0.8205198049545288\n",
      "Epoch 474: Train Loss: 0.5461715579032898, Validation Loss: 0.8149256706237793\n",
      "Epoch 475: Train Loss: 0.6022208333015442, Validation Loss: 0.8150779604911804\n",
      "Epoch 476: Train Loss: 0.5963987946510315, Validation Loss: 0.8121432662010193\n",
      "Epoch 477: Train Loss: 0.5846338152885437, Validation Loss: 0.8087716102600098\n",
      "Epoch 478: Train Loss: 0.6132964015007019, Validation Loss: 0.815682590007782\n",
      "Epoch 479: Train Loss: 0.5501459956169128, Validation Loss: 0.815826952457428\n",
      "Epoch 480: Train Loss: 0.5355707943439484, Validation Loss: 0.8165982961654663\n",
      "Epoch 481: Train Loss: 0.6066130638122559, Validation Loss: 0.8167584538459778\n",
      "Epoch 482: Train Loss: 0.630758261680603, Validation Loss: 0.8226085305213928\n",
      "Epoch 483: Train Loss: 0.5816266298294067, Validation Loss: 0.8295277953147888\n",
      "Epoch 484: Train Loss: 0.5898245334625244, Validation Loss: 0.8301084637641907\n",
      "Epoch 485: Train Loss: 0.5826801061630249, Validation Loss: 0.8410258889198303\n",
      "Epoch 486: Train Loss: 0.5424263894557952, Validation Loss: 0.8394503593444824\n",
      "Epoch 487: Train Loss: 0.5829530358314514, Validation Loss: 0.8427133560180664\n",
      "Epoch 488: Train Loss: 0.5516798853874206, Validation Loss: 0.8385604023933411\n",
      "Epoch 489: Train Loss: 0.5865500211715698, Validation Loss: 0.8355221748352051\n",
      "Epoch 490: Train Loss: 0.5490534663200378, Validation Loss: 0.8395566344261169\n",
      "Epoch 491: Train Loss: 0.5364866733551026, Validation Loss: 0.849369466304779\n",
      "Epoch 492: Train Loss: 0.5551169395446778, Validation Loss: 0.8482045531272888\n",
      "Epoch 493: Train Loss: 0.5386726319789886, Validation Loss: 0.8524420857429504\n",
      "Epoch 494: Train Loss: 0.5439077913761139, Validation Loss: 0.8590428829193115\n",
      "Epoch 495: Train Loss: 0.5592504501342773, Validation Loss: 0.8620449304580688\n",
      "Epoch 496: Train Loss: 0.537898701429367, Validation Loss: 0.8621309399604797\n",
      "Epoch 497: Train Loss: 0.5536002576351166, Validation Loss: 0.8648149371147156\n",
      "Epoch 498: Train Loss: 0.6004271030426025, Validation Loss: 0.8717575073242188\n",
      "Epoch 499: Train Loss: 0.5527220368385315, Validation Loss: 0.8759576678276062\n",
      "Fold 11 Metrics:\n",
      "Accuracy: 0.2727272727272727, Precision: 0.0, Recall: 0.0, F1-score: 0.0, AUC: 0.3\n",
      "Confusion Matrix:\n",
      "[[3 2]\n",
      " [6 0]]\n",
      "Completed fold 11\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples from subject 2 to test set\n",
      "Adding 6 truth samples from subject 2 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 500)\n",
      "(132, 65, 500)\n",
      "Epoch 0: Train Loss: 0.6816924810409546, Validation Loss: 0.6909772753715515\n",
      "Epoch 1: Train Loss: 0.7189926981925965, Validation Loss: 0.6904840469360352\n",
      "Epoch 2: Train Loss: 0.6890928387641907, Validation Loss: 0.6906675100326538\n",
      "Epoch 3: Train Loss: 0.7179399967193604, Validation Loss: 0.6916874647140503\n",
      "Epoch 4: Train Loss: 0.7321070313453675, Validation Loss: 0.6926344037055969\n",
      "Epoch 5: Train Loss: 0.6996555805206299, Validation Loss: 0.6919555068016052\n",
      "Epoch 6: Train Loss: 0.7392439007759094, Validation Loss: 0.6941142678260803\n",
      "Epoch 7: Train Loss: 0.8303667902946472, Validation Loss: 0.6942858695983887\n",
      "Epoch 8: Train Loss: 0.6616907000541687, Validation Loss: 0.6951206922531128\n",
      "Epoch 9: Train Loss: 0.7090878963470459, Validation Loss: 0.695259153842926\n",
      "Epoch 10: Train Loss: 0.7483145952224731, Validation Loss: 0.6955673098564148\n",
      "Epoch 11: Train Loss: 0.6833554744720459, Validation Loss: 0.695404589176178\n",
      "Epoch 12: Train Loss: 0.6673689365386963, Validation Loss: 0.6953725218772888\n",
      "Epoch 13: Train Loss: 0.7117528200149537, Validation Loss: 0.6946269869804382\n",
      "Epoch 14: Train Loss: 0.7061838030815124, Validation Loss: 0.6960651278495789\n",
      "Epoch 15: Train Loss: 0.7024314403533936, Validation Loss: 0.6958199143409729\n",
      "Epoch 16: Train Loss: 0.7251912474632263, Validation Loss: 0.6957077980041504\n",
      "Epoch 17: Train Loss: 0.6552598357200623, Validation Loss: 0.6963541507720947\n",
      "Epoch 18: Train Loss: 0.7263032913208007, Validation Loss: 0.6978944540023804\n",
      "Epoch 19: Train Loss: 0.7444785475730896, Validation Loss: 0.6979436874389648\n",
      "Epoch 20: Train Loss: 0.6902489185333252, Validation Loss: 0.699047327041626\n",
      "Epoch 21: Train Loss: 0.7439450025558472, Validation Loss: 0.6987574696540833\n",
      "Epoch 22: Train Loss: 0.7169408917427063, Validation Loss: 0.6981717944145203\n",
      "Epoch 23: Train Loss: 0.7242779016494751, Validation Loss: 0.6983403563499451\n",
      "Epoch 24: Train Loss: 0.7211385250091553, Validation Loss: 0.6979777812957764\n",
      "Epoch 25: Train Loss: 0.6784787654876709, Validation Loss: 0.6986808776855469\n",
      "Epoch 26: Train Loss: 0.691287386417389, Validation Loss: 0.6989799737930298\n",
      "Epoch 27: Train Loss: 0.6987404346466064, Validation Loss: 0.7002867460250854\n",
      "Epoch 28: Train Loss: 0.6997976422309875, Validation Loss: 0.6991870999336243\n",
      "Epoch 29: Train Loss: 0.732469093799591, Validation Loss: 0.6990437507629395\n",
      "Epoch 30: Train Loss: 0.7251707315444946, Validation Loss: 0.6985368132591248\n",
      "Epoch 31: Train Loss: 0.6766610383987427, Validation Loss: 0.6991186141967773\n",
      "Epoch 32: Train Loss: 0.7217518448829651, Validation Loss: 0.6983575820922852\n",
      "Epoch 33: Train Loss: 0.6729119300842286, Validation Loss: 0.7000598907470703\n",
      "Epoch 34: Train Loss: 0.7897958278656005, Validation Loss: 0.6991521716117859\n",
      "Epoch 35: Train Loss: 0.7475736498832702, Validation Loss: 0.6989043354988098\n",
      "Epoch 36: Train Loss: 0.7408779859542847, Validation Loss: 0.7001972198486328\n",
      "Epoch 37: Train Loss: 0.7160723686218262, Validation Loss: 0.7000932097434998\n",
      "Epoch 38: Train Loss: 0.7361963391304016, Validation Loss: 0.7004045248031616\n",
      "Epoch 39: Train Loss: 0.6852087616920471, Validation Loss: 0.7003757357597351\n",
      "Epoch 40: Train Loss: 0.7080844759941101, Validation Loss: 0.7003210783004761\n",
      "Epoch 41: Train Loss: 0.7583000659942627, Validation Loss: 0.6991381049156189\n",
      "Epoch 42: Train Loss: 0.6731868028640747, Validation Loss: 0.6962985992431641\n",
      "Epoch 43: Train Loss: 0.7153250336647033, Validation Loss: 0.6967026591300964\n",
      "Epoch 44: Train Loss: 0.7194632291793823, Validation Loss: 0.6975048780441284\n",
      "Epoch 45: Train Loss: 0.6985833644866943, Validation Loss: 0.6978880763053894\n",
      "Epoch 46: Train Loss: 0.7030613780021667, Validation Loss: 0.6983583569526672\n",
      "Epoch 47: Train Loss: 0.7319050669670105, Validation Loss: 0.6995163559913635\n",
      "Epoch 48: Train Loss: 0.713291072845459, Validation Loss: 0.6991440653800964\n",
      "Epoch 49: Train Loss: 0.6884674310684205, Validation Loss: 0.6997681856155396\n",
      "Epoch 50: Train Loss: 0.6738090991973877, Validation Loss: 0.6997906565666199\n",
      "Epoch 51: Train Loss: 0.6925165176391601, Validation Loss: 0.6997830867767334\n",
      "Epoch 52: Train Loss: 0.6566789865493774, Validation Loss: 0.6999874711036682\n",
      "Epoch 53: Train Loss: 0.7141181468963623, Validation Loss: 0.6994573473930359\n",
      "Epoch 54: Train Loss: 0.7521522164344787, Validation Loss: 0.7006021738052368\n",
      "Epoch 55: Train Loss: 0.7065478920936584, Validation Loss: 0.7008346319198608\n",
      "Epoch 56: Train Loss: 0.7771063685417176, Validation Loss: 0.7017995715141296\n",
      "Epoch 57: Train Loss: 0.7188591599464417, Validation Loss: 0.7024639248847961\n",
      "Epoch 58: Train Loss: 0.6629014372825622, Validation Loss: 0.7007986903190613\n",
      "Epoch 59: Train Loss: 0.6592651724815368, Validation Loss: 0.7009202241897583\n",
      "Epoch 60: Train Loss: 0.7098796725273132, Validation Loss: 0.7007705569267273\n",
      "Epoch 61: Train Loss: 0.7132391452789306, Validation Loss: 0.7010924220085144\n",
      "Epoch 62: Train Loss: 0.7252732396125794, Validation Loss: 0.7015938758850098\n",
      "Epoch 63: Train Loss: 0.6753525376319885, Validation Loss: 0.702350378036499\n",
      "Epoch 64: Train Loss: 0.6716751337051392, Validation Loss: 0.7011986970901489\n",
      "Epoch 65: Train Loss: 0.7058062195777893, Validation Loss: 0.7019593119621277\n",
      "Epoch 66: Train Loss: 0.6862512707710267, Validation Loss: 0.7019756436347961\n",
      "Epoch 67: Train Loss: 0.6743258237838745, Validation Loss: 0.7015218734741211\n",
      "Epoch 68: Train Loss: 0.6869608044624329, Validation Loss: 0.7019636034965515\n",
      "Epoch 69: Train Loss: 0.6952252864837647, Validation Loss: 0.7029515504837036\n",
      "Epoch 70: Train Loss: 0.7142788887023925, Validation Loss: 0.7023314237594604\n",
      "Epoch 71: Train Loss: 0.7376033425331116, Validation Loss: 0.7027527689933777\n",
      "Epoch 72: Train Loss: 0.6505773305892945, Validation Loss: 0.7026455402374268\n",
      "Epoch 73: Train Loss: 0.6884877324104309, Validation Loss: 0.7030653357505798\n",
      "Epoch 74: Train Loss: 0.6579405426979065, Validation Loss: 0.7033160924911499\n",
      "Epoch 75: Train Loss: 0.7630429983139038, Validation Loss: 0.7052622437477112\n",
      "Epoch 76: Train Loss: 0.6493926525115967, Validation Loss: 0.7045135498046875\n",
      "Epoch 77: Train Loss: 0.7095877766609192, Validation Loss: 0.7055555582046509\n",
      "Epoch 78: Train Loss: 0.6991910219192505, Validation Loss: 0.7052084803581238\n",
      "Epoch 79: Train Loss: 0.6595185399055481, Validation Loss: 0.7015561461448669\n",
      "Epoch 80: Train Loss: 0.6621718049049378, Validation Loss: 0.7029681205749512\n",
      "Epoch 81: Train Loss: 0.7292065262794495, Validation Loss: 0.7044082283973694\n",
      "Epoch 82: Train Loss: 0.6816583275794983, Validation Loss: 0.7054067850112915\n",
      "Epoch 83: Train Loss: 0.7229460597038269, Validation Loss: 0.7062093615531921\n",
      "Epoch 84: Train Loss: 0.7621667504310607, Validation Loss: 0.7042538523674011\n",
      "Epoch 85: Train Loss: 0.6991620659828186, Validation Loss: 0.7060669660568237\n",
      "Epoch 86: Train Loss: 0.7069108009338378, Validation Loss: 0.7064008712768555\n",
      "Epoch 87: Train Loss: 0.6682848691940307, Validation Loss: 0.7059008479118347\n",
      "Epoch 88: Train Loss: 0.6887999176979065, Validation Loss: 0.707030713558197\n",
      "Epoch 89: Train Loss: 0.7245773911476135, Validation Loss: 0.706947922706604\n",
      "Epoch 90: Train Loss: 0.7410665392875672, Validation Loss: 0.7076171636581421\n",
      "Epoch 91: Train Loss: 0.6821659803390503, Validation Loss: 0.7043768763542175\n",
      "Epoch 92: Train Loss: 0.6914069056510925, Validation Loss: 0.7050813436508179\n",
      "Epoch 93: Train Loss: 0.6924018144607544, Validation Loss: 0.7073191404342651\n",
      "Epoch 94: Train Loss: 0.6580235958099365, Validation Loss: 0.7068018913269043\n",
      "Epoch 95: Train Loss: 0.6782772421836853, Validation Loss: 0.7080141305923462\n",
      "Epoch 96: Train Loss: 0.7356348633766174, Validation Loss: 0.7085168957710266\n",
      "Epoch 97: Train Loss: 0.7011632442474365, Validation Loss: 0.7092380523681641\n",
      "Epoch 98: Train Loss: 0.6957149028778076, Validation Loss: 0.7090264558792114\n",
      "Epoch 99: Train Loss: 0.6797118782997131, Validation Loss: 0.7089143395423889\n",
      "Epoch 100: Train Loss: 0.7193026185035706, Validation Loss: 0.7103607058525085\n",
      "Epoch 101: Train Loss: 0.674798047542572, Validation Loss: 0.7081298828125\n",
      "Epoch 102: Train Loss: 0.6716411709785461, Validation Loss: 0.7089504599571228\n",
      "Epoch 103: Train Loss: 0.6903332471847534, Validation Loss: 0.7084265947341919\n",
      "Epoch 104: Train Loss: 0.6569036722183228, Validation Loss: 0.7089921832084656\n",
      "Epoch 105: Train Loss: 0.6776802182197571, Validation Loss: 0.710346519947052\n",
      "Epoch 106: Train Loss: 0.7002477884292603, Validation Loss: 0.7107255458831787\n",
      "Epoch 107: Train Loss: 0.6776126503944397, Validation Loss: 0.7112697958946228\n",
      "Epoch 108: Train Loss: 0.6626524567604065, Validation Loss: 0.7116910219192505\n",
      "Epoch 109: Train Loss: 0.6618531703948974, Validation Loss: 0.7103666067123413\n",
      "Epoch 110: Train Loss: 0.6624973297119141, Validation Loss: 0.7122544646263123\n",
      "Epoch 111: Train Loss: 0.6628796219825744, Validation Loss: 0.7121261358261108\n",
      "Epoch 112: Train Loss: 0.7004528164863586, Validation Loss: 0.710978090763092\n",
      "Epoch 113: Train Loss: 0.7047981023788452, Validation Loss: 0.7123041152954102\n",
      "Epoch 114: Train Loss: 0.6986559629440308, Validation Loss: 0.7109178304672241\n",
      "Epoch 115: Train Loss: 0.6793993592262269, Validation Loss: 0.7113245725631714\n",
      "Epoch 116: Train Loss: 0.7132913589477539, Validation Loss: 0.7125073075294495\n",
      "Epoch 117: Train Loss: 0.6794094204902649, Validation Loss: 0.7113000750541687\n",
      "Epoch 118: Train Loss: 0.6858237504959106, Validation Loss: 0.7131249308586121\n",
      "Epoch 119: Train Loss: 0.6432982563972474, Validation Loss: 0.7132828831672668\n",
      "Epoch 120: Train Loss: 0.7210246205329895, Validation Loss: 0.7130393981933594\n",
      "Epoch 121: Train Loss: 0.6952581882476807, Validation Loss: 0.712863028049469\n",
      "Epoch 122: Train Loss: 0.6549113988876343, Validation Loss: 0.7107005715370178\n",
      "Epoch 123: Train Loss: 0.6213227152824402, Validation Loss: 0.7127525210380554\n",
      "Epoch 124: Train Loss: 0.6810091018676758, Validation Loss: 0.7136639356613159\n",
      "Epoch 125: Train Loss: 0.6389216542243957, Validation Loss: 0.7130070924758911\n",
      "Epoch 126: Train Loss: 0.6239971876144409, Validation Loss: 0.7133769392967224\n",
      "Epoch 127: Train Loss: 0.6821864247322083, Validation Loss: 0.7115271091461182\n",
      "Epoch 128: Train Loss: 0.6941189289093017, Validation Loss: 0.7118266224861145\n",
      "Epoch 129: Train Loss: 0.6800421595573425, Validation Loss: 0.7130388021469116\n",
      "Epoch 130: Train Loss: 0.6932543992996216, Validation Loss: 0.713354766368866\n",
      "Epoch 131: Train Loss: 0.6589313626289368, Validation Loss: 0.7143996357917786\n",
      "Epoch 132: Train Loss: 0.6665288805961609, Validation Loss: 0.7150969505310059\n",
      "Epoch 133: Train Loss: 0.7030739426612854, Validation Loss: 0.7154420614242554\n",
      "Epoch 134: Train Loss: 0.7024593830108643, Validation Loss: 0.7166576981544495\n",
      "Epoch 135: Train Loss: 0.6711380481719971, Validation Loss: 0.7165545225143433\n",
      "Epoch 136: Train Loss: 0.6531975984573364, Validation Loss: 0.7147879600524902\n",
      "Epoch 137: Train Loss: 0.6727753400802612, Validation Loss: 0.7168864607810974\n",
      "Epoch 138: Train Loss: 0.7156760096549988, Validation Loss: 0.7178757190704346\n",
      "Epoch 139: Train Loss: 0.6533510565757752, Validation Loss: 0.7188153266906738\n",
      "Epoch 140: Train Loss: 0.6849551558494568, Validation Loss: 0.7189334034919739\n",
      "Epoch 141: Train Loss: 0.6761497139930726, Validation Loss: 0.7190799117088318\n",
      "Epoch 142: Train Loss: 0.6619069814682007, Validation Loss: 0.7197405695915222\n",
      "Epoch 143: Train Loss: 0.7201324462890625, Validation Loss: 0.7190297245979309\n",
      "Epoch 144: Train Loss: 0.6439677357673645, Validation Loss: 0.7196877598762512\n",
      "Epoch 145: Train Loss: 0.6448158025741577, Validation Loss: 0.7167901992797852\n",
      "Epoch 146: Train Loss: 0.668552553653717, Validation Loss: 0.7185859084129333\n",
      "Epoch 147: Train Loss: 0.6911381840705871, Validation Loss: 0.7194601893424988\n",
      "Epoch 148: Train Loss: 0.6782715916633606, Validation Loss: 0.7197651267051697\n",
      "Epoch 149: Train Loss: 0.6394954562187195, Validation Loss: 0.7201424837112427\n",
      "Epoch 150: Train Loss: 0.6571242690086365, Validation Loss: 0.7192161679267883\n",
      "Epoch 151: Train Loss: 0.6815149188041687, Validation Loss: 0.721385657787323\n",
      "Epoch 152: Train Loss: 0.7083225727081299, Validation Loss: 0.721220076084137\n",
      "Epoch 153: Train Loss: 0.6395583152770996, Validation Loss: 0.7231106758117676\n",
      "Epoch 154: Train Loss: 0.6940492033958435, Validation Loss: 0.7247998118400574\n",
      "Epoch 155: Train Loss: 0.7065367817878723, Validation Loss: 0.7213162183761597\n",
      "Epoch 156: Train Loss: 0.6291316628456116, Validation Loss: 0.7225216031074524\n",
      "Epoch 157: Train Loss: 0.7097048282623291, Validation Loss: 0.7196266055107117\n",
      "Epoch 158: Train Loss: 0.6440178275108337, Validation Loss: 0.7199523448944092\n",
      "Epoch 159: Train Loss: 0.692068898677826, Validation Loss: 0.7213073372840881\n",
      "Epoch 160: Train Loss: 0.7593290448188782, Validation Loss: 0.7217915654182434\n",
      "Epoch 161: Train Loss: 0.7587382316589355, Validation Loss: 0.7207064628601074\n",
      "Epoch 162: Train Loss: 0.6479059100151062, Validation Loss: 0.7228842973709106\n",
      "Epoch 163: Train Loss: 0.6825177311897278, Validation Loss: 0.7241432666778564\n",
      "Epoch 164: Train Loss: 0.6440361618995667, Validation Loss: 0.72541743516922\n",
      "Epoch 165: Train Loss: 0.6232794642448425, Validation Loss: 0.7223567962646484\n",
      "Epoch 166: Train Loss: 0.6607454538345336, Validation Loss: 0.7228643894195557\n",
      "Epoch 167: Train Loss: 0.6767019748687744, Validation Loss: 0.7232856154441833\n",
      "Epoch 168: Train Loss: 0.7443158268928528, Validation Loss: 0.7243037223815918\n",
      "Epoch 169: Train Loss: 0.6833714842796326, Validation Loss: 0.7214080691337585\n",
      "Epoch 170: Train Loss: 0.6689830303192139, Validation Loss: 0.7240986824035645\n",
      "Epoch 171: Train Loss: 0.65382239818573, Validation Loss: 0.7238968014717102\n",
      "Epoch 172: Train Loss: 0.6777749538421631, Validation Loss: 0.7251620292663574\n",
      "Epoch 173: Train Loss: 0.6598475694656372, Validation Loss: 0.7209365963935852\n",
      "Epoch 174: Train Loss: 0.6786872982978821, Validation Loss: 0.7243843674659729\n",
      "Epoch 175: Train Loss: 0.6835282683372498, Validation Loss: 0.7252054810523987\n",
      "Epoch 176: Train Loss: 0.6473879218101501, Validation Loss: 0.7233245968818665\n",
      "Epoch 177: Train Loss: 0.6486207842826843, Validation Loss: 0.7209153175354004\n",
      "Epoch 178: Train Loss: 0.7068673729896545, Validation Loss: 0.7249447107315063\n",
      "Epoch 179: Train Loss: 0.6230527520179748, Validation Loss: 0.7247710824012756\n",
      "Epoch 180: Train Loss: 0.6282333254814148, Validation Loss: 0.72242271900177\n",
      "Epoch 181: Train Loss: 0.6410441398620605, Validation Loss: 0.7245143055915833\n",
      "Epoch 182: Train Loss: 0.6159552156925201, Validation Loss: 0.7279361486434937\n",
      "Epoch 183: Train Loss: 0.6764544129371644, Validation Loss: 0.7269526720046997\n",
      "Epoch 184: Train Loss: 0.685339093208313, Validation Loss: 0.7262719869613647\n",
      "Epoch 185: Train Loss: 0.631093966960907, Validation Loss: 0.7282270193099976\n",
      "Epoch 186: Train Loss: 0.6485705018043518, Validation Loss: 0.7282939553260803\n",
      "Epoch 187: Train Loss: 0.6897075414657593, Validation Loss: 0.7308049201965332\n",
      "Epoch 188: Train Loss: 0.7137786269187927, Validation Loss: 0.7311208844184875\n",
      "Epoch 189: Train Loss: 0.6762423753738404, Validation Loss: 0.7266577482223511\n",
      "Epoch 190: Train Loss: 0.6655451059341431, Validation Loss: 0.7293127179145813\n",
      "Epoch 191: Train Loss: 0.6445835947990417, Validation Loss: 0.7266972661018372\n",
      "Epoch 192: Train Loss: 0.6407154560089111, Validation Loss: 0.7255303263664246\n",
      "Epoch 193: Train Loss: 0.6216347336769104, Validation Loss: 0.7274171113967896\n",
      "Epoch 194: Train Loss: 0.7155688643455506, Validation Loss: 0.7276322841644287\n",
      "Epoch 195: Train Loss: 0.611271321773529, Validation Loss: 0.7310643196105957\n",
      "Epoch 196: Train Loss: 0.6491232991218567, Validation Loss: 0.7274617552757263\n",
      "Epoch 197: Train Loss: 0.6642381906509399, Validation Loss: 0.7298914790153503\n",
      "Epoch 198: Train Loss: 0.6417765498161316, Validation Loss: 0.7312660217285156\n",
      "Epoch 199: Train Loss: 0.5973863303661346, Validation Loss: 0.7291907668113708\n",
      "Epoch 200: Train Loss: 0.604684579372406, Validation Loss: 0.7322688698768616\n",
      "Epoch 201: Train Loss: 0.6459985136985779, Validation Loss: 0.7322368621826172\n",
      "Epoch 202: Train Loss: 0.6351955771446228, Validation Loss: 0.7334231734275818\n",
      "Epoch 203: Train Loss: 0.6884170174598694, Validation Loss: 0.7379329204559326\n",
      "Epoch 204: Train Loss: 0.6705807566642761, Validation Loss: 0.7338027358055115\n",
      "Epoch 205: Train Loss: 0.6186251640319824, Validation Loss: 0.7366784811019897\n",
      "Epoch 206: Train Loss: 0.6690234661102294, Validation Loss: 0.7371841073036194\n",
      "Epoch 207: Train Loss: 0.6502901077270508, Validation Loss: 0.7397602200508118\n",
      "Epoch 208: Train Loss: 0.6078241109848023, Validation Loss: 0.7329404354095459\n",
      "Epoch 209: Train Loss: 0.6825797438621521, Validation Loss: 0.7344953417778015\n",
      "Epoch 210: Train Loss: 0.6353212714195251, Validation Loss: 0.7356542348861694\n",
      "Epoch 211: Train Loss: 0.6226901054382324, Validation Loss: 0.7316062450408936\n",
      "Epoch 212: Train Loss: 0.6570181012153625, Validation Loss: 0.7325291633605957\n",
      "Epoch 213: Train Loss: 0.6979422926902771, Validation Loss: 0.7334877848625183\n",
      "Epoch 214: Train Loss: 0.6451875925064087, Validation Loss: 0.7360431551933289\n",
      "Epoch 215: Train Loss: 0.6938538789749146, Validation Loss: 0.7359700202941895\n",
      "Epoch 216: Train Loss: 0.6176781535148621, Validation Loss: 0.7338455319404602\n",
      "Epoch 217: Train Loss: 0.6606490015983582, Validation Loss: 0.7349485158920288\n",
      "Epoch 218: Train Loss: 0.6646730303764343, Validation Loss: 0.7380807399749756\n",
      "Epoch 219: Train Loss: 0.7030975103378296, Validation Loss: 0.73902428150177\n",
      "Epoch 220: Train Loss: 0.6431749105453491, Validation Loss: 0.7353854179382324\n",
      "Epoch 221: Train Loss: 0.6161448895931244, Validation Loss: 0.7342263460159302\n",
      "Epoch 222: Train Loss: 0.6476862668991089, Validation Loss: 0.7392570376396179\n",
      "Epoch 223: Train Loss: 0.6521057963371277, Validation Loss: 0.7408550381660461\n",
      "Epoch 224: Train Loss: 0.7079543113708496, Validation Loss: 0.7419946789741516\n",
      "Epoch 225: Train Loss: 0.6242133259773255, Validation Loss: 0.7446596026420593\n",
      "Epoch 226: Train Loss: 0.6522958636283874, Validation Loss: 0.7452076077461243\n",
      "Epoch 227: Train Loss: 0.6842986941337585, Validation Loss: 0.7437960505485535\n",
      "Epoch 228: Train Loss: 0.6869746923446656, Validation Loss: 0.744638979434967\n",
      "Epoch 229: Train Loss: 0.6099167108535767, Validation Loss: 0.7431142330169678\n",
      "Epoch 230: Train Loss: 0.6374311685562134, Validation Loss: 0.7406905889511108\n",
      "Epoch 231: Train Loss: 0.6213536024093628, Validation Loss: 0.7418692708015442\n",
      "Epoch 232: Train Loss: 0.6046119809150696, Validation Loss: 0.739341676235199\n",
      "Epoch 233: Train Loss: 0.6484068036079407, Validation Loss: 0.7416917085647583\n",
      "Epoch 234: Train Loss: 0.7231085777282715, Validation Loss: 0.74496990442276\n",
      "Epoch 235: Train Loss: 0.6444439768791199, Validation Loss: 0.7455540895462036\n",
      "Epoch 236: Train Loss: 0.6820327758789062, Validation Loss: 0.7473400235176086\n",
      "Epoch 237: Train Loss: 0.6533922791481018, Validation Loss: 0.7492579817771912\n",
      "Epoch 238: Train Loss: 0.6384269952774048, Validation Loss: 0.7506756782531738\n",
      "Epoch 239: Train Loss: 0.6540337085723877, Validation Loss: 0.7488589882850647\n",
      "Epoch 240: Train Loss: 0.6646318435668945, Validation Loss: 0.7517949342727661\n",
      "Epoch 241: Train Loss: 0.5997658371925354, Validation Loss: 0.7406642436981201\n",
      "Epoch 242: Train Loss: 0.6342893481254578, Validation Loss: 0.7419255375862122\n",
      "Epoch 243: Train Loss: 0.5833013117313385, Validation Loss: 0.7386248707771301\n",
      "Epoch 244: Train Loss: 0.6189350843429565, Validation Loss: 0.7443366050720215\n",
      "Epoch 245: Train Loss: 0.6403094172477722, Validation Loss: 0.7475939989089966\n",
      "Epoch 246: Train Loss: 0.6198105096817017, Validation Loss: 0.7520965337753296\n",
      "Epoch 247: Train Loss: 0.600201016664505, Validation Loss: 0.7542998790740967\n",
      "Epoch 248: Train Loss: 0.639270293712616, Validation Loss: 0.7438336610794067\n",
      "Epoch 249: Train Loss: 0.6645372629165649, Validation Loss: 0.7496678829193115\n",
      "Epoch 250: Train Loss: 0.6431460618972779, Validation Loss: 0.7481622099876404\n",
      "Epoch 251: Train Loss: 0.5831554651260376, Validation Loss: 0.7532840371131897\n",
      "Epoch 252: Train Loss: 0.6341372013092041, Validation Loss: 0.7592494487762451\n",
      "Epoch 253: Train Loss: 0.5854412317276001, Validation Loss: 0.7555241584777832\n",
      "Epoch 254: Train Loss: 0.5896404147148132, Validation Loss: 0.756509006023407\n",
      "Epoch 255: Train Loss: 0.5928604185581208, Validation Loss: 0.7475998997688293\n",
      "Epoch 256: Train Loss: 0.6605692386627198, Validation Loss: 0.7541394233703613\n",
      "Epoch 257: Train Loss: 0.6029160380363464, Validation Loss: 0.7546764016151428\n",
      "Epoch 258: Train Loss: 0.6524027228355408, Validation Loss: 0.7509140968322754\n",
      "Epoch 259: Train Loss: 0.61706702709198, Validation Loss: 0.7529618144035339\n",
      "Epoch 260: Train Loss: 0.6187028646469116, Validation Loss: 0.7494109272956848\n",
      "Epoch 261: Train Loss: 0.6642199397087097, Validation Loss: 0.7504321932792664\n",
      "Epoch 262: Train Loss: 0.6243938088417054, Validation Loss: 0.749361515045166\n",
      "Epoch 263: Train Loss: 0.6082341074943542, Validation Loss: 0.7515904903411865\n",
      "Epoch 264: Train Loss: 0.6746932864189148, Validation Loss: 0.7490201592445374\n",
      "Epoch 265: Train Loss: 0.5830443024635314, Validation Loss: 0.7562397122383118\n",
      "Epoch 266: Train Loss: 0.6112297892570495, Validation Loss: 0.7544060945510864\n",
      "Epoch 267: Train Loss: 0.6430546402931213, Validation Loss: 0.7568950057029724\n",
      "Epoch 268: Train Loss: 0.6341135859489441, Validation Loss: 0.7586416006088257\n",
      "Epoch 269: Train Loss: 0.6674371242523194, Validation Loss: 0.7566623091697693\n",
      "Epoch 270: Train Loss: 0.61304931640625, Validation Loss: 0.7545115351676941\n",
      "Epoch 271: Train Loss: 0.6385224461555481, Validation Loss: 0.7606267333030701\n",
      "Epoch 272: Train Loss: 0.63990318775177, Validation Loss: 0.7646012902259827\n",
      "Epoch 273: Train Loss: 0.6267966866493225, Validation Loss: 0.7620052099227905\n",
      "Epoch 274: Train Loss: 0.6241429448127747, Validation Loss: 0.754473090171814\n",
      "Epoch 275: Train Loss: 0.6581626057624816, Validation Loss: 0.7621715068817139\n",
      "Epoch 276: Train Loss: 0.6386023998260498, Validation Loss: 0.7638599276542664\n",
      "Epoch 277: Train Loss: 0.6610192775726318, Validation Loss: 0.765478789806366\n",
      "Epoch 278: Train Loss: 0.5835846304893494, Validation Loss: 0.769737720489502\n",
      "Epoch 279: Train Loss: 0.6573004722595215, Validation Loss: 0.7592698335647583\n",
      "Epoch 280: Train Loss: 0.6890144109725952, Validation Loss: 0.7621879577636719\n",
      "Epoch 281: Train Loss: 0.621434235572815, Validation Loss: 0.7656084895133972\n",
      "Epoch 282: Train Loss: 0.629224157333374, Validation Loss: 0.761203944683075\n",
      "Epoch 283: Train Loss: 0.577003014087677, Validation Loss: 0.7663488388061523\n",
      "Epoch 284: Train Loss: 0.5845152020454407, Validation Loss: 0.7650648951530457\n",
      "Epoch 285: Train Loss: 0.6532414317131042, Validation Loss: 0.7698653340339661\n",
      "Epoch 286: Train Loss: 0.7033979892730713, Validation Loss: 0.7685686945915222\n",
      "Epoch 287: Train Loss: 0.6432565569877624, Validation Loss: 0.7718487977981567\n",
      "Epoch 288: Train Loss: 0.6026543855667115, Validation Loss: 0.7695645689964294\n",
      "Epoch 289: Train Loss: 0.6114511370658875, Validation Loss: 0.7647132873535156\n",
      "Epoch 290: Train Loss: 0.6502155184745788, Validation Loss: 0.7717567086219788\n",
      "Epoch 291: Train Loss: 0.6319661021232605, Validation Loss: 0.7688044905662537\n",
      "Epoch 292: Train Loss: 0.6539754509925843, Validation Loss: 0.770394504070282\n",
      "Epoch 293: Train Loss: 0.6511240005493164, Validation Loss: 0.7770905494689941\n",
      "Epoch 294: Train Loss: 0.6398801207542419, Validation Loss: 0.7756543159484863\n",
      "Epoch 295: Train Loss: 0.6903127551078796, Validation Loss: 0.7731384038925171\n",
      "Epoch 296: Train Loss: 0.6141131401062012, Validation Loss: 0.7746508717536926\n",
      "Epoch 297: Train Loss: 0.5980257630348206, Validation Loss: 0.773134708404541\n",
      "Epoch 298: Train Loss: 0.6678954720497131, Validation Loss: 0.7735166549682617\n",
      "Epoch 299: Train Loss: 0.622787356376648, Validation Loss: 0.7686232924461365\n",
      "Epoch 300: Train Loss: 0.6046887993812561, Validation Loss: 0.7667327523231506\n",
      "Epoch 301: Train Loss: 0.5779381573200226, Validation Loss: 0.7683665752410889\n",
      "Epoch 302: Train Loss: 0.6358472585678101, Validation Loss: 0.7701177000999451\n",
      "Epoch 303: Train Loss: 0.6322243213653564, Validation Loss: 0.7701148986816406\n",
      "Epoch 304: Train Loss: 0.5473203718662262, Validation Loss: 0.7715103030204773\n",
      "Epoch 305: Train Loss: 0.5784493207931518, Validation Loss: 0.7750409245491028\n",
      "Epoch 306: Train Loss: 0.6096831202507019, Validation Loss: 0.7717530727386475\n",
      "Epoch 307: Train Loss: 0.6094779968261719, Validation Loss: 0.775946319103241\n",
      "Epoch 308: Train Loss: 0.5654472231864929, Validation Loss: 0.7607823610305786\n",
      "Epoch 309: Train Loss: 0.5967472493648529, Validation Loss: 0.7656710743904114\n",
      "Epoch 310: Train Loss: 0.6762491106987, Validation Loss: 0.770545482635498\n",
      "Epoch 311: Train Loss: 0.6162840127944946, Validation Loss: 0.7734028697013855\n",
      "Epoch 312: Train Loss: 0.6293445587158203, Validation Loss: 0.772454023361206\n",
      "Epoch 313: Train Loss: 0.6801515817642212, Validation Loss: 0.7784001231193542\n",
      "Epoch 314: Train Loss: 0.6427562832832336, Validation Loss: 0.7787187099456787\n",
      "Epoch 315: Train Loss: 0.6219229102134705, Validation Loss: 0.7767265439033508\n",
      "Epoch 316: Train Loss: 0.6318324327468872, Validation Loss: 0.7663726806640625\n",
      "Epoch 317: Train Loss: 0.5658104419708252, Validation Loss: 0.7715920805931091\n",
      "Epoch 318: Train Loss: 0.6251535415649414, Validation Loss: 0.7710330486297607\n",
      "Epoch 319: Train Loss: 0.5720265090465546, Validation Loss: 0.7742511630058289\n",
      "Epoch 320: Train Loss: 0.6206637501716614, Validation Loss: 0.780495822429657\n",
      "Epoch 321: Train Loss: 0.5819993734359741, Validation Loss: 0.7699441313743591\n",
      "Epoch 322: Train Loss: 0.5826140999794006, Validation Loss: 0.769914448261261\n",
      "Epoch 323: Train Loss: 0.6448456525802613, Validation Loss: 0.7721740007400513\n",
      "Epoch 324: Train Loss: 0.6350892186164856, Validation Loss: 0.7801787257194519\n",
      "Epoch 325: Train Loss: 0.6275018334388733, Validation Loss: 0.7695819139480591\n",
      "Epoch 326: Train Loss: 0.6237723350524902, Validation Loss: 0.7740910649299622\n",
      "Epoch 327: Train Loss: 0.6805986046791077, Validation Loss: 0.7739079594612122\n",
      "Epoch 328: Train Loss: 0.5972706437110901, Validation Loss: 0.7764832377433777\n",
      "Epoch 329: Train Loss: 0.6102720975875855, Validation Loss: 0.7705358266830444\n",
      "Epoch 330: Train Loss: 0.5588594079017639, Validation Loss: 0.7759494781494141\n",
      "Epoch 331: Train Loss: 0.5879249215126038, Validation Loss: 0.7733354568481445\n",
      "Epoch 332: Train Loss: 0.5970098495483398, Validation Loss: 0.7754676938056946\n",
      "Epoch 333: Train Loss: 0.5969790577888489, Validation Loss: 0.7813196182250977\n",
      "Epoch 334: Train Loss: 0.5835924267768859, Validation Loss: 0.7888345718383789\n",
      "Epoch 335: Train Loss: 0.5882383942604065, Validation Loss: 0.7770511507987976\n",
      "Epoch 336: Train Loss: 0.5432273566722869, Validation Loss: 0.7762319445610046\n",
      "Epoch 337: Train Loss: 0.5998292446136475, Validation Loss: 0.7835534811019897\n",
      "Epoch 338: Train Loss: 0.571849811077118, Validation Loss: 0.7915050387382507\n",
      "Epoch 339: Train Loss: 0.5710858047008515, Validation Loss: 0.7972041964530945\n",
      "Epoch 340: Train Loss: 0.5887898921966552, Validation Loss: 0.7981082201004028\n",
      "Epoch 341: Train Loss: 0.5880636215209961, Validation Loss: 0.7958489656448364\n",
      "Epoch 342: Train Loss: 0.6248765110969543, Validation Loss: 0.7935607433319092\n",
      "Epoch 343: Train Loss: 0.6120757341384888, Validation Loss: 0.796527087688446\n",
      "Epoch 344: Train Loss: 0.6262384533882142, Validation Loss: 0.7904869318008423\n",
      "Epoch 345: Train Loss: 0.6005009055137634, Validation Loss: 0.78289794921875\n",
      "Epoch 346: Train Loss: 0.5982234120368958, Validation Loss: 0.7826822400093079\n",
      "Epoch 347: Train Loss: 0.6129588723182678, Validation Loss: 0.7883465886116028\n",
      "Epoch 348: Train Loss: 0.6086386561393737, Validation Loss: 0.8005898594856262\n",
      "Epoch 349: Train Loss: 0.6375686883926391, Validation Loss: 0.799954891204834\n",
      "Epoch 350: Train Loss: 0.6404559493064881, Validation Loss: 0.8050603866577148\n",
      "Epoch 351: Train Loss: 0.5681803584098816, Validation Loss: 0.7946198582649231\n",
      "Epoch 352: Train Loss: 0.5941778659820557, Validation Loss: 0.7998224496841431\n",
      "Epoch 353: Train Loss: 0.569729071855545, Validation Loss: 0.7969673275947571\n",
      "Epoch 354: Train Loss: 0.5542383074760437, Validation Loss: 0.8091214895248413\n",
      "Epoch 355: Train Loss: 0.5795840978622436, Validation Loss: 0.801063060760498\n",
      "Epoch 356: Train Loss: 0.6000401496887207, Validation Loss: 0.8090615272521973\n",
      "Epoch 357: Train Loss: 0.6247970223426819, Validation Loss: 0.8097031712532043\n",
      "Epoch 358: Train Loss: 0.5689410448074341, Validation Loss: 0.8087554574012756\n",
      "Epoch 359: Train Loss: 0.5758412182331085, Validation Loss: 0.8093789219856262\n",
      "Epoch 360: Train Loss: 0.5722020149230957, Validation Loss: 0.8153983950614929\n",
      "Epoch 361: Train Loss: 0.5488229870796204, Validation Loss: 0.8237593770027161\n",
      "Epoch 362: Train Loss: 0.5762286484241486, Validation Loss: 0.8183461427688599\n",
      "Epoch 363: Train Loss: 0.586215615272522, Validation Loss: 0.8335674405097961\n",
      "Epoch 364: Train Loss: 0.5706765174865722, Validation Loss: 0.8325173854827881\n",
      "Epoch 365: Train Loss: 0.5640693604946136, Validation Loss: 0.8314433097839355\n",
      "Epoch 366: Train Loss: 0.6059671998023987, Validation Loss: 0.816848874092102\n",
      "Epoch 367: Train Loss: 0.5788930177688598, Validation Loss: 0.815599262714386\n",
      "Epoch 368: Train Loss: 0.6248064637184143, Validation Loss: 0.8192129135131836\n",
      "Epoch 369: Train Loss: 0.6610857725143433, Validation Loss: 0.8134816884994507\n",
      "Epoch 370: Train Loss: 0.6040789783000946, Validation Loss: 0.8109343647956848\n",
      "Epoch 371: Train Loss: 0.5947585344314575, Validation Loss: 0.8105466365814209\n",
      "Epoch 372: Train Loss: 0.6509153008460998, Validation Loss: 0.7923747897148132\n",
      "Epoch 373: Train Loss: 0.5764207005500793, Validation Loss: 0.7983959317207336\n",
      "Epoch 374: Train Loss: 0.5618139386177063, Validation Loss: 0.8010272979736328\n",
      "Epoch 375: Train Loss: 0.5942255139350892, Validation Loss: 0.80585777759552\n",
      "Epoch 376: Train Loss: 0.5617234349250794, Validation Loss: 0.8039588332176208\n",
      "Epoch 377: Train Loss: 0.571365475654602, Validation Loss: 0.7982894778251648\n",
      "Epoch 378: Train Loss: 0.6074332475662232, Validation Loss: 0.8061956167221069\n",
      "Epoch 379: Train Loss: 0.6176383256912231, Validation Loss: 0.8153778314590454\n",
      "Epoch 380: Train Loss: 0.6628278136253357, Validation Loss: 0.8146064281463623\n",
      "Epoch 381: Train Loss: 0.6003566205501556, Validation Loss: 0.8137611150741577\n",
      "Epoch 382: Train Loss: 0.5488643646240234, Validation Loss: 0.8088613748550415\n",
      "Epoch 383: Train Loss: 0.5982220530509949, Validation Loss: 0.81434166431427\n",
      "Epoch 384: Train Loss: 0.5386283099651337, Validation Loss: 0.814266562461853\n",
      "Epoch 385: Train Loss: 0.6092310309410095, Validation Loss: 0.8158813118934631\n",
      "Epoch 386: Train Loss: 0.6161437511444092, Validation Loss: 0.8063533902168274\n",
      "Epoch 387: Train Loss: 0.5588956713676453, Validation Loss: 0.8027957081794739\n",
      "Epoch 388: Train Loss: 0.5577755689620971, Validation Loss: 0.8091347813606262\n",
      "Epoch 389: Train Loss: 0.5347359001636505, Validation Loss: 0.8093675374984741\n",
      "Epoch 390: Train Loss: 0.5704418599605561, Validation Loss: 0.8238368034362793\n",
      "Epoch 391: Train Loss: 0.6101338267326355, Validation Loss: 0.8189662098884583\n",
      "Epoch 392: Train Loss: 0.583074975013733, Validation Loss: 0.8126928210258484\n",
      "Epoch 393: Train Loss: 0.5518299877643585, Validation Loss: 0.8005296587944031\n",
      "Epoch 394: Train Loss: 0.5476482272148132, Validation Loss: 0.8045926094055176\n",
      "Epoch 395: Train Loss: 0.617832601070404, Validation Loss: 0.7970554232597351\n",
      "Epoch 396: Train Loss: 0.5811540901660919, Validation Loss: 0.8040770888328552\n",
      "Epoch 397: Train Loss: 0.5730623722076416, Validation Loss: 0.8134346008300781\n",
      "Epoch 398: Train Loss: 0.6033833503723145, Validation Loss: 0.8201614618301392\n",
      "Epoch 399: Train Loss: 0.5841307878494263, Validation Loss: 0.8285166025161743\n",
      "Epoch 400: Train Loss: 0.5571428596973419, Validation Loss: 0.8114087581634521\n",
      "Epoch 401: Train Loss: 0.5674746990203857, Validation Loss: 0.8141759037971497\n",
      "Epoch 402: Train Loss: 0.5432374000549316, Validation Loss: 0.8159599304199219\n",
      "Epoch 403: Train Loss: 0.5652573108673096, Validation Loss: 0.8150533437728882\n",
      "Epoch 404: Train Loss: 0.5854603290557862, Validation Loss: 0.8156571388244629\n",
      "Epoch 405: Train Loss: 0.5508927941322327, Validation Loss: 0.8110674023628235\n",
      "Epoch 406: Train Loss: 0.5651323318481445, Validation Loss: 0.81903076171875\n",
      "Epoch 407: Train Loss: 0.5634468078613282, Validation Loss: 0.8191083669662476\n",
      "Epoch 408: Train Loss: 0.5645990908145905, Validation Loss: 0.8220379948616028\n",
      "Epoch 409: Train Loss: 0.5406720578670502, Validation Loss: 0.828423261642456\n",
      "Epoch 410: Train Loss: 0.5292317271232605, Validation Loss: 0.8235790133476257\n",
      "Epoch 411: Train Loss: 0.5199333250522613, Validation Loss: 0.8319562077522278\n",
      "Epoch 412: Train Loss: 0.5060604274272918, Validation Loss: 0.8243889808654785\n",
      "Epoch 413: Train Loss: 0.5696726202964782, Validation Loss: 0.8270183205604553\n",
      "Epoch 414: Train Loss: 0.5643723785877228, Validation Loss: 0.8271798491477966\n",
      "Epoch 415: Train Loss: 0.5843027830123901, Validation Loss: 0.8343517184257507\n",
      "Epoch 416: Train Loss: 0.5979066848754883, Validation Loss: 0.8469493985176086\n",
      "Epoch 417: Train Loss: 0.5732656717300415, Validation Loss: 0.8357482552528381\n",
      "Epoch 418: Train Loss: 0.4872916638851166, Validation Loss: 0.8374528884887695\n",
      "Epoch 419: Train Loss: 0.5215186238288879, Validation Loss: 0.8409446477890015\n",
      "Epoch 420: Train Loss: 0.5337042272090912, Validation Loss: 0.8441566228866577\n",
      "Epoch 421: Train Loss: 0.5640138745307922, Validation Loss: 0.8458030223846436\n",
      "Epoch 422: Train Loss: 0.5635903954505921, Validation Loss: 0.8554638028144836\n",
      "Epoch 423: Train Loss: 0.5509988129138946, Validation Loss: 0.8401719331741333\n",
      "Epoch 424: Train Loss: 0.5573947310447693, Validation Loss: 0.8405654430389404\n",
      "Epoch 425: Train Loss: 0.5956541061401367, Validation Loss: 0.8490013480186462\n",
      "Epoch 426: Train Loss: 0.5931577146053314, Validation Loss: 0.8545951843261719\n",
      "Epoch 427: Train Loss: 0.5493710041046143, Validation Loss: 0.8577468991279602\n",
      "Epoch 428: Train Loss: 0.5402980506420135, Validation Loss: 0.8708206415176392\n",
      "Epoch 429: Train Loss: 0.5294025540351868, Validation Loss: 0.8636497259140015\n",
      "Epoch 430: Train Loss: 0.620133352279663, Validation Loss: 0.8730772137641907\n",
      "Epoch 431: Train Loss: 0.5129913687705994, Validation Loss: 0.8714680075645447\n",
      "Epoch 432: Train Loss: 0.5330025494098664, Validation Loss: 0.855307936668396\n",
      "Epoch 433: Train Loss: 0.49247615933418276, Validation Loss: 0.8513885140419006\n",
      "Epoch 434: Train Loss: 0.4572759926319122, Validation Loss: 0.8628392815589905\n",
      "Epoch 435: Train Loss: 0.5449845790863037, Validation Loss: 0.8573533892631531\n",
      "Epoch 436: Train Loss: 0.541461169719696, Validation Loss: 0.8543546795845032\n",
      "Epoch 437: Train Loss: 0.507965725660324, Validation Loss: 0.8572994470596313\n",
      "Epoch 438: Train Loss: 0.5266384422779083, Validation Loss: 0.8630897402763367\n",
      "Epoch 439: Train Loss: 0.5335682570934296, Validation Loss: 0.8650239706039429\n",
      "Epoch 440: Train Loss: 0.5972849667072296, Validation Loss: 0.8666436672210693\n",
      "Epoch 441: Train Loss: 0.5067943155765533, Validation Loss: 0.8559237718582153\n",
      "Epoch 442: Train Loss: 0.4980385422706604, Validation Loss: 0.8475068211555481\n",
      "Epoch 443: Train Loss: 0.509703540802002, Validation Loss: 0.8642800450325012\n",
      "Epoch 444: Train Loss: 0.5416531085968017, Validation Loss: 0.8628031015396118\n",
      "Epoch 445: Train Loss: 0.5385457217693329, Validation Loss: 0.8726035356521606\n",
      "Epoch 446: Train Loss: 0.6032441616058349, Validation Loss: 0.8789704442024231\n",
      "Epoch 447: Train Loss: 0.545469468832016, Validation Loss: 0.8941074013710022\n",
      "Epoch 448: Train Loss: 0.5550264775753021, Validation Loss: 0.8990764617919922\n",
      "Epoch 449: Train Loss: 0.5096262633800507, Validation Loss: 0.9059770107269287\n",
      "Epoch 450: Train Loss: 0.5423177540302276, Validation Loss: 0.8945866823196411\n",
      "Epoch 451: Train Loss: 0.525103771686554, Validation Loss: 0.8729142546653748\n",
      "Epoch 452: Train Loss: 0.5510632514953613, Validation Loss: 0.8714460730552673\n",
      "Epoch 453: Train Loss: 0.501393735408783, Validation Loss: 0.8738914132118225\n",
      "Epoch 454: Train Loss: 0.5380729258060455, Validation Loss: 0.8661075234413147\n",
      "Epoch 455: Train Loss: 0.48560338616371157, Validation Loss: 0.8772562146186829\n",
      "Epoch 456: Train Loss: 0.6046050250530243, Validation Loss: 0.8835492730140686\n",
      "Epoch 457: Train Loss: 0.4869714915752411, Validation Loss: 0.8802233934402466\n",
      "Epoch 458: Train Loss: 0.49473106265068056, Validation Loss: 0.8714927434921265\n",
      "Epoch 459: Train Loss: 0.5035664200782776, Validation Loss: 0.8707117438316345\n",
      "Epoch 460: Train Loss: 0.5029247105121613, Validation Loss: 0.8651494979858398\n",
      "Epoch 461: Train Loss: 0.5403684556484223, Validation Loss: 0.8688639998435974\n",
      "Epoch 462: Train Loss: 0.5007224142551422, Validation Loss: 0.8774661421775818\n",
      "Epoch 463: Train Loss: 0.5893776595592499, Validation Loss: 0.8699735999107361\n",
      "Epoch 464: Train Loss: 0.46610061526298524, Validation Loss: 0.8559589385986328\n",
      "Epoch 465: Train Loss: 0.5450516760349273, Validation Loss: 0.8760896921157837\n",
      "Epoch 466: Train Loss: 0.6050985038280488, Validation Loss: 0.8903144001960754\n",
      "Epoch 467: Train Loss: 0.48886761665344236, Validation Loss: 0.8759559988975525\n",
      "Epoch 468: Train Loss: 0.49321749806404114, Validation Loss: 0.8691120147705078\n",
      "Epoch 469: Train Loss: 0.4367885857820511, Validation Loss: 0.8809766173362732\n",
      "Epoch 470: Train Loss: 0.4729523241519928, Validation Loss: 0.8835259675979614\n",
      "Epoch 471: Train Loss: 0.4737675845623016, Validation Loss: 0.8814277052879333\n",
      "Epoch 472: Train Loss: 0.5087272167205811, Validation Loss: 0.8806695342063904\n",
      "Epoch 473: Train Loss: 0.5004435062408448, Validation Loss: 0.8950191140174866\n",
      "Epoch 474: Train Loss: 0.45068249106407166, Validation Loss: 0.8933885097503662\n",
      "Epoch 475: Train Loss: 0.4808511435985565, Validation Loss: 0.8930515050888062\n",
      "Epoch 476: Train Loss: 0.4761891603469849, Validation Loss: 0.9012855291366577\n",
      "Epoch 477: Train Loss: 0.5095512390136718, Validation Loss: 0.9027301073074341\n",
      "Epoch 478: Train Loss: 0.45977922081947326, Validation Loss: 0.9135547280311584\n",
      "Epoch 479: Train Loss: 0.5076610743999481, Validation Loss: 0.8949598670005798\n",
      "Epoch 480: Train Loss: 0.5091682195663452, Validation Loss: 0.9091575741767883\n",
      "Epoch 481: Train Loss: 0.5029797315597534, Validation Loss: 0.8967317342758179\n",
      "Epoch 482: Train Loss: 0.5009058952331543, Validation Loss: 0.9101016521453857\n",
      "Epoch 483: Train Loss: 0.4850971281528473, Validation Loss: 0.9168605208396912\n",
      "Epoch 484: Train Loss: 0.4963667392730713, Validation Loss: 0.9254230260848999\n",
      "Epoch 485: Train Loss: 0.4264754831790924, Validation Loss: 0.9169589281082153\n",
      "Epoch 486: Train Loss: 0.5195441365242004, Validation Loss: 0.9189085364341736\n",
      "Epoch 487: Train Loss: 0.4949060320854187, Validation Loss: 0.9190087914466858\n",
      "Epoch 488: Train Loss: 0.45606288909912107, Validation Loss: 0.9106742739677429\n",
      "Epoch 489: Train Loss: 0.48511186242103577, Validation Loss: 0.9013761281967163\n",
      "Epoch 490: Train Loss: 0.466579532623291, Validation Loss: 0.9099233150482178\n",
      "Epoch 491: Train Loss: 0.46701760292053224, Validation Loss: 0.9275104999542236\n",
      "Epoch 492: Train Loss: 0.5002701878547668, Validation Loss: 0.9435878992080688\n",
      "Epoch 493: Train Loss: 0.4697306275367737, Validation Loss: 0.9492576122283936\n",
      "Epoch 494: Train Loss: 0.4171564817428589, Validation Loss: 0.9658973813056946\n",
      "Epoch 495: Train Loss: 0.4487354516983032, Validation Loss: 0.9718401432037354\n",
      "Epoch 496: Train Loss: 0.5072129189968109, Validation Loss: 0.9674449563026428\n",
      "Epoch 497: Train Loss: 0.44560967683792113, Validation Loss: 0.9680274724960327\n",
      "Epoch 498: Train Loss: 0.4836828887462616, Validation Loss: 0.9815958142280579\n",
      "Epoch 499: Train Loss: 0.48269320130348203, Validation Loss: 0.9707944989204407\n",
      "Fold 12 Metrics:\n",
      "Accuracy: 0.45454545454545453, Precision: 0.0, Recall: 0.0, F1-score: 0.0, AUC: 0.5\n",
      "Confusion Matrix:\n",
      "[[5 0]\n",
      " [6 0]]\n",
      "Completed fold 12\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples from subject 11 to test set\n",
      "Adding 6 truth samples from subject 11 to test set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 500)\n",
      "(132, 65, 500)\n",
      "Epoch 0: Train Loss: 0.6970247983932495, Validation Loss: 0.6950665712356567\n",
      "Epoch 1: Train Loss: 0.7598681211471557, Validation Loss: 0.6952936053276062\n",
      "Epoch 2: Train Loss: 0.7447726964950562, Validation Loss: 0.6952589154243469\n",
      "Epoch 3: Train Loss: 0.6952788352966308, Validation Loss: 0.6949342489242554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\detectlie\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.7131210207939148, Validation Loss: 0.6949276924133301\n",
      "Epoch 5: Train Loss: 0.7096806406974793, Validation Loss: 0.6946588754653931\n",
      "Epoch 6: Train Loss: 0.6652545630931854, Validation Loss: 0.6942596435546875\n",
      "Epoch 7: Train Loss: 0.6787576496601104, Validation Loss: 0.6946641206741333\n",
      "Epoch 8: Train Loss: 0.7410647749900818, Validation Loss: 0.6945067048072815\n",
      "Epoch 9: Train Loss: 0.6845633864402771, Validation Loss: 0.6945503354072571\n",
      "Epoch 10: Train Loss: 0.7267278552055358, Validation Loss: 0.694369912147522\n",
      "Epoch 11: Train Loss: 0.7494747161865234, Validation Loss: 0.6948657035827637\n",
      "Epoch 12: Train Loss: 0.6923669934272766, Validation Loss: 0.6946220993995667\n",
      "Epoch 13: Train Loss: 0.7346237778663636, Validation Loss: 0.6944574117660522\n",
      "Epoch 14: Train Loss: 0.7173344492912292, Validation Loss: 0.6943864822387695\n",
      "Epoch 15: Train Loss: 0.733373761177063, Validation Loss: 0.6945481300354004\n",
      "Epoch 16: Train Loss: 0.7990216255187989, Validation Loss: 0.6944004893302917\n",
      "Epoch 17: Train Loss: 0.7457878947257995, Validation Loss: 0.6948123574256897\n",
      "Epoch 18: Train Loss: 0.6937025308609008, Validation Loss: 0.6950778961181641\n",
      "Epoch 19: Train Loss: 0.7258593320846558, Validation Loss: 0.6948288083076477\n",
      "Epoch 20: Train Loss: 0.7189799547195435, Validation Loss: 0.6955372095108032\n",
      "Epoch 21: Train Loss: 0.6976280212402344, Validation Loss: 0.6950855255126953\n",
      "Epoch 22: Train Loss: 0.7277171373367309, Validation Loss: 0.6951193809509277\n",
      "Epoch 23: Train Loss: 0.7350358486175537, Validation Loss: 0.6951352953910828\n",
      "Epoch 24: Train Loss: 0.7397226810455322, Validation Loss: 0.6952798962593079\n",
      "Epoch 25: Train Loss: 0.6823636054992676, Validation Loss: 0.6951596736907959\n",
      "Epoch 26: Train Loss: 0.6863966226577759, Validation Loss: 0.6958379745483398\n",
      "Epoch 27: Train Loss: 0.7323895335197449, Validation Loss: 0.695742130279541\n",
      "Epoch 28: Train Loss: 0.7011921405792236, Validation Loss: 0.6959827542304993\n",
      "Epoch 29: Train Loss: 0.759730851650238, Validation Loss: 0.6961382031440735\n",
      "Epoch 30: Train Loss: 0.7329981207847596, Validation Loss: 0.6966935992240906\n",
      "Epoch 31: Train Loss: 0.6558213949203491, Validation Loss: 0.6974237561225891\n",
      "Epoch 32: Train Loss: 0.7473979830741883, Validation Loss: 0.6980981826782227\n",
      "Epoch 33: Train Loss: 0.7266223549842834, Validation Loss: 0.6984962224960327\n",
      "Epoch 34: Train Loss: 0.713475501537323, Validation Loss: 0.6990331411361694\n",
      "Epoch 35: Train Loss: 0.7186332583427429, Validation Loss: 0.6989716291427612\n",
      "Epoch 36: Train Loss: 0.7066908121109009, Validation Loss: 0.6994869709014893\n",
      "Epoch 37: Train Loss: 0.692184317111969, Validation Loss: 0.6990215182304382\n",
      "Epoch 38: Train Loss: 0.7204624295234681, Validation Loss: 0.6993250250816345\n",
      "Epoch 39: Train Loss: 0.7694589257240295, Validation Loss: 0.6991523504257202\n",
      "Epoch 40: Train Loss: 0.6942366838455201, Validation Loss: 0.6994635462760925\n",
      "Epoch 41: Train Loss: 0.7206264734268188, Validation Loss: 0.6994799375534058\n",
      "Epoch 42: Train Loss: 0.7083348870277405, Validation Loss: 0.6998257040977478\n",
      "Epoch 43: Train Loss: 0.7361100077629089, Validation Loss: 0.6996349692344666\n",
      "Epoch 44: Train Loss: 0.6986334085464477, Validation Loss: 0.6994155645370483\n",
      "Epoch 45: Train Loss: 0.6438253402709961, Validation Loss: 0.6989867091178894\n",
      "Epoch 46: Train Loss: 0.7511262416839599, Validation Loss: 0.6988858580589294\n",
      "Epoch 47: Train Loss: 0.7031198143959045, Validation Loss: 0.6992179155349731\n",
      "Epoch 48: Train Loss: 0.7303095817565918, Validation Loss: 0.6990261077880859\n",
      "Epoch 49: Train Loss: 0.7627273917198181, Validation Loss: 0.6988980174064636\n",
      "Epoch 50: Train Loss: 0.7296425938606262, Validation Loss: 0.699637770652771\n",
      "Epoch 51: Train Loss: 0.7274111866950989, Validation Loss: 0.699887216091156\n",
      "Epoch 52: Train Loss: 0.7011156439781189, Validation Loss: 0.7002266645431519\n",
      "Epoch 53: Train Loss: 0.7000097632408142, Validation Loss: 0.7005278468132019\n",
      "Epoch 54: Train Loss: 0.7389376997947693, Validation Loss: 0.7006435990333557\n",
      "Epoch 55: Train Loss: 0.6497081160545349, Validation Loss: 0.701038122177124\n",
      "Epoch 56: Train Loss: 0.7037257552146912, Validation Loss: 0.7011739611625671\n",
      "Epoch 57: Train Loss: 0.7040327072143555, Validation Loss: 0.7017800211906433\n",
      "Epoch 58: Train Loss: 0.6890655398368836, Validation Loss: 0.7018718719482422\n",
      "Epoch 59: Train Loss: 0.6699274778366089, Validation Loss: 0.7016165852546692\n",
      "Epoch 60: Train Loss: 0.6848915100097657, Validation Loss: 0.7014300227165222\n",
      "Epoch 61: Train Loss: 0.742069411277771, Validation Loss: 0.7008475065231323\n",
      "Epoch 62: Train Loss: 0.6804011344909668, Validation Loss: 0.7007964253425598\n",
      "Epoch 63: Train Loss: 0.7050586700439453, Validation Loss: 0.7010825872421265\n",
      "Epoch 64: Train Loss: 0.7001037240028382, Validation Loss: 0.7011103630065918\n",
      "Epoch 65: Train Loss: 0.7190826177597046, Validation Loss: 0.7021892070770264\n",
      "Epoch 66: Train Loss: 0.6961840271949769, Validation Loss: 0.7023488879203796\n",
      "Epoch 67: Train Loss: 0.6847827672958374, Validation Loss: 0.7020949125289917\n",
      "Epoch 68: Train Loss: 0.7168702721595764, Validation Loss: 0.7024726867675781\n",
      "Epoch 69: Train Loss: 0.730111300945282, Validation Loss: 0.7027979493141174\n",
      "Epoch 70: Train Loss: 0.6777576923370361, Validation Loss: 0.7028771638870239\n",
      "Epoch 71: Train Loss: 0.7229578614234924, Validation Loss: 0.704216480255127\n",
      "Epoch 72: Train Loss: 0.7191024422645569, Validation Loss: 0.7046274542808533\n",
      "Epoch 73: Train Loss: 0.7195173859596252, Validation Loss: 0.7042285799980164\n",
      "Epoch 74: Train Loss: 0.724609375, Validation Loss: 0.7040550708770752\n",
      "Epoch 75: Train Loss: 0.6906421542167663, Validation Loss: 0.7049307823181152\n",
      "Epoch 76: Train Loss: 0.7117340326309204, Validation Loss: 0.7053434252738953\n",
      "Epoch 77: Train Loss: 0.6720651507377624, Validation Loss: 0.70536869764328\n",
      "Epoch 78: Train Loss: 0.664618194103241, Validation Loss: 0.7050506472587585\n",
      "Epoch 79: Train Loss: 0.7155398726463318, Validation Loss: 0.7050273418426514\n",
      "Epoch 80: Train Loss: 0.6750533938407898, Validation Loss: 0.7047738432884216\n",
      "Epoch 81: Train Loss: 0.7534405350685119, Validation Loss: 0.7044224143028259\n",
      "Epoch 82: Train Loss: 0.6786092519760132, Validation Loss: 0.7038019299507141\n",
      "Epoch 83: Train Loss: 0.7188194036483765, Validation Loss: 0.7033137679100037\n",
      "Epoch 84: Train Loss: 0.7012155652046204, Validation Loss: 0.7033671736717224\n",
      "Epoch 85: Train Loss: 0.6984254837036132, Validation Loss: 0.70361328125\n",
      "Epoch 86: Train Loss: 0.7061513423919678, Validation Loss: 0.7037163972854614\n",
      "Epoch 87: Train Loss: 0.6972432017326355, Validation Loss: 0.7042417526245117\n",
      "Epoch 88: Train Loss: 0.6974995493888855, Validation Loss: 0.7044048309326172\n",
      "Epoch 89: Train Loss: 0.6947845578193664, Validation Loss: 0.7042168974876404\n",
      "Epoch 90: Train Loss: 0.695033586025238, Validation Loss: 0.7043781280517578\n",
      "Epoch 91: Train Loss: 0.7055512428283691, Validation Loss: 0.7047145366668701\n",
      "Epoch 92: Train Loss: 0.7507108569145202, Validation Loss: 0.704502284526825\n",
      "Epoch 93: Train Loss: 0.7150031447410583, Validation Loss: 0.7047608494758606\n",
      "Epoch 94: Train Loss: 0.7317359566688537, Validation Loss: 0.705285370349884\n",
      "Epoch 95: Train Loss: 0.7226918458938598, Validation Loss: 0.7052580714225769\n",
      "Epoch 96: Train Loss: 0.7107500195503235, Validation Loss: 0.7045043110847473\n",
      "Epoch 97: Train Loss: 0.7240847826004029, Validation Loss: 0.7044079303741455\n",
      "Epoch 98: Train Loss: 0.6936322927474976, Validation Loss: 0.705007016658783\n",
      "Epoch 99: Train Loss: 0.713814640045166, Validation Loss: 0.705154299736023\n",
      "Epoch 100: Train Loss: 0.6758695244789124, Validation Loss: 0.7048040628433228\n",
      "Epoch 101: Train Loss: 0.7129815697669983, Validation Loss: 0.7047650218009949\n",
      "Epoch 102: Train Loss: 0.7469743013381958, Validation Loss: 0.7041052579879761\n",
      "Epoch 103: Train Loss: 0.7034080266952515, Validation Loss: 0.7039036750793457\n",
      "Epoch 104: Train Loss: 0.7006584048271179, Validation Loss: 0.7044622302055359\n",
      "Epoch 105: Train Loss: 0.7454012989997864, Validation Loss: 0.704110324382782\n",
      "Epoch 106: Train Loss: 0.6361862540245056, Validation Loss: 0.7038216590881348\n",
      "Epoch 107: Train Loss: 0.6745012402534485, Validation Loss: 0.7038837671279907\n",
      "Epoch 108: Train Loss: 0.6556983947753906, Validation Loss: 0.7039756774902344\n",
      "Epoch 109: Train Loss: 0.7299892663955688, Validation Loss: 0.7040521502494812\n",
      "Epoch 110: Train Loss: 0.7418776631355286, Validation Loss: 0.7039976119995117\n",
      "Epoch 111: Train Loss: 0.6581438779830933, Validation Loss: 0.7033340334892273\n",
      "Epoch 112: Train Loss: 0.7114430427551269, Validation Loss: 0.7029269933700562\n",
      "Epoch 113: Train Loss: 0.6475230693817139, Validation Loss: 0.7029921412467957\n",
      "Epoch 114: Train Loss: 0.6746305465698242, Validation Loss: 0.703086793422699\n",
      "Epoch 115: Train Loss: 0.6977444648742676, Validation Loss: 0.7032239437103271\n",
      "Epoch 116: Train Loss: 0.6709294319152832, Validation Loss: 0.703445315361023\n",
      "Epoch 117: Train Loss: 0.6432103037834167, Validation Loss: 0.7036340832710266\n",
      "Epoch 118: Train Loss: 0.6500174045562744, Validation Loss: 0.7034826278686523\n",
      "Epoch 119: Train Loss: 0.7356260061264038, Validation Loss: 0.7035046219825745\n",
      "Epoch 120: Train Loss: 0.724277651309967, Validation Loss: 0.7030382752418518\n",
      "Epoch 121: Train Loss: 0.6551268219947814, Validation Loss: 0.7032245397567749\n",
      "Epoch 122: Train Loss: 0.711934494972229, Validation Loss: 0.7024444341659546\n",
      "Epoch 123: Train Loss: 0.6470333933830261, Validation Loss: 0.7023996114730835\n",
      "Epoch 124: Train Loss: 0.6917777776718139, Validation Loss: 0.7021008729934692\n",
      "Epoch 125: Train Loss: 0.7304497122764587, Validation Loss: 0.7025443315505981\n",
      "Epoch 126: Train Loss: 0.6616322636604309, Validation Loss: 0.7025938034057617\n",
      "Epoch 127: Train Loss: 0.7143349528312684, Validation Loss: 0.702049732208252\n",
      "Epoch 128: Train Loss: 0.7392523169517518, Validation Loss: 0.7021771669387817\n",
      "Epoch 129: Train Loss: 0.6879646420478821, Validation Loss: 0.70235276222229\n",
      "Epoch 130: Train Loss: 0.6511502504348755, Validation Loss: 0.7021910548210144\n",
      "Epoch 131: Train Loss: 0.6832366347312927, Validation Loss: 0.7018402218818665\n",
      "Epoch 132: Train Loss: 0.7213839530944824, Validation Loss: 0.7015376091003418\n",
      "Epoch 133: Train Loss: 0.6920092463493347, Validation Loss: 0.7019505500793457\n",
      "Epoch 134: Train Loss: 0.71440669298172, Validation Loss: 0.7027366757392883\n",
      "Epoch 135: Train Loss: 0.7053067326545716, Validation Loss: 0.7025668025016785\n",
      "Epoch 136: Train Loss: 0.6993789672851562, Validation Loss: 0.7026934027671814\n",
      "Epoch 137: Train Loss: 0.6774526834487915, Validation Loss: 0.7029154300689697\n",
      "Epoch 138: Train Loss: 0.6878459215164184, Validation Loss: 0.7024965286254883\n",
      "Epoch 139: Train Loss: 0.6689718127250671, Validation Loss: 0.7027259469032288\n",
      "Epoch 140: Train Loss: 0.734724473953247, Validation Loss: 0.7022445201873779\n",
      "Epoch 141: Train Loss: 0.689061963558197, Validation Loss: 0.7024384140968323\n",
      "Epoch 142: Train Loss: 0.7016763329505921, Validation Loss: 0.7026745676994324\n",
      "Epoch 143: Train Loss: 0.7133785367012024, Validation Loss: 0.7034173011779785\n",
      "Epoch 144: Train Loss: 0.7063355445861816, Validation Loss: 0.7029349207878113\n",
      "Epoch 145: Train Loss: 0.6864025831222534, Validation Loss: 0.7032981514930725\n",
      "Epoch 146: Train Loss: 0.7041065931320191, Validation Loss: 0.7031347155570984\n",
      "Epoch 147: Train Loss: 0.6497648596763611, Validation Loss: 0.7027451992034912\n",
      "Epoch 148: Train Loss: 0.7256618618965149, Validation Loss: 0.7028126120567322\n",
      "Epoch 149: Train Loss: 0.7320724368095398, Validation Loss: 0.7029942870140076\n",
      "Epoch 150: Train Loss: 0.719883382320404, Validation Loss: 0.7026376724243164\n",
      "Epoch 151: Train Loss: 0.6518120527267456, Validation Loss: 0.703150749206543\n",
      "Epoch 152: Train Loss: 0.678620982170105, Validation Loss: 0.7025223970413208\n",
      "Epoch 153: Train Loss: 0.6424720525741577, Validation Loss: 0.7023976445198059\n",
      "Epoch 154: Train Loss: 0.685023283958435, Validation Loss: 0.7028934359550476\n",
      "Epoch 155: Train Loss: 0.683879268169403, Validation Loss: 0.7024650573730469\n",
      "Epoch 156: Train Loss: 0.6559874653816223, Validation Loss: 0.7028988599777222\n",
      "Epoch 157: Train Loss: 0.6925114154815674, Validation Loss: 0.7025763988494873\n",
      "Epoch 158: Train Loss: 0.768696665763855, Validation Loss: 0.7022883296012878\n",
      "Epoch 159: Train Loss: 0.6928264856338501, Validation Loss: 0.7028219103813171\n",
      "Epoch 160: Train Loss: 0.712666130065918, Validation Loss: 0.7027071714401245\n",
      "Epoch 161: Train Loss: 0.659969687461853, Validation Loss: 0.7027589082717896\n",
      "Epoch 162: Train Loss: 0.6845050096511841, Validation Loss: 0.702584445476532\n",
      "Epoch 163: Train Loss: 0.7137391090393066, Validation Loss: 0.7027482390403748\n",
      "Epoch 164: Train Loss: 0.6654440522193908, Validation Loss: 0.7026949524879456\n",
      "Epoch 165: Train Loss: 0.7123160362243652, Validation Loss: 0.7030550837516785\n",
      "Epoch 166: Train Loss: 0.6948071360588074, Validation Loss: 0.7032701373100281\n",
      "Epoch 167: Train Loss: 0.6830225229263306, Validation Loss: 0.7030376195907593\n",
      "Epoch 168: Train Loss: 0.6483265280723571, Validation Loss: 0.7027040719985962\n",
      "Epoch 169: Train Loss: 0.7207228302955627, Validation Loss: 0.702779233455658\n",
      "Epoch 170: Train Loss: 0.7064127087593078, Validation Loss: 0.7032999396324158\n",
      "Epoch 171: Train Loss: 0.6625901341438294, Validation Loss: 0.7040742635726929\n",
      "Epoch 172: Train Loss: 0.6853342175483703, Validation Loss: 0.7045808434486389\n",
      "Epoch 173: Train Loss: 0.681985342502594, Validation Loss: 0.7038562297821045\n",
      "Epoch 174: Train Loss: 0.6784454584121704, Validation Loss: 0.7038984298706055\n",
      "Epoch 175: Train Loss: 0.7293851852416993, Validation Loss: 0.7038495540618896\n",
      "Epoch 176: Train Loss: 0.6800723075866699, Validation Loss: 0.7033613920211792\n",
      "Epoch 177: Train Loss: 0.6842371582984924, Validation Loss: 0.7033316493034363\n",
      "Epoch 178: Train Loss: 0.698639166355133, Validation Loss: 0.7040531635284424\n",
      "Epoch 179: Train Loss: 0.6352996587753296, Validation Loss: 0.7040125727653503\n",
      "Epoch 180: Train Loss: 0.6456601858139038, Validation Loss: 0.7037740349769592\n",
      "Epoch 181: Train Loss: 0.6592136025428772, Validation Loss: 0.7039049863815308\n",
      "Epoch 182: Train Loss: 0.6787466645240784, Validation Loss: 0.7037785053253174\n",
      "Epoch 183: Train Loss: 0.6870444416999817, Validation Loss: 0.7032051086425781\n",
      "Epoch 184: Train Loss: 0.7155432820320129, Validation Loss: 0.7031848430633545\n",
      "Epoch 185: Train Loss: 0.6637129783630371, Validation Loss: 0.7025949954986572\n",
      "Epoch 186: Train Loss: 0.6788551807403564, Validation Loss: 0.7028406858444214\n",
      "Epoch 187: Train Loss: 0.6684174180030823, Validation Loss: 0.702704131603241\n",
      "Epoch 188: Train Loss: 0.7011647820472717, Validation Loss: 0.702917218208313\n",
      "Epoch 189: Train Loss: 0.6321059584617614, Validation Loss: 0.7026929259300232\n",
      "Epoch 190: Train Loss: 0.6715002417564392, Validation Loss: 0.7029278874397278\n",
      "Epoch 191: Train Loss: 0.6520163416862488, Validation Loss: 0.7029287219047546\n",
      "Epoch 192: Train Loss: 0.6902402877807617, Validation Loss: 0.7037383317947388\n",
      "Epoch 193: Train Loss: 0.6835590481758118, Validation Loss: 0.703934371471405\n",
      "Epoch 194: Train Loss: 0.6491825819015503, Validation Loss: 0.7041548490524292\n",
      "Epoch 195: Train Loss: 0.6567642569541932, Validation Loss: 0.7035905122756958\n",
      "Epoch 196: Train Loss: 0.7045540690422059, Validation Loss: 0.7040466070175171\n",
      "Epoch 197: Train Loss: 0.68548663854599, Validation Loss: 0.7042721509933472\n",
      "Epoch 198: Train Loss: 0.727219569683075, Validation Loss: 0.7039923071861267\n",
      "Epoch 199: Train Loss: 0.6670014142990113, Validation Loss: 0.7038644552230835\n",
      "Epoch 200: Train Loss: 0.7132110476493836, Validation Loss: 0.7042853236198425\n",
      "Epoch 201: Train Loss: 0.6839394450187684, Validation Loss: 0.7046663165092468\n",
      "Epoch 202: Train Loss: 0.7119877696037292, Validation Loss: 0.7048618793487549\n",
      "Epoch 203: Train Loss: 0.6570377349853516, Validation Loss: 0.7053696513175964\n",
      "Epoch 204: Train Loss: 0.668793261051178, Validation Loss: 0.705379843711853\n",
      "Epoch 205: Train Loss: 0.6561201333999633, Validation Loss: 0.7057598829269409\n",
      "Epoch 206: Train Loss: 0.6724973320960999, Validation Loss: 0.7058056592941284\n",
      "Epoch 207: Train Loss: 0.6534676074981689, Validation Loss: 0.7060068845748901\n",
      "Epoch 208: Train Loss: 0.6926817417144775, Validation Loss: 0.7059316039085388\n",
      "Epoch 209: Train Loss: 0.7003911018371582, Validation Loss: 0.7058426737785339\n",
      "Epoch 210: Train Loss: 0.7040313243865967, Validation Loss: 0.7053855657577515\n",
      "Epoch 211: Train Loss: 0.725540554523468, Validation Loss: 0.7056374549865723\n",
      "Epoch 212: Train Loss: 0.7063670992851258, Validation Loss: 0.7052890062332153\n",
      "Epoch 213: Train Loss: 0.6377607345581054, Validation Loss: 0.7049366235733032\n",
      "Epoch 214: Train Loss: 0.629223644733429, Validation Loss: 0.7046434879302979\n",
      "Epoch 215: Train Loss: 0.7000532388687134, Validation Loss: 0.7048598527908325\n",
      "Epoch 216: Train Loss: 0.6722661018371582, Validation Loss: 0.7050446271896362\n",
      "Epoch 217: Train Loss: 0.6710826396942139, Validation Loss: 0.7052656412124634\n",
      "Epoch 218: Train Loss: 0.698969566822052, Validation Loss: 0.7047995328903198\n",
      "Epoch 219: Train Loss: 0.7156971096992493, Validation Loss: 0.7048241496086121\n",
      "Epoch 220: Train Loss: 0.6997556090354919, Validation Loss: 0.7050355076789856\n",
      "Epoch 221: Train Loss: 0.6667754650115967, Validation Loss: 0.70567387342453\n",
      "Epoch 222: Train Loss: 0.68994699716568, Validation Loss: 0.7059175968170166\n",
      "Epoch 223: Train Loss: 0.7233148336410522, Validation Loss: 0.7059656381607056\n",
      "Epoch 224: Train Loss: 0.6504960179328918, Validation Loss: 0.7058236598968506\n",
      "Epoch 225: Train Loss: 0.7084251046180725, Validation Loss: 0.7059891223907471\n",
      "Epoch 226: Train Loss: 0.6945217370986938, Validation Loss: 0.7064378261566162\n",
      "Epoch 227: Train Loss: 0.7148745656013489, Validation Loss: 0.7062665820121765\n",
      "Epoch 228: Train Loss: 0.6400159955024719, Validation Loss: 0.7065930962562561\n",
      "Epoch 229: Train Loss: 0.6795120716094971, Validation Loss: 0.7064951658248901\n",
      "Epoch 230: Train Loss: 0.683612072467804, Validation Loss: 0.7065361738204956\n",
      "Epoch 231: Train Loss: 0.7083574414253235, Validation Loss: 0.7066384553909302\n",
      "Epoch 232: Train Loss: 0.6519989728927612, Validation Loss: 0.7068558931350708\n",
      "Epoch 233: Train Loss: 0.6763854503631592, Validation Loss: 0.7073574662208557\n",
      "Epoch 234: Train Loss: 0.6143722295761108, Validation Loss: 0.7072638273239136\n",
      "Epoch 235: Train Loss: 0.6134277403354644, Validation Loss: 0.7078874111175537\n",
      "Epoch 236: Train Loss: 0.6436022520065308, Validation Loss: 0.7078472971916199\n",
      "Epoch 237: Train Loss: 0.6345941662788391, Validation Loss: 0.7080880403518677\n",
      "Epoch 238: Train Loss: 0.6430444121360779, Validation Loss: 0.7082436680793762\n",
      "Epoch 239: Train Loss: 0.6642029285430908, Validation Loss: 0.7085090279579163\n",
      "Epoch 240: Train Loss: 0.6456780552864074, Validation Loss: 0.7077948451042175\n",
      "Epoch 241: Train Loss: 0.6719380974769592, Validation Loss: 0.7074470520019531\n",
      "Epoch 242: Train Loss: 0.6741246104240417, Validation Loss: 0.7069826126098633\n",
      "Epoch 243: Train Loss: 0.7032765030860901, Validation Loss: 0.7073524594306946\n",
      "Epoch 244: Train Loss: 0.6271907687187195, Validation Loss: 0.7073085904121399\n",
      "Epoch 245: Train Loss: 0.700409996509552, Validation Loss: 0.7066220641136169\n",
      "Epoch 246: Train Loss: 0.6516943335533142, Validation Loss: 0.7065942287445068\n",
      "Epoch 247: Train Loss: 0.6916533708572388, Validation Loss: 0.706426203250885\n",
      "Epoch 248: Train Loss: 0.7057571887969971, Validation Loss: 0.7066053152084351\n",
      "Epoch 249: Train Loss: 0.7053439497947693, Validation Loss: 0.7076267004013062\n",
      "Epoch 250: Train Loss: 0.663109815120697, Validation Loss: 0.7078710198402405\n",
      "Epoch 251: Train Loss: 0.693743634223938, Validation Loss: 0.7079198360443115\n",
      "Epoch 252: Train Loss: 0.670497989654541, Validation Loss: 0.7084938883781433\n",
      "Epoch 253: Train Loss: 0.6819430947303772, Validation Loss: 0.7084434032440186\n",
      "Epoch 254: Train Loss: 0.6479822874069214, Validation Loss: 0.7085136771202087\n",
      "Epoch 255: Train Loss: 0.6775651931762695, Validation Loss: 0.7081612348556519\n",
      "Epoch 256: Train Loss: 0.6481606245040894, Validation Loss: 0.7081872820854187\n",
      "Epoch 257: Train Loss: 0.7085814595222473, Validation Loss: 0.7080959677696228\n",
      "Epoch 258: Train Loss: 0.6819353461265564, Validation Loss: 0.7081199884414673\n",
      "Epoch 259: Train Loss: 0.6495697617530822, Validation Loss: 0.7078505158424377\n",
      "Epoch 260: Train Loss: 0.6457137107849121, Validation Loss: 0.7085018157958984\n",
      "Epoch 261: Train Loss: 0.7162166595458984, Validation Loss: 0.7089916467666626\n",
      "Epoch 262: Train Loss: 0.7056153535842895, Validation Loss: 0.708931565284729\n",
      "Epoch 263: Train Loss: 0.610685795545578, Validation Loss: 0.7095941305160522\n",
      "Epoch 264: Train Loss: 0.6835705876350403, Validation Loss: 0.7100286483764648\n",
      "Epoch 265: Train Loss: 0.6868939161300659, Validation Loss: 0.7102269530296326\n",
      "Epoch 266: Train Loss: 0.6503697872161865, Validation Loss: 0.7105795741081238\n",
      "Epoch 267: Train Loss: 0.6955482840538025, Validation Loss: 0.7099091410636902\n",
      "Epoch 268: Train Loss: 0.6374631643295288, Validation Loss: 0.7103338241577148\n",
      "Epoch 269: Train Loss: 0.6852119088172912, Validation Loss: 0.7101735472679138\n",
      "Epoch 270: Train Loss: 0.6794938802719116, Validation Loss: 0.7106494903564453\n",
      "Epoch 271: Train Loss: 0.656403648853302, Validation Loss: 0.7104620337486267\n",
      "Epoch 272: Train Loss: 0.6769899606704712, Validation Loss: 0.7099204063415527\n",
      "Epoch 273: Train Loss: 0.7050583720207214, Validation Loss: 0.7102464437484741\n",
      "Epoch 274: Train Loss: 0.6565886735916138, Validation Loss: 0.7103008031845093\n",
      "Epoch 275: Train Loss: 0.657320785522461, Validation Loss: 0.7110374569892883\n",
      "Epoch 276: Train Loss: 0.6557953238487244, Validation Loss: 0.7108443975448608\n",
      "Epoch 277: Train Loss: 0.634755551815033, Validation Loss: 0.7106523513793945\n",
      "Epoch 278: Train Loss: 0.6979617238044739, Validation Loss: 0.7109427452087402\n",
      "Epoch 279: Train Loss: 0.6216951251029968, Validation Loss: 0.711197555065155\n",
      "Epoch 280: Train Loss: 0.6509686470031738, Validation Loss: 0.7118275761604309\n",
      "Epoch 281: Train Loss: 0.6433974862098694, Validation Loss: 0.7107483744621277\n",
      "Epoch 282: Train Loss: 0.5955267071723938, Validation Loss: 0.7106959223747253\n",
      "Epoch 283: Train Loss: 0.6332052230834961, Validation Loss: 0.7103797197341919\n",
      "Epoch 284: Train Loss: 0.6264778017997742, Validation Loss: 0.7111822962760925\n",
      "Epoch 285: Train Loss: 0.6309433937072754, Validation Loss: 0.7106793522834778\n",
      "Epoch 286: Train Loss: 0.6040037393569946, Validation Loss: 0.7109429240226746\n",
      "Epoch 287: Train Loss: 0.6283135175704956, Validation Loss: 0.7116027474403381\n",
      "Epoch 288: Train Loss: 0.6448909163475036, Validation Loss: 0.7116430401802063\n",
      "Epoch 289: Train Loss: 0.6637073397636414, Validation Loss: 0.7115612030029297\n",
      "Epoch 290: Train Loss: 0.6714968204498291, Validation Loss: 0.7109323143959045\n",
      "Epoch 291: Train Loss: 0.6190600514411926, Validation Loss: 0.7107937335968018\n",
      "Epoch 292: Train Loss: 0.6523907899856567, Validation Loss: 0.7109904885292053\n",
      "Epoch 293: Train Loss: 0.6974656581878662, Validation Loss: 0.7110980749130249\n",
      "Epoch 294: Train Loss: 0.6553485274314881, Validation Loss: 0.7119728922843933\n",
      "Epoch 295: Train Loss: 0.6521243214607239, Validation Loss: 0.7122840881347656\n",
      "Epoch 296: Train Loss: 0.6481595635414124, Validation Loss: 0.7125045657157898\n",
      "Epoch 297: Train Loss: 0.6326624393463135, Validation Loss: 0.7124183177947998\n",
      "Epoch 298: Train Loss: 0.6565656900405884, Validation Loss: 0.711819589138031\n",
      "Epoch 299: Train Loss: 0.6797379612922668, Validation Loss: 0.7119083404541016\n",
      "Epoch 300: Train Loss: 0.6767098069190979, Validation Loss: 0.71161288022995\n",
      "Epoch 301: Train Loss: 0.66296226978302, Validation Loss: 0.7123861312866211\n",
      "Epoch 302: Train Loss: 0.6793301343917847, Validation Loss: 0.7133101224899292\n",
      "Epoch 303: Train Loss: 0.6487371563911438, Validation Loss: 0.7143362760543823\n",
      "Epoch 304: Train Loss: 0.6360057353973388, Validation Loss: 0.7143595814704895\n",
      "Epoch 305: Train Loss: 0.6483428597450256, Validation Loss: 0.7149885892868042\n",
      "Epoch 306: Train Loss: 0.6273903965950012, Validation Loss: 0.715721845626831\n",
      "Epoch 307: Train Loss: 0.6444370746612549, Validation Loss: 0.7153063416481018\n",
      "Epoch 308: Train Loss: 0.6363770484924316, Validation Loss: 0.7154145240783691\n",
      "Epoch 309: Train Loss: 0.6581443905830383, Validation Loss: 0.7156163454055786\n",
      "Epoch 310: Train Loss: 0.686300265789032, Validation Loss: 0.7151517868041992\n",
      "Epoch 311: Train Loss: 0.6495599150657654, Validation Loss: 0.714957058429718\n",
      "Epoch 312: Train Loss: 0.633348572254181, Validation Loss: 0.7152029871940613\n",
      "Epoch 313: Train Loss: 0.6529669761657715, Validation Loss: 0.7154344916343689\n",
      "Epoch 314: Train Loss: 0.6353385508060455, Validation Loss: 0.7151288986206055\n",
      "Epoch 315: Train Loss: 0.6404968857765198, Validation Loss: 0.7161092758178711\n",
      "Epoch 316: Train Loss: 0.6615641355514527, Validation Loss: 0.7157072424888611\n",
      "Epoch 317: Train Loss: 0.6411793470382691, Validation Loss: 0.7158113121986389\n",
      "Epoch 318: Train Loss: 0.6501758694648743, Validation Loss: 0.7155376672744751\n",
      "Epoch 319: Train Loss: 0.6623003959655762, Validation Loss: 0.715060293674469\n",
      "Epoch 320: Train Loss: 0.7081398963928223, Validation Loss: 0.7151647210121155\n",
      "Epoch 321: Train Loss: 0.6700226068496704, Validation Loss: 0.7156123518943787\n",
      "Epoch 322: Train Loss: 0.6200957417488098, Validation Loss: 0.7152480483055115\n",
      "Epoch 323: Train Loss: 0.6289756894111633, Validation Loss: 0.7161070108413696\n",
      "Epoch 324: Train Loss: 0.6486997365951538, Validation Loss: 0.7170055508613586\n",
      "Epoch 325: Train Loss: 0.6739356279373169, Validation Loss: 0.7170529365539551\n",
      "Epoch 326: Train Loss: 0.681191623210907, Validation Loss: 0.716401219367981\n",
      "Epoch 327: Train Loss: 0.6730703949928284, Validation Loss: 0.7171139717102051\n",
      "Epoch 328: Train Loss: 0.6221909046173095, Validation Loss: 0.7170734405517578\n",
      "Epoch 329: Train Loss: 0.6404656529426574, Validation Loss: 0.7169545292854309\n",
      "Epoch 330: Train Loss: 0.6619413018226623, Validation Loss: 0.7173758149147034\n",
      "Epoch 331: Train Loss: 0.6583415865898132, Validation Loss: 0.7178862690925598\n",
      "Epoch 332: Train Loss: 0.6458189010620117, Validation Loss: 0.7179166078567505\n",
      "Epoch 333: Train Loss: 0.6718383669853211, Validation Loss: 0.7180724740028381\n",
      "Epoch 334: Train Loss: 0.5982436656951904, Validation Loss: 0.7185773849487305\n",
      "Epoch 335: Train Loss: 0.6448448538780213, Validation Loss: 0.7185853719711304\n",
      "Epoch 336: Train Loss: 0.6322778582572937, Validation Loss: 0.7190035581588745\n",
      "Epoch 337: Train Loss: 0.657375180721283, Validation Loss: 0.718829333782196\n",
      "Epoch 338: Train Loss: 0.7110048174858093, Validation Loss: 0.7187103629112244\n",
      "Epoch 339: Train Loss: 0.6215251803398132, Validation Loss: 0.7186347246170044\n",
      "Epoch 340: Train Loss: 0.6439569115638732, Validation Loss: 0.7187705039978027\n",
      "Epoch 341: Train Loss: 0.6671123504638672, Validation Loss: 0.7189290523529053\n",
      "Epoch 342: Train Loss: 0.6152183890342713, Validation Loss: 0.719125509262085\n",
      "Epoch 343: Train Loss: 0.6445380091667176, Validation Loss: 0.7205268144607544\n",
      "Epoch 344: Train Loss: 0.6088456153869629, Validation Loss: 0.7197626233100891\n",
      "Epoch 345: Train Loss: 0.5842602372169494, Validation Loss: 0.7201155424118042\n",
      "Epoch 346: Train Loss: 0.5873807907104492, Validation Loss: 0.7195781469345093\n",
      "Epoch 347: Train Loss: 0.6924624085426331, Validation Loss: 0.720086395740509\n",
      "Epoch 348: Train Loss: 0.6207866191864013, Validation Loss: 0.720358669757843\n",
      "Epoch 349: Train Loss: 0.6094337105751038, Validation Loss: 0.7202507853507996\n",
      "Epoch 350: Train Loss: 0.6558467268943786, Validation Loss: 0.7205300331115723\n",
      "Epoch 351: Train Loss: 0.6661611557006836, Validation Loss: 0.7216327786445618\n",
      "Epoch 352: Train Loss: 0.6247270464897156, Validation Loss: 0.7223877906799316\n",
      "Epoch 353: Train Loss: 0.662785279750824, Validation Loss: 0.7233762145042419\n",
      "Epoch 354: Train Loss: 0.6491427659988404, Validation Loss: 0.7239398956298828\n",
      "Epoch 355: Train Loss: 0.6233153581619263, Validation Loss: 0.7237887382507324\n",
      "Epoch 356: Train Loss: 0.622717821598053, Validation Loss: 0.7244900465011597\n",
      "Epoch 357: Train Loss: 0.6825638532638549, Validation Loss: 0.7242364883422852\n",
      "Epoch 358: Train Loss: 0.612169885635376, Validation Loss: 0.7236259579658508\n",
      "Epoch 359: Train Loss: 0.6170938849449158, Validation Loss: 0.7236658930778503\n",
      "Epoch 360: Train Loss: 0.6445732355117798, Validation Loss: 0.7238996624946594\n",
      "Epoch 361: Train Loss: 0.6674073815345765, Validation Loss: 0.7244593501091003\n",
      "Epoch 362: Train Loss: 0.6724317908287049, Validation Loss: 0.7243019938468933\n",
      "Epoch 363: Train Loss: 0.6351131796836853, Validation Loss: 0.7241657972335815\n",
      "Epoch 364: Train Loss: 0.667425560951233, Validation Loss: 0.7244773507118225\n",
      "Epoch 365: Train Loss: 0.667050814628601, Validation Loss: 0.7249119877815247\n",
      "Epoch 366: Train Loss: 0.632233738899231, Validation Loss: 0.7253961563110352\n",
      "Epoch 367: Train Loss: 0.6404850482940674, Validation Loss: 0.7254288792610168\n",
      "Epoch 368: Train Loss: 0.645181941986084, Validation Loss: 0.7255626320838928\n",
      "Epoch 369: Train Loss: 0.6616947650909424, Validation Loss: 0.7253983616828918\n",
      "Epoch 370: Train Loss: 0.6212744951248169, Validation Loss: 0.7263653874397278\n",
      "Epoch 371: Train Loss: 0.6308154225349426, Validation Loss: 0.7261402010917664\n",
      "Epoch 372: Train Loss: 0.6302485823631286, Validation Loss: 0.7274043560028076\n",
      "Epoch 373: Train Loss: 0.6637330174446106, Validation Loss: 0.7274516820907593\n",
      "Epoch 374: Train Loss: 0.6635176062583923, Validation Loss: 0.7278493046760559\n",
      "Epoch 375: Train Loss: 0.6634802341461181, Validation Loss: 0.7276002168655396\n",
      "Epoch 376: Train Loss: 0.6726915121078492, Validation Loss: 0.7275481224060059\n",
      "Epoch 377: Train Loss: 0.5957104623317718, Validation Loss: 0.7278123497962952\n",
      "Epoch 378: Train Loss: 0.6588469862937927, Validation Loss: 0.7279849648475647\n",
      "Epoch 379: Train Loss: 0.6567975401878356, Validation Loss: 0.7279744148254395\n",
      "Epoch 380: Train Loss: 0.6372207760810852, Validation Loss: 0.7284262776374817\n",
      "Epoch 381: Train Loss: 0.6558389902114868, Validation Loss: 0.7299486994743347\n",
      "Epoch 382: Train Loss: 0.6406181454658508, Validation Loss: 0.7302026748657227\n",
      "Epoch 383: Train Loss: 0.6303076624870301, Validation Loss: 0.7305148839950562\n",
      "Epoch 384: Train Loss: 0.5778531432151794, Validation Loss: 0.7313353419303894\n",
      "Epoch 385: Train Loss: 0.6626427173614502, Validation Loss: 0.7317453026771545\n",
      "Epoch 386: Train Loss: 0.6281882882118225, Validation Loss: 0.7318278551101685\n",
      "Epoch 387: Train Loss: 0.6173677086830139, Validation Loss: 0.7321383357048035\n",
      "Epoch 388: Train Loss: 0.6258672833442688, Validation Loss: 0.7325534820556641\n",
      "Epoch 389: Train Loss: 0.6136250019073486, Validation Loss: 0.7324957847595215\n",
      "Epoch 390: Train Loss: 0.6491306304931641, Validation Loss: 0.7314541935920715\n",
      "Epoch 391: Train Loss: 0.6593466997146606, Validation Loss: 0.7316328883171082\n",
      "Epoch 392: Train Loss: 0.6937981247901917, Validation Loss: 0.732414186000824\n",
      "Epoch 393: Train Loss: 0.6484750628471374, Validation Loss: 0.7321910858154297\n",
      "Epoch 394: Train Loss: 0.6682156205177308, Validation Loss: 0.7320809960365295\n",
      "Epoch 395: Train Loss: 0.6028401970863342, Validation Loss: 0.7322806715965271\n",
      "Epoch 396: Train Loss: 0.6151988446712494, Validation Loss: 0.7328471541404724\n",
      "Epoch 397: Train Loss: 0.6101589798927307, Validation Loss: 0.7331317067146301\n",
      "Epoch 398: Train Loss: 0.6388803362846375, Validation Loss: 0.7342332601547241\n",
      "Epoch 399: Train Loss: 0.6672983050346375, Validation Loss: 0.7343724370002747\n",
      "Epoch 400: Train Loss: 0.5791094124317169, Validation Loss: 0.7337020039558411\n",
      "Epoch 401: Train Loss: 0.6366868019104004, Validation Loss: 0.7341741323471069\n",
      "Epoch 402: Train Loss: 0.6240679621696472, Validation Loss: 0.7345755696296692\n",
      "Epoch 403: Train Loss: 0.5886642634868622, Validation Loss: 0.7349783778190613\n",
      "Epoch 404: Train Loss: 0.6123816847801209, Validation Loss: 0.7348967790603638\n",
      "Epoch 405: Train Loss: 0.6214568138122558, Validation Loss: 0.7354045510292053\n",
      "Epoch 406: Train Loss: 0.6419096708297729, Validation Loss: 0.7363052368164062\n",
      "Epoch 407: Train Loss: 0.6188153028488159, Validation Loss: 0.7372683882713318\n",
      "Epoch 408: Train Loss: 0.604686164855957, Validation Loss: 0.7378672957420349\n",
      "Epoch 409: Train Loss: 0.6519038319587708, Validation Loss: 0.7374467849731445\n",
      "Epoch 410: Train Loss: 0.6555601358413696, Validation Loss: 0.7387763857841492\n",
      "Epoch 411: Train Loss: 0.6555876731872559, Validation Loss: 0.7411128282546997\n",
      "Epoch 412: Train Loss: 0.6263091802597046, Validation Loss: 0.7428712844848633\n",
      "Epoch 413: Train Loss: 0.624608325958252, Validation Loss: 0.744543731212616\n",
      "Epoch 414: Train Loss: 0.699879276752472, Validation Loss: 0.7456573247909546\n",
      "Epoch 415: Train Loss: 0.6122706174850464, Validation Loss: 0.7459302544593811\n",
      "Epoch 416: Train Loss: 0.6620645642280578, Validation Loss: 0.7456708550453186\n",
      "Epoch 417: Train Loss: 0.6062676787376404, Validation Loss: 0.7458673119544983\n",
      "Epoch 418: Train Loss: 0.6381640315055848, Validation Loss: 0.7458175420761108\n",
      "Epoch 419: Train Loss: 0.657764482498169, Validation Loss: 0.7461289763450623\n",
      "Epoch 420: Train Loss: 0.6409827589988708, Validation Loss: 0.7464984059333801\n",
      "Epoch 421: Train Loss: 0.637631356716156, Validation Loss: 0.7477166056632996\n",
      "Epoch 422: Train Loss: 0.6188976168632507, Validation Loss: 0.7485194802284241\n",
      "Epoch 423: Train Loss: 0.6784709215164184, Validation Loss: 0.7497926354408264\n",
      "Epoch 424: Train Loss: 0.6372536540031433, Validation Loss: 0.7506330609321594\n",
      "Epoch 425: Train Loss: 0.6328129053115845, Validation Loss: 0.7511301040649414\n",
      "Epoch 426: Train Loss: 0.6667153835296631, Validation Loss: 0.7518147826194763\n",
      "Epoch 427: Train Loss: 0.615272831916809, Validation Loss: 0.7520972490310669\n",
      "Epoch 428: Train Loss: 0.6262086272239685, Validation Loss: 0.7519038319587708\n",
      "Epoch 429: Train Loss: 0.6411140203475952, Validation Loss: 0.7524245381355286\n",
      "Epoch 430: Train Loss: 0.5800439357757569, Validation Loss: 0.7523742318153381\n",
      "Epoch 431: Train Loss: 0.6522144675254822, Validation Loss: 0.7531295418739319\n",
      "Epoch 432: Train Loss: 0.5720540761947632, Validation Loss: 0.7555356025695801\n",
      "Epoch 433: Train Loss: 0.6089291095733642, Validation Loss: 0.7567464113235474\n",
      "Epoch 434: Train Loss: 0.5804900765419007, Validation Loss: 0.7574825286865234\n",
      "Epoch 435: Train Loss: 0.6149721622467041, Validation Loss: 0.7574340105056763\n",
      "Epoch 436: Train Loss: 0.6786560654640198, Validation Loss: 0.7572696805000305\n",
      "Epoch 437: Train Loss: 0.581886601448059, Validation Loss: 0.7583550810813904\n",
      "Epoch 438: Train Loss: 0.616952645778656, Validation Loss: 0.7583969235420227\n",
      "Epoch 439: Train Loss: 0.601602828502655, Validation Loss: 0.7596285939216614\n",
      "Epoch 440: Train Loss: 0.590347409248352, Validation Loss: 0.7588477730751038\n",
      "Epoch 441: Train Loss: 0.6586303949356079, Validation Loss: 0.7609610557556152\n",
      "Epoch 442: Train Loss: 0.5755756735801697, Validation Loss: 0.7605305910110474\n",
      "Epoch 443: Train Loss: 0.5912272334098816, Validation Loss: 0.7626378536224365\n",
      "Epoch 444: Train Loss: 0.6283006548881531, Validation Loss: 0.764114499092102\n",
      "Epoch 445: Train Loss: 0.6382649302482605, Validation Loss: 0.7646233439445496\n",
      "Epoch 446: Train Loss: 0.5963342070579529, Validation Loss: 0.7651651501655579\n",
      "Epoch 447: Train Loss: 0.605027151107788, Validation Loss: 0.7657595872879028\n",
      "Epoch 448: Train Loss: 0.6227548837661743, Validation Loss: 0.7656457424163818\n",
      "Epoch 449: Train Loss: 0.6401586771011353, Validation Loss: 0.7648952007293701\n",
      "Epoch 450: Train Loss: 0.6225202441215515, Validation Loss: 0.7671409249305725\n",
      "Epoch 451: Train Loss: 0.5549792051315308, Validation Loss: 0.7687724232673645\n",
      "Epoch 452: Train Loss: 0.6343127608299255, Validation Loss: 0.7698418498039246\n",
      "Epoch 453: Train Loss: 0.5794151961803437, Validation Loss: 0.7719411849975586\n",
      "Epoch 454: Train Loss: 0.6123949885368347, Validation Loss: 0.7745240330696106\n",
      "Epoch 455: Train Loss: 0.6328312397003174, Validation Loss: 0.7766788005828857\n",
      "Epoch 456: Train Loss: 0.5933530688285827, Validation Loss: 0.7767476439476013\n",
      "Epoch 457: Train Loss: 0.6164130330085754, Validation Loss: 0.7780349850654602\n",
      "Epoch 458: Train Loss: 0.6072245240211487, Validation Loss: 0.7776576280593872\n",
      "Epoch 459: Train Loss: 0.5888581991195678, Validation Loss: 0.7791139483451843\n",
      "Epoch 460: Train Loss: 0.5220369517803192, Validation Loss: 0.7823171019554138\n",
      "Epoch 461: Train Loss: 0.6149754643440246, Validation Loss: 0.7851775884628296\n",
      "Epoch 462: Train Loss: 0.6099471926689148, Validation Loss: 0.7879959344863892\n",
      "Epoch 463: Train Loss: 0.555271577835083, Validation Loss: 0.7909889221191406\n",
      "Epoch 464: Train Loss: 0.5974552512168885, Validation Loss: 0.7929906845092773\n",
      "Epoch 465: Train Loss: 0.5815102517604828, Validation Loss: 0.7956839799880981\n",
      "Epoch 466: Train Loss: 0.5661821484565734, Validation Loss: 0.796623706817627\n",
      "Epoch 467: Train Loss: 0.5564513325691223, Validation Loss: 0.7979799509048462\n",
      "Epoch 468: Train Loss: 0.5569080114364624, Validation Loss: 0.7989721298217773\n",
      "Epoch 469: Train Loss: 0.6083622395992279, Validation Loss: 0.7997481822967529\n",
      "Epoch 470: Train Loss: 0.5402848482131958, Validation Loss: 0.8011320233345032\n",
      "Epoch 471: Train Loss: 0.5850160360336304, Validation Loss: 0.8049285411834717\n",
      "Epoch 472: Train Loss: 0.547823041677475, Validation Loss: 0.8030202388763428\n",
      "Epoch 473: Train Loss: 0.6027904391288758, Validation Loss: 0.8064336776733398\n",
      "Epoch 474: Train Loss: 0.5920753240585327, Validation Loss: 0.8086792230606079\n",
      "Epoch 475: Train Loss: 0.5126062631607056, Validation Loss: 0.8090469241142273\n",
      "Epoch 476: Train Loss: 0.5818904757499694, Validation Loss: 0.812876284122467\n",
      "Epoch 477: Train Loss: 0.5889064550399781, Validation Loss: 0.8137081265449524\n",
      "Epoch 478: Train Loss: 0.5329061210155487, Validation Loss: 0.8146608471870422\n",
      "Epoch 479: Train Loss: 0.6331761002540588, Validation Loss: 0.8134936690330505\n",
      "Epoch 480: Train Loss: 0.5856619000434875, Validation Loss: 0.8170871734619141\n",
      "Epoch 481: Train Loss: 0.5895170927047729, Validation Loss: 0.8198624849319458\n",
      "Epoch 482: Train Loss: 0.566410219669342, Validation Loss: 0.82197105884552\n",
      "Epoch 483: Train Loss: 0.5349789261817932, Validation Loss: 0.8242813348770142\n",
      "Epoch 484: Train Loss: 0.5447033166885376, Validation Loss: 0.8278640508651733\n",
      "Epoch 485: Train Loss: 0.6247968673706055, Validation Loss: 0.8320550918579102\n",
      "Epoch 486: Train Loss: 0.5605152308940887, Validation Loss: 0.8346103429794312\n",
      "Epoch 487: Train Loss: 0.5459480047225952, Validation Loss: 0.8324172496795654\n",
      "Epoch 488: Train Loss: 0.5862685680389405, Validation Loss: 0.832233190536499\n",
      "Epoch 489: Train Loss: 0.5747386813163757, Validation Loss: 0.8333232402801514\n",
      "Epoch 490: Train Loss: 0.5964765310287475, Validation Loss: 0.8374231457710266\n",
      "Epoch 491: Train Loss: 0.5669610023498535, Validation Loss: 0.8349916338920593\n",
      "Epoch 492: Train Loss: 0.5897780179977417, Validation Loss: 0.8336835503578186\n",
      "Epoch 493: Train Loss: 0.6249306559562683, Validation Loss: 0.8342233300209045\n",
      "Epoch 494: Train Loss: 0.5540413141250611, Validation Loss: 0.8344453573226929\n",
      "Epoch 495: Train Loss: 0.5600430727005005, Validation Loss: 0.8327533602714539\n",
      "Epoch 496: Train Loss: 0.5109111607074738, Validation Loss: 0.8345413208007812\n",
      "Epoch 497: Train Loss: 0.5758795619010926, Validation Loss: 0.8353276252746582\n",
      "Epoch 498: Train Loss: 0.6043238937854767, Validation Loss: 0.8339242339134216\n",
      "Epoch 499: Train Loss: 0.5633544325828552, Validation Loss: 0.8323457837104797\n",
      "Fold 13 Metrics:\n",
      "Accuracy: 0.45454545454545453, Precision: 0.5, Recall: 0.8333333333333334, F1-score: 0.625, AUC: 0.4166666666666667\n",
      "Confusion Matrix:\n",
      "[[0 5]\n",
      " [1 5]]\n",
      "Completed fold 13\n",
      "--------------------------------------------------\n",
      "Overall Metrics:\n",
      "Accuracy: 0.3986013986013986, Precision: 0.44285714285714284, Recall: 0.3974358974358974, F1-score: 0.4189189189189189, AUC: 0.39871794871794874\n",
      "Overall Confusion Matrix:\n",
      "[[26 39]\n",
      " [47 31]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "model_save_dir = r'C:\\Users\\User\\Documents\\Lie detect data\\Model'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion Matrix'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='viridis', xticklabels=['lie', 'truth'], yticklabels=['lie', 'truth'])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.show()\n",
    "\n",
    "# Define a function to load and pad data\n",
    "def load_data(data_dir, max_length):\n",
    "    subject_data = {'lie': {}, 'truth': {}}\n",
    "    \n",
    "    file_list = os.listdir(data_dir)\n",
    "    \n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Determine if the file is 'truth' or 'lie'\n",
    "        label_type = 'truth' if 'truth' in file else 'lie'\n",
    "        subj_id = int(file.split('_')[-1].split('.')[0])\n",
    "        \n",
    "        # Grouping logic\n",
    "        if label_type == 'lie':\n",
    "            # Mapping each 5 lie samples to one subject\n",
    "            subject_key = (subj_id - 1) // 5 + 1\n",
    "        else:  # 'truth'\n",
    "            # Mapping each 6 truth samples to one subject\n",
    "            subject_key = (subj_id - 1) // 6 + 1\n",
    "            \n",
    "        # Initialize the subject's list if it doesn't exist\n",
    "        if subject_key not in subject_data[label_type]:\n",
    "            subject_data[label_type][subject_key] = []\n",
    "            \n",
    "        # Extract only the portion of the data from time length 3000 to 3750\n",
    "        data_subset = data[:, 2625:3125]\n",
    "\n",
    "        # Pad or truncate the data to match max_length\n",
    "        if data_subset.shape[1] > max_length:\n",
    "            processed_data = data_subset[:, :max_length]  # Truncate if it exceeds max_length\n",
    "        else:\n",
    "            processed_data = np.zeros((data_subset.shape[0], max_length))\n",
    "            processed_data[:, :data_subset.shape[1]] = data_subset  # Pad if it is shorter than max_length\n",
    "        \n",
    "        # Add the processed data to the appropriate list\n",
    "        subject_data[label_type][subject_key].append(processed_data)\n",
    "    \n",
    "    return subject_data\n",
    "\n",
    "# Load dataset and pad/truncate the data\n",
    "data_dir = \"C:\\\\Users\\\\User\\\\Documents\\\\Lie detect data\\\\7M_EEGData\"\n",
    "max_length = 500 # Define maximum length for padding\n",
    "subject_data = load_data(data_dir, max_length)\n",
    "\n",
    "# Count the total number of samples\n",
    "num_lie_samples = sum(len(subject_data['lie'][subject_key]) for subject_key in subject_data['lie'])\n",
    "num_truth_samples = sum(len(subject_data['truth'][subject_key]) for subject_key in subject_data['truth'])\n",
    "\n",
    "print(f\"Number of 'lie' samples: {num_lie_samples}\")\n",
    "print(f\"Number of 'truth' samples: {num_truth_samples}\")\n",
    "print(f\"Total number of samples: {num_lie_samples + num_truth_samples}\")\n",
    "\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure the data is reshaped to [1, Chans, Samples] for Conv2d input\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32).unsqueeze(0), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "nb_classes = 2\n",
    "Chans = 65\n",
    "Samples = 500\n",
    "dropoutRate = 0.5 \n",
    "kernLength = 125\n",
    "F1 = 8\n",
    "D = 4\n",
    "F2 = 32\n",
    "\n",
    "# New EEGNet Model Definition in PyTorch\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, nb_classes, Chans=65, Samples=3750,\n",
    "                 dropoutRate=0.6, kernLength=125, F1=8, \n",
    "                 D=2, F2=16, norm_rate=0.25, dropoutType='Dropout'):\n",
    "        super(EEGNet, self).__init__()\n",
    "\n",
    "        if dropoutType == 'SpatialDropout2D':\n",
    "            dropoutLayer = nn.Dropout2d\n",
    "        elif dropoutType == 'Dropout':\n",
    "            dropoutLayer = nn.Dropout\n",
    "        else:\n",
    "            raise ValueError('dropoutType must be one of \"SpatialDropout2D\" or \"Dropout\".')\n",
    "\n",
    "        # Block 1\n",
    "        self.conv1 = nn.Conv2d(1, F1, (1, kernLength), padding='same', bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "        \n",
    "        self.depthwiseConv = nn.Conv2d(F1, F1 * D, (Chans, 1), \n",
    "                                       groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(F1 * D)\n",
    "        self.elu = nn.ELU()\n",
    "        self.avgpool1 = nn.AvgPool2d((1, 4))\n",
    "        self.dropout1 = dropoutLayer(dropoutRate)\n",
    "\n",
    "        # Block 2\n",
    "        self.separableConv = nn.Sequential(\n",
    "            nn.Conv2d(F1 * D, F1 * D, (1, 16), padding='same', groups=F1 * D, bias=False),\n",
    "            nn.Conv2d(F1 * D, F2, (1, 1), bias=False)\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(F2)\n",
    "        self.avgpool2 = nn.AvgPool2d((1, 8))\n",
    "        self.dropout2 = dropoutLayer(dropoutRate)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(F2 * (Samples // (4 * 8)), nb_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input block\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.depthwiseConv(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.avgpool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Block 2\n",
    "        x = self.separableConv(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.avgpool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Flatten before the fully connected layer\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        # Fully connected output\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_and_evaluate(train_loader, val_loader, y_train):\n",
    "    # Initialize the updated EEGNet model\n",
    "    model = EEGNet(nb_classes=nb_classes, Chans=Chans, Samples=Samples,\n",
    "                   dropoutRate=dropoutRate, kernLength=kernLength, F1=F1, D=D, F2=F2, norm_rate=0.25, dropoutType='Dropout').to(device)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "    num_epochs = 500\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 500\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "            fold_model_path = os.path.join(model_save_dir, f'revise_model_fold_{fold_idx}.pth')\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': best_val_loss,\n",
    "            }, fold_model_path)\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize the cross-validation\n",
    "subject_ids = list(range(1, 14))  # Since there are 13 subjects for lie and truth\n",
    "random.shuffle(subject_ids)  # Shuffle to ensure random selection\n",
    "\n",
    "# Initialize arrays to store all labels, predictions, and metrics\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "fold_accuracies = []\n",
    "fold_precisions = []\n",
    "fold_recalls = []\n",
    "fold_f1_scores = []\n",
    "fold_aucs = []\n",
    "fold_conf_matrices = []\n",
    "\n",
    "for fold_idx in range(13):\n",
    "    # Split subjects into test and training sets\n",
    "    test_subject = subject_ids[fold_idx]\n",
    "\n",
    "    # Prepare training and test data\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for subject_id in subject_ids:\n",
    "        lie_samples = subject_data['lie'].get(subject_id, [])\n",
    "        truth_samples = subject_data['truth'].get(subject_id, [])\n",
    "\n",
    "        if subject_id == test_subject:\n",
    "            X_test.extend(lie_samples)\n",
    "            y_test.extend([0] * len(lie_samples))\n",
    "            print(f\"Adding {len(lie_samples)} lie samples from subject {subject_id} to test set\")\n",
    "            X_test.extend(truth_samples)\n",
    "            y_test.extend([1] * len(truth_samples))\n",
    "            print(f\"Adding {len(truth_samples)} truth samples from subject {subject_id} to test set\")\n",
    "        else:\n",
    "            X_train.extend(lie_samples)\n",
    "            y_train.extend([0] * len(lie_samples))\n",
    "            X_train.extend(truth_samples)\n",
    "            y_train.extend([1] * len(truth_samples))\n",
    "            print(f\"Adding {len(lie_samples)} lie samples and {len(truth_samples)} truth samples from subject {subject_id} to train set\")\n",
    "\n",
    "    print(f\"Total training samples: {len(X_train)}, Total test samples: {len(X_test)}\")\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    # Normalize data using scaler fitted on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_test = scaler.transform(X_test.reshape(X_test.shape[0], -1))\n",
    "    X_train = X_train.reshape(-1, 65, max_length)\n",
    "    X_test = X_test.reshape(-1, 65, max_length)\n",
    "\n",
    "    print(X_test.shape)\n",
    "    print(X_train.shape)\n",
    "\n",
    "    # Save the scaler to a file\n",
    "    with open(r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\RevisedEEGNet_scaler_{}.pkl'.format(fold_idx), 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = EEGDataset(X_train, y_train)\n",
    "    test_dataset = EEGDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    model = train_and_evaluate(train_loader, test_loader, y_train)\n",
    "\n",
    "    # Evaluate on the test set and calculate metrics\n",
    "    model.eval()\n",
    "    fold_labels = []\n",
    "    fold_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            fold_labels.extend(y_batch.cpu().numpy())\n",
    "            fold_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics for the current fold\n",
    "    accuracy = accuracy_score(fold_labels, fold_predictions)\n",
    "    precision = precision_score(fold_labels, fold_predictions)\n",
    "    recall = recall_score(fold_labels, fold_predictions)\n",
    "    f1 = f1_score(fold_labels, fold_predictions)\n",
    "    auc = roc_auc_score(fold_labels, fold_predictions)\n",
    "    conf_matrix = confusion_matrix(fold_labels, fold_predictions)\n",
    "\n",
    "    # Store fold metrics\n",
    "    fold_accuracies.append(accuracy)\n",
    "    fold_precisions.append(precision)\n",
    "    fold_recalls.append(recall)\n",
    "    fold_f1_scores.append(f1)\n",
    "    fold_aucs.append(auc)\n",
    "    fold_conf_matrices.append(conf_matrix)\n",
    "\n",
    "    # Print metrics for the current fold\n",
    "    print(f'Fold {fold_idx + 1} Metrics:')\n",
    "    print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Aggregate all labels and predictions for final evaluation across all folds\n",
    "    all_labels.extend(fold_labels)\n",
    "    all_predictions.extend(fold_predictions)\n",
    "\n",
    "    print(f'Completed fold {fold_idx + 1}')\n",
    "    print('-' * 50)\n",
    "\n",
    "# Calculate overall metrics across all folds\n",
    "overall_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "overall_precision = precision_score(all_labels, all_predictions)\n",
    "overall_recall = recall_score(all_labels, all_predictions)\n",
    "overall_f1 = f1_score(all_labels, all_predictions)\n",
    "overall_auc = roc_auc_score(all_labels, all_predictions)\n",
    "overall_conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(f'Overall Metrics:')\n",
    "print(f'Accuracy: {overall_accuracy}, Precision: {overall_precision}, Recall: {overall_recall}, F1-score: {overall_f1}, AUC: {overall_auc}')\n",
    "print('Overall Confusion Matrix:')\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "plot_confusion_matrix(overall_conf_matrix, title='Overall Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae639325-1800-4763-ad37-b5931e4391c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
