{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b7bb66d-7268-4596-9d35-bb1e3ddcfd74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\detectlie\\Lib\\site-packages\\torch\\nn\\init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.9543, Val Loss: 0.6554\n",
      "Epoch 2/100, Train Loss: 0.5493, Val Loss: 0.6562\n",
      "Epoch 3/100, Train Loss: 0.5053, Val Loss: 0.6117\n",
      "Epoch 4/100, Train Loss: 0.4281, Val Loss: 0.5671\n",
      "Epoch 5/100, Train Loss: 0.3936, Val Loss: 0.5345\n",
      "Epoch 6/100, Train Loss: 0.3801, Val Loss: 0.5372\n",
      "Epoch 7/100, Train Loss: 0.3132, Val Loss: 0.5711\n",
      "Epoch 8/100, Train Loss: 0.2671, Val Loss: 0.6251\n",
      "Epoch 9/100, Train Loss: 0.2822, Val Loss: 0.6984\n",
      "Epoch 10/100, Train Loss: 0.2001, Val Loss: 0.6743\n",
      "Epoch 11/100, Train Loss: 0.2014, Val Loss: 0.6858\n",
      "Epoch 12/100, Train Loss: 0.3009, Val Loss: 0.6358\n",
      "Epoch 13/100, Train Loss: 0.2299, Val Loss: 0.6390\n",
      "Epoch 14/100, Train Loss: 0.2435, Val Loss: 0.6486\n",
      "Epoch 15/100, Train Loss: 0.2473, Val Loss: 0.5985\n",
      "Epoch 16/100, Train Loss: 0.2146, Val Loss: 0.6224\n",
      "Epoch 17/100, Train Loss: 0.1971, Val Loss: 0.6056\n",
      "Epoch 18/100, Train Loss: 0.2941, Val Loss: 0.5794\n",
      "Epoch 19/100, Train Loss: 0.5614, Val Loss: 0.5702\n",
      "Epoch 20/100, Train Loss: 0.1799, Val Loss: 0.5831\n",
      "Epoch 21/100, Train Loss: 0.1837, Val Loss: 0.5857\n",
      "Epoch 22/100, Train Loss: 0.3012, Val Loss: 0.5908\n",
      "Epoch 23/100, Train Loss: 0.1712, Val Loss: 0.6029\n",
      "Epoch 24/100, Train Loss: 0.1769, Val Loss: 0.6053\n",
      "Epoch 25/100, Train Loss: 0.2097, Val Loss: 0.5903\n",
      "Early stopping at epoch 25\n",
      "Final Accuracy for fold 1: 66.67%\n",
      "Final Validation Loss for fold 1: 0.6026\n",
      "Final Precision for fold 1: 0.71\n",
      "Final Recall for fold 1: 0.69\n",
      "Final F1-Score for fold 1: 0.66\n",
      "Final AUC for fold 1: 0.72\n",
      "\n",
      "Fold 2\n",
      "Epoch 1/100, Train Loss: 0.6326, Val Loss: 0.6484\n",
      "Epoch 2/100, Train Loss: 0.4945, Val Loss: 0.6111\n",
      "Epoch 3/100, Train Loss: 0.4905, Val Loss: 0.5715\n",
      "Epoch 4/100, Train Loss: 0.4162, Val Loss: 0.5270\n",
      "Epoch 5/100, Train Loss: 0.3974, Val Loss: 0.4762\n",
      "Epoch 6/100, Train Loss: 0.3027, Val Loss: 0.4410\n",
      "Epoch 7/100, Train Loss: 0.3449, Val Loss: 0.4084\n",
      "Epoch 8/100, Train Loss: 0.2608, Val Loss: 0.4161\n",
      "Epoch 9/100, Train Loss: 0.3243, Val Loss: 0.4094\n",
      "Epoch 10/100, Train Loss: 0.3293, Val Loss: 0.3673\n",
      "Epoch 11/100, Train Loss: 0.2665, Val Loss: 0.3682\n",
      "Epoch 12/100, Train Loss: 0.2740, Val Loss: 0.3649\n",
      "Epoch 13/100, Train Loss: 0.2341, Val Loss: 0.3542\n",
      "Epoch 14/100, Train Loss: 0.2288, Val Loss: 0.3532\n",
      "Epoch 15/100, Train Loss: 0.2289, Val Loss: 0.3520\n",
      "Epoch 16/100, Train Loss: 0.2404, Val Loss: 0.3503\n",
      "Epoch 17/100, Train Loss: 0.2445, Val Loss: 0.3559\n",
      "Epoch 18/100, Train Loss: 0.1980, Val Loss: 0.3389\n",
      "Epoch 19/100, Train Loss: 0.2332, Val Loss: 0.3529\n",
      "Epoch 20/100, Train Loss: 0.2008, Val Loss: 0.3364\n",
      "Epoch 21/100, Train Loss: 0.1746, Val Loss: 0.3510\n",
      "Epoch 22/100, Train Loss: 0.2770, Val Loss: 0.3471\n",
      "Epoch 23/100, Train Loss: 0.2513, Val Loss: 0.3442\n",
      "Epoch 24/100, Train Loss: 0.2210, Val Loss: 0.3341\n",
      "Epoch 25/100, Train Loss: 0.1646, Val Loss: 0.3443\n",
      "Epoch 26/100, Train Loss: 0.1948, Val Loss: 0.3356\n",
      "Epoch 27/100, Train Loss: 0.2327, Val Loss: 0.3396\n",
      "Epoch 28/100, Train Loss: 0.1916, Val Loss: 0.3325\n",
      "Epoch 29/100, Train Loss: 0.2276, Val Loss: 0.3304\n",
      "Epoch 30/100, Train Loss: 0.2251, Val Loss: 0.3378\n",
      "Epoch 31/100, Train Loss: 0.2900, Val Loss: 0.3376\n",
      "Epoch 32/100, Train Loss: 0.1932, Val Loss: 0.3374\n",
      "Epoch 33/100, Train Loss: 0.2184, Val Loss: 0.3454\n",
      "Epoch 34/100, Train Loss: 0.1734, Val Loss: 0.3374\n",
      "Epoch 35/100, Train Loss: 0.1742, Val Loss: 0.3347\n",
      "Epoch 36/100, Train Loss: 0.2125, Val Loss: 0.3507\n",
      "Epoch 37/100, Train Loss: 0.2190, Val Loss: 0.3473\n",
      "Epoch 38/100, Train Loss: 0.2629, Val Loss: 0.3363\n",
      "Epoch 39/100, Train Loss: 0.2047, Val Loss: 0.3327\n",
      "Epoch 40/100, Train Loss: 0.2506, Val Loss: 0.3452\n",
      "Epoch 41/100, Train Loss: 0.2678, Val Loss: 0.3462\n",
      "Epoch 42/100, Train Loss: 0.2128, Val Loss: 0.3427\n",
      "Epoch 43/100, Train Loss: 0.2167, Val Loss: 0.3386\n",
      "Epoch 44/100, Train Loss: 0.1997, Val Loss: 0.3525\n",
      "Epoch 45/100, Train Loss: 0.1906, Val Loss: 0.3466\n",
      "Epoch 46/100, Train Loss: 0.1902, Val Loss: 0.3428\n",
      "Epoch 47/100, Train Loss: 0.2068, Val Loss: 0.3336\n",
      "Epoch 48/100, Train Loss: 0.2005, Val Loss: 0.3374\n",
      "Epoch 49/100, Train Loss: 0.2460, Val Loss: 0.3463\n",
      "Early stopping at epoch 49\n",
      "Final Accuracy for fold 2: 88.89%\n",
      "Final Validation Loss for fold 2: 0.3339\n",
      "Final Precision for fold 2: 0.90\n",
      "Final Recall for fold 2: 0.90\n",
      "Final F1-Score for fold 2: 0.89\n",
      "Final AUC for fold 2: 0.91\n",
      "\n",
      "Fold 3\n",
      "Epoch 1/100, Train Loss: 0.6885, Val Loss: 0.6431\n",
      "Epoch 2/100, Train Loss: 0.7222, Val Loss: 0.5777\n",
      "Epoch 3/100, Train Loss: 0.5983, Val Loss: 0.5702\n",
      "Epoch 4/100, Train Loss: 0.5182, Val Loss: 0.5133\n",
      "Epoch 5/100, Train Loss: 0.4696, Val Loss: 0.4100\n",
      "Epoch 6/100, Train Loss: 0.4065, Val Loss: 0.3358\n",
      "Epoch 7/100, Train Loss: 0.3708, Val Loss: 0.3128\n",
      "Epoch 8/100, Train Loss: 0.3088, Val Loss: 0.3476\n",
      "Epoch 9/100, Train Loss: 0.3165, Val Loss: 0.3236\n",
      "Epoch 10/100, Train Loss: 0.2561, Val Loss: 0.3158\n",
      "Epoch 11/100, Train Loss: 0.2446, Val Loss: 0.3155\n",
      "Epoch 12/100, Train Loss: 0.2634, Val Loss: 0.3074\n",
      "Epoch 13/100, Train Loss: 0.2396, Val Loss: 0.3186\n",
      "Epoch 14/100, Train Loss: 0.2489, Val Loss: 0.3209\n",
      "Epoch 15/100, Train Loss: 0.2513, Val Loss: 0.3324\n",
      "Epoch 16/100, Train Loss: 0.2158, Val Loss: 0.3295\n",
      "Epoch 17/100, Train Loss: 0.2315, Val Loss: 0.3415\n",
      "Epoch 18/100, Train Loss: 0.1920, Val Loss: 0.3393\n",
      "Epoch 19/100, Train Loss: 0.2229, Val Loss: 0.3494\n",
      "Epoch 20/100, Train Loss: 0.2397, Val Loss: 0.3446\n",
      "Epoch 21/100, Train Loss: 0.1820, Val Loss: 0.3461\n",
      "Epoch 22/100, Train Loss: 0.2221, Val Loss: 0.3576\n",
      "Epoch 23/100, Train Loss: 0.3587, Val Loss: 0.3406\n",
      "Epoch 24/100, Train Loss: 0.1995, Val Loss: 0.3364\n",
      "Epoch 25/100, Train Loss: 0.1826, Val Loss: 0.3363\n",
      "Epoch 26/100, Train Loss: 0.2021, Val Loss: 0.3450\n",
      "Epoch 27/100, Train Loss: 0.2291, Val Loss: 0.3531\n",
      "Epoch 28/100, Train Loss: 0.2082, Val Loss: 0.3548\n",
      "Epoch 29/100, Train Loss: 0.1764, Val Loss: 0.3518\n",
      "Epoch 30/100, Train Loss: 0.1946, Val Loss: 0.3541\n",
      "Epoch 31/100, Train Loss: 0.1787, Val Loss: 0.3711\n",
      "Epoch 32/100, Train Loss: 0.1852, Val Loss: 0.3681\n",
      "Early stopping at epoch 32\n",
      "Final Accuracy for fold 3: 83.33%\n",
      "Final Validation Loss for fold 3: 0.3680\n",
      "Final Precision for fold 3: 0.81\n",
      "Final Recall for fold 3: 0.88\n",
      "Final F1-Score for fold 3: 0.82\n",
      "Final AUC for fold 3: 0.95\n",
      "\n",
      "Fold 4\n",
      "Epoch 1/100, Train Loss: 0.9101, Val Loss: 0.6459\n",
      "Epoch 2/100, Train Loss: 0.5902, Val Loss: 0.6575\n",
      "Epoch 3/100, Train Loss: 0.5224, Val Loss: 0.6470\n",
      "Epoch 4/100, Train Loss: 0.4513, Val Loss: 0.6225\n",
      "Epoch 5/100, Train Loss: 0.4084, Val Loss: 0.6151\n",
      "Epoch 6/100, Train Loss: 0.3855, Val Loss: 0.6356\n",
      "Epoch 7/100, Train Loss: 0.4574, Val Loss: 0.6035\n",
      "Epoch 8/100, Train Loss: 0.3718, Val Loss: 0.5696\n",
      "Epoch 9/100, Train Loss: 0.2781, Val Loss: 0.5563\n",
      "Epoch 10/100, Train Loss: 0.3300, Val Loss: 0.4805\n",
      "Epoch 11/100, Train Loss: 0.3208, Val Loss: 0.5004\n",
      "Epoch 12/100, Train Loss: 0.2697, Val Loss: 0.5006\n",
      "Epoch 13/100, Train Loss: 0.2903, Val Loss: 0.4289\n",
      "Epoch 14/100, Train Loss: 0.2851, Val Loss: 0.4737\n",
      "Epoch 15/100, Train Loss: 0.2952, Val Loss: 0.4934\n",
      "Epoch 16/100, Train Loss: 0.2802, Val Loss: 0.4691\n",
      "Epoch 17/100, Train Loss: 0.3103, Val Loss: 0.4984\n",
      "Epoch 18/100, Train Loss: 0.2845, Val Loss: 0.4812\n",
      "Epoch 19/100, Train Loss: 0.3060, Val Loss: 0.4677\n",
      "Epoch 20/100, Train Loss: 0.2633, Val Loss: 0.4988\n",
      "Epoch 21/100, Train Loss: 0.2546, Val Loss: 0.4588\n",
      "Epoch 22/100, Train Loss: 0.2965, Val Loss: 0.4715\n",
      "Epoch 23/100, Train Loss: 0.3263, Val Loss: 0.4518\n",
      "Epoch 24/100, Train Loss: 0.2369, Val Loss: 0.4466\n",
      "Epoch 25/100, Train Loss: 0.2985, Val Loss: 0.5056\n",
      "Epoch 26/100, Train Loss: 0.2723, Val Loss: 0.4534\n",
      "Epoch 27/100, Train Loss: 0.2378, Val Loss: 0.5210\n",
      "Epoch 28/100, Train Loss: 0.2488, Val Loss: 0.4811\n",
      "Epoch 29/100, Train Loss: 0.2640, Val Loss: 0.4902\n",
      "Epoch 30/100, Train Loss: 0.2055, Val Loss: 0.5117\n",
      "Epoch 31/100, Train Loss: 0.2401, Val Loss: 0.4870\n",
      "Epoch 32/100, Train Loss: 0.2614, Val Loss: 0.4999\n",
      "Epoch 33/100, Train Loss: 0.3761, Val Loss: 0.5049\n",
      "Early stopping at epoch 33\n",
      "Final Accuracy for fold 4: 94.44%\n",
      "Final Validation Loss for fold 4: 0.4873\n",
      "Final Precision for fold 4: 0.96\n",
      "Final Recall for fold 4: 0.93\n",
      "Final F1-Score for fold 4: 0.94\n",
      "Final AUC for fold 4: 0.88\n",
      "\n",
      "Fold 5\n",
      "Epoch 1/100, Train Loss: 0.7191, Val Loss: 0.6277\n",
      "Epoch 2/100, Train Loss: 0.5659, Val Loss: 0.6191\n",
      "Epoch 3/100, Train Loss: 0.5043, Val Loss: 0.6183\n",
      "Epoch 4/100, Train Loss: 0.4572, Val Loss: 0.6365\n",
      "Epoch 5/100, Train Loss: 0.4043, Val Loss: 0.6266\n",
      "Epoch 6/100, Train Loss: 0.3469, Val Loss: 0.6091\n",
      "Epoch 7/100, Train Loss: 0.3719, Val Loss: 0.6008\n",
      "Epoch 8/100, Train Loss: 0.2849, Val Loss: 0.6224\n",
      "Epoch 9/100, Train Loss: 0.2796, Val Loss: 0.6986\n",
      "Epoch 10/100, Train Loss: 0.3129, Val Loss: 0.7071\n",
      "Epoch 11/100, Train Loss: 0.2746, Val Loss: 0.6924\n",
      "Epoch 12/100, Train Loss: 0.2163, Val Loss: 0.7025\n",
      "Epoch 13/100, Train Loss: 0.1893, Val Loss: 0.6904\n",
      "Epoch 14/100, Train Loss: 0.1780, Val Loss: 0.6739\n",
      "Epoch 15/100, Train Loss: 0.1936, Val Loss: 0.7008\n",
      "Epoch 16/100, Train Loss: 0.1680, Val Loss: 0.6905\n",
      "Epoch 17/100, Train Loss: 0.2018, Val Loss: 0.7105\n",
      "Epoch 18/100, Train Loss: 0.1862, Val Loss: 0.6943\n",
      "Epoch 19/100, Train Loss: 0.1917, Val Loss: 0.7012\n",
      "Epoch 20/100, Train Loss: 0.1713, Val Loss: 0.6660\n",
      "Epoch 21/100, Train Loss: 0.1813, Val Loss: 0.6745\n",
      "Epoch 22/100, Train Loss: 0.1802, Val Loss: 0.6801\n",
      "Epoch 23/100, Train Loss: 0.1845, Val Loss: 0.6898\n",
      "Epoch 24/100, Train Loss: 0.1678, Val Loss: 0.6750\n",
      "Epoch 25/100, Train Loss: 0.2226, Val Loss: 0.6680\n",
      "Epoch 26/100, Train Loss: 0.1777, Val Loss: 0.6813\n",
      "Epoch 27/100, Train Loss: 0.1571, Val Loss: 0.7169\n",
      "Early stopping at epoch 27\n",
      "Final Accuracy for fold 5: 72.22%\n",
      "Final Validation Loss for fold 5: 0.7016\n",
      "Final Precision for fold 5: 0.79\n",
      "Final Recall for fold 5: 0.77\n",
      "Final F1-Score for fold 5: 0.72\n",
      "Final AUC for fold 5: 0.82\n",
      "\n",
      "Average Accuracy: 81.11%\n",
      "Average Validation Loss: 0.4987\n",
      "Average Precision: 0.83\n",
      "Average Recall: 0.83\n",
      "Average F1-Score: 0.81\n",
      "Average AUC: 0.86\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Define constants\n",
    "DATA_DIR = 'C:/Users/User/Documents/Lie detect data/EEGData'\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100  # Increased to allow early stopping\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_FOLDS = 5\n",
    "PATIENCE = 20  # Early stopping patience\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Custom Dataset class for EEG data\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.load_data(data_dir)\n",
    "        self.normalize_data()\n",
    "\n",
    "    def load_data(self, data_dir):\n",
    "        max_length = 0\n",
    "        temp_data = []\n",
    "        \n",
    "        for file_name in os.listdir(data_dir):\n",
    "            file_path = os.path.join(data_dir, file_name)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                eeg_data = pickle.load(f)\n",
    "                label = 1 if 'lie' in file_name else 0  # Assuming file names contain 'lie' or 'truth'\n",
    "                temp_data.append((eeg_data, label))\n",
    "                max_length = max(max_length, eeg_data.shape[1])\n",
    "\n",
    "        for eeg_data, label in temp_data:\n",
    "            padded_data = np.pad(eeg_data, ((0, 0), (0, max_length - eeg_data.shape[1])), mode='constant')\n",
    "            self.data.append(padded_data)\n",
    "            self.labels.append(label)\n",
    "        \n",
    "        self.data = [torch.tensor(d, dtype=torch.float32, device=device) for d in self.data]\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long, device=device)\n",
    "    \n",
    "    def normalize_data(self):\n",
    "        all_data = torch.cat([d.unsqueeze(0) for d in self.data], dim=0)\n",
    "        mean = all_data.mean()\n",
    "        std = all_data.std()\n",
    "        self.data = [(d - mean) / std for d in self.data]\n",
    "\n",
    "    def augment_data(self, data):\n",
    "        # Advanced augmentations: Gaussian noise, time shift, scaling\n",
    "        noise = torch.randn_like(data, device=device) * 0.01\n",
    "        shift = torch.roll(data, shifts=int(data.shape[1] * 0.1), dims=1)\n",
    "        scale = data * (1 + 0.1 * torch.randn(1, device=device))\n",
    "        return noise + shift + scale\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.data[idx], self.labels[idx]\n",
    "        data = self.augment_data(data)  # Apply augmentation\n",
    "        return data, label\n",
    "\n",
    "# Define the EEGNet model\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.firstconv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, (1, 51), padding=(0, 25)),\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "        self.depthwiseConv = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, (65, 1), groups=16),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d((1, 4)),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.separableConv = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, (1, 15), padding=(0, 7)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d((1, 8)),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.classify = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(output_size, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.firstconv(x)\n",
    "        x = self.depthwiseConv(x)\n",
    "        x = self.separableConv(x)\n",
    "        return self.classify(x)\n",
    "\n",
    "# Function to determine the output size of the EEGNet model before the linear layer\n",
    "def get_output_size(model, shape):\n",
    "    with torch.no_grad():\n",
    "        x = torch.zeros(shape, device=device)\n",
    "        x = model.firstconv(x)\n",
    "        x = model.depthwiseConv(x)\n",
    "        x = model.separableConv(x)\n",
    "        return x.view(x.size(0), -1).size(1)\n",
    "\n",
    "# Load data\n",
    "dataset = EEGDataset(DATA_DIR)\n",
    "\n",
    "# Determine the correct input size for the linear layer\n",
    "dummy_input_shape = (1, 1, 65, max([d.shape[1] for d in dataset.data]))  # (batch_size, channels, height, width)\n",
    "output_size = get_output_size(EEGNet(output_size=0).to(device), dummy_input_shape)\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "kf = KFold(n_splits=NUM_FOLDS, shuffle=True)\n",
    "\n",
    "final_accuracies = []\n",
    "final_precisions = []\n",
    "final_recalls = []\n",
    "final_f1s = []\n",
    "final_aucs = []\n",
    "final_val_losses = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(dataset)):\n",
    "    print(f'Fold {fold+1}')\n",
    "    \n",
    "    train_subset = Subset(dataset, train_index)\n",
    "    val_subset = Subset(dataset, val_index)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = EEGNet(output_size=output_size).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.unsqueeze(1))\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs.unsqueeze(1))\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "\n",
    "        val_loss = val_running_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probs = []\n",
    "    final_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs.unsqueeze(1))\n",
    "            loss = criterion(outputs, labels)\n",
    "            final_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_probs.extend(torch.softmax(outputs, dim=1)[:, 1].cpu().numpy())\n",
    "\n",
    "    final_val_loss /= len(val_loader)\n",
    "    final_accuracy = 100 * correct / total\n",
    "    final_precision = precision_score(all_labels, all_predictions, average='macro')\n",
    "    final_recall = recall_score(all_labels, all_predictions, average='macro')\n",
    "    final_f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    final_auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    # Store final metrics\n",
    "    final_accuracies.append(final_accuracy)\n",
    "    final_precisions.append(final_precision)\n",
    "    final_recalls.append(final_recall)\n",
    "    final_f1s.append(final_f1)\n",
    "    final_aucs.append(final_auc)\n",
    "    final_val_losses.append(final_val_loss)\n",
    "\n",
    "    print(f'Final Accuracy for fold {fold+1}: {final_accuracy:.2f}%')\n",
    "    print(f'Final Validation Loss for fold {fold+1}: {final_val_loss:.4f}')\n",
    "    print(f'Final Precision for fold {fold+1}: {final_precision:.2f}')\n",
    "    print(f'Final Recall for fold {fold+1}: {final_recall:.2f}')\n",
    "    print(f'Final F1-Score for fold {fold+1}: {final_f1:.2f}')\n",
    "    print(f'Final AUC for fold {fold+1}: {final_auc:.2f}\\n')\n",
    "\n",
    "# Report average performance across all folds\n",
    "print(f'Average Accuracy: {np.mean(final_accuracies):.2f}%')\n",
    "print(f'Average Validation Loss: {np.mean(final_val_losses):.4f}')\n",
    "print(f'Average Precision: {np.mean(final_precisions):.2f}')\n",
    "print(f'Average Recall: {np.mean(final_recalls):.2f}')\n",
    "print(f'Average F1-Score: {np.mean(final_f1s):.2f}')\n",
    "print(f'Average AUC: {np.mean(final_aucs):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faffd59-d784-4037-a0f7-d591f8cb26d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
