{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "656cdbc2-da54-45ba-97fc-195b59bd7820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 0: Train Loss: 0.6841477751731873, Validation Loss: 0.6788529753684998\n",
      "Epoch 1: Train Loss: 0.6387658913930258, Validation Loss: 0.6638750433921814\n",
      "Epoch 2: Train Loss: 0.6211890776952108, Validation Loss: 0.6385936141014099\n",
      "Epoch 3: Train Loss: 0.5965638359387716, Validation Loss: 0.6122031807899475\n",
      "Epoch 4: Train Loss: 0.5795794129371643, Validation Loss: 0.5921217203140259\n",
      "Epoch 5: Train Loss: 0.5689776341120402, Validation Loss: 0.577282190322876\n",
      "Epoch 6: Train Loss: 0.5700750946998596, Validation Loss: 0.5647788047790527\n",
      "Epoch 7: Train Loss: 0.555071234703064, Validation Loss: 0.555070161819458\n",
      "Epoch 8: Train Loss: 0.5555931329727173, Validation Loss: 0.5468003749847412\n",
      "Epoch 9: Train Loss: 0.552112857500712, Validation Loss: 0.5405316352844238\n",
      "Epoch 10: Train Loss: 0.543188730875651, Validation Loss: 0.5284601449966431\n",
      "Epoch 11: Train Loss: 0.5234002868334452, Validation Loss: 0.5171597003936768\n",
      "Epoch 12: Train Loss: 0.5129938920338949, Validation Loss: 0.5064069628715515\n",
      "Epoch 13: Train Loss: 0.5001097818215688, Validation Loss: 0.4953766465187073\n",
      "Epoch 14: Train Loss: 0.48496074477831524, Validation Loss: 0.48723104596138\n",
      "Epoch 15: Train Loss: 0.47381091117858887, Validation Loss: 0.4828973710536957\n",
      "Epoch 16: Train Loss: 0.46736417214075726, Validation Loss: 0.47822335362434387\n",
      "Epoch 17: Train Loss: 0.4627976914246877, Validation Loss: 0.4749007225036621\n",
      "Epoch 18: Train Loss: 0.46743529041608173, Validation Loss: 0.4720917046070099\n",
      "Epoch 19: Train Loss: 0.4582405885060628, Validation Loss: 0.47019365429878235\n",
      "Epoch 20: Train Loss: 0.44328134258588153, Validation Loss: 0.4682779610157013\n",
      "Epoch 21: Train Loss: 0.44082359472910565, Validation Loss: 0.4650740325450897\n",
      "Epoch 22: Train Loss: 0.4221915602684021, Validation Loss: 0.4597744345664978\n",
      "Epoch 23: Train Loss: 0.4461659292380015, Validation Loss: 0.4547138512134552\n",
      "Epoch 24: Train Loss: 0.4042571187019348, Validation Loss: 0.451121062040329\n",
      "Epoch 25: Train Loss: 0.39685720205307007, Validation Loss: 0.45022085309028625\n",
      "Epoch 26: Train Loss: 0.3923487067222595, Validation Loss: 0.4483335614204407\n",
      "Epoch 27: Train Loss: 0.38962961236635846, Validation Loss: 0.4478667974472046\n",
      "Epoch 28: Train Loss: 0.38296399513880414, Validation Loss: 0.4465424716472626\n",
      "Epoch 29: Train Loss: 0.3929781913757324, Validation Loss: 0.4453551769256592\n",
      "Epoch 30: Train Loss: 0.38920178016026813, Validation Loss: 0.43586790561676025\n",
      "Epoch 31: Train Loss: 0.3798389732837677, Validation Loss: 0.4305627942085266\n",
      "Epoch 32: Train Loss: 0.36345765988032025, Validation Loss: 0.4323638379573822\n",
      "Epoch 33: Train Loss: 0.34706318378448486, Validation Loss: 0.4370570778846741\n",
      "Epoch 34: Train Loss: 0.3440820376078288, Validation Loss: 0.4444814622402191\n",
      "Epoch 35: Train Loss: 0.32819414138793945, Validation Loss: 0.44753775000572205\n",
      "Epoch 36: Train Loss: 0.3218263586362203, Validation Loss: 0.4475628435611725\n",
      "Epoch 37: Train Loss: 0.32638688882191974, Validation Loss: 0.4460642635822296\n",
      "Epoch 38: Train Loss: 0.3176814218362172, Validation Loss: 0.44462427496910095\n",
      "Epoch 39: Train Loss: 0.30725230773289997, Validation Loss: 0.4449670612812042\n",
      "Epoch 40: Train Loss: 0.30816034475962323, Validation Loss: 0.43988704681396484\n",
      "Epoch 41: Train Loss: 0.29822323222955066, Validation Loss: 0.4451533257961273\n",
      "Epoch 42: Train Loss: 0.29563285907109577, Validation Loss: 0.45390743017196655\n",
      "Epoch 43: Train Loss: 0.31085360050201416, Validation Loss: 0.4666944146156311\n",
      "Epoch 44: Train Loss: 0.2721794197956721, Validation Loss: 0.4553682804107666\n",
      "Epoch 45: Train Loss: 0.2925550639629364, Validation Loss: 0.44650179147720337\n",
      "Epoch 46: Train Loss: 0.26623180011908215, Validation Loss: 0.4416552484035492\n",
      "Epoch 47: Train Loss: 0.2688967287540436, Validation Loss: 0.4408513605594635\n",
      "Epoch 48: Train Loss: 0.2637357711791992, Validation Loss: 0.4426954388618469\n",
      "Epoch 49: Train Loss: 0.2665779044230779, Validation Loss: 0.44483786821365356\n",
      "Epoch 50: Train Loss: 0.2671891550223033, Validation Loss: 0.4676009714603424\n",
      "Epoch 51: Train Loss: 0.253001203139623, Validation Loss: 0.4605954885482788\n",
      "Epoch 52: Train Loss: 0.2527819623549779, Validation Loss: 0.47210273146629333\n",
      "Epoch 53: Train Loss: 0.2486958603064219, Validation Loss: 0.47129809856414795\n",
      "Epoch 54: Train Loss: 0.2544749826192856, Validation Loss: 0.4559667110443115\n",
      "Epoch 55: Train Loss: 0.2394747088352839, Validation Loss: 0.4532948434352875\n",
      "Epoch 56: Train Loss: 0.224306121468544, Validation Loss: 0.46677032113075256\n",
      "Epoch 57: Train Loss: 0.22595315178235373, Validation Loss: 0.47536757588386536\n",
      "Epoch 58: Train Loss: 0.2557632823785146, Validation Loss: 0.4761488437652588\n",
      "Epoch 59: Train Loss: 0.2249309072891871, Validation Loss: 0.4762629568576813\n",
      "Epoch 60: Train Loss: 0.24120259781678519, Validation Loss: 0.4630957841873169\n",
      "Epoch 61: Train Loss: 0.2238410065571467, Validation Loss: 0.4927311837673187\n",
      "Epoch 62: Train Loss: 0.22501198947429657, Validation Loss: 0.48929980397224426\n",
      "Epoch 63: Train Loss: 0.2107904851436615, Validation Loss: 0.5053527355194092\n",
      "Epoch 64: Train Loss: 0.20480233430862427, Validation Loss: 0.5253438949584961\n",
      "Epoch 65: Train Loss: 0.2152441293001175, Validation Loss: 0.5061907172203064\n",
      "Epoch 66: Train Loss: 0.20090046028296152, Validation Loss: 0.4886721968650818\n",
      "Epoch 67: Train Loss: 0.199758380651474, Validation Loss: 0.48151010274887085\n",
      "Epoch 68: Train Loss: 0.1960812658071518, Validation Loss: 0.4805232286453247\n",
      "Epoch 69: Train Loss: 0.1973110338052114, Validation Loss: 0.4816589057445526\n",
      "Epoch 70: Train Loss: 0.20161230862140656, Validation Loss: 0.4911108613014221\n",
      "Epoch 71: Train Loss: 0.20639005303382874, Validation Loss: 0.5303198099136353\n",
      "Epoch 72: Train Loss: 0.18467223644256592, Validation Loss: 0.5152142643928528\n",
      "Epoch 73: Train Loss: 0.18743220468362173, Validation Loss: 0.5009705424308777\n",
      "Epoch 74: Train Loss: 0.1821484516064326, Validation Loss: 0.5097367167472839\n",
      "Epoch 75: Train Loss: 0.17389380931854248, Validation Loss: 0.5379165410995483\n",
      "Epoch 76: Train Loss: 0.18830432494481406, Validation Loss: 0.5368087291717529\n",
      "Epoch 77: Train Loss: 0.17551479240258536, Validation Loss: 0.528705894947052\n",
      "Epoch 78: Train Loss: 0.16343512634436289, Validation Loss: 0.5216686129570007\n",
      "Epoch 79: Train Loss: 0.17558002968629202, Validation Loss: 0.5207011103630066\n",
      "Epoch 80: Train Loss: 0.1710765759150187, Validation Loss: 0.5243346691131592\n",
      "Epoch 81: Train Loss: 0.17367351055145264, Validation Loss: 0.5193208456039429\n",
      "Epoch 82: Train Loss: 0.167973592877388, Validation Loss: 0.527611494064331\n",
      "Epoch 83: Train Loss: 0.1726293166478475, Validation Loss: 0.521725594997406\n",
      "Epoch 84: Train Loss: 0.16473402579625449, Validation Loss: 0.5325338244438171\n",
      "Epoch 85: Train Loss: 0.16866063078244528, Validation Loss: 0.5062310695648193\n",
      "Epoch 86: Train Loss: 0.14751015106836954, Validation Loss: 0.5013740062713623\n",
      "Epoch 87: Train Loss: 0.14876417815685272, Validation Loss: 0.5066203474998474\n",
      "Epoch 88: Train Loss: 0.1533074975013733, Validation Loss: 0.5109001398086548\n",
      "Epoch 89: Train Loss: 0.14662430435419083, Validation Loss: 0.5100497603416443\n",
      "Epoch 90: Train Loss: 0.15437972297271094, Validation Loss: 0.595030665397644\n",
      "Epoch 91: Train Loss: 0.1689313997824987, Validation Loss: 0.6205732226371765\n",
      "Epoch 92: Train Loss: 0.15235292414824167, Validation Loss: 0.5648790001869202\n",
      "Epoch 93: Train Loss: 0.14142866432666779, Validation Loss: 0.5361398458480835\n",
      "Epoch 94: Train Loss: 0.14887942870457968, Validation Loss: 0.5388185977935791\n",
      "Epoch 95: Train Loss: 0.14374463756879172, Validation Loss: 0.5529741048812866\n",
      "Epoch 96: Train Loss: 0.14132761458555856, Validation Loss: 0.5737133026123047\n",
      "Epoch 97: Train Loss: 0.14006865521272024, Validation Loss: 0.5775371789932251\n",
      "Epoch 98: Train Loss: 0.15122964481512705, Validation Loss: 0.5776228904724121\n",
      "Epoch 99: Train Loss: 0.13167641311883926, Validation Loss: 0.5748488306999207\n",
      "Epoch 100: Train Loss: 0.13237739354372025, Validation Loss: 0.5444275140762329\n",
      "Epoch 101: Train Loss: 0.12789581964413324, Validation Loss: 0.5587950944900513\n",
      "Epoch 102: Train Loss: 0.13138713439305624, Validation Loss: 0.5952416062355042\n",
      "Epoch 103: Train Loss: 0.1241891011595726, Validation Loss: 0.5682182908058167\n",
      "Epoch 104: Train Loss: 0.14602934320767721, Validation Loss: 0.5353186130523682\n",
      "Epoch 105: Train Loss: 0.11841757595539093, Validation Loss: 0.5173392295837402\n",
      "Epoch 106: Train Loss: 0.11848253260056178, Validation Loss: 0.5368092656135559\n",
      "Epoch 107: Train Loss: 0.1206780696908633, Validation Loss: 0.5564144253730774\n",
      "Epoch 108: Train Loss: 0.11178098618984222, Validation Loss: 0.5776523351669312\n",
      "Epoch 109: Train Loss: 0.12886904428402582, Validation Loss: 0.5796419978141785\n",
      "Epoch 110: Train Loss: 0.13996729254722595, Validation Loss: 0.6311725974082947\n",
      "Epoch 111: Train Loss: 0.12078305582205455, Validation Loss: 0.5751901865005493\n",
      "Epoch 112: Train Loss: 0.12618490805228552, Validation Loss: 0.6716494560241699\n",
      "Epoch 113: Train Loss: 0.1185416504740715, Validation Loss: 0.7223414778709412\n",
      "Epoch 114: Train Loss: 0.1320485696196556, Validation Loss: 0.660801351070404\n",
      "Epoch 115: Train Loss: 0.10382136205832164, Validation Loss: 0.6408119797706604\n",
      "Epoch 116: Train Loss: 0.11175646632909775, Validation Loss: 0.6448119878768921\n",
      "Epoch 117: Train Loss: 0.10930639505386353, Validation Loss: 0.6510253548622131\n",
      "Epoch 118: Train Loss: 0.11135446031888326, Validation Loss: 0.6458523869514465\n",
      "Epoch 119: Train Loss: 0.10134506225585938, Validation Loss: 0.6523882150650024\n",
      "Epoch 120: Train Loss: 0.10997694979111354, Validation Loss: 0.6879871487617493\n",
      "Epoch 121: Train Loss: 0.10733974725008011, Validation Loss: 0.6435662508010864\n",
      "Epoch 122: Train Loss: 0.1307273507118225, Validation Loss: 0.6156175136566162\n",
      "Epoch 123: Train Loss: 0.10371534774700801, Validation Loss: 0.559428870677948\n",
      "Epoch 124: Train Loss: 0.10663367807865143, Validation Loss: 0.5989636182785034\n",
      "Epoch 125: Train Loss: 0.09819003194570541, Validation Loss: 0.6418779492378235\n",
      "Epoch 126: Train Loss: 0.10404541095097859, Validation Loss: 0.6375723481178284\n",
      "Epoch 127: Train Loss: 0.10004370907942454, Validation Loss: 0.6044136881828308\n",
      "Epoch 128: Train Loss: 0.09574367602666219, Validation Loss: 0.5956557393074036\n",
      "Epoch 129: Train Loss: 0.09509739776452382, Validation Loss: 0.5952890515327454\n",
      "Epoch 130: Train Loss: 0.10652279357115428, Validation Loss: 0.6137418150901794\n",
      "Epoch 131: Train Loss: 0.11588372414310773, Validation Loss: 0.624991774559021\n",
      "Epoch 132: Train Loss: 0.09964609394470851, Validation Loss: 0.6580871939659119\n",
      "Epoch 133: Train Loss: 0.1005372479557991, Validation Loss: 0.6209061145782471\n",
      "Epoch 134: Train Loss: 0.10052894800901413, Validation Loss: 0.6355023980140686\n",
      "Epoch 135: Train Loss: 0.09578386942545573, Validation Loss: 0.6337816119194031\n",
      "Epoch 136: Train Loss: 0.09174236903587978, Validation Loss: 0.6461465358734131\n",
      "Epoch 137: Train Loss: 0.08129107455412547, Validation Loss: 0.6594205498695374\n",
      "Epoch 138: Train Loss: 0.08337478836377461, Validation Loss: 0.6603556871414185\n",
      "Epoch 139: Train Loss: 0.0906418040394783, Validation Loss: 0.6502297520637512\n",
      "Epoch 140: Train Loss: 0.10028044258554776, Validation Loss: 0.6179990768432617\n",
      "Epoch 141: Train Loss: 0.09429974853992462, Validation Loss: 0.6357429623603821\n",
      "Epoch 142: Train Loss: 0.09060161064068477, Validation Loss: 0.6141035556793213\n",
      "Epoch 143: Train Loss: 0.10562800864378612, Validation Loss: 0.5808506608009338\n",
      "Epoch 144: Train Loss: 0.08487544332941373, Validation Loss: 0.5983355045318604\n",
      "Epoch 145: Train Loss: 0.0826616957783699, Validation Loss: 0.646091103553772\n",
      "Epoch 146: Train Loss: 0.08866136272748311, Validation Loss: 0.6617278456687927\n",
      "Epoch 147: Train Loss: 0.07974268247683843, Validation Loss: 0.6593389511108398\n",
      "Epoch 148: Train Loss: 0.08304879566033681, Validation Loss: 0.655396580696106\n",
      "Epoch 149: Train Loss: 0.07332793871561687, Validation Loss: 0.6503781676292419\n",
      "Epoch 150: Train Loss: 0.08074316630760829, Validation Loss: 0.6089146733283997\n",
      "Epoch 151: Train Loss: 0.07486866911252339, Validation Loss: 0.6205899119377136\n",
      "Epoch 152: Train Loss: 0.10618634770313899, Validation Loss: 0.6862220764160156\n",
      "Epoch 153: Train Loss: 0.07537126044432323, Validation Loss: 0.786523699760437\n",
      "Epoch 154: Train Loss: 0.08567189921935399, Validation Loss: 0.7631159424781799\n",
      "Epoch 155: Train Loss: 0.07365453615784645, Validation Loss: 0.6865752339363098\n",
      "Epoch 156: Train Loss: 0.08956244587898254, Validation Loss: 0.67307049036026\n",
      "Epoch 157: Train Loss: 0.06681690365076065, Validation Loss: 0.670706033706665\n",
      "Epoch 158: Train Loss: 0.06843075901269913, Validation Loss: 0.6677110195159912\n",
      "Epoch 159: Train Loss: 0.07150671134392421, Validation Loss: 0.6630302667617798\n",
      "Epoch 160: Train Loss: 0.08810421824455261, Validation Loss: 0.6670196652412415\n",
      "Epoch 161: Train Loss: 0.10276168088118236, Validation Loss: 0.6172003149986267\n",
      "Epoch 162: Train Loss: 0.09237118437886238, Validation Loss: 0.6968940496444702\n",
      "Epoch 163: Train Loss: 0.09012901534636815, Validation Loss: 0.6773763298988342\n",
      "Epoch 164: Train Loss: 0.08122189591328303, Validation Loss: 0.6216554045677185\n",
      "Epoch 165: Train Loss: 0.08647690216700236, Validation Loss: 0.6293134689331055\n",
      "Epoch 166: Train Loss: 0.07839205364386241, Validation Loss: 0.6330960988998413\n",
      "Epoch 167: Train Loss: 0.07046204184492429, Validation Loss: 0.6466801166534424\n",
      "Epoch 168: Train Loss: 0.06880935281515121, Validation Loss: 0.6517903804779053\n",
      "Epoch 169: Train Loss: 0.09007243439555168, Validation Loss: 0.6552242636680603\n",
      "Epoch 170: Train Loss: 0.08404433975617091, Validation Loss: 0.6376272439956665\n",
      "Epoch 171: Train Loss: 0.07593633482853572, Validation Loss: 0.655211329460144\n",
      "Epoch 172: Train Loss: 0.07093820472558339, Validation Loss: 0.7082677483558655\n",
      "Epoch 173: Train Loss: 0.080893079439799, Validation Loss: 0.7282958626747131\n",
      "Epoch 174: Train Loss: 0.08150237798690796, Validation Loss: 0.728454053401947\n",
      "Epoch 175: Train Loss: 0.09458797921737035, Validation Loss: 0.7708808183670044\n",
      "Epoch 176: Train Loss: 0.07821326206127803, Validation Loss: 0.7479690313339233\n",
      "Epoch 177: Train Loss: 0.08133761088053386, Validation Loss: 0.7082268595695496\n",
      "Epoch 178: Train Loss: 0.06456124782562256, Validation Loss: 0.6794459819793701\n",
      "Epoch 179: Train Loss: 0.059254822631676994, Validation Loss: 0.6690548062324524\n",
      "Epoch 180: Train Loss: 0.06340577205022176, Validation Loss: 0.6011215448379517\n",
      "Epoch 181: Train Loss: 0.06955240045984586, Validation Loss: 0.6585379242897034\n",
      "Epoch 182: Train Loss: 0.06606562808156013, Validation Loss: 0.6757567524909973\n",
      "Epoch 183: Train Loss: 0.06698039794961612, Validation Loss: 0.6592938899993896\n",
      "Epoch 184: Train Loss: 0.06453318148851395, Validation Loss: 0.6856071949005127\n",
      "Epoch 185: Train Loss: 0.08366599058111508, Validation Loss: 0.6552373766899109\n",
      "Epoch 186: Train Loss: 0.06439927717049916, Validation Loss: 0.6521319150924683\n",
      "Epoch 187: Train Loss: 0.06348159288366635, Validation Loss: 0.6487354636192322\n",
      "Epoch 188: Train Loss: 0.056845015535751976, Validation Loss: 0.6446338891983032\n",
      "Epoch 189: Train Loss: 0.05964502568046252, Validation Loss: 0.6490933895111084\n",
      "Epoch 190: Train Loss: 0.05492013692855835, Validation Loss: 0.6497529745101929\n",
      "Epoch 191: Train Loss: 0.07242349411050479, Validation Loss: 0.6820916533470154\n",
      "Epoch 192: Train Loss: 0.058158453553915024, Validation Loss: 0.670681357383728\n",
      "Epoch 193: Train Loss: 0.05140481889247894, Validation Loss: 0.652618944644928\n",
      "Epoch 194: Train Loss: 0.06214216227332751, Validation Loss: 0.6380802989006042\n",
      "Epoch 195: Train Loss: 0.0515618659555912, Validation Loss: 0.650906503200531\n",
      "Epoch 196: Train Loss: 0.06938273832201958, Validation Loss: 0.678631603717804\n",
      "Epoch 197: Train Loss: 0.05219788352648417, Validation Loss: 0.679311215877533\n",
      "Epoch 198: Train Loss: 0.060402831683556236, Validation Loss: 0.6758615970611572\n",
      "Epoch 199: Train Loss: 0.0588698536157608, Validation Loss: 0.6784461736679077\n",
      "Fold 2\n",
      "Epoch 0: Train Loss: 0.6807426810264587, Validation Loss: 0.6847732067108154\n",
      "Epoch 1: Train Loss: 0.622234026590983, Validation Loss: 0.68367600440979\n",
      "Epoch 2: Train Loss: 0.5972331563631693, Validation Loss: 0.6823948621749878\n",
      "Epoch 3: Train Loss: 0.5679527521133423, Validation Loss: 0.6807800531387329\n",
      "Epoch 4: Train Loss: 0.5550009409586588, Validation Loss: 0.6806753873825073\n",
      "Epoch 5: Train Loss: 0.544039785861969, Validation Loss: 0.6805709004402161\n",
      "Epoch 6: Train Loss: 0.5377901196479797, Validation Loss: 0.6836935877799988\n",
      "Epoch 7: Train Loss: 0.5295806129773458, Validation Loss: 0.687454104423523\n",
      "Epoch 8: Train Loss: 0.5350978374481201, Validation Loss: 0.693035364151001\n",
      "Epoch 9: Train Loss: 0.5274578928947449, Validation Loss: 0.7023783326148987\n",
      "Epoch 10: Train Loss: 0.5205435752868652, Validation Loss: 0.7073699235916138\n",
      "Epoch 11: Train Loss: 0.5066649417082468, Validation Loss: 0.7178081274032593\n",
      "Epoch 12: Train Loss: 0.47806554039319354, Validation Loss: 0.7338384389877319\n",
      "Epoch 13: Train Loss: 0.46363353729248047, Validation Loss: 0.7461605072021484\n",
      "Epoch 14: Train Loss: 0.47386545936266583, Validation Loss: 0.7589182257652283\n",
      "Epoch 15: Train Loss: 0.4478523830572764, Validation Loss: 0.7771518230438232\n",
      "Epoch 16: Train Loss: 0.43975449601809186, Validation Loss: 0.7901577353477478\n",
      "Epoch 17: Train Loss: 0.44951818386713666, Validation Loss: 0.7974679470062256\n",
      "Epoch 18: Train Loss: 0.4234493275483449, Validation Loss: 0.808337390422821\n",
      "Epoch 19: Train Loss: 0.43080152074495953, Validation Loss: 0.8140523433685303\n",
      "Epoch 20: Train Loss: 0.4270966251691182, Validation Loss: 0.8292407393455505\n",
      "Epoch 21: Train Loss: 0.4271629949410756, Validation Loss: 0.8439034819602966\n",
      "Epoch 22: Train Loss: 0.3864952325820923, Validation Loss: 0.8432565927505493\n",
      "Epoch 23: Train Loss: 0.3986932039260864, Validation Loss: 0.8435654044151306\n",
      "Epoch 24: Train Loss: 0.38621167341868085, Validation Loss: 0.8630958795547485\n",
      "Epoch 25: Train Loss: 0.36179062724113464, Validation Loss: 0.8618194460868835\n",
      "Epoch 26: Train Loss: 0.3600940803686778, Validation Loss: 0.8436228632926941\n",
      "Epoch 27: Train Loss: 0.35424907008806866, Validation Loss: 0.843302845954895\n",
      "Epoch 28: Train Loss: 0.3497852583726247, Validation Loss: 0.8351345658302307\n",
      "Epoch 29: Train Loss: 0.35902169346809387, Validation Loss: 0.8438370823860168\n",
      "Epoch 30: Train Loss: 0.3347933888435364, Validation Loss: 0.8452823162078857\n",
      "Epoch 31: Train Loss: 0.33110856016476947, Validation Loss: 0.8373308777809143\n",
      "Epoch 32: Train Loss: 0.32229140400886536, Validation Loss: 0.8171496391296387\n",
      "Epoch 33: Train Loss: 0.3254951635996501, Validation Loss: 0.7889751195907593\n",
      "Epoch 34: Train Loss: 0.30736778179804486, Validation Loss: 0.7727349400520325\n",
      "Epoch 35: Train Loss: 0.30409754316012066, Validation Loss: 0.7638096809387207\n",
      "Epoch 36: Train Loss: 0.28977201382319134, Validation Loss: 0.7532447576522827\n",
      "Epoch 37: Train Loss: 0.29666654268900555, Validation Loss: 0.7516781687736511\n",
      "Epoch 38: Train Loss: 0.32596444586912793, Validation Loss: 0.7565503120422363\n",
      "Epoch 39: Train Loss: 0.280321071545283, Validation Loss: 0.763296902179718\n",
      "Epoch 40: Train Loss: 0.29082860549290973, Validation Loss: 0.7520412802696228\n",
      "Epoch 41: Train Loss: 0.2693597724040349, Validation Loss: 0.7500596046447754\n",
      "Epoch 42: Train Loss: 0.27393285433451336, Validation Loss: 0.7430410981178284\n",
      "Epoch 43: Train Loss: 0.2701460272073746, Validation Loss: 0.6928305625915527\n",
      "Epoch 44: Train Loss: 0.2593994339307149, Validation Loss: 0.6832628846168518\n",
      "Epoch 45: Train Loss: 0.25166163345177966, Validation Loss: 0.680094301700592\n",
      "Epoch 46: Train Loss: 0.2523371825615565, Validation Loss: 0.6907550096511841\n",
      "Epoch 47: Train Loss: 0.26193589965502423, Validation Loss: 0.6658996939659119\n",
      "Epoch 48: Train Loss: 0.24962057173252106, Validation Loss: 0.6576772332191467\n",
      "Epoch 49: Train Loss: 0.2315596491098404, Validation Loss: 0.6499769687652588\n",
      "Epoch 50: Train Loss: 0.2660653392473857, Validation Loss: 0.6380854249000549\n",
      "Epoch 51: Train Loss: 0.22071578601996103, Validation Loss: 0.6273163557052612\n",
      "Epoch 52: Train Loss: 0.21657326817512512, Validation Loss: 0.6088334918022156\n",
      "Epoch 53: Train Loss: 0.2276007135709127, Validation Loss: 0.6102980375289917\n",
      "Epoch 54: Train Loss: 0.2203975518544515, Validation Loss: 0.5908645987510681\n",
      "Epoch 55: Train Loss: 0.22272947430610657, Validation Loss: 0.5755525827407837\n",
      "Epoch 56: Train Loss: 0.19974125921726227, Validation Loss: 0.5799870491027832\n",
      "Epoch 57: Train Loss: 0.19736777245998383, Validation Loss: 0.5966416001319885\n",
      "Epoch 58: Train Loss: 0.20380447804927826, Validation Loss: 0.5908535718917847\n",
      "Epoch 59: Train Loss: 0.19726008673508963, Validation Loss: 0.5983536839485168\n",
      "Epoch 60: Train Loss: 0.2024716486533483, Validation Loss: 0.5372911095619202\n",
      "Epoch 61: Train Loss: 0.2114812582731247, Validation Loss: 0.4828905165195465\n",
      "Epoch 62: Train Loss: 0.20067635675271353, Validation Loss: 0.47017449140548706\n",
      "Epoch 63: Train Loss: 0.18502249817053476, Validation Loss: 0.5216897130012512\n",
      "Epoch 64: Train Loss: 0.19803842902183533, Validation Loss: 0.5156143307685852\n",
      "Epoch 65: Train Loss: 0.17399486899375916, Validation Loss: 0.49015259742736816\n",
      "Epoch 66: Train Loss: 0.16624756157398224, Validation Loss: 0.47370538115501404\n",
      "Epoch 67: Train Loss: 0.16575792928536734, Validation Loss: 0.4598708748817444\n",
      "Epoch 68: Train Loss: 0.1642275849978129, Validation Loss: 0.4525023400783539\n",
      "Epoch 69: Train Loss: 0.19782703121503195, Validation Loss: 0.45254677534103394\n",
      "Epoch 70: Train Loss: 0.17598397533098856, Validation Loss: 0.4256230592727661\n",
      "Epoch 71: Train Loss: 0.16812717417875925, Validation Loss: 0.4386116862297058\n",
      "Epoch 72: Train Loss: 0.16586574415365854, Validation Loss: 0.4880375266075134\n",
      "Epoch 73: Train Loss: 0.17953715721766153, Validation Loss: 0.46240100264549255\n",
      "Epoch 74: Train Loss: 0.1613109235962232, Validation Loss: 0.4705309569835663\n",
      "Epoch 75: Train Loss: 0.15336284538110098, Validation Loss: 0.47560933232307434\n",
      "Epoch 76: Train Loss: 0.15579937398433685, Validation Loss: 0.46923699975013733\n",
      "Epoch 77: Train Loss: 0.14175818612178168, Validation Loss: 0.4672762453556061\n",
      "Epoch 78: Train Loss: 0.1453587462504705, Validation Loss: 0.4655047357082367\n",
      "Epoch 79: Train Loss: 0.14135989298423132, Validation Loss: 0.46613141894340515\n",
      "Epoch 80: Train Loss: 0.13860069712003073, Validation Loss: 0.44349542260169983\n",
      "Epoch 81: Train Loss: 0.13829506436983743, Validation Loss: 0.39982715249061584\n",
      "Epoch 82: Train Loss: 0.12906916936238608, Validation Loss: 0.41669774055480957\n",
      "Epoch 83: Train Loss: 0.13122150301933289, Validation Loss: 0.46196654438972473\n",
      "Epoch 84: Train Loss: 0.1233085145552953, Validation Loss: 0.49914148449897766\n",
      "Epoch 85: Train Loss: 0.12305840104818344, Validation Loss: 0.45382562279701233\n",
      "Epoch 86: Train Loss: 0.12949537485837936, Validation Loss: 0.43658924102783203\n",
      "Epoch 87: Train Loss: 0.11677907903989156, Validation Loss: 0.4355909824371338\n",
      "Epoch 88: Train Loss: 0.11418157815933228, Validation Loss: 0.4330073297023773\n",
      "Epoch 89: Train Loss: 0.11697811384995778, Validation Loss: 0.43331336975097656\n",
      "Epoch 90: Train Loss: 0.12066218256950378, Validation Loss: 0.469814658164978\n",
      "Epoch 91: Train Loss: 0.1395372524857521, Validation Loss: 0.48041006922721863\n",
      "Epoch 92: Train Loss: 0.11357851326465607, Validation Loss: 0.4544500708580017\n",
      "Epoch 93: Train Loss: 0.13938334584236145, Validation Loss: 0.45461541414260864\n",
      "Epoch 94: Train Loss: 0.11516332626342773, Validation Loss: 0.44068169593811035\n",
      "Epoch 95: Train Loss: 0.11513447761535645, Validation Loss: 0.4568086266517639\n",
      "Epoch 96: Train Loss: 0.11613441507021587, Validation Loss: 0.4898851215839386\n",
      "Epoch 97: Train Loss: 0.10156749188899994, Validation Loss: 0.4833458662033081\n",
      "Epoch 98: Train Loss: 0.10528505841890971, Validation Loss: 0.4768696427345276\n",
      "Epoch 99: Train Loss: 0.12040720631678899, Validation Loss: 0.4727397859096527\n",
      "Epoch 100: Train Loss: 0.09671139717102051, Validation Loss: 0.40801918506622314\n",
      "Epoch 101: Train Loss: 0.10886541257301967, Validation Loss: 0.4448640048503876\n",
      "Epoch 102: Train Loss: 0.10853888342777888, Validation Loss: 0.41995835304260254\n",
      "Epoch 103: Train Loss: 0.09881133089462917, Validation Loss: 0.4271896481513977\n",
      "Epoch 104: Train Loss: 0.10593541711568832, Validation Loss: 0.4769364595413208\n",
      "Epoch 105: Train Loss: 0.10954611251751582, Validation Loss: 0.5297297239303589\n",
      "Epoch 106: Train Loss: 0.09555907547473907, Validation Loss: 0.4759569466114044\n",
      "Epoch 107: Train Loss: 0.0988008330265681, Validation Loss: 0.4506504237651825\n",
      "Epoch 108: Train Loss: 0.10914286722739537, Validation Loss: 0.4351225197315216\n",
      "Epoch 109: Train Loss: 0.09012337774038315, Validation Loss: 0.4372473359107971\n",
      "Epoch 110: Train Loss: 0.09815689673026402, Validation Loss: 0.47300389409065247\n",
      "Epoch 111: Train Loss: 0.10373571515083313, Validation Loss: 0.45651668310165405\n",
      "Epoch 112: Train Loss: 0.0902515451113383, Validation Loss: 0.4193723499774933\n",
      "Epoch 113: Train Loss: 0.09792209664980571, Validation Loss: 0.457345187664032\n",
      "Epoch 114: Train Loss: 0.09659953912099202, Validation Loss: 0.5111669898033142\n",
      "Epoch 115: Train Loss: 0.09810239573319753, Validation Loss: 0.45330944657325745\n",
      "Epoch 116: Train Loss: 0.08477662255366643, Validation Loss: 0.43322423100471497\n",
      "Epoch 117: Train Loss: 0.08482882628838222, Validation Loss: 0.4286104440689087\n",
      "Epoch 118: Train Loss: 0.08117996901273727, Validation Loss: 0.4274766445159912\n",
      "Epoch 119: Train Loss: 0.08530604839324951, Validation Loss: 0.42705675959587097\n",
      "Epoch 120: Train Loss: 0.07622202237447102, Validation Loss: 0.4528396725654602\n",
      "Epoch 121: Train Loss: 0.07907989869515102, Validation Loss: 0.450832724571228\n",
      "Epoch 122: Train Loss: 0.08543262382348378, Validation Loss: 0.47573772072792053\n",
      "Epoch 123: Train Loss: 0.08190630376338959, Validation Loss: 0.4752005934715271\n",
      "Epoch 124: Train Loss: 0.08277196437120438, Validation Loss: 0.448852002620697\n",
      "Epoch 125: Train Loss: 0.08864887058734894, Validation Loss: 0.4487404525279999\n",
      "Epoch 126: Train Loss: 0.07438127075632413, Validation Loss: 0.453910231590271\n",
      "Epoch 127: Train Loss: 0.0728813111782074, Validation Loss: 0.4534229636192322\n",
      "Epoch 128: Train Loss: 0.06814821188648541, Validation Loss: 0.45108699798583984\n",
      "Epoch 129: Train Loss: 0.07042700797319412, Validation Loss: 0.45231449604034424\n",
      "Epoch 130: Train Loss: 0.09121765941381454, Validation Loss: 0.535271942615509\n",
      "Epoch 131: Train Loss: 0.07345162332057953, Validation Loss: 0.5846936106681824\n",
      "Epoch 132: Train Loss: 0.08384014914433162, Validation Loss: 0.5452125668525696\n",
      "Epoch 133: Train Loss: 0.07392631719509761, Validation Loss: 0.46429747343063354\n",
      "Epoch 134: Train Loss: 0.07232997318108876, Validation Loss: 0.42432424426078796\n",
      "Epoch 135: Train Loss: 0.07404284924268723, Validation Loss: 0.4412654638290405\n",
      "Epoch 136: Train Loss: 0.07358011603355408, Validation Loss: 0.4708653688430786\n",
      "Epoch 137: Train Loss: 0.07030979171395302, Validation Loss: 0.4866000711917877\n",
      "Epoch 138: Train Loss: 0.06960753972331683, Validation Loss: 0.48892149329185486\n",
      "Epoch 139: Train Loss: 0.08220462252696355, Validation Loss: 0.48189643025398254\n",
      "Epoch 140: Train Loss: 0.06778827557961146, Validation Loss: 0.45800623297691345\n",
      "Epoch 141: Train Loss: 0.07547205686569214, Validation Loss: 0.4828881323337555\n",
      "Epoch 142: Train Loss: 0.07991958409547806, Validation Loss: 0.5291363596916199\n",
      "Epoch 143: Train Loss: 0.06917430832982063, Validation Loss: 0.48638737201690674\n",
      "Epoch 144: Train Loss: 0.06983371327320735, Validation Loss: 0.46441546082496643\n",
      "Epoch 145: Train Loss: 0.06445718308289845, Validation Loss: 0.4723423719406128\n",
      "Epoch 146: Train Loss: 0.07762693117062251, Validation Loss: 0.48733147978782654\n",
      "Epoch 147: Train Loss: 0.06336871162056923, Validation Loss: 0.5096144676208496\n",
      "Epoch 148: Train Loss: 0.06814559797445933, Validation Loss: 0.5114343166351318\n",
      "Epoch 149: Train Loss: 0.060658144454161324, Validation Loss: 0.5062369108200073\n",
      "Epoch 150: Train Loss: 0.06303634122014046, Validation Loss: 0.46195000410079956\n",
      "Epoch 151: Train Loss: 0.0602886937558651, Validation Loss: 0.4579460918903351\n",
      "Epoch 152: Train Loss: 0.058358182509740196, Validation Loss: 0.47602763772010803\n",
      "Epoch 153: Train Loss: 0.062278603514035545, Validation Loss: 0.46596652269363403\n",
      "Epoch 154: Train Loss: 0.057885212202866874, Validation Loss: 0.4451836943626404\n",
      "Epoch 155: Train Loss: 0.05407893533507983, Validation Loss: 0.4446314871311188\n",
      "Epoch 156: Train Loss: 0.055887063344319664, Validation Loss: 0.45083487033843994\n",
      "Epoch 157: Train Loss: 0.0559544675052166, Validation Loss: 0.46176305413246155\n",
      "Epoch 158: Train Loss: 0.051375798881053925, Validation Loss: 0.45836007595062256\n",
      "Epoch 159: Train Loss: 0.05726365000009537, Validation Loss: 0.4562903940677643\n",
      "Epoch 160: Train Loss: 0.06758484865228336, Validation Loss: 0.46389973163604736\n",
      "Epoch 161: Train Loss: 0.07770490025480588, Validation Loss: 0.5532958507537842\n",
      "Epoch 162: Train Loss: 0.059424933046102524, Validation Loss: 0.5769832134246826\n",
      "Epoch 163: Train Loss: 0.0581942026813825, Validation Loss: 0.49414336681365967\n",
      "Epoch 164: Train Loss: 0.05818425491452217, Validation Loss: 0.48437026143074036\n",
      "Epoch 165: Train Loss: 0.05511028319597244, Validation Loss: 0.48427051305770874\n",
      "Epoch 166: Train Loss: 0.05683701733748118, Validation Loss: 0.5041342377662659\n",
      "Epoch 167: Train Loss: 0.05514022707939148, Validation Loss: 0.5018284320831299\n",
      "Epoch 168: Train Loss: 0.05042035132646561, Validation Loss: 0.5012937784194946\n",
      "Epoch 169: Train Loss: 0.05270396297176679, Validation Loss: 0.49957138299942017\n",
      "Epoch 170: Train Loss: 0.05598384762803713, Validation Loss: 0.4520159363746643\n",
      "Epoch 171: Train Loss: 0.05862431849042574, Validation Loss: 0.4809074401855469\n",
      "Epoch 172: Train Loss: 0.04861209293206533, Validation Loss: 0.4876846671104431\n",
      "Epoch 173: Train Loss: 0.04903337421516577, Validation Loss: 0.47176122665405273\n",
      "Epoch 174: Train Loss: 0.05559938525160154, Validation Loss: 0.4498959481716156\n",
      "Epoch 175: Train Loss: 0.04719094311197599, Validation Loss: 0.45207569003105164\n",
      "Epoch 176: Train Loss: 0.045681923627853394, Validation Loss: 0.47811517119407654\n",
      "Epoch 177: Train Loss: 0.05560224068661531, Validation Loss: 0.4905422031879425\n",
      "Epoch 178: Train Loss: 0.04618668556213379, Validation Loss: 0.49883946776390076\n",
      "Epoch 179: Train Loss: 0.04969130828976631, Validation Loss: 0.5005567073822021\n",
      "Epoch 180: Train Loss: 0.042858559638261795, Validation Loss: 0.5099241733551025\n",
      "Epoch 181: Train Loss: 0.050799320141474404, Validation Loss: 0.5147311091423035\n",
      "Epoch 182: Train Loss: 0.04667466754714648, Validation Loss: 0.5014099478721619\n",
      "Epoch 183: Train Loss: 0.06141561331848303, Validation Loss: 0.502826452255249\n",
      "Epoch 184: Train Loss: 0.04607662186026573, Validation Loss: 0.558205246925354\n",
      "Epoch 185: Train Loss: 0.05239793658256531, Validation Loss: 0.5026566386222839\n",
      "Epoch 186: Train Loss: 0.05188606182734171, Validation Loss: 0.4651174545288086\n",
      "Epoch 187: Train Loss: 0.04536510383089384, Validation Loss: 0.46430346369743347\n",
      "Epoch 188: Train Loss: 0.04284479282796383, Validation Loss: 0.47173529863357544\n",
      "Epoch 189: Train Loss: 0.04420066252350807, Validation Loss: 0.4766564965248108\n",
      "Epoch 190: Train Loss: 0.04840087021390597, Validation Loss: 0.578704297542572\n",
      "Epoch 191: Train Loss: 0.05675578489899635, Validation Loss: 0.621173083782196\n",
      "Epoch 192: Train Loss: 0.04522675151626269, Validation Loss: 0.5206606984138489\n",
      "Epoch 193: Train Loss: 0.049639554073413215, Validation Loss: 0.47100749611854553\n",
      "Epoch 194: Train Loss: 0.042275745421648026, Validation Loss: 0.4747724235057831\n",
      "Epoch 195: Train Loss: 0.04224421953161558, Validation Loss: 0.4938206076622009\n",
      "Epoch 196: Train Loss: 0.04669704226156076, Validation Loss: 0.48601052165031433\n",
      "Epoch 197: Train Loss: 0.060150460650523506, Validation Loss: 0.4643242359161377\n",
      "Epoch 198: Train Loss: 0.042511935656269394, Validation Loss: 0.465151309967041\n",
      "Epoch 199: Train Loss: 0.04213506542146206, Validation Loss: 0.4630237817764282\n",
      "Fold 3\n",
      "Epoch 0: Train Loss: 0.6804497241973877, Validation Loss: 0.6630085110664368\n",
      "Epoch 1: Train Loss: 0.6120573282241821, Validation Loss: 0.6581000685691833\n",
      "Epoch 2: Train Loss: 0.5906813542048136, Validation Loss: 0.6554563045501709\n",
      "Epoch 3: Train Loss: 0.560797373453776, Validation Loss: 0.6554808020591736\n",
      "Epoch 4: Train Loss: 0.5568894942601522, Validation Loss: 0.6553106307983398\n",
      "Epoch 5: Train Loss: 0.5394837458928426, Validation Loss: 0.6551315188407898\n",
      "Epoch 6: Train Loss: 0.5424326658248901, Validation Loss: 0.6559613943099976\n",
      "Epoch 7: Train Loss: 0.5314535101254781, Validation Loss: 0.6569061875343323\n",
      "Epoch 8: Train Loss: 0.5337267716725668, Validation Loss: 0.6580652594566345\n",
      "Epoch 9: Train Loss: 0.5252039035161337, Validation Loss: 0.6601749658584595\n",
      "Epoch 10: Train Loss: 0.5179632504781088, Validation Loss: 0.6536254286766052\n",
      "Epoch 11: Train Loss: 0.49971744418144226, Validation Loss: 0.6356368064880371\n",
      "Epoch 12: Train Loss: 0.4891338249047597, Validation Loss: 0.6139096021652222\n",
      "Epoch 13: Train Loss: 0.4610843559106191, Validation Loss: 0.6017107963562012\n",
      "Epoch 14: Train Loss: 0.4591912031173706, Validation Loss: 0.5907781720161438\n",
      "Epoch 15: Train Loss: 0.44542451699574787, Validation Loss: 0.5830500721931458\n",
      "Epoch 16: Train Loss: 0.43968509634335834, Validation Loss: 0.5857649445533752\n",
      "Epoch 17: Train Loss: 0.44322968522707623, Validation Loss: 0.5852352380752563\n",
      "Epoch 18: Train Loss: 0.4293496012687683, Validation Loss: 0.5825825333595276\n",
      "Epoch 19: Train Loss: 0.4271329840024312, Validation Loss: 0.5799187421798706\n",
      "Epoch 20: Train Loss: 0.42499833305676776, Validation Loss: 0.6007053256034851\n",
      "Epoch 21: Train Loss: 0.42040036122004193, Validation Loss: 0.6304000616073608\n",
      "Epoch 22: Train Loss: 0.40448154012362164, Validation Loss: 0.6711370944976807\n",
      "Epoch 23: Train Loss: 0.38563599189122516, Validation Loss: 0.6520536541938782\n",
      "Epoch 24: Train Loss: 0.3779184917608897, Validation Loss: 0.6528428792953491\n",
      "Epoch 25: Train Loss: 0.36227451761563617, Validation Loss: 0.6514156460762024\n",
      "Epoch 26: Train Loss: 0.35996153950691223, Validation Loss: 0.6460798978805542\n",
      "Epoch 27: Train Loss: 0.37713801860809326, Validation Loss: 0.6369926333427429\n",
      "Epoch 28: Train Loss: 0.35957959294319153, Validation Loss: 0.6329096555709839\n",
      "Epoch 29: Train Loss: 0.35912448167800903, Validation Loss: 0.6355111002922058\n",
      "Epoch 30: Train Loss: 0.3484293520450592, Validation Loss: 0.6145204305648804\n",
      "Epoch 31: Train Loss: 0.3453102906545003, Validation Loss: 0.5905879139900208\n",
      "Epoch 32: Train Loss: 0.33882102370262146, Validation Loss: 0.6113725900650024\n",
      "Epoch 33: Train Loss: 0.33361849188804626, Validation Loss: 0.6511532068252563\n",
      "Epoch 34: Train Loss: 0.32607786854108173, Validation Loss: 0.6905539035797119\n",
      "Epoch 35: Train Loss: 0.32126439611117047, Validation Loss: 0.6981557011604309\n",
      "Epoch 36: Train Loss: 0.3082532584667206, Validation Loss: 0.6796728372573853\n",
      "Epoch 37: Train Loss: 0.2985249658425649, Validation Loss: 0.6745656132698059\n",
      "Epoch 38: Train Loss: 0.2993210256099701, Validation Loss: 0.6575583219528198\n",
      "Epoch 39: Train Loss: 0.31345394253730774, Validation Loss: 0.6545644998550415\n",
      "Epoch 40: Train Loss: 0.31023871898651123, Validation Loss: 0.6820504665374756\n",
      "Epoch 41: Train Loss: 0.3280392388502757, Validation Loss: 0.6483921408653259\n",
      "Epoch 42: Train Loss: 0.31087586283683777, Validation Loss: 0.5725444555282593\n",
      "Epoch 43: Train Loss: 0.278642863035202, Validation Loss: 0.5347413420677185\n",
      "Epoch 44: Train Loss: 0.27145321170488995, Validation Loss: 0.5523731112480164\n",
      "Epoch 45: Train Loss: 0.27383385598659515, Validation Loss: 0.5701472163200378\n",
      "Epoch 46: Train Loss: 0.2716738084952037, Validation Loss: 0.6082330346107483\n",
      "Epoch 47: Train Loss: 0.2684296866257985, Validation Loss: 0.6367804408073425\n",
      "Epoch 48: Train Loss: 0.2599014590183894, Validation Loss: 0.6333116292953491\n",
      "Epoch 49: Train Loss: 0.27433369557062787, Validation Loss: 0.6254352927207947\n",
      "Epoch 50: Train Loss: 0.25717389583587646, Validation Loss: 0.5803566575050354\n",
      "Epoch 51: Train Loss: 0.24475465714931488, Validation Loss: 0.5817619562149048\n",
      "Epoch 52: Train Loss: 0.2699621319770813, Validation Loss: 0.5334039926528931\n",
      "Epoch 53: Train Loss: 0.2489392509063085, Validation Loss: 0.5408029556274414\n",
      "Epoch 54: Train Loss: 0.24188581109046936, Validation Loss: 0.6051272749900818\n",
      "Epoch 55: Train Loss: 0.22777697443962097, Validation Loss: 0.6121001243591309\n",
      "Epoch 56: Train Loss: 0.21724533041318259, Validation Loss: 0.5721914172172546\n",
      "Epoch 57: Train Loss: 0.22085569302241007, Validation Loss: 0.5329095125198364\n",
      "Epoch 58: Train Loss: 0.2260312338670095, Validation Loss: 0.531686544418335\n",
      "Epoch 59: Train Loss: 0.22181108593940735, Validation Loss: 0.534465491771698\n",
      "Epoch 60: Train Loss: 0.22245576481024423, Validation Loss: 0.48016831278800964\n",
      "Epoch 61: Train Loss: 0.23483316600322723, Validation Loss: 0.48709768056869507\n",
      "Epoch 62: Train Loss: 0.20889157056808472, Validation Loss: 0.5338282585144043\n",
      "Epoch 63: Train Loss: 0.20250972112019858, Validation Loss: 0.5396498441696167\n",
      "Epoch 64: Train Loss: 0.19591833651065826, Validation Loss: 0.49530917406082153\n",
      "Epoch 65: Train Loss: 0.19388542075951895, Validation Loss: 0.5101357102394104\n",
      "Epoch 66: Train Loss: 0.19647233684857687, Validation Loss: 0.5095416307449341\n",
      "Epoch 67: Train Loss: 0.20036581158638, Validation Loss: 0.5057454109191895\n",
      "Epoch 68: Train Loss: 0.1830903341372808, Validation Loss: 0.4948146641254425\n",
      "Epoch 69: Train Loss: 0.18702146907647452, Validation Loss: 0.4862648546695709\n",
      "Epoch 70: Train Loss: 0.21131330728530884, Validation Loss: 0.44620126485824585\n",
      "Epoch 71: Train Loss: 0.1871762921412786, Validation Loss: 0.4241005778312683\n",
      "Epoch 72: Train Loss: 0.2108677178621292, Validation Loss: 0.42430415749549866\n",
      "Epoch 73: Train Loss: 0.18320450683434805, Validation Loss: 0.42450541257858276\n",
      "Epoch 74: Train Loss: 0.17789819339911142, Validation Loss: 0.42883193492889404\n",
      "Epoch 75: Train Loss: 0.18695734441280365, Validation Loss: 0.44396448135375977\n",
      "Epoch 76: Train Loss: 0.16793555517991385, Validation Loss: 0.4503827691078186\n",
      "Epoch 77: Train Loss: 0.1785182555516561, Validation Loss: 0.44635090231895447\n",
      "Epoch 78: Train Loss: 0.1865107168753942, Validation Loss: 0.43942561745643616\n",
      "Epoch 79: Train Loss: 0.16344340393940607, Validation Loss: 0.43958115577697754\n",
      "Epoch 80: Train Loss: 0.17306950688362122, Validation Loss: 0.44110846519470215\n",
      "Epoch 81: Train Loss: 0.16854038834571838, Validation Loss: 0.45919424295425415\n",
      "Epoch 82: Train Loss: 0.15735086798667908, Validation Loss: 0.4993058145046234\n",
      "Epoch 83: Train Loss: 0.15582622587680817, Validation Loss: 0.48510488867759705\n",
      "Epoch 84: Train Loss: 0.14991564551989237, Validation Loss: 0.43866610527038574\n",
      "Epoch 85: Train Loss: 0.14237685253222784, Validation Loss: 0.43508708477020264\n",
      "Epoch 86: Train Loss: 0.15277455747127533, Validation Loss: 0.4395048916339874\n",
      "Epoch 87: Train Loss: 0.15360361337661743, Validation Loss: 0.4404747784137726\n",
      "Epoch 88: Train Loss: 0.13849753886461258, Validation Loss: 0.4410306215286255\n",
      "Epoch 89: Train Loss: 0.14695726335048676, Validation Loss: 0.4431135356426239\n",
      "Epoch 90: Train Loss: 0.13795174161593118, Validation Loss: 0.45206496119499207\n",
      "Epoch 91: Train Loss: 0.1376365746061007, Validation Loss: 0.4470595419406891\n",
      "Epoch 92: Train Loss: 0.15841785073280334, Validation Loss: 0.440415620803833\n",
      "Epoch 93: Train Loss: 0.13686769704023996, Validation Loss: 0.45743465423583984\n",
      "Epoch 94: Train Loss: 0.1439563607176145, Validation Loss: 0.4751153886318207\n",
      "Epoch 95: Train Loss: 0.12983193745215735, Validation Loss: 0.4725192189216614\n",
      "Epoch 96: Train Loss: 0.13024695217609406, Validation Loss: 0.46853259205818176\n",
      "Epoch 97: Train Loss: 0.12919611980517706, Validation Loss: 0.4662274718284607\n",
      "Epoch 98: Train Loss: 0.12420388062795003, Validation Loss: 0.4641653597354889\n",
      "Epoch 99: Train Loss: 0.12860787411530814, Validation Loss: 0.46373429894447327\n",
      "Epoch 100: Train Loss: 0.1316243683298429, Validation Loss: 0.4893025755882263\n",
      "Epoch 101: Train Loss: 0.15710417181253433, Validation Loss: 0.4524116516113281\n",
      "Epoch 102: Train Loss: 0.14424372712771097, Validation Loss: 0.46373334527015686\n",
      "Epoch 103: Train Loss: 0.16833103199799856, Validation Loss: 0.5221335887908936\n",
      "Epoch 104: Train Loss: 0.1381898894906044, Validation Loss: 0.48817920684814453\n",
      "Epoch 105: Train Loss: 0.12193645536899567, Validation Loss: 0.4852399230003357\n",
      "Epoch 106: Train Loss: 0.1222634216149648, Validation Loss: 0.48579972982406616\n",
      "Epoch 107: Train Loss: 0.11507515609264374, Validation Loss: 0.48454877734184265\n",
      "Epoch 108: Train Loss: 0.11480991542339325, Validation Loss: 0.4826622009277344\n",
      "Epoch 109: Train Loss: 0.1123790293931961, Validation Loss: 0.482757031917572\n",
      "Epoch 110: Train Loss: 0.11923373987277348, Validation Loss: 0.49836039543151855\n",
      "Epoch 111: Train Loss: 0.11364151040712993, Validation Loss: 0.4775373935699463\n",
      "Epoch 112: Train Loss: 0.11121760308742523, Validation Loss: 0.49144691228866577\n",
      "Epoch 113: Train Loss: 0.10556056847174962, Validation Loss: 0.47435441613197327\n",
      "Epoch 114: Train Loss: 0.11535167694091797, Validation Loss: 0.46991655230522156\n",
      "Epoch 115: Train Loss: 0.11197152733802795, Validation Loss: 0.47266870737075806\n",
      "Epoch 116: Train Loss: 0.10605457921822865, Validation Loss: 0.48036396503448486\n",
      "Epoch 117: Train Loss: 0.10136872281630833, Validation Loss: 0.48770368099212646\n",
      "Epoch 118: Train Loss: 0.10901796321074168, Validation Loss: 0.486984521150589\n",
      "Epoch 119: Train Loss: 0.09735370675722758, Validation Loss: 0.4903835654258728\n",
      "Epoch 120: Train Loss: 0.11622818062702815, Validation Loss: 0.46854203939437866\n",
      "Epoch 121: Train Loss: 0.10682216535011928, Validation Loss: 0.482128769159317\n",
      "Epoch 122: Train Loss: 0.11866579949855804, Validation Loss: 0.5053431391716003\n",
      "Epoch 123: Train Loss: 0.12012456854184468, Validation Loss: 0.48900306224823\n",
      "Epoch 124: Train Loss: 0.10844186693429947, Validation Loss: 0.5009490251541138\n",
      "Epoch 125: Train Loss: 0.1283660133679708, Validation Loss: 0.5503439903259277\n",
      "Epoch 126: Train Loss: 0.11085201054811478, Validation Loss: 0.5546205639839172\n",
      "Epoch 127: Train Loss: 0.1053933451573054, Validation Loss: 0.5366855263710022\n",
      "Epoch 128: Train Loss: 0.0832902913292249, Validation Loss: 0.5249046683311462\n",
      "Epoch 129: Train Loss: 0.09438615292310715, Validation Loss: 0.5186595320701599\n",
      "Epoch 130: Train Loss: 0.0938633605837822, Validation Loss: 0.5120488405227661\n",
      "Epoch 131: Train Loss: 0.094914510846138, Validation Loss: 0.5419537425041199\n",
      "Epoch 132: Train Loss: 0.11769820004701614, Validation Loss: 0.5381211042404175\n",
      "Epoch 133: Train Loss: 0.08132476856311162, Validation Loss: 0.5202273726463318\n",
      "Epoch 134: Train Loss: 0.09119925896326701, Validation Loss: 0.5201634168624878\n",
      "Epoch 135: Train Loss: 0.07294211536645889, Validation Loss: 0.5395336747169495\n",
      "Epoch 136: Train Loss: 0.07150796428322792, Validation Loss: 0.5462554693222046\n",
      "Epoch 137: Train Loss: 0.0752926121155421, Validation Loss: 0.5352274179458618\n",
      "Epoch 138: Train Loss: 0.0731722538669904, Validation Loss: 0.5346016883850098\n",
      "Epoch 139: Train Loss: 0.07138752813140552, Validation Loss: 0.5369786024093628\n",
      "Epoch 140: Train Loss: 0.0802033469080925, Validation Loss: 0.5125594735145569\n",
      "Epoch 141: Train Loss: 0.0822434996565183, Validation Loss: 0.5068586468696594\n",
      "Epoch 142: Train Loss: 0.07506955539186795, Validation Loss: 0.5320417284965515\n",
      "Epoch 143: Train Loss: 0.07316858073075612, Validation Loss: 0.5287579298019409\n",
      "Epoch 144: Train Loss: 0.07750935355822246, Validation Loss: 0.5169556140899658\n",
      "Epoch 145: Train Loss: 0.07459131628274918, Validation Loss: 0.5244914889335632\n",
      "Epoch 146: Train Loss: 0.06880390768249829, Validation Loss: 0.536950945854187\n",
      "Epoch 147: Train Loss: 0.06344829872250557, Validation Loss: 0.5379055738449097\n",
      "Epoch 148: Train Loss: 0.06146275872985522, Validation Loss: 0.5348824858665466\n",
      "Epoch 149: Train Loss: 0.0629763590792815, Validation Loss: 0.5348031520843506\n",
      "Epoch 150: Train Loss: 0.07218499854207039, Validation Loss: 0.540778636932373\n",
      "Epoch 151: Train Loss: 0.07101104905207951, Validation Loss: 0.5413818955421448\n",
      "Epoch 152: Train Loss: 0.07155896847446759, Validation Loss: 0.540551483631134\n",
      "Epoch 153: Train Loss: 0.06824603180090587, Validation Loss: 0.5477864146232605\n",
      "Epoch 154: Train Loss: 0.056462338815132775, Validation Loss: 0.5631814002990723\n",
      "Epoch 155: Train Loss: 0.05672010282675425, Validation Loss: 0.5549814105033875\n",
      "Epoch 156: Train Loss: 0.05344964936375618, Validation Loss: 0.5608421564102173\n",
      "Epoch 157: Train Loss: 0.07192288090785344, Validation Loss: 0.564504086971283\n",
      "Epoch 158: Train Loss: 0.06069497267405192, Validation Loss: 0.5583869218826294\n",
      "Epoch 159: Train Loss: 0.053743816912174225, Validation Loss: 0.5566971898078918\n",
      "Epoch 160: Train Loss: 0.06457336371143658, Validation Loss: 0.5476585030555725\n",
      "Epoch 161: Train Loss: 0.06091145301858584, Validation Loss: 0.5528345704078674\n",
      "Epoch 162: Train Loss: 0.06200943390528361, Validation Loss: 0.563501238822937\n",
      "Epoch 163: Train Loss: 0.050922137995560966, Validation Loss: 0.5583720803260803\n",
      "Epoch 164: Train Loss: 0.05493738874793053, Validation Loss: 0.5720787048339844\n",
      "Epoch 165: Train Loss: 0.058796416968107224, Validation Loss: 0.5670435428619385\n",
      "Epoch 166: Train Loss: 0.05349964275956154, Validation Loss: 0.5665432214736938\n",
      "Epoch 167: Train Loss: 0.05362652366360029, Validation Loss: 0.5730962753295898\n",
      "Epoch 168: Train Loss: 0.04867349689205488, Validation Loss: 0.5727476477622986\n",
      "Epoch 169: Train Loss: 0.05669265488783518, Validation Loss: 0.5808873772621155\n",
      "Epoch 170: Train Loss: 0.04901228845119476, Validation Loss: 0.5842046141624451\n",
      "Epoch 171: Train Loss: 0.057445596903562546, Validation Loss: 0.5919345617294312\n",
      "Epoch 172: Train Loss: 0.05089752872784933, Validation Loss: 0.6063894033432007\n",
      "Epoch 173: Train Loss: 0.05810618648926417, Validation Loss: 0.5896403789520264\n",
      "Epoch 174: Train Loss: 0.055940729876359306, Validation Loss: 0.5889937877655029\n",
      "Epoch 175: Train Loss: 0.04983539630969366, Validation Loss: 0.595832347869873\n",
      "Epoch 176: Train Loss: 0.04150370880961418, Validation Loss: 0.6237062215805054\n",
      "Epoch 177: Train Loss: 0.04557077959179878, Validation Loss: 0.6286574006080627\n",
      "Epoch 178: Train Loss: 0.04929924011230469, Validation Loss: 0.6257447004318237\n",
      "Epoch 179: Train Loss: 0.05080000932017962, Validation Loss: 0.6291412115097046\n",
      "Epoch 180: Train Loss: 0.04254524161418279, Validation Loss: 0.6167946457862854\n",
      "Epoch 181: Train Loss: 0.05214362343152364, Validation Loss: 0.6084693074226379\n",
      "Epoch 182: Train Loss: 0.0457480326294899, Validation Loss: 0.6073697209358215\n",
      "Epoch 183: Train Loss: 0.05216644207636515, Validation Loss: 0.5833848714828491\n",
      "Epoch 184: Train Loss: 0.04629152392347654, Validation Loss: 0.5686525106430054\n",
      "Epoch 185: Train Loss: 0.042313517381747566, Validation Loss: 0.5730341672897339\n",
      "Epoch 186: Train Loss: 0.043785897394021354, Validation Loss: 0.5747016668319702\n",
      "Epoch 187: Train Loss: 0.041670069098472595, Validation Loss: 0.567308247089386\n",
      "Epoch 188: Train Loss: 0.03878135606646538, Validation Loss: 0.5825605392456055\n",
      "Epoch 189: Train Loss: 0.03772147744894028, Validation Loss: 0.5908090472221375\n",
      "Epoch 190: Train Loss: 0.05546086529890696, Validation Loss: 0.6581651568412781\n",
      "Epoch 191: Train Loss: 0.041161363323529564, Validation Loss: 0.679949939250946\n",
      "Epoch 192: Train Loss: 0.04181370884180069, Validation Loss: 0.6611425876617432\n",
      "Epoch 193: Train Loss: 0.03794428581992785, Validation Loss: 0.6517657041549683\n",
      "Epoch 194: Train Loss: 0.036276016384363174, Validation Loss: 0.6437554955482483\n",
      "Epoch 195: Train Loss: 0.03426149611671766, Validation Loss: 0.6441448926925659\n",
      "Epoch 196: Train Loss: 0.0378613801052173, Validation Loss: 0.6562306880950928\n",
      "Epoch 197: Train Loss: 0.035845366617043815, Validation Loss: 0.6498738527297974\n",
      "Epoch 198: Train Loss: 0.03595456356803576, Validation Loss: 0.6318523287773132\n",
      "Epoch 199: Train Loss: 0.05147969660659631, Validation Loss: 0.6408272385597229\n",
      "Fold 4\n",
      "Epoch 0: Train Loss: 0.7199166417121887, Validation Loss: 0.6963354349136353\n",
      "Epoch 1: Train Loss: 0.6691357890764872, Validation Loss: 0.6941315531730652\n",
      "Epoch 2: Train Loss: 0.6327934463818868, Validation Loss: 0.6924113035202026\n",
      "Epoch 3: Train Loss: 0.601449728012085, Validation Loss: 0.6910151243209839\n",
      "Epoch 4: Train Loss: 0.5893680850664774, Validation Loss: 0.6902875900268555\n",
      "Epoch 5: Train Loss: 0.5689035058021545, Validation Loss: 0.6895666122436523\n",
      "Epoch 6: Train Loss: 0.5565517346064249, Validation Loss: 0.6887486577033997\n",
      "Epoch 7: Train Loss: 0.5540236234664917, Validation Loss: 0.6876497864723206\n",
      "Epoch 8: Train Loss: 0.552565336227417, Validation Loss: 0.687858521938324\n",
      "Epoch 9: Train Loss: 0.5564485589663187, Validation Loss: 0.6885449886322021\n",
      "Epoch 10: Train Loss: 0.5494755705197653, Validation Loss: 0.6773439645767212\n",
      "Epoch 11: Train Loss: 0.5284509062767029, Validation Loss: 0.6649473309516907\n",
      "Epoch 12: Train Loss: 0.5127239525318146, Validation Loss: 0.6566261053085327\n",
      "Epoch 13: Train Loss: 0.5038207074006399, Validation Loss: 0.6525663137435913\n",
      "Epoch 14: Train Loss: 0.48375972112019855, Validation Loss: 0.6484857201576233\n",
      "Epoch 15: Train Loss: 0.47017623980840045, Validation Loss: 0.6437365412712097\n",
      "Epoch 16: Train Loss: 0.4641699691613515, Validation Loss: 0.6392778754234314\n",
      "Epoch 17: Train Loss: 0.46529709299405414, Validation Loss: 0.6365960836410522\n",
      "Epoch 18: Train Loss: 0.47731852531433105, Validation Loss: 0.6364209055900574\n",
      "Epoch 19: Train Loss: 0.4607181449731191, Validation Loss: 0.6367098689079285\n",
      "Epoch 20: Train Loss: 0.45008522272109985, Validation Loss: 0.6211851835250854\n",
      "Epoch 21: Train Loss: 0.44056342045466107, Validation Loss: 0.6129428744316101\n",
      "Epoch 22: Train Loss: 0.436795840660731, Validation Loss: 0.6092384457588196\n",
      "Epoch 23: Train Loss: 0.42406509319941205, Validation Loss: 0.6051467657089233\n",
      "Epoch 24: Train Loss: 0.4005230963230133, Validation Loss: 0.6051170825958252\n",
      "Epoch 25: Train Loss: 0.3982400099436442, Validation Loss: 0.600975513458252\n",
      "Epoch 26: Train Loss: 0.4007137715816498, Validation Loss: 0.5995585918426514\n",
      "Epoch 27: Train Loss: 0.381249338388443, Validation Loss: 0.5949023962020874\n",
      "Epoch 28: Train Loss: 0.3811883330345154, Validation Loss: 0.592487633228302\n",
      "Epoch 29: Train Loss: 0.385701447725296, Validation Loss: 0.5916149020195007\n",
      "Epoch 30: Train Loss: 0.3877246578534444, Validation Loss: 0.5692866444587708\n",
      "Epoch 31: Train Loss: 0.36739055315653485, Validation Loss: 0.5593079328536987\n",
      "Epoch 32: Train Loss: 0.37518293658892315, Validation Loss: 0.5542213320732117\n",
      "Epoch 33: Train Loss: 0.3531218270460765, Validation Loss: 0.5571917295455933\n",
      "Epoch 34: Train Loss: 0.3363397816816966, Validation Loss: 0.5677299499511719\n",
      "Epoch 35: Train Loss: 0.35558493932088214, Validation Loss: 0.5628583431243896\n",
      "Epoch 36: Train Loss: 0.33041004339853924, Validation Loss: 0.5539580583572388\n",
      "Epoch 37: Train Loss: 0.3533131579558055, Validation Loss: 0.553259015083313\n",
      "Epoch 38: Train Loss: 0.32181187470753986, Validation Loss: 0.5532636642456055\n",
      "Epoch 39: Train Loss: 0.36397812763849896, Validation Loss: 0.552687406539917\n",
      "Epoch 40: Train Loss: 0.3244871993859609, Validation Loss: 0.5497284531593323\n",
      "Epoch 41: Train Loss: 0.31327370802561444, Validation Loss: 0.5527054667472839\n",
      "Epoch 42: Train Loss: 0.2990689476331075, Validation Loss: 0.5490836501121521\n",
      "Epoch 43: Train Loss: 0.30534205337365466, Validation Loss: 0.5444750785827637\n",
      "Epoch 44: Train Loss: 0.2906234959761302, Validation Loss: 0.5431157946586609\n",
      "Epoch 45: Train Loss: 0.3026123344898224, Validation Loss: 0.5283950567245483\n",
      "Epoch 46: Train Loss: 0.2864948312441508, Validation Loss: 0.5268900394439697\n",
      "Epoch 47: Train Loss: 0.2878422538439433, Validation Loss: 0.5252888202667236\n",
      "Epoch 48: Train Loss: 0.28478387991587323, Validation Loss: 0.5234293341636658\n",
      "Epoch 49: Train Loss: 0.2886248826980591, Validation Loss: 0.5247175097465515\n",
      "Epoch 50: Train Loss: 0.27701710164546967, Validation Loss: 0.5120529532432556\n",
      "Epoch 51: Train Loss: 0.277433305978775, Validation Loss: 0.5055691003799438\n",
      "Epoch 52: Train Loss: 0.27499451736609143, Validation Loss: 0.527162492275238\n",
      "Epoch 53: Train Loss: 0.26195456087589264, Validation Loss: 0.5198045969009399\n",
      "Epoch 54: Train Loss: 0.24862388769785562, Validation Loss: 0.49972981214523315\n",
      "Epoch 55: Train Loss: 0.2516280710697174, Validation Loss: 0.5024662017822266\n",
      "Epoch 56: Train Loss: 0.2487791726986567, Validation Loss: 0.49977943301200867\n",
      "Epoch 57: Train Loss: 0.2438703030347824, Validation Loss: 0.500800609588623\n",
      "Epoch 58: Train Loss: 0.23489629725615183, Validation Loss: 0.5023570656776428\n",
      "Epoch 59: Train Loss: 0.2409841020901998, Validation Loss: 0.5019561648368835\n",
      "Epoch 60: Train Loss: 0.23973880211512247, Validation Loss: 0.5071732997894287\n",
      "Epoch 61: Train Loss: 0.24461697041988373, Validation Loss: 0.5138403177261353\n",
      "Epoch 62: Train Loss: 0.23697551091512045, Validation Loss: 0.4777771234512329\n",
      "Epoch 63: Train Loss: 0.2268005112806956, Validation Loss: 0.4790259003639221\n",
      "Epoch 64: Train Loss: 0.2204629530509313, Validation Loss: 0.49154847860336304\n",
      "Epoch 65: Train Loss: 0.21308709184328714, Validation Loss: 0.49873214960098267\n",
      "Epoch 66: Train Loss: 0.21933158238728842, Validation Loss: 0.500767707824707\n",
      "Epoch 67: Train Loss: 0.23197747270266214, Validation Loss: 0.49530553817749023\n",
      "Epoch 68: Train Loss: 0.21235612034797668, Validation Loss: 0.493783563375473\n",
      "Epoch 69: Train Loss: 0.22337564329306284, Validation Loss: 0.4934772551059723\n",
      "Epoch 70: Train Loss: 0.22549158334732056, Validation Loss: 0.4823864698410034\n",
      "Epoch 71: Train Loss: 0.22526223957538605, Validation Loss: 0.4937320649623871\n",
      "Epoch 72: Train Loss: 0.2176295965909958, Validation Loss: 0.4879015386104584\n",
      "Epoch 73: Train Loss: 0.2035421927769979, Validation Loss: 0.4697202444076538\n",
      "Epoch 74: Train Loss: 0.19805873930454254, Validation Loss: 0.4690028727054596\n",
      "Epoch 75: Train Loss: 0.20185373226801553, Validation Loss: 0.47076305747032166\n",
      "Epoch 76: Train Loss: 0.1840421805779139, Validation Loss: 0.47879746556282043\n",
      "Epoch 77: Train Loss: 0.17960366110006967, Validation Loss: 0.48363593220710754\n",
      "Epoch 78: Train Loss: 0.1902612497409185, Validation Loss: 0.48339155316352844\n",
      "Epoch 79: Train Loss: 0.1827898919582367, Validation Loss: 0.4851166009902954\n",
      "Epoch 80: Train Loss: 0.1862004597981771, Validation Loss: 0.47206953167915344\n",
      "Epoch 81: Train Loss: 0.18348775307337442, Validation Loss: 0.4811714291572571\n",
      "Epoch 82: Train Loss: 0.18462743361790976, Validation Loss: 0.4878513216972351\n",
      "Epoch 83: Train Loss: 0.1715953399737676, Validation Loss: 0.5103309154510498\n",
      "Epoch 84: Train Loss: 0.17250983913739523, Validation Loss: 0.4909215569496155\n",
      "Epoch 85: Train Loss: 0.17214073240756989, Validation Loss: 0.4943138062953949\n",
      "Epoch 86: Train Loss: 0.18644851446151733, Validation Loss: 0.48277294635772705\n",
      "Epoch 87: Train Loss: 0.18057983120282492, Validation Loss: 0.4845709204673767\n",
      "Epoch 88: Train Loss: 0.18829646209875742, Validation Loss: 0.48055899143218994\n",
      "Epoch 89: Train Loss: 0.1653080383936564, Validation Loss: 0.47570186853408813\n",
      "Epoch 90: Train Loss: 0.17370502154032388, Validation Loss: 0.4717695713043213\n",
      "Epoch 91: Train Loss: 0.15972096224625906, Validation Loss: 0.4974726736545563\n",
      "Epoch 92: Train Loss: 0.17874538401762644, Validation Loss: 0.502241849899292\n",
      "Epoch 93: Train Loss: 0.15946134428183237, Validation Loss: 0.48492974042892456\n",
      "Epoch 94: Train Loss: 0.1549637963374456, Validation Loss: 0.4892776608467102\n",
      "Epoch 95: Train Loss: 0.1498655527830124, Validation Loss: 0.49592894315719604\n",
      "Epoch 96: Train Loss: 0.14642928540706635, Validation Loss: 0.4968785345554352\n",
      "Epoch 97: Train Loss: 0.14184711376825967, Validation Loss: 0.48315680027008057\n",
      "Epoch 98: Train Loss: 0.1434250921010971, Validation Loss: 0.48320117592811584\n",
      "Epoch 99: Train Loss: 0.13823467989762625, Validation Loss: 0.48287832736968994\n",
      "Epoch 100: Train Loss: 0.15244496365388235, Validation Loss: 0.46660542488098145\n",
      "Epoch 101: Train Loss: 0.15842014302810034, Validation Loss: 0.4712584614753723\n",
      "Epoch 102: Train Loss: 0.14139490822950998, Validation Loss: 0.5116190314292908\n",
      "Epoch 103: Train Loss: 0.14587337772051492, Validation Loss: 0.502217710018158\n",
      "Epoch 104: Train Loss: 0.14642015347878137, Validation Loss: 0.4774799048900604\n",
      "Epoch 105: Train Loss: 0.12966035306453705, Validation Loss: 0.4693429470062256\n",
      "Epoch 106: Train Loss: 0.13360334187746048, Validation Loss: 0.4564313590526581\n",
      "Epoch 107: Train Loss: 0.12896612038215002, Validation Loss: 0.4548083543777466\n",
      "Epoch 108: Train Loss: 0.12974410007397333, Validation Loss: 0.4585424065589905\n",
      "Epoch 109: Train Loss: 0.13163754095633826, Validation Loss: 0.4569946825504303\n",
      "Epoch 110: Train Loss: 0.12580483655134836, Validation Loss: 0.5087335705757141\n",
      "Epoch 111: Train Loss: 0.12639744331439337, Validation Loss: 0.4604051411151886\n",
      "Epoch 112: Train Loss: 0.12287875264883041, Validation Loss: 0.4496432840824127\n",
      "Epoch 113: Train Loss: 0.12122827519973119, Validation Loss: 0.47286754846572876\n",
      "Epoch 114: Train Loss: 0.112553504606088, Validation Loss: 0.47220155596733093\n",
      "Epoch 115: Train Loss: 0.12008540332317352, Validation Loss: 0.4728243350982666\n",
      "Epoch 116: Train Loss: 0.11440802862246831, Validation Loss: 0.47280243039131165\n",
      "Epoch 117: Train Loss: 0.11593687285979588, Validation Loss: 0.46338558197021484\n",
      "Epoch 118: Train Loss: 0.11972469588120778, Validation Loss: 0.4668325185775757\n",
      "Epoch 119: Train Loss: 0.1158409093817075, Validation Loss: 0.4659344255924225\n",
      "Epoch 120: Train Loss: 0.11125010997056961, Validation Loss: 0.45152759552001953\n",
      "Epoch 121: Train Loss: 0.12240145355463028, Validation Loss: 0.4955194294452667\n",
      "Epoch 122: Train Loss: 0.11328691989183426, Validation Loss: 0.4818001091480255\n",
      "Epoch 123: Train Loss: 0.10843720038731892, Validation Loss: 0.47185996174812317\n",
      "Epoch 124: Train Loss: 0.10397808005412419, Validation Loss: 0.46624502539634705\n",
      "Epoch 125: Train Loss: 0.11391471574703853, Validation Loss: 0.4512355327606201\n",
      "Epoch 126: Train Loss: 0.09797737747430801, Validation Loss: 0.4386079013347626\n",
      "Epoch 127: Train Loss: 0.10631197939316432, Validation Loss: 0.44945836067199707\n",
      "Epoch 128: Train Loss: 0.09459732472896576, Validation Loss: 0.4554724395275116\n",
      "Epoch 129: Train Loss: 0.0987189660469691, Validation Loss: 0.45131397247314453\n",
      "Epoch 130: Train Loss: 0.10044894615809123, Validation Loss: 0.4939253330230713\n",
      "Epoch 131: Train Loss: 0.1036221260825793, Validation Loss: 0.47262272238731384\n",
      "Epoch 132: Train Loss: 0.11321781327327092, Validation Loss: 0.47056421637535095\n",
      "Epoch 133: Train Loss: 0.09263722598552704, Validation Loss: 0.4833877682685852\n",
      "Epoch 134: Train Loss: 0.09462294975916545, Validation Loss: 0.458164244890213\n",
      "Epoch 135: Train Loss: 0.08744683861732483, Validation Loss: 0.46520233154296875\n",
      "Epoch 136: Train Loss: 0.08692291378974915, Validation Loss: 0.47776535153388977\n",
      "Epoch 137: Train Loss: 0.08295240501562755, Validation Loss: 0.4771597683429718\n",
      "Epoch 138: Train Loss: 0.08584297448396683, Validation Loss: 0.4780113995075226\n",
      "Epoch 139: Train Loss: 0.08176133533318837, Validation Loss: 0.4793417453765869\n",
      "Epoch 140: Train Loss: 0.08769700179497401, Validation Loss: 0.45882150530815125\n",
      "Epoch 141: Train Loss: 0.08430594205856323, Validation Loss: 0.46140938997268677\n",
      "Epoch 142: Train Loss: 0.08702943474054337, Validation Loss: 0.4587763845920563\n",
      "Epoch 143: Train Loss: 0.08291487395763397, Validation Loss: 0.5067036747932434\n",
      "Epoch 144: Train Loss: 0.08429387211799622, Validation Loss: 0.4571695625782013\n",
      "Epoch 145: Train Loss: 0.08191135028998058, Validation Loss: 0.43213412165641785\n",
      "Epoch 146: Train Loss: 0.09208675225575765, Validation Loss: 0.44143804907798767\n",
      "Epoch 147: Train Loss: 0.07485346247752507, Validation Loss: 0.45172637701034546\n",
      "Epoch 148: Train Loss: 0.07231743385394414, Validation Loss: 0.45655813813209534\n",
      "Epoch 149: Train Loss: 0.07220542679230373, Validation Loss: 0.46171459555625916\n",
      "Epoch 150: Train Loss: 0.07458957533041637, Validation Loss: 0.48626646399497986\n",
      "Epoch 151: Train Loss: 0.07824158171812694, Validation Loss: 0.4998418390750885\n",
      "Epoch 152: Train Loss: 0.07654649019241333, Validation Loss: 0.4846874475479126\n",
      "Epoch 153: Train Loss: 0.07999546080827713, Validation Loss: 0.4557097256183624\n",
      "Epoch 154: Train Loss: 0.06905603408813477, Validation Loss: 0.456216037273407\n",
      "Epoch 155: Train Loss: 0.07115993027885754, Validation Loss: 0.4677172601222992\n",
      "Epoch 156: Train Loss: 0.07863005499045055, Validation Loss: 0.47670748829841614\n",
      "Epoch 157: Train Loss: 0.06618079170584679, Validation Loss: 0.4795883893966675\n",
      "Epoch 158: Train Loss: 0.068889319896698, Validation Loss: 0.47741520404815674\n",
      "Epoch 159: Train Loss: 0.06914155185222626, Validation Loss: 0.4768243432044983\n",
      "Epoch 160: Train Loss: 0.07197724531094234, Validation Loss: 0.46162697672843933\n",
      "Epoch 161: Train Loss: 0.07545250157515208, Validation Loss: 0.48450398445129395\n",
      "Epoch 162: Train Loss: 0.07165634632110596, Validation Loss: 0.4701618254184723\n",
      "Epoch 163: Train Loss: 0.06980659315983455, Validation Loss: 0.4504348933696747\n",
      "Epoch 164: Train Loss: 0.061811779936154686, Validation Loss: 0.4467291831970215\n",
      "Epoch 165: Train Loss: 0.07663008322318395, Validation Loss: 0.4638935327529907\n",
      "Epoch 166: Train Loss: 0.058212343603372574, Validation Loss: 0.4904530942440033\n",
      "Epoch 167: Train Loss: 0.058111206938823066, Validation Loss: 0.507234513759613\n",
      "Epoch 168: Train Loss: 0.057388139267762504, Validation Loss: 0.5059303045272827\n",
      "Epoch 169: Train Loss: 0.05726095661520958, Validation Loss: 0.5018602013587952\n",
      "Epoch 170: Train Loss: 0.07053075482447942, Validation Loss: 0.4795953631401062\n",
      "Epoch 171: Train Loss: 0.06149437899390856, Validation Loss: 0.5022892355918884\n",
      "Epoch 172: Train Loss: 0.06440446898341179, Validation Loss: 0.47206854820251465\n",
      "Epoch 173: Train Loss: 0.08770903199911118, Validation Loss: 0.47506409883499146\n",
      "Epoch 174: Train Loss: 0.06760847692688306, Validation Loss: 0.5072535276412964\n",
      "Epoch 175: Train Loss: 0.05359665180246035, Validation Loss: 0.4996336102485657\n",
      "Epoch 176: Train Loss: 0.06571703031659126, Validation Loss: 0.4856209456920624\n",
      "Epoch 177: Train Loss: 0.061943204452594124, Validation Loss: 0.48089584708213806\n",
      "Epoch 178: Train Loss: 0.05169498175382614, Validation Loss: 0.4809860289096832\n",
      "Epoch 179: Train Loss: 0.05165276055534681, Validation Loss: 0.4931657612323761\n",
      "Epoch 180: Train Loss: 0.0678999088704586, Validation Loss: 0.5083182454109192\n",
      "Epoch 181: Train Loss: 0.05199939012527466, Validation Loss: 0.5004895329475403\n",
      "Epoch 182: Train Loss: 0.06299307694037755, Validation Loss: 0.5259687900543213\n",
      "Epoch 183: Train Loss: 0.05462636798620224, Validation Loss: 0.5280761122703552\n",
      "Epoch 184: Train Loss: 0.06125548854470253, Validation Loss: 0.4826183319091797\n",
      "Epoch 185: Train Loss: 0.06040633345643679, Validation Loss: 0.4680696725845337\n",
      "Epoch 186: Train Loss: 0.058058528850475945, Validation Loss: 0.4865700304508209\n",
      "Epoch 187: Train Loss: 0.0511397918065389, Validation Loss: 0.5101391673088074\n",
      "Epoch 188: Train Loss: 0.054987495144208275, Validation Loss: 0.4990181028842926\n",
      "Epoch 189: Train Loss: 0.053261986623207726, Validation Loss: 0.49602723121643066\n",
      "Epoch 190: Train Loss: 0.04636772722005844, Validation Loss: 0.5000728964805603\n",
      "Epoch 191: Train Loss: 0.05504199986656507, Validation Loss: 0.4902355968952179\n",
      "Epoch 192: Train Loss: 0.05017997572819392, Validation Loss: 0.5644587278366089\n",
      "Epoch 193: Train Loss: 0.05374262109398842, Validation Loss: 0.5845332145690918\n",
      "Epoch 194: Train Loss: 0.04564730698863665, Validation Loss: 0.4941225051879883\n",
      "Epoch 195: Train Loss: 0.047232020646333694, Validation Loss: 0.4754514694213867\n",
      "Epoch 196: Train Loss: 0.04385493819912275, Validation Loss: 0.4806738495826721\n",
      "Epoch 197: Train Loss: 0.04086950669685999, Validation Loss: 0.5048729181289673\n",
      "Epoch 198: Train Loss: 0.05990994473298391, Validation Loss: 0.4852571189403534\n",
      "Epoch 199: Train Loss: 0.04470740631222725, Validation Loss: 0.48482969403266907\n",
      "Fold 5\n",
      "Epoch 0: Train Loss: 0.7021830876668295, Validation Loss: 0.6898509860038757\n",
      "Epoch 1: Train Loss: 0.6464404861132304, Validation Loss: 0.686640202999115\n",
      "Epoch 2: Train Loss: 0.6131129860877991, Validation Loss: 0.683218777179718\n",
      "Epoch 3: Train Loss: 0.593768318494161, Validation Loss: 0.6783851385116577\n",
      "Epoch 4: Train Loss: 0.5811898708343506, Validation Loss: 0.6726217269897461\n",
      "Epoch 5: Train Loss: 0.5616026918093363, Validation Loss: 0.668398380279541\n",
      "Epoch 6: Train Loss: 0.5505707859992981, Validation Loss: 0.6659590601921082\n",
      "Epoch 7: Train Loss: 0.5382341345151266, Validation Loss: 0.6662935614585876\n",
      "Epoch 8: Train Loss: 0.5546259085337321, Validation Loss: 0.6670817732810974\n",
      "Epoch 9: Train Loss: 0.5407346685727438, Validation Loss: 0.668438732624054\n",
      "Epoch 10: Train Loss: 0.5286614100138346, Validation Loss: 0.6644372344017029\n",
      "Epoch 11: Train Loss: 0.5059231022993723, Validation Loss: 0.6599986553192139\n",
      "Epoch 12: Train Loss: 0.49621595939000446, Validation Loss: 0.6543005108833313\n",
      "Epoch 13: Train Loss: 0.48021673162778217, Validation Loss: 0.6542800068855286\n",
      "Epoch 14: Train Loss: 0.4620056649049123, Validation Loss: 0.6566783785820007\n",
      "Epoch 15: Train Loss: 0.44810977578163147, Validation Loss: 0.6626805067062378\n",
      "Epoch 16: Train Loss: 0.4421467383702596, Validation Loss: 0.6643157005310059\n",
      "Epoch 17: Train Loss: 0.44472888112068176, Validation Loss: 0.6625441312789917\n",
      "Epoch 18: Train Loss: 0.43217172225316364, Validation Loss: 0.6606352925300598\n",
      "Epoch 19: Train Loss: 0.4351024230321248, Validation Loss: 0.6641470789909363\n",
      "Epoch 20: Train Loss: 0.43482157588005066, Validation Loss: 0.6692517995834351\n",
      "Epoch 21: Train Loss: 0.42223697900772095, Validation Loss: 0.6755473017692566\n",
      "Epoch 22: Train Loss: 0.3988646864891052, Validation Loss: 0.6839350461959839\n",
      "Epoch 23: Train Loss: 0.39139553904533386, Validation Loss: 0.6798204183578491\n",
      "Epoch 24: Train Loss: 0.37406091888745624, Validation Loss: 0.6801221966743469\n",
      "Epoch 25: Train Loss: 0.37241466840108234, Validation Loss: 0.6796480417251587\n",
      "Epoch 26: Train Loss: 0.3680950403213501, Validation Loss: 0.6836788058280945\n",
      "Epoch 27: Train Loss: 0.3542315562566121, Validation Loss: 0.6866435408592224\n",
      "Epoch 28: Train Loss: 0.3847754995028178, Validation Loss: 0.6866215467453003\n",
      "Epoch 29: Train Loss: 0.3493108848730723, Validation Loss: 0.6829804182052612\n",
      "Epoch 30: Train Loss: 0.3535675307114919, Validation Loss: 0.6963663101196289\n",
      "Epoch 31: Train Loss: 0.3377583622932434, Validation Loss: 0.6890226006507874\n",
      "Epoch 32: Train Loss: 0.34021209677060443, Validation Loss: 0.7040151953697205\n",
      "Epoch 33: Train Loss: 0.32484936714172363, Validation Loss: 0.72856205701828\n",
      "Epoch 34: Train Loss: 0.31141941746075946, Validation Loss: 0.7342637777328491\n",
      "Epoch 35: Train Loss: 0.314189076423645, Validation Loss: 0.75808185338974\n",
      "Epoch 36: Train Loss: 0.31119468808174133, Validation Loss: 0.761223554611206\n",
      "Epoch 37: Train Loss: 0.28920312722524005, Validation Loss: 0.7583039999008179\n",
      "Epoch 38: Train Loss: 0.2977362771828969, Validation Loss: 0.7532661557197571\n",
      "Epoch 39: Train Loss: 0.2927403549353282, Validation Loss: 0.7487928867340088\n",
      "Epoch 40: Train Loss: 0.2857956290245056, Validation Loss: 0.716792106628418\n",
      "Epoch 41: Train Loss: 0.2895690103371938, Validation Loss: 0.7056323885917664\n",
      "Epoch 42: Train Loss: 0.2678757905960083, Validation Loss: 0.7135792374610901\n",
      "Epoch 43: Train Loss: 0.26913996040821075, Validation Loss: 0.7442467212677002\n",
      "Epoch 44: Train Loss: 0.2515169580777486, Validation Loss: 0.7601912021636963\n",
      "Epoch 45: Train Loss: 0.24667716523011526, Validation Loss: 0.7572981715202332\n",
      "Epoch 46: Train Loss: 0.25093983113765717, Validation Loss: 0.7555761337280273\n",
      "Epoch 47: Train Loss: 0.2536977678537369, Validation Loss: 0.7409266233444214\n",
      "Epoch 48: Train Loss: 0.2530452311038971, Validation Loss: 0.7296032309532166\n",
      "Epoch 49: Train Loss: 0.2426612228155136, Validation Loss: 0.7359369993209839\n",
      "Epoch 50: Train Loss: 0.257494792342186, Validation Loss: 0.7400482892990112\n",
      "Epoch 51: Train Loss: 0.23795882860819498, Validation Loss: 0.7641052007675171\n",
      "Epoch 52: Train Loss: 0.24198336899280548, Validation Loss: 0.7556332349777222\n",
      "Epoch 53: Train Loss: 0.22736522058645883, Validation Loss: 0.7459156513214111\n",
      "Epoch 54: Train Loss: 0.22929976880550385, Validation Loss: 0.7404977083206177\n",
      "Epoch 55: Train Loss: 0.21170777082443237, Validation Loss: 0.7439364790916443\n",
      "Epoch 56: Train Loss: 0.2186757375796636, Validation Loss: 0.7596145272254944\n",
      "Epoch 57: Train Loss: 0.20672554771105447, Validation Loss: 0.7667970657348633\n",
      "Epoch 58: Train Loss: 0.21576725443204245, Validation Loss: 0.7693963646888733\n",
      "Epoch 59: Train Loss: 0.20305283864339194, Validation Loss: 0.7651063799858093\n",
      "Epoch 60: Train Loss: 0.21106835703055063, Validation Loss: 0.7337459325790405\n",
      "Epoch 61: Train Loss: 0.20645974079767862, Validation Loss: 0.7478944063186646\n",
      "Epoch 62: Train Loss: 0.2000595728556315, Validation Loss: 0.8069362640380859\n",
      "Epoch 63: Train Loss: 0.2012597918510437, Validation Loss: 0.8672041893005371\n",
      "Epoch 64: Train Loss: 0.18589819967746735, Validation Loss: 0.8335604071617126\n",
      "Epoch 65: Train Loss: 0.2067475219567617, Validation Loss: 0.8083798289299011\n",
      "Epoch 66: Train Loss: 0.19703667859236398, Validation Loss: 0.8070098757743835\n",
      "Epoch 67: Train Loss: 0.17619741956392923, Validation Loss: 0.8100056648254395\n",
      "Epoch 68: Train Loss: 0.1688446750243505, Validation Loss: 0.8003458380699158\n",
      "Epoch 69: Train Loss: 0.17135215799013773, Validation Loss: 0.8010779023170471\n",
      "Epoch 70: Train Loss: 0.17227617899576822, Validation Loss: 0.7709242701530457\n",
      "Epoch 71: Train Loss: 0.1999542067448298, Validation Loss: 0.7652232646942139\n",
      "Epoch 72: Train Loss: 0.17771785954634348, Validation Loss: 0.8127408027648926\n",
      "Epoch 73: Train Loss: 0.19857609768708548, Validation Loss: 0.8413107395172119\n",
      "Epoch 74: Train Loss: 0.16161192953586578, Validation Loss: 0.8560027480125427\n",
      "Epoch 75: Train Loss: 0.1618147442738215, Validation Loss: 0.860434889793396\n",
      "Epoch 76: Train Loss: 0.17762541770935059, Validation Loss: 0.862194836139679\n",
      "Epoch 77: Train Loss: 0.1777785668770472, Validation Loss: 0.8682664036750793\n",
      "Epoch 78: Train Loss: 0.1584945097565651, Validation Loss: 0.8622910380363464\n",
      "Epoch 79: Train Loss: 0.15122413635253906, Validation Loss: 0.866112232208252\n",
      "Epoch 80: Train Loss: 0.1563659111658732, Validation Loss: 0.8363030552864075\n",
      "Epoch 81: Train Loss: 0.1572966972986857, Validation Loss: 0.8143782615661621\n",
      "Epoch 82: Train Loss: 0.14798476050297418, Validation Loss: 0.8292815089225769\n",
      "Epoch 83: Train Loss: 0.14452687402566275, Validation Loss: 0.8392570614814758\n",
      "Epoch 84: Train Loss: 0.14445633192857107, Validation Loss: 0.8350980281829834\n",
      "Epoch 85: Train Loss: 0.14272226889928183, Validation Loss: 0.8516749739646912\n",
      "Epoch 86: Train Loss: 0.1278693899512291, Validation Loss: 0.8470876216888428\n",
      "Epoch 87: Train Loss: 0.13775217036406198, Validation Loss: 0.8364459872245789\n",
      "Epoch 88: Train Loss: 0.1320899228254954, Validation Loss: 0.8406851887702942\n",
      "Epoch 89: Train Loss: 0.1387504612406095, Validation Loss: 0.8408221006393433\n",
      "Epoch 90: Train Loss: 0.1287740021944046, Validation Loss: 0.8722293376922607\n",
      "Epoch 91: Train Loss: 0.14443800846735635, Validation Loss: 0.815272867679596\n",
      "Epoch 92: Train Loss: 0.139011820157369, Validation Loss: 0.840419352054596\n",
      "Epoch 93: Train Loss: 0.12632580349842706, Validation Loss: 0.8820145726203918\n",
      "Epoch 94: Train Loss: 0.13035578777392706, Validation Loss: 0.927603006362915\n",
      "Epoch 95: Train Loss: 0.1342982475956281, Validation Loss: 0.8786113262176514\n",
      "Epoch 96: Train Loss: 0.11833363771438599, Validation Loss: 0.8779775500297546\n",
      "Epoch 97: Train Loss: 0.11550110081831615, Validation Loss: 0.8640024065971375\n",
      "Epoch 98: Train Loss: 0.1200644572575887, Validation Loss: 0.8607720732688904\n",
      "Epoch 99: Train Loss: 0.1075375700990359, Validation Loss: 0.8569318056106567\n",
      "Epoch 100: Train Loss: 0.11640739192565282, Validation Loss: 0.7774830460548401\n",
      "Epoch 101: Train Loss: 0.12191716581583023, Validation Loss: 0.8159279227256775\n",
      "Epoch 102: Train Loss: 0.10907330860694249, Validation Loss: 0.8918202519416809\n",
      "Epoch 103: Train Loss: 0.11612614740928014, Validation Loss: 0.8784000873565674\n",
      "Epoch 104: Train Loss: 0.10475775351126988, Validation Loss: 0.8770582675933838\n",
      "Epoch 105: Train Loss: 0.1133774071931839, Validation Loss: 0.8835410475730896\n",
      "Epoch 106: Train Loss: 0.10097340246041615, Validation Loss: 0.8962332010269165\n",
      "Epoch 107: Train Loss: 0.09971302996079127, Validation Loss: 0.8955718874931335\n",
      "Epoch 108: Train Loss: 0.09445178757111232, Validation Loss: 0.8941934704780579\n",
      "Epoch 109: Train Loss: 0.09499061107635498, Validation Loss: 0.893982470035553\n",
      "Epoch 110: Train Loss: 0.09837735444307327, Validation Loss: 0.9123559594154358\n",
      "Epoch 111: Train Loss: 0.10428997129201889, Validation Loss: 0.9062493443489075\n",
      "Epoch 112: Train Loss: 0.12236938625574112, Validation Loss: 0.8581607341766357\n",
      "Epoch 113: Train Loss: 0.09908106178045273, Validation Loss: 0.8465210795402527\n",
      "Epoch 114: Train Loss: 0.09425341586271922, Validation Loss: 0.8757521510124207\n",
      "Epoch 115: Train Loss: 0.09790426741043727, Validation Loss: 0.8894655108451843\n",
      "Epoch 116: Train Loss: 0.08972959220409393, Validation Loss: 0.8995850086212158\n",
      "Epoch 117: Train Loss: 0.09312550723552704, Validation Loss: 0.8942769765853882\n",
      "Epoch 118: Train Loss: 0.0940548727909724, Validation Loss: 0.8915805816650391\n",
      "Epoch 119: Train Loss: 0.0934474915266037, Validation Loss: 0.8918681144714355\n",
      "Epoch 120: Train Loss: 0.08791520446538925, Validation Loss: 0.808375895023346\n",
      "Epoch 121: Train Loss: 0.10621146112680435, Validation Loss: 0.8746886253356934\n",
      "Epoch 122: Train Loss: 0.08527438094218572, Validation Loss: 0.9751668572425842\n",
      "Epoch 123: Train Loss: 0.0886663869023323, Validation Loss: 1.0718919038772583\n",
      "Epoch 124: Train Loss: 0.08721919109423955, Validation Loss: 0.9879512190818787\n",
      "Epoch 125: Train Loss: 0.0823976347843806, Validation Loss: 0.949732780456543\n",
      "Epoch 126: Train Loss: 0.07824381192525227, Validation Loss: 0.9339267611503601\n",
      "Epoch 127: Train Loss: 0.08793922637899716, Validation Loss: 0.9448841214179993\n",
      "Epoch 128: Train Loss: 0.0790560357272625, Validation Loss: 0.9536692500114441\n",
      "Epoch 129: Train Loss: 0.07464748620986938, Validation Loss: 0.9629130959510803\n",
      "Epoch 130: Train Loss: 0.0847549041112264, Validation Loss: 1.0637212991714478\n",
      "Epoch 131: Train Loss: 0.08766743292411168, Validation Loss: 1.0207884311676025\n",
      "Epoch 132: Train Loss: 0.08078209931651752, Validation Loss: 0.9738561511039734\n",
      "Epoch 133: Train Loss: 0.07552595188220342, Validation Loss: 0.9832121133804321\n",
      "Epoch 134: Train Loss: 0.06911103924115498, Validation Loss: 0.9744763374328613\n",
      "Epoch 135: Train Loss: 0.07518807301918666, Validation Loss: 0.9847298860549927\n",
      "Epoch 136: Train Loss: 0.07008979469537735, Validation Loss: 0.9981669783592224\n",
      "Epoch 137: Train Loss: 0.070479782919089, Validation Loss: 1.0057271718978882\n",
      "Epoch 138: Train Loss: 0.07928005854288737, Validation Loss: 0.9856438636779785\n",
      "Epoch 139: Train Loss: 0.12133332093556722, Validation Loss: 0.9855765700340271\n",
      "Epoch 140: Train Loss: 0.07851757109165192, Validation Loss: 0.945035994052887\n",
      "Epoch 141: Train Loss: 0.07138881087303162, Validation Loss: 0.9797053933143616\n",
      "Epoch 142: Train Loss: 0.0861026793718338, Validation Loss: 1.0441547632217407\n",
      "Epoch 143: Train Loss: 0.08165817707777023, Validation Loss: 1.0501686334609985\n",
      "Epoch 144: Train Loss: 0.07831886038184166, Validation Loss: 1.0123584270477295\n",
      "Epoch 145: Train Loss: 0.06731643279393514, Validation Loss: 0.9813724756240845\n",
      "Epoch 146: Train Loss: 0.07058533032735188, Validation Loss: 0.9913914203643799\n",
      "Epoch 147: Train Loss: 0.07587538659572601, Validation Loss: 0.9921576380729675\n",
      "Epoch 148: Train Loss: 0.07022117078304291, Validation Loss: 0.9999367594718933\n",
      "Epoch 149: Train Loss: 0.06394202510515849, Validation Loss: 1.0046569108963013\n",
      "Epoch 150: Train Loss: 0.07258731126785278, Validation Loss: 1.0004968643188477\n",
      "Epoch 151: Train Loss: 0.08413874854644139, Validation Loss: 1.0123498439788818\n",
      "Epoch 152: Train Loss: 0.06776932875315349, Validation Loss: 1.0578418970108032\n",
      "Epoch 153: Train Loss: 0.06642014409104983, Validation Loss: 1.032407283782959\n",
      "Epoch 154: Train Loss: 0.06437789027889569, Validation Loss: 1.0186576843261719\n",
      "Epoch 155: Train Loss: 0.05893991763393084, Validation Loss: 1.0267267227172852\n",
      "Epoch 156: Train Loss: 0.07008987044294675, Validation Loss: 1.0137258768081665\n",
      "Epoch 157: Train Loss: 0.059841045488913856, Validation Loss: 1.0195900201797485\n",
      "Epoch 158: Train Loss: 0.06556147833665212, Validation Loss: 1.0199252367019653\n",
      "Epoch 159: Train Loss: 0.05179008717338244, Validation Loss: 1.0267643928527832\n",
      "Epoch 160: Train Loss: 0.06459662814935048, Validation Loss: 1.0130653381347656\n",
      "Epoch 161: Train Loss: 0.08468260367711385, Validation Loss: 1.0467448234558105\n",
      "Epoch 162: Train Loss: 0.06075112770001093, Validation Loss: 1.085654854774475\n",
      "Epoch 163: Train Loss: 0.06173780684669813, Validation Loss: 1.057345986366272\n",
      "Epoch 164: Train Loss: 0.06260753174622853, Validation Loss: 1.0417060852050781\n",
      "Epoch 165: Train Loss: 0.059047107895215355, Validation Loss: 1.0657459497451782\n",
      "Epoch 166: Train Loss: 0.05828560640414556, Validation Loss: 1.0865811109542847\n",
      "Epoch 167: Train Loss: 0.05867629746596018, Validation Loss: 1.0772196054458618\n",
      "Epoch 168: Train Loss: 0.05581394707163175, Validation Loss: 1.0859447717666626\n",
      "Epoch 169: Train Loss: 0.06387131909529369, Validation Loss: 1.0693318843841553\n",
      "Epoch 170: Train Loss: 0.055693684766689934, Validation Loss: 1.0682083368301392\n",
      "Epoch 171: Train Loss: 0.06306418528159459, Validation Loss: 1.0471230745315552\n",
      "Epoch 172: Train Loss: 0.05776477853457133, Validation Loss: 1.0656239986419678\n",
      "Epoch 173: Train Loss: 0.051461540162563324, Validation Loss: 1.0951045751571655\n",
      "Epoch 174: Train Loss: 0.05754585067431132, Validation Loss: 1.0892558097839355\n",
      "Epoch 175: Train Loss: 0.060969422260920204, Validation Loss: 1.0856050252914429\n",
      "Epoch 176: Train Loss: 0.049885121484597526, Validation Loss: 1.0644729137420654\n",
      "Epoch 177: Train Loss: 0.04847172896067301, Validation Loss: 1.0495561361312866\n",
      "Epoch 178: Train Loss: 0.0483934630950292, Validation Loss: 1.0394060611724854\n",
      "Epoch 179: Train Loss: 0.05624523510535558, Validation Loss: 1.0475828647613525\n",
      "Epoch 180: Train Loss: 0.058988820761442184, Validation Loss: 1.0833414793014526\n",
      "Epoch 181: Train Loss: 0.04674664263923963, Validation Loss: 1.1737812757492065\n",
      "Epoch 182: Train Loss: 0.05112472176551819, Validation Loss: 1.1744489669799805\n",
      "Epoch 183: Train Loss: 0.05008488272627195, Validation Loss: 1.1359590291976929\n",
      "Epoch 184: Train Loss: 0.05212982495625814, Validation Loss: 1.1434444189071655\n",
      "Epoch 185: Train Loss: 0.06068794180949529, Validation Loss: 1.1506315469741821\n",
      "Epoch 186: Train Loss: 0.05256621601680914, Validation Loss: 1.1522222757339478\n",
      "Epoch 187: Train Loss: 0.04845391089717547, Validation Loss: 1.1542365550994873\n",
      "Epoch 188: Train Loss: 0.04365025212367376, Validation Loss: 1.162745475769043\n",
      "Epoch 189: Train Loss: 0.04671347513794899, Validation Loss: 1.1638851165771484\n",
      "Epoch 190: Train Loss: 0.05349567160010338, Validation Loss: 1.1329045295715332\n",
      "Epoch 191: Train Loss: 0.048997122794389725, Validation Loss: 1.08982253074646\n",
      "Epoch 192: Train Loss: 0.05468162149190903, Validation Loss: 1.1222847700119019\n",
      "Epoch 193: Train Loss: 0.05236783685783545, Validation Loss: 1.1827030181884766\n",
      "Epoch 194: Train Loss: 0.05565220738450686, Validation Loss: 1.2115615606307983\n",
      "Epoch 195: Train Loss: 0.0624952440460523, Validation Loss: 1.2098990678787231\n",
      "Epoch 196: Train Loss: 0.07550240804751714, Validation Loss: 1.2010029554367065\n",
      "Epoch 197: Train Loss: 0.04726151625315348, Validation Loss: 1.1530379056930542\n",
      "Epoch 198: Train Loss: 0.04493485018610954, Validation Loss: 1.1398828029632568\n",
      "Epoch 199: Train Loss: 0.05058668926358223, Validation Loss: 1.131013035774231\n",
      "Accuracy: 0.7416666666666667,Precision: 0.7457627118644068, Recall: 0.7333333333333333, F1-score: 0.7394957983193278, AUC: 0.7416666666666667\n",
      "Confusion Matrix:\n",
      "[[45 15]\n",
      " [16 44]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nweight decay = 1e-7\\nlearning rate = 0.0001\\nepoch = 100\\nbatch size = 32\\nearly stopping patience = 10\\nstandard scaler\\nReLU\\ncross entropy loss\\ndrop out = 0.8\\n\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "model_save_dir = r'C:\\Users\\User\\Documents\\Lie detect data\\Model'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Define a function to load and pad data\n",
    "def load_data(data_dir, max_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    file_list = os.listdir(data_dir)\n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        label = 1 if 'truth' in file else 0\n",
    "        padded_data = np.zeros((65, max_length))\n",
    "        length = min(data.shape[1], max_length)\n",
    "        padded_data[:, :length] = data[:, :length]\n",
    "        X.append(padded_data)\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load dataset and pad the data\n",
    "data_dir = \"C:\\\\Users\\\\User\\\\Documents\\\\Lie detect data\\\\56M_DWTEEGData\"\n",
    "max_length = 1400  # Define maximum length for padding\n",
    "X, y = load_data(data_dir, max_length)\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# Define EEGNet model\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(65, 32, kernel_size=63, padding=31)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "        self.depthwiseConv1d = nn.Conv1d(32, 64, kernel_size=65, groups=32, padding=32)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)  # Additional convolutional layer\n",
    "        self.batchnorm3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.pooling = nn.AvgPool1d(kernel_size=4)\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        \n",
    "        self._calculate_num_features()\n",
    "        self.fc = nn.Linear(self.num_features, num_classes)\n",
    "\n",
    "    def _calculate_num_features(self):\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.zeros(1, 65, 1400)\n",
    "            sample_output = self._forward_features(sample_input)\n",
    "            self.num_features = sample_output.shape[1]\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.depthwiseConv1d(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.conv2(x)  # Additional convolutional layer\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.pooling(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.global_pool(x)  # Global average pooling layer\n",
    "        x = torch.flatten(x, 1)  # Flatten the output for the fully connected layer\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_and_evaluate(train_loader, val_loader, y_train):\n",
    "    model = EEGNet(num_classes=2).to(device)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-7)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "    num_epochs = 200\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 200\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "            fold_model_path = os.path.join(model_save_dir, f'fold3_model_fold_{fold_idx}.pth')\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': best_val_loss,\n",
    "            }, fold_model_path)\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_idx = 0\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "for train_index, val_index in kf.split(X, y):\n",
    "    print(f'Fold {fold_idx + 1}')\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Normalize data using scaler fitted on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_val = scaler.transform(X_val.reshape(X_val.shape[0], -1))\n",
    "    X_train = X_train.reshape(-1, 65, max_length)\n",
    "    X_val = X_val.reshape(-1, 65, max_length)\n",
    "\n",
    "    # Save the scaler to a file\n",
    "    with open(r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\simpleEEGNet_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = EEGDataset(X_train, y_train)\n",
    "    val_dataset = EEGDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = train_and_evaluate(train_loader, val_loader, y_train)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    fold_idx += 1\n",
    "\n",
    "\n",
    "# Calculate additional metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "auc = roc_auc_score(all_labels, all_predictions)\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy},Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "weight decay = 1e-7\n",
    "learning rate = 0.0001\n",
    "epoch = 100\n",
    "batch size = 32\n",
    "early stopping patience = 10\n",
    "standard scaler\n",
    "ReLU\n",
    "cross entropy loss\n",
    "drop out = 0.8\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66654d63-c450-4d9f-a3e3-63701fd1d41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934cb89-5ea1-4eb0-a152-00c71e49e06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
