{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "656cdbc2-da54-45ba-97fc-195b59bd7820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 0: Train Loss: 0.684169352054596, Validation Loss: 0.6795186400413513\n",
      "Epoch 1: Train Loss: 0.603224515914917, Validation Loss: 0.6633535027503967\n",
      "Epoch 2: Train Loss: 0.5643516580263773, Validation Loss: 0.6437851786613464\n",
      "Epoch 3: Train Loss: 0.5447646776835123, Validation Loss: 0.6248903870582581\n",
      "Epoch 4: Train Loss: 0.5192426939805349, Validation Loss: 0.6054286956787109\n",
      "Epoch 5: Train Loss: 0.5056215822696686, Validation Loss: 0.5885299444198608\n",
      "Epoch 6: Train Loss: 0.5021647612253824, Validation Loss: 0.5719761252403259\n",
      "Epoch 7: Train Loss: 0.48730186621348065, Validation Loss: 0.5575671792030334\n",
      "Epoch 8: Train Loss: 0.4924500584602356, Validation Loss: 0.5450038313865662\n",
      "Epoch 9: Train Loss: 0.4765230218569438, Validation Loss: 0.5350608825683594\n",
      "Epoch 10: Train Loss: 0.4713878234227498, Validation Loss: 0.5249186754226685\n",
      "Epoch 11: Train Loss: 0.4447943369547526, Validation Loss: 0.5181313753128052\n",
      "Epoch 12: Train Loss: 0.4382547636826833, Validation Loss: 0.5100656151771545\n",
      "Epoch 13: Train Loss: 0.4148433605829875, Validation Loss: 0.5037165880203247\n",
      "Epoch 14: Train Loss: 0.4124240775903066, Validation Loss: 0.5036950707435608\n",
      "Epoch 15: Train Loss: 0.40235960483551025, Validation Loss: 0.5040748119354248\n",
      "Epoch 16: Train Loss: 0.3680960536003113, Validation Loss: 0.5042238831520081\n",
      "Epoch 17: Train Loss: 0.3645391861597697, Validation Loss: 0.5046496987342834\n",
      "Epoch 18: Train Loss: 0.3599972625573476, Validation Loss: 0.5037114024162292\n",
      "Epoch 19: Train Loss: 0.36499064167340595, Validation Loss: 0.5036336183547974\n",
      "Epoch 20: Train Loss: 0.3491812249024709, Validation Loss: 0.507803738117218\n",
      "Epoch 21: Train Loss: 0.33675480882326764, Validation Loss: 0.49832096695899963\n",
      "Epoch 22: Train Loss: 0.31429357330004376, Validation Loss: 0.4972536265850067\n",
      "Epoch 23: Train Loss: 0.31796880563100177, Validation Loss: 0.4941309690475464\n",
      "Epoch 24: Train Loss: 0.2923102875550588, Validation Loss: 0.492260217666626\n",
      "Epoch 25: Train Loss: 0.2796477973461151, Validation Loss: 0.49578043818473816\n",
      "Epoch 26: Train Loss: 0.28267674148082733, Validation Loss: 0.49674755334854126\n",
      "Epoch 27: Train Loss: 0.281230166554451, Validation Loss: 0.49440985918045044\n",
      "Epoch 28: Train Loss: 0.2764786084493001, Validation Loss: 0.4951193928718567\n",
      "Epoch 29: Train Loss: 0.2689369022846222, Validation Loss: 0.49541884660720825\n",
      "Epoch 30: Train Loss: 0.28290554881095886, Validation Loss: 0.5106652975082397\n",
      "Epoch 31: Train Loss: 0.259090319275856, Validation Loss: 0.5288664698600769\n",
      "Epoch 32: Train Loss: 0.24170926213264465, Validation Loss: 0.5114321708679199\n",
      "Epoch 33: Train Loss: 0.23502001663049063, Validation Loss: 0.49501270055770874\n",
      "Epoch 34: Train Loss: 0.23301298916339874, Validation Loss: 0.4763662815093994\n",
      "Epoch 35: Train Loss: 0.23579712212085724, Validation Loss: 0.47240540385246277\n",
      "Epoch 36: Train Loss: 0.2750846942265828, Validation Loss: 0.47863632440567017\n",
      "Epoch 37: Train Loss: 0.21290120979150137, Validation Loss: 0.474960058927536\n",
      "Epoch 38: Train Loss: 0.20985752840836844, Validation Loss: 0.475830614566803\n",
      "Epoch 39: Train Loss: 0.21681022147337595, Validation Loss: 0.4754238724708557\n",
      "Epoch 40: Train Loss: 0.21278528372446695, Validation Loss: 0.5068485140800476\n",
      "Epoch 41: Train Loss: 0.19913743436336517, Validation Loss: 0.5390028357505798\n",
      "Epoch 42: Train Loss: 0.19187726577123007, Validation Loss: 0.5404557585716248\n",
      "Epoch 43: Train Loss: 0.18872235218683878, Validation Loss: 0.5502699613571167\n",
      "Epoch 44: Train Loss: 0.18409049014250436, Validation Loss: 0.5536733865737915\n",
      "Early stopping at epoch 45\n",
      "Fold 2\n",
      "Epoch 0: Train Loss: 0.6484758853912354, Validation Loss: 0.6756094098091125\n",
      "Epoch 1: Train Loss: 0.5830923318862915, Validation Loss: 0.6542724370956421\n",
      "Epoch 2: Train Loss: 0.544488807519277, Validation Loss: 0.6377037763595581\n",
      "Epoch 3: Train Loss: 0.501512865225474, Validation Loss: 0.620619535446167\n",
      "Epoch 4: Train Loss: 0.49284879366556805, Validation Loss: 0.6039525866508484\n",
      "Epoch 5: Train Loss: 0.47285155455271405, Validation Loss: 0.5923367142677307\n",
      "Epoch 6: Train Loss: 0.46050923069318134, Validation Loss: 0.5805244445800781\n",
      "Epoch 7: Train Loss: 0.47139347592989606, Validation Loss: 0.5693238377571106\n",
      "Epoch 8: Train Loss: 0.4507892926534017, Validation Loss: 0.5597559809684753\n",
      "Epoch 9: Train Loss: 0.4473361571629842, Validation Loss: 0.5525026321411133\n",
      "Epoch 10: Train Loss: 0.42602062225341797, Validation Loss: 0.5407058596611023\n",
      "Epoch 11: Train Loss: 0.41264768441518146, Validation Loss: 0.5249258279800415\n",
      "Epoch 12: Train Loss: 0.39054320255915326, Validation Loss: 0.5153712630271912\n",
      "Epoch 13: Train Loss: 0.3722859025001526, Validation Loss: 0.5004569888114929\n",
      "Epoch 14: Train Loss: 0.36228257417678833, Validation Loss: 0.49342820048332214\n",
      "Epoch 15: Train Loss: 0.335888534784317, Validation Loss: 0.4856468141078949\n",
      "Epoch 16: Train Loss: 0.3526211977005005, Validation Loss: 0.47918379306793213\n",
      "Epoch 17: Train Loss: 0.33870260914166767, Validation Loss: 0.4778969883918762\n",
      "Epoch 18: Train Loss: 0.3247131605943044, Validation Loss: 0.47632208466529846\n",
      "Epoch 19: Train Loss: 0.3193429410457611, Validation Loss: 0.4748772084712982\n",
      "Epoch 20: Train Loss: 0.3279936412970225, Validation Loss: 0.4525213837623596\n",
      "Epoch 21: Train Loss: 0.30256710449854535, Validation Loss: 0.4532001316547394\n",
      "Epoch 22: Train Loss: 0.2880505422751109, Validation Loss: 0.4679190516471863\n",
      "Epoch 23: Train Loss: 0.25777555008729297, Validation Loss: 0.4534832239151001\n",
      "Epoch 24: Train Loss: 0.2621348450581233, Validation Loss: 0.46427175402641296\n",
      "Epoch 25: Train Loss: 0.255482276280721, Validation Loss: 0.4549487829208374\n",
      "Epoch 26: Train Loss: 0.2589668532212575, Validation Loss: 0.4408521056175232\n",
      "Epoch 27: Train Loss: 0.23407049973805746, Validation Loss: 0.43896326422691345\n",
      "Epoch 28: Train Loss: 0.23270555337270102, Validation Loss: 0.4386484920978546\n",
      "Epoch 29: Train Loss: 0.24103939036528269, Validation Loss: 0.4386107921600342\n",
      "Epoch 30: Train Loss: 0.23545029759407043, Validation Loss: 0.43506813049316406\n",
      "Epoch 31: Train Loss: 0.21974679827690125, Validation Loss: 0.4204238951206207\n",
      "Epoch 32: Train Loss: 0.21504692236582437, Validation Loss: 0.4574625790119171\n",
      "Epoch 33: Train Loss: 0.2057866503794988, Validation Loss: 0.4672527611255646\n",
      "Epoch 34: Train Loss: 0.19843452175458273, Validation Loss: 0.45251742005348206\n",
      "Epoch 35: Train Loss: 0.19185024003187814, Validation Loss: 0.4280734956264496\n",
      "Epoch 36: Train Loss: 0.18144920468330383, Validation Loss: 0.4257117211818695\n",
      "Epoch 37: Train Loss: 0.19022633135318756, Validation Loss: 0.43440428376197815\n",
      "Epoch 38: Train Loss: 0.17716075479984283, Validation Loss: 0.43991225957870483\n",
      "Epoch 39: Train Loss: 0.18473176658153534, Validation Loss: 0.4394633173942566\n",
      "Epoch 40: Train Loss: 0.18513154983520508, Validation Loss: 0.45260998606681824\n",
      "Epoch 41: Train Loss: 0.1632992078860601, Validation Loss: 0.3999428153038025\n",
      "Epoch 42: Train Loss: 0.18177878856658936, Validation Loss: 0.4122927784919739\n",
      "Epoch 43: Train Loss: 0.15631859004497528, Validation Loss: 0.4824658930301666\n",
      "Epoch 44: Train Loss: 0.19471891721089682, Validation Loss: 0.4311116337776184\n",
      "Epoch 45: Train Loss: 0.15773800015449524, Validation Loss: 0.41144445538520813\n",
      "Epoch 46: Train Loss: 0.1619091977675756, Validation Loss: 0.4255715310573578\n",
      "Epoch 47: Train Loss: 0.1444074735045433, Validation Loss: 0.44294318556785583\n",
      "Epoch 48: Train Loss: 0.14997919897238413, Validation Loss: 0.45107024908065796\n",
      "Epoch 49: Train Loss: 0.14272228876749674, Validation Loss: 0.4528507590293884\n",
      "Epoch 50: Train Loss: 0.14156254132588705, Validation Loss: 0.5088497400283813\n",
      "Early stopping at epoch 51\n",
      "Fold 3\n",
      "Epoch 0: Train Loss: 0.7323587934176127, Validation Loss: 0.6989032626152039\n",
      "Epoch 1: Train Loss: 0.6160936156908671, Validation Loss: 0.6918889284133911\n",
      "Epoch 2: Train Loss: 0.5574429233868917, Validation Loss: 0.6812726855278015\n",
      "Epoch 3: Train Loss: 0.5392953952153524, Validation Loss: 0.6693505048751831\n",
      "Epoch 4: Train Loss: 0.5072267254193624, Validation Loss: 0.6586554646492004\n",
      "Epoch 5: Train Loss: 0.4962437053521474, Validation Loss: 0.6476251482963562\n",
      "Epoch 6: Train Loss: 0.49277953306833905, Validation Loss: 0.6372332572937012\n",
      "Epoch 7: Train Loss: 0.4911504884560903, Validation Loss: 0.62836754322052\n",
      "Epoch 8: Train Loss: 0.4865881601969401, Validation Loss: 0.6210216879844666\n",
      "Epoch 9: Train Loss: 0.46814072132110596, Validation Loss: 0.6144350171089172\n",
      "Epoch 10: Train Loss: 0.4685198465983073, Validation Loss: 0.6042667031288147\n",
      "Epoch 11: Train Loss: 0.4469914237658183, Validation Loss: 0.5943660736083984\n",
      "Epoch 12: Train Loss: 0.42751004298528034, Validation Loss: 0.582081139087677\n",
      "Epoch 13: Train Loss: 0.42752301692962646, Validation Loss: 0.5656759142875671\n",
      "Epoch 14: Train Loss: 0.3804079790910085, Validation Loss: 0.5553691983222961\n",
      "Epoch 15: Train Loss: 0.36882826685905457, Validation Loss: 0.5505377650260925\n",
      "Epoch 16: Train Loss: 0.3588060438632965, Validation Loss: 0.5480506420135498\n",
      "Epoch 17: Train Loss: 0.3560209373633067, Validation Loss: 0.5470135807991028\n",
      "Epoch 18: Train Loss: 0.3580130736033122, Validation Loss: 0.5456936359405518\n",
      "Epoch 19: Train Loss: 0.3720689018567403, Validation Loss: 0.5449057221412659\n",
      "Epoch 20: Train Loss: 0.3629927337169647, Validation Loss: 0.5445137619972229\n",
      "Epoch 21: Train Loss: 0.3554205397764842, Validation Loss: 0.5524837970733643\n",
      "Epoch 22: Train Loss: 0.3183789551258087, Validation Loss: 0.551760196685791\n",
      "Epoch 23: Train Loss: 0.295053094625473, Validation Loss: 0.5610120892524719\n",
      "Epoch 24: Train Loss: 0.2951475878556569, Validation Loss: 0.5706722140312195\n",
      "Epoch 25: Train Loss: 0.2777126630147298, Validation Loss: 0.571428120136261\n",
      "Epoch 26: Train Loss: 0.2706688642501831, Validation Loss: 0.5689489245414734\n",
      "Epoch 27: Train Loss: 0.270800585548083, Validation Loss: 0.5664775371551514\n",
      "Epoch 28: Train Loss: 0.2608275016148885, Validation Loss: 0.5651007890701294\n",
      "Epoch 29: Train Loss: 0.2697741985321045, Validation Loss: 0.5651248097419739\n",
      "Epoch 30: Train Loss: 0.254815215865771, Validation Loss: 0.5406324863433838\n",
      "Epoch 31: Train Loss: 0.25235875447591144, Validation Loss: 0.530613362789154\n",
      "Epoch 32: Train Loss: 0.2530166357755661, Validation Loss: 0.5340395569801331\n",
      "Epoch 33: Train Loss: 0.23962914943695068, Validation Loss: 0.5480387806892395\n",
      "Epoch 34: Train Loss: 0.21385449667771658, Validation Loss: 0.5604031085968018\n",
      "Epoch 35: Train Loss: 0.2304925173521042, Validation Loss: 0.5741349458694458\n",
      "Epoch 36: Train Loss: 0.2591403126716614, Validation Loss: 0.5801826119422913\n",
      "Epoch 37: Train Loss: 0.21269646286964417, Validation Loss: 0.5828033089637756\n",
      "Epoch 38: Train Loss: 0.21032468974590302, Validation Loss: 0.5857539772987366\n",
      "Epoch 39: Train Loss: 0.19881711403528848, Validation Loss: 0.586076021194458\n",
      "Epoch 40: Train Loss: 0.2159984509150187, Validation Loss: 0.5928354263305664\n",
      "Early stopping at epoch 41\n",
      "Fold 4\n",
      "Epoch 0: Train Loss: 0.6940699418385824, Validation Loss: 0.694641649723053\n",
      "Epoch 1: Train Loss: 0.6008172631263733, Validation Loss: 0.6848639845848083\n",
      "Epoch 2: Train Loss: 0.5588512221972147, Validation Loss: 0.6764554381370544\n",
      "Epoch 3: Train Loss: 0.5277071197827657, Validation Loss: 0.6675297617912292\n",
      "Epoch 4: Train Loss: 0.5188924372196198, Validation Loss: 0.6586575508117676\n",
      "Epoch 5: Train Loss: 0.5014661947886149, Validation Loss: 0.6495336294174194\n",
      "Epoch 6: Train Loss: 0.4738614360491435, Validation Loss: 0.6412472128868103\n",
      "Epoch 7: Train Loss: 0.47310782472292584, Validation Loss: 0.633642852306366\n",
      "Epoch 8: Train Loss: 0.45889169971148175, Validation Loss: 0.6260455846786499\n",
      "Epoch 9: Train Loss: 0.47221459945042926, Validation Loss: 0.6194354891777039\n",
      "Epoch 10: Train Loss: 0.4612019956111908, Validation Loss: 0.6115626692771912\n",
      "Epoch 11: Train Loss: 0.4416451354821523, Validation Loss: 0.6013430953025818\n",
      "Epoch 12: Train Loss: 0.41330333550771076, Validation Loss: 0.5908096432685852\n",
      "Epoch 13: Train Loss: 0.39800973733266193, Validation Loss: 0.5759825110435486\n",
      "Epoch 14: Train Loss: 0.3845366636912028, Validation Loss: 0.5674366354942322\n",
      "Epoch 15: Train Loss: 0.36268192529678345, Validation Loss: 0.5606427192687988\n",
      "Epoch 16: Train Loss: 0.35262882709503174, Validation Loss: 0.5541952252388\n",
      "Epoch 17: Train Loss: 0.35760097702344257, Validation Loss: 0.5508696436882019\n",
      "Epoch 18: Train Loss: 0.3518952826658885, Validation Loss: 0.5489485263824463\n",
      "Epoch 19: Train Loss: 0.3716597358385722, Validation Loss: 0.5473757386207581\n",
      "Epoch 20: Train Loss: 0.34870466589927673, Validation Loss: 0.5403720140457153\n",
      "Epoch 21: Train Loss: 0.32601088285446167, Validation Loss: 0.5323452949523926\n",
      "Epoch 22: Train Loss: 0.3328693111737569, Validation Loss: 0.5215494632720947\n",
      "Epoch 23: Train Loss: 0.29645130038261414, Validation Loss: 0.5076956152915955\n",
      "Epoch 24: Train Loss: 0.2889397044976552, Validation Loss: 0.5000801086425781\n",
      "Epoch 25: Train Loss: 0.3017982542514801, Validation Loss: 0.4981290400028229\n",
      "Epoch 26: Train Loss: 0.2863237609465917, Validation Loss: 0.4936818480491638\n",
      "Epoch 27: Train Loss: 0.2641923228899638, Validation Loss: 0.48989999294281006\n",
      "Epoch 28: Train Loss: 0.2621685067812602, Validation Loss: 0.48805850744247437\n",
      "Epoch 29: Train Loss: 0.279519259929657, Validation Loss: 0.4867188334465027\n",
      "Epoch 30: Train Loss: 0.26448198159535724, Validation Loss: 0.4811277687549591\n",
      "Epoch 31: Train Loss: 0.24429989357789358, Validation Loss: 0.47459304332733154\n",
      "Epoch 32: Train Loss: 0.2453989932934443, Validation Loss: 0.46715232729911804\n",
      "Epoch 33: Train Loss: 0.2413072238365809, Validation Loss: 0.47582101821899414\n",
      "Epoch 34: Train Loss: 0.2178431898355484, Validation Loss: 0.47085249423980713\n",
      "Epoch 35: Train Loss: 0.22150974969069162, Validation Loss: 0.46484774351119995\n",
      "Epoch 36: Train Loss: 0.21025466918945312, Validation Loss: 0.45724451541900635\n",
      "Epoch 37: Train Loss: 0.2116104712088903, Validation Loss: 0.4550067186355591\n",
      "Epoch 38: Train Loss: 0.21688441435496011, Validation Loss: 0.45472726225852966\n",
      "Epoch 39: Train Loss: 0.22588000694910684, Validation Loss: 0.45532122254371643\n",
      "Epoch 40: Train Loss: 0.2018574426571528, Validation Loss: 0.44209471344947815\n",
      "Epoch 41: Train Loss: 0.23362540702025095, Validation Loss: 0.45286300778388977\n",
      "Epoch 42: Train Loss: 0.21724557876586914, Validation Loss: 0.443211168050766\n",
      "Epoch 43: Train Loss: 0.17735955119132996, Validation Loss: 0.4186858832836151\n",
      "Epoch 44: Train Loss: 0.18286100029945374, Validation Loss: 0.41453057527542114\n",
      "Epoch 45: Train Loss: 0.18735651671886444, Validation Loss: 0.43288594484329224\n",
      "Epoch 46: Train Loss: 0.17037296295166016, Validation Loss: 0.44224604964256287\n",
      "Epoch 47: Train Loss: 0.17922632892926535, Validation Loss: 0.4397602677345276\n",
      "Epoch 48: Train Loss: 0.16278399030367532, Validation Loss: 0.43812718987464905\n",
      "Epoch 49: Train Loss: 0.1834530234336853, Validation Loss: 0.43664664030075073\n",
      "Epoch 50: Train Loss: 0.1817996104558309, Validation Loss: 0.4183834493160248\n",
      "Epoch 51: Train Loss: 0.19813929001490274, Validation Loss: 0.43776118755340576\n",
      "Epoch 52: Train Loss: 0.15198401113351187, Validation Loss: 0.4598870575428009\n",
      "Epoch 53: Train Loss: 0.153767558435599, Validation Loss: 0.4666054844856262\n",
      "Early stopping at epoch 54\n",
      "Fold 5\n",
      "Epoch 0: Train Loss: 0.6756677428881327, Validation Loss: 0.6807886362075806\n",
      "Epoch 1: Train Loss: 0.6152436137199402, Validation Loss: 0.6772233843803406\n",
      "Epoch 2: Train Loss: 0.5730535785357157, Validation Loss: 0.6757623553276062\n",
      "Epoch 3: Train Loss: 0.5458670457204183, Validation Loss: 0.6754303574562073\n",
      "Epoch 4: Train Loss: 0.5302229126294454, Validation Loss: 0.6780338883399963\n",
      "Epoch 5: Train Loss: 0.5060833096504211, Validation Loss: 0.6806657910346985\n",
      "Epoch 6: Train Loss: 0.495987464984258, Validation Loss: 0.6849424242973328\n",
      "Epoch 7: Train Loss: 0.48689013719558716, Validation Loss: 0.689294695854187\n",
      "Epoch 8: Train Loss: 0.4772622883319855, Validation Loss: 0.6945236325263977\n",
      "Epoch 9: Train Loss: 0.48976537585258484, Validation Loss: 0.7017284631729126\n",
      "Epoch 10: Train Loss: 0.47948580980300903, Validation Loss: 0.7031224370002747\n",
      "Epoch 11: Train Loss: 0.4412440260251363, Validation Loss: 0.7022475004196167\n",
      "Epoch 12: Train Loss: 0.4192044933636983, Validation Loss: 0.6917844414710999\n",
      "Early stopping at epoch 13\n",
      "Accuracy: 0.7416666666666667,Precision: 0.7959183673469388, Recall: 0.65, F1-score: 0.7155963302752294, AUC: 0.7416666666666667\n",
      "Confusion Matrix:\n",
      "[[50 10]\n",
      " [21 39]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nweight decay = 1e-7\\nlearning rate = 0.0001\\nepoch = 100\\nbatch size = 32\\nearly stopping patience = 10\\nstandard scaler\\nReLU\\ncross entropy loss\\ndrop out = 0.8\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "model_save_dir = r'C:\\Users\\User\\Documents\\Lie detect data\\Model'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Define a function to load, cut and pad data\n",
    "def load_data(data_dir, max_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    file_list = os.listdir(data_dir)\n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        label = 1 if 'truth' in file else 0\n",
    "        if data.shape[1] > max_length:\n",
    "            processed_data = data[:, :max_length]  # Cut data if it exceeds max_length\n",
    "        else:\n",
    "            processed_data = np.zeros((data.shape[0], max_length))\n",
    "            processed_data[:, :data.shape[1]] = data  # Pad data if it is shorter than max_length\n",
    "        X.append(processed_data)\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load dataset and process the data\n",
    "data_dir = \"C:\\\\Users\\\\User\\\\Documents\\\\Lie detect data\\\\56M_DWTEEGData\"\n",
    "max_length = 500  # Define maximum length for cutting and padding\n",
    "X, y = load_data(data_dir, max_length)\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# Define EEGNet model\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(65, 32, kernel_size=63, padding=31)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "        self.depthwiseConv1d = nn.Conv1d(32, 64, kernel_size=65, groups=32, padding=32)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)  # Additional convolutional layer\n",
    "        self.batchnorm3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.pooling = nn.AvgPool1d(kernel_size=4)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        \n",
    "        self._calculate_num_features()\n",
    "        self.fc = nn.Linear(self.num_features, num_classes)\n",
    "\n",
    "    def _calculate_num_features(self):\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.zeros(1, 65, 1400)\n",
    "            sample_output = self._forward_features(sample_input)\n",
    "            self.num_features = sample_output.shape[1]\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.depthwiseConv1d(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.conv2(x)  # Additional convolutional layer\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.pooling(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.global_pool(x)  # Global average pooling layer\n",
    "        x = torch.flatten(x, 1)  # Flatten the output for the fully connected layer\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_and_evaluate(train_loader, val_loader, y_train):\n",
    "    model = EEGNet(num_classes=2).to(device)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "    num_epochs = 100\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "            fold_model_path = os.path.join(model_save_dir, f'fold3_model_fold_{fold_idx}.pth')\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': best_val_loss,\n",
    "            }, fold_model_path)\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_idx = 0\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "for train_index, val_index in kf.split(X, y):\n",
    "    print(f'Fold {fold_idx + 1}')\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Normalize data using scaler fitted on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_val = scaler.transform(X_val.reshape(X_val.shape[0], -1))\n",
    "    X_train = X_train.reshape(-1, 65, max_length)\n",
    "    X_val = X_val.reshape(-1, 65, max_length)\n",
    "\n",
    "    # Save the scaler to a file\n",
    "    with open(r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\simpleEEGNet_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = EEGDataset(X_train, y_train)\n",
    "    val_dataset = EEGDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = train_and_evaluate(train_loader, val_loader, y_train)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    fold_idx += 1\n",
    "\n",
    "\n",
    "# Calculate additional metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "auc = roc_auc_score(all_labels, all_predictions)\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy},Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "weight decay = 1e-7\n",
    "learning rate = 0.0001\n",
    "epoch = 100\n",
    "batch size = 32\n",
    "early stopping patience = 10\n",
    "standard scaler\n",
    "ReLU\n",
    "cross entropy loss\n",
    "drop out = 0.8\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66654d63-c450-4d9f-a3e3-63701fd1d41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934cb89-5ea1-4eb0-a152-00c71e49e06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
