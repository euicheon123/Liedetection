{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "656cdbc2-da54-45ba-97fc-195b59bd7820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 0: Train Loss: 0.7023430864016215, Validation Loss: 0.689907431602478\n",
      "Epoch 1: Train Loss: 0.6715287367502848, Validation Loss: 0.6867794990539551\n",
      "Epoch 2: Train Loss: 0.650465190410614, Validation Loss: 0.6826143264770508\n",
      "Epoch 3: Train Loss: 0.6275926232337952, Validation Loss: 0.6765142679214478\n",
      "Epoch 4: Train Loss: 0.6231590906778971, Validation Loss: 0.6684725880622864\n",
      "Epoch 5: Train Loss: 0.6045318245887756, Validation Loss: 0.6581513285636902\n",
      "Epoch 6: Train Loss: 0.5984524289766947, Validation Loss: 0.646645188331604\n",
      "Epoch 7: Train Loss: 0.5952226122220358, Validation Loss: 0.6325746178627014\n",
      "Epoch 8: Train Loss: 0.5829063455263773, Validation Loss: 0.6185172200202942\n",
      "Epoch 9: Train Loss: 0.5916460752487183, Validation Loss: 0.6054142713546753\n",
      "Epoch 10: Train Loss: 0.578544040520986, Validation Loss: 0.5703256726264954\n",
      "Epoch 11: Train Loss: 0.5651192863782247, Validation Loss: 0.5396777391433716\n",
      "Epoch 12: Train Loss: 0.5471428036689758, Validation Loss: 0.5214055180549622\n",
      "Epoch 13: Train Loss: 0.5331645210584005, Validation Loss: 0.5048184990882874\n",
      "Epoch 14: Train Loss: 0.5108115275700887, Validation Loss: 0.48986712098121643\n",
      "Epoch 15: Train Loss: 0.5107545355955759, Validation Loss: 0.4862140417098999\n",
      "Epoch 16: Train Loss: 0.5044477681318918, Validation Loss: 0.48028531670570374\n",
      "Epoch 17: Train Loss: 0.5058458348115286, Validation Loss: 0.47383636236190796\n",
      "Epoch 18: Train Loss: 0.4888640344142914, Validation Loss: 0.46388012170791626\n",
      "Epoch 19: Train Loss: 0.49274394909540814, Validation Loss: 0.4550207257270813\n",
      "Epoch 20: Train Loss: 0.49216581384340924, Validation Loss: 0.4804399609565735\n",
      "Epoch 21: Train Loss: 0.46840285261472064, Validation Loss: 0.523338258266449\n",
      "Epoch 22: Train Loss: 0.46266178290049237, Validation Loss: 0.573147714138031\n",
      "Epoch 23: Train Loss: 0.4489755829175313, Validation Loss: 0.5960646271705627\n",
      "Epoch 24: Train Loss: 0.43029112617174786, Validation Loss: 0.5750163793563843\n",
      "Epoch 25: Train Loss: 0.43633894125620526, Validation Loss: 0.5545364618301392\n",
      "Epoch 26: Train Loss: 0.4235919713973999, Validation Loss: 0.5231238007545471\n",
      "Epoch 27: Train Loss: 0.4190903306007385, Validation Loss: 0.4804271459579468\n",
      "Epoch 28: Train Loss: 0.4004075030485789, Validation Loss: 0.45478615164756775\n",
      "Epoch 29: Train Loss: 0.4153488377730052, Validation Loss: 0.4380550980567932\n",
      "Epoch 30: Train Loss: 0.4021455744902293, Validation Loss: 0.4759480953216553\n",
      "Epoch 31: Train Loss: 0.4008125265439351, Validation Loss: 0.4821884334087372\n",
      "Epoch 32: Train Loss: 0.3874320387840271, Validation Loss: 0.5063546299934387\n",
      "Epoch 33: Train Loss: 0.36798304319381714, Validation Loss: 0.4929363429546356\n",
      "Epoch 34: Train Loss: 0.39009687304496765, Validation Loss: 0.4684852361679077\n",
      "Epoch 35: Train Loss: 0.3621431688467662, Validation Loss: 0.4634512662887573\n",
      "Epoch 36: Train Loss: 0.3533145785331726, Validation Loss: 0.464506596326828\n",
      "Epoch 37: Train Loss: 0.36200210452079773, Validation Loss: 0.4549518823623657\n",
      "Epoch 38: Train Loss: 0.3484647770722707, Validation Loss: 0.4469950199127197\n",
      "Early stopping at epoch 39\n",
      "Fold 2\n",
      "Epoch 0: Train Loss: 0.7274444897969564, Validation Loss: 0.6941461563110352\n",
      "Epoch 1: Train Loss: 0.6880584359169006, Validation Loss: 0.6930782198905945\n",
      "Epoch 2: Train Loss: 0.6648643811543783, Validation Loss: 0.6915714740753174\n",
      "Epoch 3: Train Loss: 0.6427547534306844, Validation Loss: 0.6877422332763672\n",
      "Epoch 4: Train Loss: 0.6314753293991089, Validation Loss: 0.6839112043380737\n",
      "Epoch 5: Train Loss: 0.611808697382609, Validation Loss: 0.6778212785720825\n",
      "Epoch 6: Train Loss: 0.6035506725311279, Validation Loss: 0.6715947389602661\n",
      "Epoch 7: Train Loss: 0.608320931593577, Validation Loss: 0.6651227474212646\n",
      "Epoch 8: Train Loss: 0.5937758485476176, Validation Loss: 0.6581647992134094\n",
      "Epoch 9: Train Loss: 0.5963290731112162, Validation Loss: 0.6517979502677917\n",
      "Epoch 10: Train Loss: 0.5913500388463339, Validation Loss: 0.6481480598449707\n",
      "Epoch 11: Train Loss: 0.5739366809527079, Validation Loss: 0.6425151228904724\n",
      "Epoch 12: Train Loss: 0.5544038017590841, Validation Loss: 0.631007730960846\n",
      "Epoch 13: Train Loss: 0.5585029522577921, Validation Loss: 0.6219654679298401\n",
      "Epoch 14: Train Loss: 0.528315524260203, Validation Loss: 0.6176941990852356\n",
      "Epoch 15: Train Loss: 0.5185633897781372, Validation Loss: 0.621878981590271\n",
      "Epoch 16: Train Loss: 0.5077428122361501, Validation Loss: 0.6221503019332886\n",
      "Epoch 17: Train Loss: 0.5068957507610321, Validation Loss: 0.6198856234550476\n",
      "Epoch 18: Train Loss: 0.5057369470596313, Validation Loss: 0.6181179881095886\n",
      "Epoch 19: Train Loss: 0.5052302877108256, Validation Loss: 0.6183055639266968\n",
      "Epoch 20: Train Loss: 0.5054599245389303, Validation Loss: 0.6054336428642273\n",
      "Epoch 21: Train Loss: 0.48331762353579205, Validation Loss: 0.5997291803359985\n",
      "Epoch 22: Train Loss: 0.47497695684432983, Validation Loss: 0.5969551205635071\n",
      "Epoch 23: Train Loss: 0.455474317073822, Validation Loss: 0.6042458415031433\n",
      "Epoch 24: Train Loss: 0.449301819006602, Validation Loss: 0.6112434267997742\n",
      "Epoch 25: Train Loss: 0.43636272350947064, Validation Loss: 0.6053663492202759\n",
      "Epoch 26: Train Loss: 0.4410504500071208, Validation Loss: 0.5963606238365173\n",
      "Epoch 27: Train Loss: 0.4278181493282318, Validation Loss: 0.593628466129303\n",
      "Epoch 28: Train Loss: 0.4277117649714152, Validation Loss: 0.5929072499275208\n",
      "Epoch 29: Train Loss: 0.43602827191352844, Validation Loss: 0.5927450060844421\n",
      "Epoch 30: Train Loss: 0.43080101410547894, Validation Loss: 0.6066734194755554\n",
      "Epoch 31: Train Loss: 0.4026324152946472, Validation Loss: 0.594875156879425\n",
      "Epoch 32: Train Loss: 0.3995041847229004, Validation Loss: 0.5868116617202759\n",
      "Epoch 33: Train Loss: 0.3978479603926341, Validation Loss: 0.5921828150749207\n",
      "Epoch 34: Train Loss: 0.3929543693860372, Validation Loss: 0.5870794653892517\n",
      "Epoch 35: Train Loss: 0.36807551980018616, Validation Loss: 0.5862812399864197\n",
      "Epoch 36: Train Loss: 0.36388452847798664, Validation Loss: 0.5789667367935181\n",
      "Epoch 37: Train Loss: 0.3781055808067322, Validation Loss: 0.5709413290023804\n",
      "Epoch 38: Train Loss: 0.35904090603192645, Validation Loss: 0.5656207203865051\n",
      "Epoch 39: Train Loss: 0.36240166425704956, Validation Loss: 0.5633693933486938\n",
      "Epoch 40: Train Loss: 0.3502088487148285, Validation Loss: 0.5491317510604858\n",
      "Epoch 41: Train Loss: 0.34925606846809387, Validation Loss: 0.5391486287117004\n",
      "Epoch 42: Train Loss: 0.34209689497947693, Validation Loss: 0.5413780808448792\n",
      "Epoch 43: Train Loss: 0.355543980995814, Validation Loss: 0.5284419655799866\n",
      "Epoch 44: Train Loss: 0.33195985356966656, Validation Loss: 0.5476032495498657\n",
      "Epoch 45: Train Loss: 0.33763156334559125, Validation Loss: 0.5283279418945312\n",
      "Epoch 46: Train Loss: 0.3051534394423167, Validation Loss: 0.5132982730865479\n",
      "Epoch 47: Train Loss: 0.3135463297367096, Validation Loss: 0.517399251461029\n",
      "Epoch 48: Train Loss: 0.30241914590199787, Validation Loss: 0.5210623741149902\n",
      "Epoch 49: Train Loss: 0.30590641498565674, Validation Loss: 0.5200032591819763\n",
      "Epoch 50: Train Loss: 0.2977696160475413, Validation Loss: 0.5516136884689331\n",
      "Epoch 51: Train Loss: 0.2931170066197713, Validation Loss: 0.526237964630127\n",
      "Epoch 52: Train Loss: 0.28871890902519226, Validation Loss: 0.5662184953689575\n",
      "Epoch 53: Train Loss: 0.281839519739151, Validation Loss: 0.5425519943237305\n",
      "Epoch 54: Train Loss: 0.27010361353556317, Validation Loss: 0.5234371423721313\n",
      "Epoch 55: Train Loss: 0.2652566184600194, Validation Loss: 0.48118749260902405\n",
      "Epoch 56: Train Loss: 0.27485550940036774, Validation Loss: 0.46927696466445923\n",
      "Epoch 57: Train Loss: 0.26194386184215546, Validation Loss: 0.47188785672187805\n",
      "Epoch 58: Train Loss: 0.25461754699548084, Validation Loss: 0.4792819321155548\n",
      "Epoch 59: Train Loss: 0.2561926047007243, Validation Loss: 0.47900643944740295\n",
      "Epoch 60: Train Loss: 0.26896729071935016, Validation Loss: 0.510890781879425\n",
      "Epoch 61: Train Loss: 0.2589186777671178, Validation Loss: 0.46707114577293396\n",
      "Epoch 62: Train Loss: 0.24383026858170828, Validation Loss: 0.44553595781326294\n",
      "Epoch 63: Train Loss: 0.2543506771326065, Validation Loss: 0.4934859573841095\n",
      "Epoch 64: Train Loss: 0.2629324346780777, Validation Loss: 0.5040248036384583\n",
      "Epoch 65: Train Loss: 0.22790292898813883, Validation Loss: 0.5023936629295349\n",
      "Epoch 66: Train Loss: 0.23525930444399515, Validation Loss: 0.4817841649055481\n",
      "Epoch 67: Train Loss: 0.2593924303849538, Validation Loss: 0.4976966381072998\n",
      "Epoch 68: Train Loss: 0.22122345368067423, Validation Loss: 0.4897474944591522\n",
      "Epoch 69: Train Loss: 0.22709059218565622, Validation Loss: 0.48613402247428894\n",
      "Epoch 70: Train Loss: 0.23590107758839926, Validation Loss: 0.5129621028900146\n",
      "Epoch 71: Train Loss: 0.21780913571516672, Validation Loss: 0.6185216307640076\n",
      "Early stopping at epoch 72\n",
      "Fold 3\n",
      "Epoch 0: Train Loss: 0.7144414385159811, Validation Loss: 0.7118842005729675\n",
      "Epoch 1: Train Loss: 0.6707759499549866, Validation Loss: 0.710172176361084\n",
      "Epoch 2: Train Loss: 0.6495983004570007, Validation Loss: 0.7074077725410461\n",
      "Epoch 3: Train Loss: 0.6200730800628662, Validation Loss: 0.7037609219551086\n",
      "Epoch 4: Train Loss: 0.6080683072408041, Validation Loss: 0.7003849744796753\n",
      "Epoch 5: Train Loss: 0.5961487690607706, Validation Loss: 0.6960477828979492\n",
      "Epoch 6: Train Loss: 0.5855176051457723, Validation Loss: 0.6903437376022339\n",
      "Epoch 7: Train Loss: 0.5834586024284363, Validation Loss: 0.6831109523773193\n",
      "Epoch 8: Train Loss: 0.5802135268847147, Validation Loss: 0.6740813255310059\n",
      "Epoch 9: Train Loss: 0.5933846831321716, Validation Loss: 0.6654351353645325\n",
      "Epoch 10: Train Loss: 0.5725800196329752, Validation Loss: 0.6531497836112976\n",
      "Epoch 11: Train Loss: 0.5559577345848083, Validation Loss: 0.6437085866928101\n",
      "Epoch 12: Train Loss: 0.5433469812075297, Validation Loss: 0.6345946788787842\n",
      "Epoch 13: Train Loss: 0.5240967472394308, Validation Loss: 0.6203974485397339\n",
      "Epoch 14: Train Loss: 0.5119165579477946, Validation Loss: 0.5978043675422668\n",
      "Epoch 15: Train Loss: 0.49311143159866333, Validation Loss: 0.5840327143669128\n",
      "Epoch 16: Train Loss: 0.492215891679128, Validation Loss: 0.5772331953048706\n",
      "Epoch 17: Train Loss: 0.48432305455207825, Validation Loss: 0.5730205774307251\n",
      "Epoch 18: Train Loss: 0.4805799126625061, Validation Loss: 0.5661824345588684\n",
      "Epoch 19: Train Loss: 0.4764611820379893, Validation Loss: 0.5625904202461243\n",
      "Epoch 20: Train Loss: 0.4800492326418559, Validation Loss: 0.5806910395622253\n",
      "Epoch 21: Train Loss: 0.4564777612686157, Validation Loss: 0.5956670641899109\n",
      "Epoch 22: Train Loss: 0.4474155902862549, Validation Loss: 0.6385959386825562\n",
      "Epoch 23: Train Loss: 0.444677213827769, Validation Loss: 0.5797417759895325\n",
      "Epoch 24: Train Loss: 0.41610384980837506, Validation Loss: 0.6007990837097168\n",
      "Epoch 25: Train Loss: 0.40826939543088275, Validation Loss: 0.6130079627037048\n",
      "Epoch 26: Train Loss: 0.4069850842158, Validation Loss: 0.5943410992622375\n",
      "Epoch 27: Train Loss: 0.3991688887278239, Validation Loss: 0.5683897733688354\n",
      "Epoch 28: Train Loss: 0.3970028758049011, Validation Loss: 0.5493782758712769\n",
      "Epoch 29: Train Loss: 0.3874187966187795, Validation Loss: 0.5365309119224548\n",
      "Epoch 30: Train Loss: 0.38710202773412067, Validation Loss: 0.5542944073677063\n",
      "Epoch 31: Train Loss: 0.3820010920365651, Validation Loss: 0.539763867855072\n",
      "Epoch 32: Train Loss: 0.3701928953329722, Validation Loss: 0.5205292105674744\n",
      "Epoch 33: Train Loss: 0.36616116762161255, Validation Loss: 0.5603336095809937\n",
      "Epoch 34: Train Loss: 0.3471941153208415, Validation Loss: 0.5803412199020386\n",
      "Epoch 35: Train Loss: 0.3467904527982076, Validation Loss: 0.5505831241607666\n",
      "Epoch 36: Train Loss: 0.35248637199401855, Validation Loss: 0.5452669262886047\n",
      "Epoch 37: Train Loss: 0.33287861943244934, Validation Loss: 0.5409833788871765\n",
      "Epoch 38: Train Loss: 0.32751088341077167, Validation Loss: 0.5318392515182495\n",
      "Epoch 39: Train Loss: 0.32668094833691913, Validation Loss: 0.5254982709884644\n",
      "Epoch 40: Train Loss: 0.32385027408599854, Validation Loss: 0.5212024450302124\n",
      "Epoch 41: Train Loss: 0.3323809007803599, Validation Loss: 0.609542191028595\n",
      "Early stopping at epoch 42\n",
      "Fold 4\n",
      "Epoch 0: Train Loss: 0.6990987658500671, Validation Loss: 0.6825913190841675\n",
      "Epoch 1: Train Loss: 0.6672314008076986, Validation Loss: 0.6831510066986084\n",
      "Epoch 2: Train Loss: 0.6376989285151163, Validation Loss: 0.6840139031410217\n",
      "Epoch 3: Train Loss: 0.624589721361796, Validation Loss: 0.6850336194038391\n",
      "Epoch 4: Train Loss: 0.6071260372797648, Validation Loss: 0.6859782338142395\n",
      "Epoch 5: Train Loss: 0.5941402912139893, Validation Loss: 0.6870364546775818\n",
      "Epoch 6: Train Loss: 0.5922676126162211, Validation Loss: 0.688422679901123\n",
      "Epoch 7: Train Loss: 0.5828190048535665, Validation Loss: 0.6898408532142639\n",
      "Epoch 8: Train Loss: 0.5768645902474722, Validation Loss: 0.6908872723579407\n",
      "Epoch 9: Train Loss: 0.5753988027572632, Validation Loss: 0.6910945773124695\n",
      "Early stopping at epoch 10\n",
      "Fold 5\n",
      "Epoch 0: Train Loss: 0.688457985719045, Validation Loss: 0.6782912015914917\n",
      "Epoch 1: Train Loss: 0.656217614809672, Validation Loss: 0.6775640249252319\n",
      "Epoch 2: Train Loss: 0.6250325242678324, Validation Loss: 0.6778048872947693\n",
      "Epoch 3: Train Loss: 0.6126905481020609, Validation Loss: 0.6793866157531738\n",
      "Epoch 4: Train Loss: 0.605540136496226, Validation Loss: 0.6815932989120483\n",
      "Epoch 5: Train Loss: 0.5772703886032104, Validation Loss: 0.6840454936027527\n",
      "Epoch 6: Train Loss: 0.5725323557853699, Validation Loss: 0.6866756081581116\n",
      "Epoch 7: Train Loss: 0.56002809604009, Validation Loss: 0.6897815465927124\n",
      "Epoch 8: Train Loss: 0.5584814349810282, Validation Loss: 0.6937912702560425\n",
      "Epoch 9: Train Loss: 0.5555726488431295, Validation Loss: 0.6988180875778198\n",
      "Epoch 10: Train Loss: 0.5558392802874247, Validation Loss: 0.6978446841239929\n",
      "Early stopping at epoch 11\n",
      "Accuracy: 0.6916666666666667,Precision: 0.676923076923077, Recall: 0.7333333333333333, F1-score: 0.704, AUC: 0.6916666666666667\n",
      "Confusion Matrix:\n",
      "[[39 21]\n",
      " [16 44]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Define a function to load and pad data\n",
    "def load_data(data_dir, max_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    file_list = os.listdir(data_dir)\n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        label = 0 if 'truth' in file else 1\n",
    "        padded_data = np.zeros((65, max_length))\n",
    "        length = min(data.shape[1], max_length)\n",
    "        padded_data[:, :length] = data[:, :length]\n",
    "        X.append(padded_data)\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load dataset and pad the data\n",
    "data_dir = \"C:\\\\Users\\\\User\\\\Documents\\\\Lie detect data\\\\56M_DWTEEGData\"\n",
    "max_length = 1400  # Define maximum length for padding\n",
    "X, y = load_data(data_dir, max_length)\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# Define EEGNet model\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(65, 32, kernel_size=63, padding=31)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "        self.depthwiseConv1d = nn.Conv1d(32, 64, kernel_size=65, groups=32, padding=32)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)  # Additional convolutional layer\n",
    "        self.batchnorm3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.pooling = nn.AvgPool1d(kernel_size=4)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        \n",
    "        self._calculate_num_features()\n",
    "        self.fc = nn.Linear(self.num_features, num_classes)\n",
    "\n",
    "    def _calculate_num_features(self):\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.zeros(1, 65, 1400)\n",
    "            sample_output = self._forward_features(sample_input)\n",
    "            self.num_features = sample_output.shape[1]\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.depthwiseConv1d(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.conv2(x)  # Additional convolutional layer\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.pooling(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.global_pool(x)  # Global average pooling layer\n",
    "        x = torch.flatten(x, 1)  # Flatten the output for the fully connected layer\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_and_evaluate(train_loader, val_loader, y_train):\n",
    "    model = EEGNet(num_classes=2).to(device)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "    num_epochs = 100\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_idx = 0\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "for train_index, val_index in kf.split(X, y):\n",
    "    print(f'Fold {fold_idx + 1}')\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Normalize data using scaler fitted on training data\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_val = scaler.transform(X_val.reshape(X_val.shape[0], -1))\n",
    "    X_train = X_train.reshape(-1, 65, max_length)\n",
    "    X_val = X_val.reshape(-1, 65, max_length)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = EEGDataset(X_train, y_train)\n",
    "    val_dataset = EEGDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = train_and_evaluate(train_loader, val_loader, y_train)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    fold_idx += 1\n",
    "\n",
    "\n",
    "# Calculate additional metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "auc = roc_auc_score(all_labels, all_predictions)\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy},Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66654d63-c450-4d9f-a3e3-63701fd1d41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934cb89-5ea1-4eb0-a152-00c71e49e06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
