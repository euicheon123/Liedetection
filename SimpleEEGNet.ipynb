{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656cdbc2-da54-45ba-97fc-195b59bd7820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 0: Train Loss: 0.675963282585144, Validation Loss: 0.675963282585144\n",
      "Epoch 1: Train Loss: 0.6797987222671509, Validation Loss: 0.6797987222671509\n",
      "Epoch 2: Train Loss: 0.664971113204956, Validation Loss: 0.664971113204956\n",
      "Epoch 3: Train Loss: 0.6679155230522156, Validation Loss: 0.6679155230522156\n",
      "Epoch 4: Train Loss: 0.6694998145103455, Validation Loss: 0.6694998145103455\n",
      "Epoch 5: Train Loss: 0.668910026550293, Validation Loss: 0.668910026550293\n",
      "Epoch 6: Train Loss: 0.6596152186393738, Validation Loss: 0.6596152186393738\n",
      "Epoch 7: Train Loss: 0.6534891724586487, Validation Loss: 0.6534891724586487\n",
      "Epoch 8: Train Loss: 0.6464304327964783, Validation Loss: 0.6464304327964783\n",
      "Epoch 9: Train Loss: 0.6408336758613586, Validation Loss: 0.6408336758613586\n",
      "Epoch 10: Train Loss: 0.6090766787528992, Validation Loss: 0.6090766787528992\n",
      "Epoch 11: Train Loss: 0.593525767326355, Validation Loss: 0.593525767326355\n",
      "Epoch 12: Train Loss: 0.5923876762390137, Validation Loss: 0.5923876762390137\n",
      "Epoch 13: Train Loss: 0.585578203201294, Validation Loss: 0.585578203201294\n",
      "Epoch 14: Train Loss: 0.5648589134216309, Validation Loss: 0.5648589134216309\n",
      "Epoch 15: Train Loss: 0.5968329310417175, Validation Loss: 0.5968329310417175\n",
      "Epoch 16: Train Loss: 0.5860287547111511, Validation Loss: 0.5860287547111511\n",
      "Epoch 17: Train Loss: 0.5675830245018005, Validation Loss: 0.5675830245018005\n",
      "Epoch 18: Train Loss: 0.5501070022583008, Validation Loss: 0.5501070022583008\n",
      "Epoch 19: Train Loss: 0.5459749698638916, Validation Loss: 0.5459749698638916\n",
      "Epoch 20: Train Loss: 0.5053545832633972, Validation Loss: 0.5053545832633972\n",
      "Epoch 21: Train Loss: 0.5993809700012207, Validation Loss: 0.5993809700012207\n",
      "Epoch 22: Train Loss: 0.4918273091316223, Validation Loss: 0.4918273091316223\n",
      "Epoch 23: Train Loss: 0.4838499426841736, Validation Loss: 0.4838499426841736\n",
      "Epoch 24: Train Loss: 0.48633965849876404, Validation Loss: 0.48633965849876404\n",
      "Epoch 25: Train Loss: 0.5305513739585876, Validation Loss: 0.5305513739585876\n",
      "Epoch 26: Train Loss: 0.4829389154911041, Validation Loss: 0.4829389154911041\n",
      "Epoch 27: Train Loss: 0.46105602383613586, Validation Loss: 0.46105602383613586\n",
      "Epoch 28: Train Loss: 0.46196702122688293, Validation Loss: 0.46196702122688293\n",
      "Epoch 29: Train Loss: 0.46457427740097046, Validation Loss: 0.46457427740097046\n",
      "Epoch 30: Train Loss: 0.738198459148407, Validation Loss: 0.738198459148407\n",
      "Epoch 31: Train Loss: 0.4610498249530792, Validation Loss: 0.4610498249530792\n",
      "Epoch 32: Train Loss: 0.5301194787025452, Validation Loss: 0.5301194787025452\n",
      "Epoch 33: Train Loss: 0.7826393842697144, Validation Loss: 0.7826393842697144\n",
      "Epoch 34: Train Loss: 0.5067680478096008, Validation Loss: 0.5067680478096008\n",
      "Epoch 35: Train Loss: 0.4634729325771332, Validation Loss: 0.4634729325771332\n",
      "Epoch 36: Train Loss: 0.5117638111114502, Validation Loss: 0.5117638111114502\n",
      "Epoch 37: Train Loss: 0.5413409471511841, Validation Loss: 0.5413409471511841\n",
      "Epoch 38: Train Loss: 0.5331693887710571, Validation Loss: 0.5331693887710571\n",
      "Epoch 39: Train Loss: 0.5221019983291626, Validation Loss: 0.5221019983291626\n",
      "Epoch 40: Train Loss: 0.4151057004928589, Validation Loss: 0.4151057004928589\n",
      "Epoch 41: Train Loss: 0.5246225595474243, Validation Loss: 0.5246225595474243\n",
      "Epoch 42: Train Loss: 0.5182411074638367, Validation Loss: 0.5182411074638367\n",
      "Epoch 43: Train Loss: 0.4185336232185364, Validation Loss: 0.4185336232185364\n",
      "Epoch 44: Train Loss: 0.4922593832015991, Validation Loss: 0.4922593832015991\n",
      "Epoch 45: Train Loss: 0.6128047704696655, Validation Loss: 0.6128047704696655\n",
      "Epoch 46: Train Loss: 0.4796288311481476, Validation Loss: 0.4796288311481476\n",
      "Epoch 47: Train Loss: 0.47494637966156006, Validation Loss: 0.47494637966156006\n",
      "Epoch 48: Train Loss: 0.4858942925930023, Validation Loss: 0.4858942925930023\n",
      "Epoch 49: Train Loss: 0.4865977466106415, Validation Loss: 0.4865977466106415\n",
      "Epoch 50: Train Loss: 0.5327908396720886, Validation Loss: 0.5327908396720886\n",
      "Epoch 51: Train Loss: 0.467599093914032, Validation Loss: 0.467599093914032\n",
      "Epoch 52: Train Loss: 0.5635567903518677, Validation Loss: 0.5635567903518677\n",
      "Epoch 53: Train Loss: 0.47408348321914673, Validation Loss: 0.47408348321914673\n",
      "Epoch 54: Train Loss: 0.5256025791168213, Validation Loss: 0.5256025791168213\n",
      "Epoch 55: Train Loss: 0.4990426003932953, Validation Loss: 0.4990426003932953\n",
      "Epoch 56: Train Loss: 0.45346251130104065, Validation Loss: 0.45346251130104065\n",
      "Epoch 57: Train Loss: 0.499228835105896, Validation Loss: 0.499228835105896\n",
      "Epoch 58: Train Loss: 0.5466945171356201, Validation Loss: 0.5466945171356201\n",
      "Epoch 59: Train Loss: 0.5519335865974426, Validation Loss: 0.5519335865974426\n",
      "Early stopping at epoch 60\n",
      "Fold 2\n",
      "Epoch 0: Train Loss: 0.6910082697868347, Validation Loss: 0.6910082697868347\n",
      "Epoch 1: Train Loss: 0.6856476068496704, Validation Loss: 0.6856476068496704\n",
      "Epoch 2: Train Loss: 0.676224410533905, Validation Loss: 0.676224410533905\n",
      "Epoch 3: Train Loss: 0.6645190119743347, Validation Loss: 0.6645190119743347\n",
      "Epoch 4: Train Loss: 0.6569027900695801, Validation Loss: 0.6569027900695801\n",
      "Epoch 5: Train Loss: 0.6437930464744568, Validation Loss: 0.6437930464744568\n",
      "Epoch 6: Train Loss: 0.6350268721580505, Validation Loss: 0.6350268721580505\n",
      "Epoch 7: Train Loss: 0.622963547706604, Validation Loss: 0.622963547706604\n",
      "Epoch 8: Train Loss: 0.6094801425933838, Validation Loss: 0.6094801425933838\n",
      "Epoch 9: Train Loss: 0.5983951091766357, Validation Loss: 0.5983951091766357\n",
      "Epoch 10: Train Loss: 0.5850183963775635, Validation Loss: 0.5850183963775635\n",
      "Epoch 11: Train Loss: 0.5739232301712036, Validation Loss: 0.5739232301712036\n",
      "Epoch 12: Train Loss: 0.5705748796463013, Validation Loss: 0.5705748796463013\n",
      "Epoch 13: Train Loss: 0.5570505261421204, Validation Loss: 0.5570505261421204\n",
      "Epoch 14: Train Loss: 0.5592256784439087, Validation Loss: 0.5592256784439087\n",
      "Epoch 15: Train Loss: 0.5375915765762329, Validation Loss: 0.5375915765762329\n",
      "Epoch 16: Train Loss: 0.5323920249938965, Validation Loss: 0.5323920249938965\n",
      "Epoch 17: Train Loss: 0.5275038480758667, Validation Loss: 0.5275038480758667\n",
      "Epoch 18: Train Loss: 0.5256481766700745, Validation Loss: 0.5256481766700745\n",
      "Epoch 19: Train Loss: 0.5226079821586609, Validation Loss: 0.5226079821586609\n",
      "Epoch 20: Train Loss: 0.8320316672325134, Validation Loss: 0.8320316672325134\n",
      "Epoch 21: Train Loss: 0.5507444143295288, Validation Loss: 0.5507444143295288\n",
      "Epoch 22: Train Loss: 0.5060423016548157, Validation Loss: 0.5060423016548157\n",
      "Epoch 23: Train Loss: 0.6598536372184753, Validation Loss: 0.6598536372184753\n",
      "Epoch 24: Train Loss: 0.6235374808311462, Validation Loss: 0.6235374808311462\n",
      "Epoch 25: Train Loss: 0.5530707240104675, Validation Loss: 0.5530707240104675\n",
      "Epoch 26: Train Loss: 0.5631119608879089, Validation Loss: 0.5631119608879089\n",
      "Epoch 27: Train Loss: 0.5820985436439514, Validation Loss: 0.5820985436439514\n",
      "Epoch 28: Train Loss: 0.5871140360832214, Validation Loss: 0.5871140360832214\n",
      "Epoch 29: Train Loss: 0.5709802508354187, Validation Loss: 0.5709802508354187\n",
      "Epoch 30: Train Loss: 0.527111828327179, Validation Loss: 0.527111828327179\n",
      "Epoch 31: Train Loss: 0.8191670179367065, Validation Loss: 0.8191670179367065\n",
      "Epoch 32: Train Loss: 0.5222218036651611, Validation Loss: 0.5222218036651611\n",
      "Epoch 33: Train Loss: 0.6023977398872375, Validation Loss: 0.6023977398872375\n",
      "Epoch 34: Train Loss: 0.641180694103241, Validation Loss: 0.641180694103241\n",
      "Epoch 35: Train Loss: 0.644404947757721, Validation Loss: 0.644404947757721\n",
      "Epoch 36: Train Loss: 0.5873149633407593, Validation Loss: 0.5873149633407593\n",
      "Epoch 37: Train Loss: 0.5724127888679504, Validation Loss: 0.5724127888679504\n",
      "Epoch 38: Train Loss: 0.5768641829490662, Validation Loss: 0.5768641829490662\n",
      "Epoch 39: Train Loss: 0.5778624415397644, Validation Loss: 0.5778624415397644\n",
      "Epoch 40: Train Loss: 0.6459465622901917, Validation Loss: 0.6459465622901917\n",
      "Epoch 41: Train Loss: 0.740280270576477, Validation Loss: 0.740280270576477\n",
      "Early stopping at epoch 42\n",
      "Fold 3\n",
      "Epoch 0: Train Loss: 0.6581417322158813, Validation Loss: 0.6581417322158813\n",
      "Epoch 1: Train Loss: 0.6378636360168457, Validation Loss: 0.6378636360168457\n",
      "Epoch 2: Train Loss: 0.6303514838218689, Validation Loss: 0.6303514838218689\n",
      "Epoch 3: Train Loss: 0.6240693926811218, Validation Loss: 0.6240693926811218\n",
      "Epoch 4: Train Loss: 0.6047273278236389, Validation Loss: 0.6047273278236389\n",
      "Epoch 5: Train Loss: 0.5973012447357178, Validation Loss: 0.5973012447357178\n",
      "Epoch 6: Train Loss: 0.595767617225647, Validation Loss: 0.595767617225647\n",
      "Epoch 7: Train Loss: 0.5947077870368958, Validation Loss: 0.5947077870368958\n",
      "Epoch 8: Train Loss: 0.5925881266593933, Validation Loss: 0.5925881266593933\n",
      "Epoch 9: Train Loss: 0.5923324823379517, Validation Loss: 0.5923324823379517\n",
      "Epoch 10: Train Loss: 0.5643260478973389, Validation Loss: 0.5643260478973389\n",
      "Epoch 11: Train Loss: 0.5569308996200562, Validation Loss: 0.5569308996200562\n",
      "Epoch 12: Train Loss: 0.5894302129745483, Validation Loss: 0.5894302129745483\n",
      "Epoch 13: Train Loss: 0.5844879150390625, Validation Loss: 0.5844879150390625\n",
      "Epoch 14: Train Loss: 0.529339075088501, Validation Loss: 0.529339075088501\n",
      "Epoch 15: Train Loss: 0.5260042548179626, Validation Loss: 0.5260042548179626\n",
      "Epoch 16: Train Loss: 0.5763887166976929, Validation Loss: 0.5763887166976929\n",
      "Epoch 17: Train Loss: 0.6003106236457825, Validation Loss: 0.6003106236457825\n",
      "Epoch 18: Train Loss: 0.5869796276092529, Validation Loss: 0.5869796276092529\n",
      "Epoch 19: Train Loss: 0.5807807445526123, Validation Loss: 0.5807807445526123\n",
      "Epoch 20: Train Loss: 0.5122779607772827, Validation Loss: 0.5122779607772827\n",
      "Epoch 21: Train Loss: 0.552913248538971, Validation Loss: 0.552913248538971\n",
      "Epoch 22: Train Loss: 0.503411054611206, Validation Loss: 0.503411054611206\n",
      "Epoch 23: Train Loss: 0.5267207026481628, Validation Loss: 0.5267207026481628\n",
      "Epoch 24: Train Loss: 0.4731207489967346, Validation Loss: 0.4731207489967346\n",
      "Epoch 25: Train Loss: 0.4837281107902527, Validation Loss: 0.4837281107902527\n",
      "Epoch 26: Train Loss: 0.47396162152290344, Validation Loss: 0.47396162152290344\n",
      "Epoch 27: Train Loss: 0.4582797884941101, Validation Loss: 0.4582797884941101\n",
      "Epoch 28: Train Loss: 0.4460688829421997, Validation Loss: 0.4460688829421997\n",
      "Epoch 29: Train Loss: 0.4477445185184479, Validation Loss: 0.4477445185184479\n",
      "Epoch 30: Train Loss: 0.44252949953079224, Validation Loss: 0.44252949953079224\n",
      "Epoch 31: Train Loss: 0.4358009099960327, Validation Loss: 0.4358009099960327\n",
      "Epoch 32: Train Loss: 0.42407557368278503, Validation Loss: 0.42407557368278503\n",
      "Epoch 33: Train Loss: 0.4263620972633362, Validation Loss: 0.4263620972633362\n",
      "Epoch 34: Train Loss: 0.42744195461273193, Validation Loss: 0.42744195461273193\n",
      "Epoch 35: Train Loss: 0.4392063319683075, Validation Loss: 0.4392063319683075\n",
      "Epoch 36: Train Loss: 0.5575167536735535, Validation Loss: 0.5575167536735535\n",
      "Epoch 37: Train Loss: 0.5616485476493835, Validation Loss: 0.5616485476493835\n",
      "Epoch 38: Train Loss: 0.47810760140419006, Validation Loss: 0.47810760140419006\n",
      "Epoch 39: Train Loss: 0.4646630883216858, Validation Loss: 0.4646630883216858\n",
      "Epoch 40: Train Loss: 0.5240211486816406, Validation Loss: 0.5240211486816406\n",
      "Epoch 41: Train Loss: 0.8866388201713562, Validation Loss: 0.8866388201713562\n",
      "Epoch 42: Train Loss: 0.567725658416748, Validation Loss: 0.567725658416748\n",
      "Epoch 43: Train Loss: 0.4756085276603699, Validation Loss: 0.4756085276603699\n",
      "Epoch 44: Train Loss: 0.5509333610534668, Validation Loss: 0.5509333610534668\n",
      "Epoch 45: Train Loss: 0.6126541495323181, Validation Loss: 0.6126541495323181\n",
      "Epoch 46: Train Loss: 0.45163705945014954, Validation Loss: 0.45163705945014954\n",
      "Epoch 47: Train Loss: 0.4287613332271576, Validation Loss: 0.4287613332271576\n",
      "Epoch 48: Train Loss: 0.4355185329914093, Validation Loss: 0.4355185329914093\n",
      "Epoch 49: Train Loss: 0.4401401877403259, Validation Loss: 0.4401401877403259\n",
      "Epoch 50: Train Loss: 0.4572542905807495, Validation Loss: 0.4572542905807495\n",
      "Epoch 51: Train Loss: 0.564202606678009, Validation Loss: 0.564202606678009\n",
      "Early stopping at epoch 52\n",
      "Fold 4\n",
      "Epoch 0: Train Loss: 0.6997292041778564, Validation Loss: 0.6997292041778564\n",
      "Epoch 1: Train Loss: 0.7039334774017334, Validation Loss: 0.7039334774017334\n",
      "Epoch 2: Train Loss: 0.6859952211380005, Validation Loss: 0.6859952211380005\n",
      "Epoch 3: Train Loss: 0.7042114734649658, Validation Loss: 0.7042114734649658\n",
      "Epoch 4: Train Loss: 0.7062607407569885, Validation Loss: 0.7062607407569885\n",
      "Epoch 5: Train Loss: 0.6934143900871277, Validation Loss: 0.6934143900871277\n",
      "Epoch 6: Train Loss: 0.6819337606430054, Validation Loss: 0.6819337606430054\n",
      "Epoch 7: Train Loss: 0.6755649447441101, Validation Loss: 0.6755649447441101\n",
      "Epoch 8: Train Loss: 0.6722856163978577, Validation Loss: 0.6722856163978577\n",
      "Epoch 9: Train Loss: 0.6682235598564148, Validation Loss: 0.6682235598564148\n",
      "Epoch 10: Train Loss: 0.7213307619094849, Validation Loss: 0.7213307619094849\n",
      "Epoch 11: Train Loss: 0.6931108236312866, Validation Loss: 0.6931108236312866\n",
      "Epoch 12: Train Loss: 0.6258915066719055, Validation Loss: 0.6258915066719055\n",
      "Epoch 13: Train Loss: 0.6917493343353271, Validation Loss: 0.6917493343353271\n",
      "Epoch 14: Train Loss: 0.5940953493118286, Validation Loss: 0.5940953493118286\n",
      "Epoch 15: Train Loss: 0.5898512601852417, Validation Loss: 0.5898512601852417\n",
      "Epoch 16: Train Loss: 0.5840064287185669, Validation Loss: 0.5840064287185669\n",
      "Epoch 17: Train Loss: 0.5798647999763489, Validation Loss: 0.5798647999763489\n",
      "Epoch 18: Train Loss: 0.5752035975456238, Validation Loss: 0.5752035975456238\n",
      "Epoch 19: Train Loss: 0.5719698071479797, Validation Loss: 0.5719698071479797\n",
      "Epoch 20: Train Loss: 0.5835728645324707, Validation Loss: 0.5835728645324707\n",
      "Epoch 21: Train Loss: 0.5662784576416016, Validation Loss: 0.5662784576416016\n",
      "Epoch 22: Train Loss: 0.6359834671020508, Validation Loss: 0.6359834671020508\n",
      "Epoch 23: Train Loss: 0.5275647044181824, Validation Loss: 0.5275647044181824\n",
      "Epoch 24: Train Loss: 0.5270052552223206, Validation Loss: 0.5270052552223206\n",
      "Epoch 25: Train Loss: 0.5947588086128235, Validation Loss: 0.5947588086128235\n",
      "Epoch 26: Train Loss: 0.5077229738235474, Validation Loss: 0.5077229738235474\n",
      "Epoch 27: Train Loss: 0.5193483233451843, Validation Loss: 0.5193483233451843\n",
      "Epoch 28: Train Loss: 0.5356948375701904, Validation Loss: 0.5356948375701904\n",
      "Epoch 29: Train Loss: 0.5387815833091736, Validation Loss: 0.5387815833091736\n",
      "Epoch 30: Train Loss: 0.5947353839874268, Validation Loss: 0.5947353839874268\n",
      "Epoch 31: Train Loss: 0.517452597618103, Validation Loss: 0.517452597618103\n",
      "Epoch 32: Train Loss: 0.5246818661689758, Validation Loss: 0.5246818661689758\n",
      "Epoch 33: Train Loss: 0.7823586463928223, Validation Loss: 0.7823586463928223\n",
      "Epoch 34: Train Loss: 0.4877004027366638, Validation Loss: 0.4877004027366638\n",
      "Epoch 35: Train Loss: 0.4828977882862091, Validation Loss: 0.4828977882862091\n",
      "Epoch 36: Train Loss: 0.4921037256717682, Validation Loss: 0.4921037256717682\n",
      "Epoch 37: Train Loss: 0.4805994927883148, Validation Loss: 0.4805994927883148\n",
      "Epoch 38: Train Loss: 0.4771287441253662, Validation Loss: 0.4771287441253662\n",
      "Epoch 39: Train Loss: 0.48396027088165283, Validation Loss: 0.48396027088165283\n",
      "Epoch 40: Train Loss: 1.04718017578125, Validation Loss: 1.04718017578125\n",
      "Epoch 41: Train Loss: 0.6539889574050903, Validation Loss: 0.6539889574050903\n",
      "Epoch 42: Train Loss: 0.4973316490650177, Validation Loss: 0.4973316490650177\n",
      "Epoch 43: Train Loss: 0.5692556500434875, Validation Loss: 0.5692556500434875\n",
      "Epoch 44: Train Loss: 0.5084466934204102, Validation Loss: 0.5084466934204102\n",
      "Epoch 45: Train Loss: 0.5136769413948059, Validation Loss: 0.5136769413948059\n",
      "Epoch 46: Train Loss: 0.47463274002075195, Validation Loss: 0.47463274002075195\n",
      "Epoch 47: Train Loss: 0.49760401248931885, Validation Loss: 0.49760401248931885\n",
      "Epoch 48: Train Loss: 0.52913498878479, Validation Loss: 0.52913498878479\n",
      "Epoch 49: Train Loss: 0.5356999039649963, Validation Loss: 0.5356999039649963\n",
      "Epoch 50: Train Loss: 0.5842745900154114, Validation Loss: 0.5842745900154114\n",
      "Epoch 51: Train Loss: 0.4753131866455078, Validation Loss: 0.4753131866455078\n",
      "Epoch 52: Train Loss: 0.5059627294540405, Validation Loss: 0.5059627294540405\n",
      "Epoch 53: Train Loss: 0.6959858536720276, Validation Loss: 0.6959858536720276\n",
      "Epoch 54: Train Loss: 0.48531872034072876, Validation Loss: 0.48531872034072876\n",
      "Epoch 55: Train Loss: 0.48231399059295654, Validation Loss: 0.48231399059295654\n",
      "Epoch 56: Train Loss: 0.4993526339530945, Validation Loss: 0.4993526339530945\n",
      "Epoch 57: Train Loss: 0.5054391026496887, Validation Loss: 0.5054391026496887\n",
      "Epoch 58: Train Loss: 0.4966423511505127, Validation Loss: 0.4966423511505127\n",
      "Epoch 59: Train Loss: 0.49711745977401733, Validation Loss: 0.49711745977401733\n",
      "Epoch 60: Train Loss: 0.515529990196228, Validation Loss: 0.515529990196228\n",
      "Epoch 61: Train Loss: 0.7200607061386108, Validation Loss: 0.7200607061386108\n",
      "Epoch 62: Train Loss: 0.6667882800102234, Validation Loss: 0.6667882800102234\n",
      "Epoch 63: Train Loss: 0.6838936805725098, Validation Loss: 0.6838936805725098\n",
      "Epoch 64: Train Loss: 0.5583061575889587, Validation Loss: 0.5583061575889587\n",
      "Epoch 65: Train Loss: 0.539253294467926, Validation Loss: 0.539253294467926\n",
      "Early stopping at epoch 66\n",
      "Fold 5\n",
      "Epoch 0: Train Loss: 0.6949828267097473, Validation Loss: 0.6949828267097473\n",
      "Epoch 1: Train Loss: 0.6855222582817078, Validation Loss: 0.6855222582817078\n",
      "Epoch 2: Train Loss: 0.7022140026092529, Validation Loss: 0.7022140026092529\n",
      "Epoch 3: Train Loss: 0.7282335162162781, Validation Loss: 0.7282335162162781\n",
      "Epoch 4: Train Loss: 0.7252177596092224, Validation Loss: 0.7252177596092224\n",
      "Epoch 5: Train Loss: 0.7129236459732056, Validation Loss: 0.7129236459732056\n",
      "Epoch 6: Train Loss: 0.7164873480796814, Validation Loss: 0.7164873480796814\n",
      "Epoch 7: Train Loss: 0.7231261134147644, Validation Loss: 0.7231261134147644\n",
      "Epoch 8: Train Loss: 0.7296326160430908, Validation Loss: 0.7296326160430908\n",
      "Epoch 9: Train Loss: 0.7340576648712158, Validation Loss: 0.7340576648712158\n",
      "Epoch 10: Train Loss: 0.8084981441497803, Validation Loss: 0.8084981441497803\n",
      "Epoch 11: Train Loss: 0.7933030724525452, Validation Loss: 0.7933030724525452\n",
      "Epoch 12: Train Loss: 0.7578815817832947, Validation Loss: 0.7578815817832947\n",
      "Epoch 13: Train Loss: 0.7938792705535889, Validation Loss: 0.7938792705535889\n",
      "Epoch 14: Train Loss: 0.8825573325157166, Validation Loss: 0.8825573325157166\n",
      "Epoch 15: Train Loss: 0.8594474196434021, Validation Loss: 0.8594474196434021\n",
      "Epoch 16: Train Loss: 0.7915340662002563, Validation Loss: 0.7915340662002563\n",
      "Epoch 17: Train Loss: 0.7488805055618286, Validation Loss: 0.7488805055618286\n",
      "Epoch 18: Train Loss: 0.735104501247406, Validation Loss: 0.735104501247406\n",
      "Epoch 19: Train Loss: 0.7326132655143738, Validation Loss: 0.7326132655143738\n",
      "Epoch 20: Train Loss: 0.7059017419815063, Validation Loss: 0.7059017419815063\n",
      "Early stopping at epoch 21\n",
      "Accuracy: 0.7,Precision: 0.7, Recall: 0.7, F1-score: 0.7, AUC: 0.7\n",
      "Confusion Matrix:\n",
      "[[42 18]\n",
      " [18 42]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "# Define a function to load and pad data\n",
    "def load_data(data_dir, max_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    file_list = os.listdir(data_dir)\n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        label = 0 if 'truth' in file else 1\n",
    "        padded_data = np.zeros((65, max_length))\n",
    "        length = min(data.shape[1], max_length)\n",
    "        padded_data[:, :length] = data[:, :length]\n",
    "        X.append(padded_data)\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load dataset and pad the data\n",
    "data_dir = \"C:\\\\Users\\\\User\\\\Documents\\\\Lie detect data\\\\56M_DWTEEGData\"\n",
    "max_length = 1400  # Define maximum length for padding\n",
    "X, y = load_data(data_dir, max_length)\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# Define EEG model\n",
    "class EnhancedEEGNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(EnhancedEEGNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, 63), padding='same')\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "        self.depthwiseConv = nn.Conv2d(16, 32, (65, 1), groups=16, padding='same')\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
    "        self.separableConv = nn.Conv2d(32, 64, (1, 16), padding='same')\n",
    "        self.batchnorm3 = nn.BatchNorm2d(64)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        self.avgPool = nn.AvgPool2d((1, 4))\n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(64 * 65 * 15, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.depthwiseConv(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.avgPool(x)\n",
    "        x = self.separableConv(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.avgPool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_and_evaluate(train_loader, val_loader, y_train):\n",
    "    model = EEGNet(num_classes=2).to(device)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "    num_epochs = 100\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 20\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch}: Train Loss: {loss.item()}, Validation Loss: {val_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_idx = 0\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "for train_index, val_index in kf.split(X, y):\n",
    "    print(f'Fold {fold_idx + 1}')\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Normalize data\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_train = X_train.reshape(-1, 65, max_length)\n",
    "\n",
    "    X_val = X_val.reshape(X_val.shape[0], -1)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_val = X_val.reshape(-1, 65, max_length)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = EEGDataset(X_train, y_train)\n",
    "    val_dataset = EEGDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = train_and_evaluate(train_loader, val_loader, y_train)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    fold_idx += 1\n",
    "\n",
    "\n",
    "# Calculate additional metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "auc = roc_auc_score(all_labels, all_predictions)\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy},Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66654d63-c450-4d9f-a3e3-63701fd1d41b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
