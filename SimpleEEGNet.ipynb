{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "656cdbc2-da54-45ba-97fc-195b59bd7820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 0: Train Loss: 0.6814963221549988, Validation Loss: 0.6608911156654358\n",
      "Epoch 1: Train Loss: 0.6389466921488444, Validation Loss: 0.6514667868614197\n",
      "Epoch 2: Train Loss: 0.6173742612202963, Validation Loss: 0.6393904089927673\n",
      "Epoch 3: Train Loss: 0.6038448015848795, Validation Loss: 0.6249602437019348\n",
      "Epoch 4: Train Loss: 0.5907721122105917, Validation Loss: 0.6103783845901489\n",
      "Epoch 5: Train Loss: 0.5858342051506042, Validation Loss: 0.5982075929641724\n",
      "Epoch 6: Train Loss: 0.5770986477533976, Validation Loss: 0.5865899920463562\n",
      "Epoch 7: Train Loss: 0.5675041278203329, Validation Loss: 0.5767109990119934\n",
      "Epoch 8: Train Loss: 0.5798966884613037, Validation Loss: 0.5682998895645142\n",
      "Epoch 9: Train Loss: 0.5644915699958801, Validation Loss: 0.5615315437316895\n",
      "Epoch 10: Train Loss: 0.5641041398048401, Validation Loss: 0.5489822030067444\n",
      "Epoch 11: Train Loss: 0.5607494711875916, Validation Loss: 0.5356813669204712\n",
      "Epoch 12: Train Loss: 0.5302018324534098, Validation Loss: 0.5239405632019043\n",
      "Epoch 13: Train Loss: 0.5230075319608053, Validation Loss: 0.5109614729881287\n",
      "Epoch 14: Train Loss: 0.503886491060257, Validation Loss: 0.5000759959220886\n",
      "Epoch 15: Train Loss: 0.5018652280171713, Validation Loss: 0.4934251308441162\n",
      "Epoch 16: Train Loss: 0.49151472250620526, Validation Loss: 0.48780950903892517\n",
      "Epoch 17: Train Loss: 0.4884544809659322, Validation Loss: 0.48403987288475037\n",
      "Epoch 18: Train Loss: 0.4884767234325409, Validation Loss: 0.4831157326698303\n",
      "Epoch 19: Train Loss: 0.47734297315279645, Validation Loss: 0.4813663065433502\n",
      "Epoch 20: Train Loss: 0.487648864587148, Validation Loss: 0.47305694222450256\n",
      "Epoch 21: Train Loss: 0.474550594886144, Validation Loss: 0.4620766043663025\n",
      "Epoch 22: Train Loss: 0.45906012256940204, Validation Loss: 0.4595333933830261\n",
      "Epoch 23: Train Loss: 0.43920950094858807, Validation Loss: 0.4598439335823059\n",
      "Epoch 24: Train Loss: 0.4263397951920827, Validation Loss: 0.4604552090167999\n",
      "Epoch 25: Train Loss: 0.4166989326477051, Validation Loss: 0.45647189021110535\n",
      "Epoch 26: Train Loss: 0.4123894472916921, Validation Loss: 0.4531632959842682\n",
      "Epoch 27: Train Loss: 0.423108438650767, Validation Loss: 0.45201894640922546\n",
      "Epoch 28: Train Loss: 0.4067161480585734, Validation Loss: 0.45115604996681213\n",
      "Epoch 29: Train Loss: 0.41526006658871967, Validation Loss: 0.45095303654670715\n",
      "Epoch 30: Train Loss: 0.40119900306065875, Validation Loss: 0.4462617039680481\n",
      "Epoch 31: Train Loss: 0.3883879880110423, Validation Loss: 0.4418404996395111\n",
      "Epoch 32: Train Loss: 0.3813985784848531, Validation Loss: 0.4309568405151367\n",
      "Epoch 33: Train Loss: 0.3747061292330424, Validation Loss: 0.4308193027973175\n",
      "Epoch 34: Train Loss: 0.3778066138426463, Validation Loss: 0.43625715374946594\n",
      "Epoch 35: Train Loss: 0.3665582338968913, Validation Loss: 0.4389050304889679\n",
      "Epoch 36: Train Loss: 0.35676465431849164, Validation Loss: 0.4388698637485504\n",
      "Epoch 37: Train Loss: 0.3426949481169383, Validation Loss: 0.43922892212867737\n",
      "Epoch 38: Train Loss: 0.33835504452387494, Validation Loss: 0.4402419924736023\n",
      "Epoch 39: Train Loss: 0.34323662519454956, Validation Loss: 0.44110286235809326\n",
      "Epoch 40: Train Loss: 0.3480580647786458, Validation Loss: 0.4374966025352478\n",
      "Epoch 41: Train Loss: 0.3366261621316274, Validation Loss: 0.4358218312263489\n",
      "Epoch 42: Train Loss: 0.33379117647806805, Validation Loss: 0.4445442259311676\n",
      "Epoch 43: Train Loss: 0.32249656319618225, Validation Loss: 0.4451541304588318\n",
      "Epoch 44: Train Loss: 0.3151458998521169, Validation Loss: 0.4451392590999603\n",
      "Epoch 45: Train Loss: 0.29590608676274616, Validation Loss: 0.4417414963245392\n",
      "Epoch 46: Train Loss: 0.3045766254266103, Validation Loss: 0.43891578912734985\n",
      "Epoch 47: Train Loss: 0.29611216982205707, Validation Loss: 0.44076773524284363\n",
      "Epoch 48: Train Loss: 0.2892647981643677, Validation Loss: 0.4404597580432892\n",
      "Epoch 49: Train Loss: 0.2996866802374522, Validation Loss: 0.44244879484176636\n",
      "Epoch 50: Train Loss: 0.2919875880082448, Validation Loss: 0.4558001756668091\n",
      "Epoch 51: Train Loss: 0.2927253146966298, Validation Loss: 0.45186683535575867\n",
      "Epoch 52: Train Loss: 0.27650851011276245, Validation Loss: 0.45005297660827637\n",
      "Epoch 53: Train Loss: 0.2804722289244334, Validation Loss: 0.45126673579216003\n",
      "Epoch 54: Train Loss: 0.27233260373274487, Validation Loss: 0.44838839769363403\n",
      "Epoch 55: Train Loss: 0.2623843898375829, Validation Loss: 0.44595229625701904\n",
      "Epoch 56: Train Loss: 0.2820754249890645, Validation Loss: 0.45489606261253357\n",
      "Epoch 57: Train Loss: 0.2697988947232564, Validation Loss: 0.45314058661460876\n",
      "Epoch 58: Train Loss: 0.267618993918101, Validation Loss: 0.4479726552963257\n",
      "Epoch 59: Train Loss: 0.2635145038366318, Validation Loss: 0.4479546546936035\n",
      "Epoch 60: Train Loss: 0.25253982841968536, Validation Loss: 0.4324494004249573\n",
      "Epoch 61: Train Loss: 0.2464961806933085, Validation Loss: 0.4340682923793793\n",
      "Epoch 62: Train Loss: 0.2594527453184128, Validation Loss: 0.43711307644844055\n",
      "Epoch 63: Train Loss: 0.24301470816135406, Validation Loss: 0.455552339553833\n",
      "Epoch 64: Train Loss: 0.24132137497266135, Validation Loss: 0.4526841938495636\n",
      "Epoch 65: Train Loss: 0.23928704857826233, Validation Loss: 0.4426139295101166\n",
      "Epoch 66: Train Loss: 0.22365741928418478, Validation Loss: 0.4387737810611725\n",
      "Epoch 67: Train Loss: 0.219970112045606, Validation Loss: 0.4457451403141022\n",
      "Epoch 68: Train Loss: 0.23215822875499725, Validation Loss: 0.449297696352005\n",
      "Epoch 69: Train Loss: 0.23701267937819162, Validation Loss: 0.44693753123283386\n",
      "Epoch 70: Train Loss: 0.22077418863773346, Validation Loss: 0.4963345229625702\n",
      "Epoch 71: Train Loss: 0.2329226682583491, Validation Loss: 0.4692695140838623\n",
      "Epoch 72: Train Loss: 0.22398966550827026, Validation Loss: 0.4380727708339691\n",
      "Epoch 73: Train Loss: 0.21304762860139212, Validation Loss: 0.4427449703216553\n",
      "Epoch 74: Train Loss: 0.21043581267197928, Validation Loss: 0.45968174934387207\n",
      "Epoch 75: Train Loss: 0.21215852598349252, Validation Loss: 0.44819411635398865\n",
      "Epoch 76: Train Loss: 0.20955715080102286, Validation Loss: 0.45656120777130127\n",
      "Epoch 77: Train Loss: 0.21008333563804626, Validation Loss: 0.46629124879837036\n",
      "Epoch 78: Train Loss: 0.19902067879835764, Validation Loss: 0.4666249454021454\n",
      "Epoch 79: Train Loss: 0.19858272870381674, Validation Loss: 0.46512746810913086\n",
      "Epoch 80: Train Loss: 0.20468772947788239, Validation Loss: 0.40752142667770386\n",
      "Epoch 81: Train Loss: 0.2007464716831843, Validation Loss: 0.44292277097702026\n",
      "Epoch 82: Train Loss: 0.19610920051733652, Validation Loss: 0.48814207315444946\n",
      "Epoch 83: Train Loss: 0.1985948383808136, Validation Loss: 0.48609453439712524\n",
      "Epoch 84: Train Loss: 0.19545067846775055, Validation Loss: 0.46201810240745544\n",
      "Epoch 85: Train Loss: 0.18913662433624268, Validation Loss: 0.43874990940093994\n",
      "Epoch 86: Train Loss: 0.17205982903639475, Validation Loss: 0.43210333585739136\n",
      "Epoch 87: Train Loss: 0.17291810611883798, Validation Loss: 0.43737637996673584\n",
      "Epoch 88: Train Loss: 0.17685348788897196, Validation Loss: 0.4417775273323059\n",
      "Epoch 89: Train Loss: 0.17892903089523315, Validation Loss: 0.4448358416557312\n",
      "Epoch 90: Train Loss: 0.19653017818927765, Validation Loss: 0.5024679899215698\n",
      "Epoch 91: Train Loss: 0.17694510022799173, Validation Loss: 0.5038099884986877\n",
      "Epoch 92: Train Loss: 0.16798004508018494, Validation Loss: 0.5080413222312927\n",
      "Epoch 93: Train Loss: 0.17276057104269663, Validation Loss: 0.47923359274864197\n",
      "Epoch 94: Train Loss: 0.17434904972712198, Validation Loss: 0.4686141312122345\n",
      "Epoch 95: Train Loss: 0.175913006067276, Validation Loss: 0.47854554653167725\n",
      "Epoch 96: Train Loss: 0.16512227058410645, Validation Loss: 0.47183656692504883\n",
      "Epoch 97: Train Loss: 0.14697894205649695, Validation Loss: 0.4713304042816162\n",
      "Epoch 98: Train Loss: 0.1579060604174932, Validation Loss: 0.47393402457237244\n",
      "Epoch 99: Train Loss: 0.16231500109036764, Validation Loss: 0.4793286919593811\n",
      "Epoch 100: Train Loss: 0.16876272857189178, Validation Loss: 0.46185579895973206\n",
      "Epoch 101: Train Loss: 0.17837591965993246, Validation Loss: 0.5295621752738953\n",
      "Epoch 102: Train Loss: 0.18138740956783295, Validation Loss: 0.48206907510757446\n",
      "Epoch 103: Train Loss: 0.14920013646284738, Validation Loss: 0.4251144528388977\n",
      "Epoch 104: Train Loss: 0.1735177387793859, Validation Loss: 0.450452983379364\n",
      "Epoch 105: Train Loss: 0.14815264443556467, Validation Loss: 0.4844099283218384\n",
      "Epoch 106: Train Loss: 0.15083463986714682, Validation Loss: 0.4951961934566498\n",
      "Epoch 107: Train Loss: 0.14719225466251373, Validation Loss: 0.496962308883667\n",
      "Epoch 108: Train Loss: 0.14685183266798654, Validation Loss: 0.4944935142993927\n",
      "Epoch 109: Train Loss: 0.14162350197633108, Validation Loss: 0.4890243709087372\n",
      "Epoch 110: Train Loss: 0.14045328150192896, Validation Loss: 0.46139103174209595\n",
      "Epoch 111: Train Loss: 0.1418845703204473, Validation Loss: 0.46850743889808655\n",
      "Epoch 112: Train Loss: 0.1364340533812841, Validation Loss: 0.4458417296409607\n",
      "Epoch 113: Train Loss: 0.13523167371749878, Validation Loss: 0.4444080889225006\n",
      "Epoch 114: Train Loss: 0.1524329831202825, Validation Loss: 0.47577616572380066\n",
      "Epoch 115: Train Loss: 0.15635495881239572, Validation Loss: 0.4851052165031433\n",
      "Epoch 116: Train Loss: 0.1367778405547142, Validation Loss: 0.47111594676971436\n",
      "Epoch 117: Train Loss: 0.1336656908194224, Validation Loss: 0.4647321105003357\n",
      "Epoch 118: Train Loss: 0.1271709750096003, Validation Loss: 0.4633994698524475\n",
      "Epoch 119: Train Loss: 0.12340446561574936, Validation Loss: 0.468713641166687\n",
      "Epoch 120: Train Loss: 0.11748640239238739, Validation Loss: 0.4846615791320801\n",
      "Epoch 121: Train Loss: 0.1374577929576238, Validation Loss: 0.4699365496635437\n",
      "Epoch 122: Train Loss: 0.1373374213774999, Validation Loss: 0.47248226404190063\n",
      "Epoch 123: Train Loss: 0.11572644859552383, Validation Loss: 0.4971674680709839\n",
      "Epoch 124: Train Loss: 0.12628823518753052, Validation Loss: 0.46584534645080566\n",
      "Epoch 125: Train Loss: 0.12796200811862946, Validation Loss: 0.45658570528030396\n",
      "Epoch 126: Train Loss: 0.11629183838764827, Validation Loss: 0.45296770334243774\n",
      "Epoch 127: Train Loss: 0.1200055181980133, Validation Loss: 0.47735002636909485\n",
      "Epoch 128: Train Loss: 0.11212463428576787, Validation Loss: 0.49195533990859985\n",
      "Epoch 129: Train Loss: 0.10921835899353027, Validation Loss: 0.49820053577423096\n",
      "Epoch 130: Train Loss: 0.11828923473755519, Validation Loss: 0.5390440821647644\n",
      "Epoch 131: Train Loss: 0.1165887142221133, Validation Loss: 0.49483561515808105\n",
      "Epoch 132: Train Loss: 0.12138252953688304, Validation Loss: 0.42955562472343445\n",
      "Epoch 133: Train Loss: 0.11438510318597157, Validation Loss: 0.4647442400455475\n",
      "Epoch 134: Train Loss: 0.10740501433610916, Validation Loss: 0.5000483989715576\n",
      "Epoch 135: Train Loss: 0.12517269452412924, Validation Loss: 0.4861913025379181\n",
      "Epoch 136: Train Loss: 0.10592268158992131, Validation Loss: 0.48721280694007874\n",
      "Epoch 137: Train Loss: 0.10355887313683827, Validation Loss: 0.48260077834129333\n",
      "Epoch 138: Train Loss: 0.12346465637286504, Validation Loss: 0.4837595224380493\n",
      "Epoch 139: Train Loss: 0.10907397419214249, Validation Loss: 0.47484204173088074\n",
      "Epoch 140: Train Loss: 0.10220580796400706, Validation Loss: 0.4689544141292572\n",
      "Epoch 141: Train Loss: 0.10725334535042445, Validation Loss: 0.4825698733329773\n",
      "Epoch 142: Train Loss: 0.10180586328109105, Validation Loss: 0.522687554359436\n",
      "Epoch 143: Train Loss: 0.10600361476341884, Validation Loss: 0.524796724319458\n",
      "Epoch 144: Train Loss: 0.09941318879524867, Validation Loss: 0.49815547466278076\n",
      "Epoch 145: Train Loss: 0.09420848141113917, Validation Loss: 0.5051822066307068\n",
      "Epoch 146: Train Loss: 0.0957498128215472, Validation Loss: 0.499346524477005\n",
      "Epoch 147: Train Loss: 0.11277600874503453, Validation Loss: 0.5036557912826538\n",
      "Epoch 148: Train Loss: 0.1047762210170428, Validation Loss: 0.5007795095443726\n",
      "Epoch 149: Train Loss: 0.09117851903041203, Validation Loss: 0.5067397952079773\n",
      "Epoch 150: Train Loss: 0.09827616810798645, Validation Loss: 0.4580700993537903\n",
      "Epoch 151: Train Loss: 0.09237948805093765, Validation Loss: 0.505192220211029\n",
      "Epoch 152: Train Loss: 0.1015884131193161, Validation Loss: 0.5338883399963379\n",
      "Epoch 153: Train Loss: 0.09635623296101888, Validation Loss: 0.4881831705570221\n",
      "Epoch 154: Train Loss: 0.08541632940371831, Validation Loss: 0.4932718575000763\n",
      "Epoch 155: Train Loss: 0.09230855604012807, Validation Loss: 0.5113968849182129\n",
      "Epoch 156: Train Loss: 0.08900451163450877, Validation Loss: 0.501761257648468\n",
      "Epoch 157: Train Loss: 0.08083894724647205, Validation Loss: 0.5011970400810242\n",
      "Epoch 158: Train Loss: 0.08331182599067688, Validation Loss: 0.5040420889854431\n",
      "Epoch 159: Train Loss: 0.09637819230556488, Validation Loss: 0.49586930871009827\n",
      "Epoch 160: Train Loss: 0.10235945632060368, Validation Loss: 0.5238621234893799\n",
      "Epoch 161: Train Loss: 0.08945245295763016, Validation Loss: 0.4657278060913086\n",
      "Epoch 162: Train Loss: 0.08680079877376556, Validation Loss: 0.5040109753608704\n",
      "Epoch 163: Train Loss: 0.09560602406660716, Validation Loss: 0.4946348965167999\n",
      "Epoch 164: Train Loss: 0.10203519215186437, Validation Loss: 0.5027159452438354\n",
      "Epoch 165: Train Loss: 0.083809994161129, Validation Loss: 0.533458411693573\n",
      "Epoch 166: Train Loss: 0.08202449977397919, Validation Loss: 0.5312414169311523\n",
      "Epoch 167: Train Loss: 0.08177535235881805, Validation Loss: 0.5272011756896973\n",
      "Epoch 168: Train Loss: 0.08109485109647115, Validation Loss: 0.5170769691467285\n",
      "Epoch 169: Train Loss: 0.07685882101456325, Validation Loss: 0.5190292596817017\n",
      "Epoch 170: Train Loss: 0.07832364737987518, Validation Loss: 0.49350541830062866\n",
      "Epoch 171: Train Loss: 0.08039057378967603, Validation Loss: 0.5271518230438232\n",
      "Epoch 172: Train Loss: 0.07673169920841853, Validation Loss: 0.5634259581565857\n",
      "Epoch 173: Train Loss: 0.11210384716590245, Validation Loss: 0.5302245020866394\n",
      "Epoch 174: Train Loss: 0.07363065828879674, Validation Loss: 0.5324375033378601\n",
      "Epoch 175: Train Loss: 0.08637151618798573, Validation Loss: 0.538057267665863\n",
      "Epoch 176: Train Loss: 0.0876836081345876, Validation Loss: 0.5317155718803406\n",
      "Epoch 177: Train Loss: 0.07751570393641789, Validation Loss: 0.5351144075393677\n",
      "Epoch 178: Train Loss: 0.07415516550342242, Validation Loss: 0.5411839485168457\n",
      "Epoch 179: Train Loss: 0.08232033004363377, Validation Loss: 0.5452640056610107\n",
      "Epoch 180: Train Loss: 0.09083605060974757, Validation Loss: 0.5784899592399597\n",
      "Epoch 181: Train Loss: 0.07512620463967323, Validation Loss: 0.6266079545021057\n",
      "Epoch 182: Train Loss: 0.10366358111302058, Validation Loss: 0.5764568448066711\n",
      "Epoch 183: Train Loss: 0.10294045507907867, Validation Loss: 0.5367397665977478\n",
      "Epoch 184: Train Loss: 0.0843264510234197, Validation Loss: 0.5089453458786011\n",
      "Epoch 185: Train Loss: 0.0742157759765784, Validation Loss: 0.5215170383453369\n",
      "Epoch 186: Train Loss: 0.07477367545167606, Validation Loss: 0.5293902158737183\n",
      "Epoch 187: Train Loss: 0.07916736602783203, Validation Loss: 0.5352540612220764\n",
      "Epoch 188: Train Loss: 0.08096953481435776, Validation Loss: 0.5223680138587952\n",
      "Epoch 189: Train Loss: 0.06965851411223412, Validation Loss: 0.5196914076805115\n",
      "Epoch 190: Train Loss: 0.06949210787812869, Validation Loss: 0.4797786474227905\n",
      "Epoch 191: Train Loss: 0.0669773705303669, Validation Loss: 0.4604324400424957\n",
      "Epoch 192: Train Loss: 0.06255831693609555, Validation Loss: 0.48041799664497375\n",
      "Epoch 193: Train Loss: 0.07023817052443822, Validation Loss: 0.4731985926628113\n",
      "Epoch 194: Train Loss: 0.06542912249763806, Validation Loss: 0.4983054995536804\n",
      "Epoch 195: Train Loss: 0.06323638185858727, Validation Loss: 0.5148156881332397\n",
      "Epoch 196: Train Loss: 0.0694285159309705, Validation Loss: 0.5302335619926453\n",
      "Epoch 197: Train Loss: 0.06370540708303452, Validation Loss: 0.5420578122138977\n",
      "Epoch 198: Train Loss: 0.05867766340573629, Validation Loss: 0.5428601503372192\n",
      "Epoch 199: Train Loss: 0.06182823826869329, Validation Loss: 0.5200749635696411\n",
      "Fold 2\n",
      "Epoch 0: Train Loss: 0.7084993322690328, Validation Loss: 0.7097263932228088\n",
      "Epoch 1: Train Loss: 0.6434996525446574, Validation Loss: 0.7079208493232727\n",
      "Epoch 2: Train Loss: 0.6211415926615397, Validation Loss: 0.7083221673965454\n",
      "Epoch 3: Train Loss: 0.5932915012041727, Validation Loss: 0.7093226909637451\n",
      "Epoch 4: Train Loss: 0.5860308806101481, Validation Loss: 0.7095602750778198\n",
      "Epoch 5: Train Loss: 0.5718832214673361, Validation Loss: 0.7110598683357239\n",
      "Epoch 6: Train Loss: 0.5686908960342407, Validation Loss: 0.7151709794998169\n",
      "Epoch 7: Train Loss: 0.5585456689198812, Validation Loss: 0.7205281257629395\n",
      "Epoch 8: Train Loss: 0.561011532942454, Validation Loss: 0.7294638752937317\n",
      "Epoch 9: Train Loss: 0.5547055999437968, Validation Loss: 0.7367774844169617\n",
      "Epoch 10: Train Loss: 0.5490642587343851, Validation Loss: 0.7483431100845337\n",
      "Epoch 11: Train Loss: 0.5314973394076029, Validation Loss: 0.7538260221481323\n",
      "Epoch 12: Train Loss: 0.518939753373464, Validation Loss: 0.7610015273094177\n",
      "Epoch 13: Train Loss: 0.5230723818143209, Validation Loss: 0.7604178190231323\n",
      "Epoch 14: Train Loss: 0.4953792790571849, Validation Loss: 0.7559528946876526\n",
      "Epoch 15: Train Loss: 0.4926789005597432, Validation Loss: 0.7487871050834656\n",
      "Epoch 16: Train Loss: 0.4764609932899475, Validation Loss: 0.7482931017875671\n",
      "Epoch 17: Train Loss: 0.4766170581181844, Validation Loss: 0.7421069741249084\n",
      "Epoch 18: Train Loss: 0.47354670365651447, Validation Loss: 0.7403314113616943\n",
      "Epoch 19: Train Loss: 0.47634029388427734, Validation Loss: 0.7403100728988647\n",
      "Epoch 20: Train Loss: 0.4598354995250702, Validation Loss: 0.7195895910263062\n",
      "Epoch 21: Train Loss: 0.4579731225967407, Validation Loss: 0.7034325003623962\n",
      "Epoch 22: Train Loss: 0.44315030177434284, Validation Loss: 0.6796190142631531\n",
      "Epoch 23: Train Loss: 0.4308855930964152, Validation Loss: 0.653774619102478\n",
      "Epoch 24: Train Loss: 0.40978426734606427, Validation Loss: 0.6393001079559326\n",
      "Epoch 25: Train Loss: 0.41013463338216144, Validation Loss: 0.6219993233680725\n",
      "Epoch 26: Train Loss: 0.41971099376678467, Validation Loss: 0.6121861934661865\n",
      "Epoch 27: Train Loss: 0.3980615735054016, Validation Loss: 0.6193759441375732\n",
      "Epoch 28: Train Loss: 0.39454445242881775, Validation Loss: 0.6176778078079224\n",
      "Epoch 29: Train Loss: 0.4127582311630249, Validation Loss: 0.6177997589111328\n",
      "Epoch 30: Train Loss: 0.38982872168223065, Validation Loss: 0.6073357462882996\n",
      "Epoch 31: Train Loss: 0.3756442864735921, Validation Loss: 0.579038679599762\n",
      "Epoch 32: Train Loss: 0.37340033054351807, Validation Loss: 0.5435811877250671\n",
      "Epoch 33: Train Loss: 0.3666720390319824, Validation Loss: 0.5212898254394531\n",
      "Epoch 34: Train Loss: 0.35394805669784546, Validation Loss: 0.49627652764320374\n",
      "Epoch 35: Train Loss: 0.35036654273668927, Validation Loss: 0.4938857853412628\n",
      "Epoch 36: Train Loss: 0.33742671211560565, Validation Loss: 0.49245715141296387\n",
      "Epoch 37: Train Loss: 0.332908570766449, Validation Loss: 0.4908953309059143\n",
      "Epoch 38: Train Loss: 0.33024267355600995, Validation Loss: 0.4930573105812073\n",
      "Epoch 39: Train Loss: 0.33374785383542377, Validation Loss: 0.4908767640590668\n",
      "Epoch 40: Train Loss: 0.3248582382996877, Validation Loss: 0.4819755554199219\n",
      "Epoch 41: Train Loss: 0.3159299890200297, Validation Loss: 0.4798167943954468\n",
      "Epoch 42: Train Loss: 0.3028977115948995, Validation Loss: 0.4786842465400696\n",
      "Epoch 43: Train Loss: 0.3041674494743347, Validation Loss: 0.4575985074043274\n",
      "Epoch 44: Train Loss: 0.29391754666964215, Validation Loss: 0.45181235671043396\n",
      "Epoch 45: Train Loss: 0.29669875899950665, Validation Loss: 0.45377683639526367\n",
      "Epoch 46: Train Loss: 0.2780839006106059, Validation Loss: 0.44746100902557373\n",
      "Epoch 47: Train Loss: 0.2782822648684184, Validation Loss: 0.4436905086040497\n",
      "Epoch 48: Train Loss: 0.27175972362359363, Validation Loss: 0.4406774044036865\n",
      "Epoch 49: Train Loss: 0.2717830588420232, Validation Loss: 0.4399150311946869\n",
      "Epoch 50: Train Loss: 0.2797767420609792, Validation Loss: 0.4467620253562927\n",
      "Epoch 51: Train Loss: 0.2603234102328618, Validation Loss: 0.44656267762184143\n",
      "Epoch 52: Train Loss: 0.2556915779908498, Validation Loss: 0.4316726326942444\n",
      "Epoch 53: Train Loss: 0.251119260986646, Validation Loss: 0.42307570576667786\n",
      "Epoch 54: Train Loss: 0.24983512361844382, Validation Loss: 0.4235261380672455\n",
      "Epoch 55: Train Loss: 0.2698698788881302, Validation Loss: 0.4377920925617218\n",
      "Epoch 56: Train Loss: 0.2253191570440928, Validation Loss: 0.43315163254737854\n",
      "Epoch 57: Train Loss: 0.23840790490309396, Validation Loss: 0.43346893787384033\n",
      "Epoch 58: Train Loss: 0.25502818326155346, Validation Loss: 0.437796950340271\n",
      "Epoch 59: Train Loss: 0.2363221893707911, Validation Loss: 0.43376341462135315\n",
      "Epoch 60: Train Loss: 0.2428609679142634, Validation Loss: 0.408598929643631\n",
      "Epoch 61: Train Loss: 0.22577478488286337, Validation Loss: 0.414374440908432\n",
      "Epoch 62: Train Loss: 0.21996836364269257, Validation Loss: 0.4048871397972107\n",
      "Epoch 63: Train Loss: 0.22299968202908835, Validation Loss: 0.40574657917022705\n",
      "Epoch 64: Train Loss: 0.24495652318000793, Validation Loss: 0.4044457674026489\n",
      "Epoch 65: Train Loss: 0.22518613437811533, Validation Loss: 0.4162381589412689\n",
      "Epoch 66: Train Loss: 0.20858635008335114, Validation Loss: 0.4266814887523651\n",
      "Epoch 67: Train Loss: 0.20261634389559427, Validation Loss: 0.420028418302536\n",
      "Epoch 68: Train Loss: 0.20293526848157248, Validation Loss: 0.4143679141998291\n",
      "Epoch 69: Train Loss: 0.2025244782368342, Validation Loss: 0.41496193408966064\n",
      "Epoch 70: Train Loss: 0.1994432657957077, Validation Loss: 0.4253672957420349\n",
      "Epoch 71: Train Loss: 0.1945213625828425, Validation Loss: 0.4081982374191284\n",
      "Epoch 72: Train Loss: 0.19093271593252817, Validation Loss: 0.3886110484600067\n",
      "Epoch 73: Train Loss: 0.18273261189460754, Validation Loss: 0.39018264412879944\n",
      "Epoch 74: Train Loss: 0.20712316036224365, Validation Loss: 0.39983028173446655\n",
      "Epoch 75: Train Loss: 0.21888557573159537, Validation Loss: 0.4089132249355316\n",
      "Epoch 76: Train Loss: 0.19276161988576254, Validation Loss: 0.4112991988658905\n",
      "Epoch 77: Train Loss: 0.1731215020020803, Validation Loss: 0.41180214285850525\n",
      "Epoch 78: Train Loss: 0.17424485584100088, Validation Loss: 0.41201066970825195\n",
      "Epoch 79: Train Loss: 0.1768090526262919, Validation Loss: 0.41267526149749756\n",
      "Epoch 80: Train Loss: 0.185694287220637, Validation Loss: 0.38880184292793274\n",
      "Epoch 81: Train Loss: 0.1968393623828888, Validation Loss: 0.40447431802749634\n",
      "Epoch 82: Train Loss: 0.16658678154150644, Validation Loss: 0.4397946000099182\n",
      "Epoch 83: Train Loss: 0.18929389119148254, Validation Loss: 0.4132719933986664\n",
      "Epoch 84: Train Loss: 0.1653427630662918, Validation Loss: 0.3943079710006714\n",
      "Epoch 85: Train Loss: 0.16119209428628287, Validation Loss: 0.4057530462741852\n",
      "Epoch 86: Train Loss: 0.1593231757481893, Validation Loss: 0.42997413873672485\n",
      "Epoch 87: Train Loss: 0.14752085506916046, Validation Loss: 0.4348537027835846\n",
      "Epoch 88: Train Loss: 0.15328973780075708, Validation Loss: 0.4356490969657898\n",
      "Epoch 89: Train Loss: 0.1504954695701599, Validation Loss: 0.4292539358139038\n",
      "Epoch 90: Train Loss: 0.17155679066975912, Validation Loss: 0.4093474745750427\n",
      "Epoch 91: Train Loss: 0.14463531970977783, Validation Loss: 0.39256197214126587\n",
      "Epoch 92: Train Loss: 0.146034965912501, Validation Loss: 0.39893805980682373\n",
      "Epoch 93: Train Loss: 0.1387007236480713, Validation Loss: 0.4196232259273529\n",
      "Epoch 94: Train Loss: 0.13010374208291373, Validation Loss: 0.4454295337200165\n",
      "Epoch 95: Train Loss: 0.1316821426153183, Validation Loss: 0.44195371866226196\n",
      "Epoch 96: Train Loss: 0.13342497746149698, Validation Loss: 0.43547284603118896\n",
      "Epoch 97: Train Loss: 0.12300668160120647, Validation Loss: 0.431057870388031\n",
      "Epoch 98: Train Loss: 0.12444077432155609, Validation Loss: 0.43070319294929504\n",
      "Epoch 99: Train Loss: 0.12970896810293198, Validation Loss: 0.43052273988723755\n",
      "Epoch 100: Train Loss: 0.14394261439641318, Validation Loss: 0.45443785190582275\n",
      "Epoch 101: Train Loss: 0.13801444818576178, Validation Loss: 0.498675674200058\n",
      "Epoch 102: Train Loss: 0.1266264021396637, Validation Loss: 0.46695342659950256\n",
      "Epoch 103: Train Loss: 0.12442441284656525, Validation Loss: 0.4414741098880768\n",
      "Epoch 104: Train Loss: 0.12268070628245671, Validation Loss: 0.4329267144203186\n",
      "Epoch 105: Train Loss: 0.11557266612847646, Validation Loss: 0.43901994824409485\n",
      "Epoch 106: Train Loss: 0.12004038443168004, Validation Loss: 0.45103803277015686\n",
      "Epoch 107: Train Loss: 0.12641270458698273, Validation Loss: 0.4529513716697693\n",
      "Epoch 108: Train Loss: 0.12731036047140756, Validation Loss: 0.4556153416633606\n",
      "Epoch 109: Train Loss: 0.12219642102718353, Validation Loss: 0.45529288053512573\n",
      "Epoch 110: Train Loss: 0.1381819819410642, Validation Loss: 0.4736284017562866\n",
      "Epoch 111: Train Loss: 0.13824143509070078, Validation Loss: 0.41468602418899536\n",
      "Epoch 112: Train Loss: 0.12780217081308365, Validation Loss: 0.4323425889015198\n",
      "Epoch 113: Train Loss: 0.16462339460849762, Validation Loss: 0.5475754141807556\n",
      "Epoch 114: Train Loss: 0.12582965195178986, Validation Loss: 0.5051974654197693\n",
      "Epoch 115: Train Loss: 0.11335790654023488, Validation Loss: 0.4147026240825653\n",
      "Epoch 116: Train Loss: 0.12405442198117574, Validation Loss: 0.4025591015815735\n",
      "Epoch 117: Train Loss: 0.11132847766081493, Validation Loss: 0.3967266082763672\n",
      "Epoch 118: Train Loss: 0.11151047050952911, Validation Loss: 0.4005555510520935\n",
      "Epoch 119: Train Loss: 0.1027211919426918, Validation Loss: 0.403462678194046\n",
      "Epoch 120: Train Loss: 0.11703350891669591, Validation Loss: 0.48927950859069824\n",
      "Epoch 121: Train Loss: 0.11114090432723363, Validation Loss: 0.4539404511451721\n",
      "Epoch 122: Train Loss: 0.13123735040426254, Validation Loss: 0.4262372851371765\n",
      "Epoch 123: Train Loss: 0.12540186941623688, Validation Loss: 0.46619713306427\n",
      "Epoch 124: Train Loss: 0.10767953346172969, Validation Loss: 0.4958585202693939\n",
      "Epoch 125: Train Loss: 0.10027542213598888, Validation Loss: 0.4682592451572418\n",
      "Epoch 126: Train Loss: 0.09670244654019673, Validation Loss: 0.4588698446750641\n",
      "Epoch 127: Train Loss: 0.11043930798768997, Validation Loss: 0.4587997496128082\n",
      "Epoch 128: Train Loss: 0.10215877244869868, Validation Loss: 0.44578900933265686\n",
      "Epoch 129: Train Loss: 0.08556939661502838, Validation Loss: 0.44946497678756714\n",
      "Epoch 130: Train Loss: 0.09210866689682007, Validation Loss: 0.47111523151397705\n",
      "Epoch 131: Train Loss: 0.1041244591275851, Validation Loss: 0.4807998836040497\n",
      "Epoch 132: Train Loss: 0.10797811796267827, Validation Loss: 0.43084973096847534\n",
      "Epoch 133: Train Loss: 0.0920901894569397, Validation Loss: 0.4396917223930359\n",
      "Epoch 134: Train Loss: 0.08187761902809143, Validation Loss: 0.47782102227211\n",
      "Epoch 135: Train Loss: 0.08232434590657552, Validation Loss: 0.5136032104492188\n",
      "Epoch 136: Train Loss: 0.0812094509601593, Validation Loss: 0.5342228412628174\n",
      "Epoch 137: Train Loss: 0.08152214189370473, Validation Loss: 0.5112184882164001\n",
      "Epoch 138: Train Loss: 0.08338313549757004, Validation Loss: 0.4893608093261719\n",
      "Epoch 139: Train Loss: 0.08768332501252492, Validation Loss: 0.49116769433021545\n",
      "Epoch 140: Train Loss: 0.09897350519895554, Validation Loss: 0.42006364464759827\n",
      "Epoch 141: Train Loss: 0.10558564464251201, Validation Loss: 0.515944242477417\n",
      "Epoch 142: Train Loss: 0.07984256496032079, Validation Loss: 0.552480161190033\n",
      "Epoch 143: Train Loss: 0.08724737664063771, Validation Loss: 0.4893834590911865\n",
      "Epoch 144: Train Loss: 0.07592166463534038, Validation Loss: 0.4662858545780182\n",
      "Epoch 145: Train Loss: 0.0852662945787112, Validation Loss: 0.4986492693424225\n",
      "Epoch 146: Train Loss: 0.08013764023780823, Validation Loss: 0.531730055809021\n",
      "Epoch 147: Train Loss: 0.07153116911649704, Validation Loss: 0.5359364151954651\n",
      "Epoch 148: Train Loss: 0.07340945055087407, Validation Loss: 0.5311012268066406\n",
      "Epoch 149: Train Loss: 0.08092771594723065, Validation Loss: 0.5333265066146851\n",
      "Epoch 150: Train Loss: 0.07172952095667522, Validation Loss: 0.4973527193069458\n",
      "Epoch 151: Train Loss: 0.07007139797012012, Validation Loss: 0.4729619026184082\n",
      "Epoch 152: Train Loss: 0.07032773643732071, Validation Loss: 0.49871206283569336\n",
      "Epoch 153: Train Loss: 0.09680500129858653, Validation Loss: 0.5013114213943481\n",
      "Epoch 154: Train Loss: 0.07679014777143796, Validation Loss: 0.4969545900821686\n",
      "Epoch 155: Train Loss: 0.06774371862411499, Validation Loss: 0.5169783234596252\n",
      "Epoch 156: Train Loss: 0.06520334507028262, Validation Loss: 0.5247751474380493\n",
      "Epoch 157: Train Loss: 0.06456845998764038, Validation Loss: 0.5309070348739624\n",
      "Epoch 158: Train Loss: 0.06577929854393005, Validation Loss: 0.5373185873031616\n",
      "Epoch 159: Train Loss: 0.07109126821160316, Validation Loss: 0.5402837991714478\n",
      "Epoch 160: Train Loss: 0.06429105872909228, Validation Loss: 0.4876566529273987\n",
      "Epoch 161: Train Loss: 0.06623602161804835, Validation Loss: 0.4992484152317047\n",
      "Epoch 162: Train Loss: 0.06308408578236897, Validation Loss: 0.5586357116699219\n",
      "Epoch 163: Train Loss: 0.07052843024333318, Validation Loss: 0.5667070150375366\n",
      "Epoch 164: Train Loss: 0.059690043330192566, Validation Loss: 0.5329651832580566\n",
      "Epoch 165: Train Loss: 0.06226010248064995, Validation Loss: 0.5215620398521423\n",
      "Epoch 166: Train Loss: 0.06453291823466618, Validation Loss: 0.5158398747444153\n",
      "Epoch 167: Train Loss: 0.11711661145091057, Validation Loss: 0.538852334022522\n",
      "Epoch 168: Train Loss: 0.059739742428064346, Validation Loss: 0.5492112636566162\n",
      "Epoch 169: Train Loss: 0.06271381055315335, Validation Loss: 0.5387101173400879\n",
      "Epoch 170: Train Loss: 0.060906581580638885, Validation Loss: 0.5668187141418457\n",
      "Epoch 171: Train Loss: 0.05867744361360868, Validation Loss: 0.5452169179916382\n",
      "Epoch 172: Train Loss: 0.06459378451108932, Validation Loss: 0.5385926961898804\n",
      "Epoch 173: Train Loss: 0.05845418324073156, Validation Loss: 0.5809054374694824\n",
      "Epoch 174: Train Loss: 0.05809313431382179, Validation Loss: 0.5836259126663208\n",
      "Epoch 175: Train Loss: 0.05871133133769035, Validation Loss: 0.5539191365242004\n",
      "Epoch 176: Train Loss: 0.07105396315455437, Validation Loss: 0.5187095999717712\n",
      "Epoch 177: Train Loss: 0.0640017328162988, Validation Loss: 0.5187090039253235\n",
      "Epoch 178: Train Loss: 0.056560629357894264, Validation Loss: 0.5190510153770447\n",
      "Epoch 179: Train Loss: 0.05128345390160879, Validation Loss: 0.5238151550292969\n",
      "Epoch 180: Train Loss: 0.053336488703886666, Validation Loss: 0.5854030847549438\n",
      "Epoch 181: Train Loss: 0.06764661024014156, Validation Loss: 0.5232691764831543\n",
      "Epoch 182: Train Loss: 0.05720884601275126, Validation Loss: 0.5460382103919983\n",
      "Epoch 183: Train Loss: 0.05601601550976435, Validation Loss: 0.5590189695358276\n",
      "Epoch 184: Train Loss: 0.05765766526261965, Validation Loss: 0.5659316182136536\n",
      "Epoch 185: Train Loss: 0.055390446136395134, Validation Loss: 0.5487885475158691\n",
      "Epoch 186: Train Loss: 0.050074249505996704, Validation Loss: 0.5548303127288818\n",
      "Epoch 187: Train Loss: 0.04986619452635447, Validation Loss: 0.5614897012710571\n",
      "Epoch 188: Train Loss: 0.04978963484366735, Validation Loss: 0.563058078289032\n",
      "Epoch 189: Train Loss: 0.0515204519033432, Validation Loss: 0.5668315291404724\n",
      "Epoch 190: Train Loss: 0.05103148023287455, Validation Loss: 0.6003022193908691\n",
      "Epoch 191: Train Loss: 0.04615941519538561, Validation Loss: 0.5775822401046753\n",
      "Epoch 192: Train Loss: 0.050328951328992844, Validation Loss: 0.5462225079536438\n",
      "Epoch 193: Train Loss: 0.07055708890159924, Validation Loss: 0.5585170388221741\n",
      "Epoch 194: Train Loss: 0.04913714031378428, Validation Loss: 0.6265947818756104\n",
      "Epoch 195: Train Loss: 0.04801618432005247, Validation Loss: 0.5981734991073608\n",
      "Epoch 196: Train Loss: 0.04447157805164655, Validation Loss: 0.577387273311615\n",
      "Epoch 197: Train Loss: 0.04979186008373896, Validation Loss: 0.5755820870399475\n",
      "Epoch 198: Train Loss: 0.05044374739130338, Validation Loss: 0.5701847076416016\n",
      "Epoch 199: Train Loss: 0.04535706341266632, Validation Loss: 0.5757298469543457\n",
      "Fold 3\n",
      "Epoch 0: Train Loss: 0.6678682764371237, Validation Loss: 0.6794288158416748\n",
      "Epoch 1: Train Loss: 0.6165901223818461, Validation Loss: 0.6698907613754272\n",
      "Epoch 2: Train Loss: 0.5871478120485941, Validation Loss: 0.6641507744789124\n",
      "Epoch 3: Train Loss: 0.5734263261159261, Validation Loss: 0.6569505333900452\n",
      "Epoch 4: Train Loss: 0.5642174681027731, Validation Loss: 0.6506736278533936\n",
      "Epoch 5: Train Loss: 0.5370633999506632, Validation Loss: 0.6471708416938782\n",
      "Epoch 6: Train Loss: 0.5361702640851339, Validation Loss: 0.6426545977592468\n",
      "Epoch 7: Train Loss: 0.5320229430993398, Validation Loss: 0.6385731101036072\n",
      "Epoch 8: Train Loss: 0.5272630552450815, Validation Loss: 0.6373120546340942\n",
      "Epoch 9: Train Loss: 0.5193037390708923, Validation Loss: 0.6383436918258667\n",
      "Epoch 10: Train Loss: 0.5171318749586741, Validation Loss: 0.6274580955505371\n",
      "Epoch 11: Train Loss: 0.5084292888641357, Validation Loss: 0.6220633387565613\n",
      "Epoch 12: Train Loss: 0.47784655292828876, Validation Loss: 0.6190534234046936\n",
      "Epoch 13: Train Loss: 0.47283735871315, Validation Loss: 0.6136399507522583\n",
      "Epoch 14: Train Loss: 0.46330440044403076, Validation Loss: 0.6130953431129456\n",
      "Epoch 15: Train Loss: 0.4546109040578206, Validation Loss: 0.6138170957565308\n",
      "Epoch 16: Train Loss: 0.4392499128977458, Validation Loss: 0.6172559261322021\n",
      "Epoch 17: Train Loss: 0.4391416609287262, Validation Loss: 0.6145554780960083\n",
      "Epoch 18: Train Loss: 0.4438125590483348, Validation Loss: 0.6143978834152222\n",
      "Epoch 19: Train Loss: 0.4282451967398326, Validation Loss: 0.614687979221344\n",
      "Epoch 20: Train Loss: 0.42849422494570416, Validation Loss: 0.6091873645782471\n",
      "Epoch 21: Train Loss: 0.4139309823513031, Validation Loss: 0.5890750885009766\n",
      "Epoch 22: Train Loss: 0.3994927207628886, Validation Loss: 0.5713297128677368\n",
      "Epoch 23: Train Loss: 0.3844675024350484, Validation Loss: 0.5562239289283752\n",
      "Epoch 24: Train Loss: 0.3729847272237142, Validation Loss: 0.5522189140319824\n",
      "Epoch 25: Train Loss: 0.37362226843833923, Validation Loss: 0.5478579998016357\n",
      "Epoch 26: Train Loss: 0.38614365458488464, Validation Loss: 0.5498549342155457\n",
      "Epoch 27: Train Loss: 0.35634546478589374, Validation Loss: 0.5523353815078735\n",
      "Epoch 28: Train Loss: 0.35410621762275696, Validation Loss: 0.5499706268310547\n",
      "Epoch 29: Train Loss: 0.34798476099967957, Validation Loss: 0.5491386651992798\n",
      "Epoch 30: Train Loss: 0.3520340124766032, Validation Loss: 0.5795612335205078\n",
      "Epoch 31: Train Loss: 0.3344106078147888, Validation Loss: 0.5975247025489807\n",
      "Epoch 32: Train Loss: 0.32331518332163495, Validation Loss: 0.596221387386322\n",
      "Epoch 33: Train Loss: 0.3129082918167114, Validation Loss: 0.5915530920028687\n",
      "Epoch 34: Train Loss: 0.3108137647310893, Validation Loss: 0.6131553649902344\n",
      "Epoch 35: Train Loss: 0.2954203983147939, Validation Loss: 0.6252880096435547\n",
      "Epoch 36: Train Loss: 0.301226407289505, Validation Loss: 0.6372202634811401\n",
      "Epoch 37: Train Loss: 0.3025399148464203, Validation Loss: 0.6387308835983276\n",
      "Epoch 38: Train Loss: 0.2888582944869995, Validation Loss: 0.6431079506874084\n",
      "Epoch 39: Train Loss: 0.2941763450702031, Validation Loss: 0.6426635980606079\n",
      "Epoch 40: Train Loss: 0.2841492493947347, Validation Loss: 0.6241559386253357\n",
      "Epoch 41: Train Loss: 0.2764759411414464, Validation Loss: 0.6068686842918396\n",
      "Epoch 42: Train Loss: 0.2708045244216919, Validation Loss: 0.6450954079627991\n",
      "Epoch 43: Train Loss: 0.28316492835680646, Validation Loss: 0.6690278053283691\n",
      "Epoch 44: Train Loss: 0.2626451849937439, Validation Loss: 0.6376386880874634\n",
      "Epoch 45: Train Loss: 0.24386138220628104, Validation Loss: 0.6229538917541504\n",
      "Epoch 46: Train Loss: 0.2561178108056386, Validation Loss: 0.6029241681098938\n",
      "Epoch 47: Train Loss: 0.3047299881776174, Validation Loss: 0.5875796675682068\n",
      "Epoch 48: Train Loss: 0.24744864801565805, Validation Loss: 0.5881833434104919\n",
      "Epoch 49: Train Loss: 0.2426639199256897, Validation Loss: 0.58443284034729\n",
      "Epoch 50: Train Loss: 0.2506055384874344, Validation Loss: 0.6526840925216675\n",
      "Epoch 51: Train Loss: 0.2449093461036682, Validation Loss: 0.7260067462921143\n",
      "Epoch 52: Train Loss: 0.24363751212755838, Validation Loss: 0.7508490681648254\n",
      "Epoch 53: Train Loss: 0.2357089618841807, Validation Loss: 0.7495349049568176\n",
      "Epoch 54: Train Loss: 0.2221531867980957, Validation Loss: 0.6914669871330261\n",
      "Epoch 55: Train Loss: 0.2102664957443873, Validation Loss: 0.6610367298126221\n",
      "Epoch 56: Train Loss: 0.22942325472831726, Validation Loss: 0.6314936876296997\n",
      "Epoch 57: Train Loss: 0.23430559039115906, Validation Loss: 0.6020155549049377\n",
      "Epoch 58: Train Loss: 0.20734572410583496, Validation Loss: 0.6042461395263672\n",
      "Epoch 59: Train Loss: 0.2084632764259974, Validation Loss: 0.6063593029975891\n",
      "Epoch 60: Train Loss: 0.20029255747795105, Validation Loss: 0.691125750541687\n",
      "Epoch 61: Train Loss: 0.20637761056423187, Validation Loss: 0.634319543838501\n",
      "Epoch 62: Train Loss: 0.2014036774635315, Validation Loss: 0.5970954298973083\n",
      "Epoch 63: Train Loss: 0.18847747643788657, Validation Loss: 0.555379331111908\n",
      "Epoch 64: Train Loss: 0.1897353728612264, Validation Loss: 0.5363295078277588\n",
      "Epoch 65: Train Loss: 0.22140020628770193, Validation Loss: 0.5072741508483887\n",
      "Epoch 66: Train Loss: 0.1744943211476008, Validation Loss: 0.5452077388763428\n",
      "Epoch 67: Train Loss: 0.17507410049438477, Validation Loss: 0.551328182220459\n",
      "Epoch 68: Train Loss: 0.18548013269901276, Validation Loss: 0.5556183457374573\n",
      "Epoch 69: Train Loss: 0.17501244942347208, Validation Loss: 0.5581228733062744\n",
      "Epoch 70: Train Loss: 0.17720660070578256, Validation Loss: 0.5375781655311584\n",
      "Epoch 71: Train Loss: 0.18021016319592795, Validation Loss: 0.5135163068771362\n",
      "Epoch 72: Train Loss: 0.1678272287050883, Validation Loss: 0.48246702551841736\n",
      "Epoch 73: Train Loss: 0.17797938485940298, Validation Loss: 0.4892618656158447\n",
      "Epoch 74: Train Loss: 0.16802764435609183, Validation Loss: 0.5052986145019531\n",
      "Epoch 75: Train Loss: 0.15718159079551697, Validation Loss: 0.5344749689102173\n",
      "Epoch 76: Train Loss: 0.1557576929529508, Validation Loss: 0.5345891118049622\n",
      "Epoch 77: Train Loss: 0.1568630039691925, Validation Loss: 0.5225016474723816\n",
      "Epoch 78: Train Loss: 0.1494558354218801, Validation Loss: 0.5211343765258789\n",
      "Epoch 79: Train Loss: 0.15472684055566788, Validation Loss: 0.5236787796020508\n",
      "Epoch 80: Train Loss: 0.144953320423762, Validation Loss: 0.5160170197486877\n",
      "Epoch 81: Train Loss: 0.1630488783121109, Validation Loss: 0.4988536834716797\n",
      "Epoch 82: Train Loss: 0.15388997395833334, Validation Loss: 0.5001375079154968\n",
      "Epoch 83: Train Loss: 0.14631679157416025, Validation Loss: 0.5059952735900879\n",
      "Epoch 84: Train Loss: 0.14770647635062537, Validation Loss: 0.48102155327796936\n",
      "Epoch 85: Train Loss: 0.15812366704146066, Validation Loss: 0.4958629608154297\n",
      "Epoch 86: Train Loss: 0.1394124999642372, Validation Loss: 0.49721992015838623\n",
      "Epoch 87: Train Loss: 0.1402226040760676, Validation Loss: 0.4902884066104889\n",
      "Epoch 88: Train Loss: 0.13929876188437143, Validation Loss: 0.48515722155570984\n",
      "Epoch 89: Train Loss: 0.13728229204813638, Validation Loss: 0.4791780710220337\n",
      "Epoch 90: Train Loss: 0.1393734018007914, Validation Loss: 0.482290118932724\n",
      "Epoch 91: Train Loss: 0.1344323754310608, Validation Loss: 0.4456356465816498\n",
      "Epoch 92: Train Loss: 0.14810981353123984, Validation Loss: 0.44814831018447876\n",
      "Epoch 93: Train Loss: 0.15523593376080194, Validation Loss: 0.4519288241863251\n",
      "Epoch 94: Train Loss: 0.1225343868136406, Validation Loss: 0.44106969237327576\n",
      "Epoch 95: Train Loss: 0.12288122127453487, Validation Loss: 0.4412519931793213\n",
      "Epoch 96: Train Loss: 0.12106002867221832, Validation Loss: 0.4398467540740967\n",
      "Epoch 97: Train Loss: 0.12511136879523596, Validation Loss: 0.4399842619895935\n",
      "Epoch 98: Train Loss: 0.12732774019241333, Validation Loss: 0.4408915936946869\n",
      "Epoch 99: Train Loss: 0.12577847888072333, Validation Loss: 0.44008585810661316\n",
      "Epoch 100: Train Loss: 0.13120131939649582, Validation Loss: 0.4394896924495697\n",
      "Epoch 101: Train Loss: 0.13950752715269724, Validation Loss: 0.4723230004310608\n",
      "Epoch 102: Train Loss: 0.135053388774395, Validation Loss: 0.5175820589065552\n",
      "Epoch 103: Train Loss: 0.1304708868265152, Validation Loss: 0.5241268873214722\n",
      "Epoch 104: Train Loss: 0.11681116620699565, Validation Loss: 0.5011315941810608\n",
      "Epoch 105: Train Loss: 0.12210267533858617, Validation Loss: 0.4672340452671051\n",
      "Epoch 106: Train Loss: 0.11468179772297542, Validation Loss: 0.4527740180492401\n",
      "Epoch 107: Train Loss: 0.10783092180887859, Validation Loss: 0.4505331814289093\n",
      "Epoch 108: Train Loss: 0.13259589423735937, Validation Loss: 0.45364734530448914\n",
      "Epoch 109: Train Loss: 0.11135349422693253, Validation Loss: 0.4517214000225067\n",
      "Epoch 110: Train Loss: 0.11301249265670776, Validation Loss: 0.44051921367645264\n",
      "Epoch 111: Train Loss: 0.10703915109237035, Validation Loss: 0.4319377839565277\n",
      "Epoch 112: Train Loss: 0.11084096382061641, Validation Loss: 0.4288386404514313\n",
      "Epoch 113: Train Loss: 0.12962080538272858, Validation Loss: 0.41777101159095764\n",
      "Epoch 114: Train Loss: 0.12095553924640019, Validation Loss: 0.4187900125980377\n",
      "Epoch 115: Train Loss: 0.10246440768241882, Validation Loss: 0.43386438488960266\n",
      "Epoch 116: Train Loss: 0.11875088512897491, Validation Loss: 0.44381943345069885\n",
      "Epoch 117: Train Loss: 0.10524419198433559, Validation Loss: 0.44106408953666687\n",
      "Epoch 118: Train Loss: 0.10264055679241817, Validation Loss: 0.4390115439891815\n",
      "Epoch 119: Train Loss: 0.09826732675234477, Validation Loss: 0.43984758853912354\n",
      "Epoch 120: Train Loss: 0.09884717812140782, Validation Loss: 0.44017505645751953\n",
      "Epoch 121: Train Loss: 0.0962838629881541, Validation Loss: 0.45576828718185425\n",
      "Epoch 122: Train Loss: 0.09726610531409581, Validation Loss: 0.46036025881767273\n",
      "Epoch 123: Train Loss: 0.08676241338253021, Validation Loss: 0.4594067633152008\n",
      "Epoch 124: Train Loss: 0.08798738817373912, Validation Loss: 0.46011465787887573\n",
      "Epoch 125: Train Loss: 0.08946711321671803, Validation Loss: 0.46056902408599854\n",
      "Epoch 126: Train Loss: 0.07920971264441808, Validation Loss: 0.4690645933151245\n",
      "Epoch 127: Train Loss: 0.09347109993298848, Validation Loss: 0.46937254071235657\n",
      "Epoch 128: Train Loss: 0.10579635202884674, Validation Loss: 0.4723549783229828\n",
      "Epoch 129: Train Loss: 0.08108716458082199, Validation Loss: 0.47076961398124695\n",
      "Epoch 130: Train Loss: 0.08839704841375351, Validation Loss: 0.46499574184417725\n",
      "Epoch 131: Train Loss: 0.08103275795777638, Validation Loss: 0.4853372275829315\n",
      "Epoch 132: Train Loss: 0.0864545280734698, Validation Loss: 0.47908854484558105\n",
      "Epoch 133: Train Loss: 0.08313013364871343, Validation Loss: 0.46474021673202515\n",
      "Epoch 134: Train Loss: 0.08821244537830353, Validation Loss: 0.4676971137523651\n",
      "Epoch 135: Train Loss: 0.08583396424849828, Validation Loss: 0.47950267791748047\n",
      "Epoch 136: Train Loss: 0.07403355836868286, Validation Loss: 0.5020264983177185\n",
      "Epoch 137: Train Loss: 0.07769931604464848, Validation Loss: 0.497460275888443\n",
      "Epoch 138: Train Loss: 0.07958831638097763, Validation Loss: 0.4914263188838959\n",
      "Epoch 139: Train Loss: 0.07980936765670776, Validation Loss: 0.4896288812160492\n",
      "Epoch 140: Train Loss: 0.09672598292430241, Validation Loss: 0.48425304889678955\n",
      "Epoch 141: Train Loss: 0.12409825623035431, Validation Loss: 0.5052139163017273\n",
      "Epoch 142: Train Loss: 0.12035041799147923, Validation Loss: 0.5435954332351685\n",
      "Epoch 143: Train Loss: 0.11741185436646144, Validation Loss: 0.4693993330001831\n",
      "Epoch 144: Train Loss: 0.1277093862493833, Validation Loss: 0.5041234493255615\n",
      "Epoch 145: Train Loss: 0.09604343151052792, Validation Loss: 0.49373120069503784\n",
      "Epoch 146: Train Loss: 0.08810282001892726, Validation Loss: 0.5224899053573608\n",
      "Epoch 147: Train Loss: 0.07064861431717873, Validation Loss: 0.5268770456314087\n",
      "Epoch 148: Train Loss: 0.08017949014902115, Validation Loss: 0.518147885799408\n",
      "Epoch 149: Train Loss: 0.07155093799034755, Validation Loss: 0.5021750926971436\n",
      "Epoch 150: Train Loss: 0.07224007944266002, Validation Loss: 0.4842824339866638\n",
      "Epoch 151: Train Loss: 0.0675707037250201, Validation Loss: 0.48944076895713806\n",
      "Epoch 152: Train Loss: 0.06480943039059639, Validation Loss: 0.5108447670936584\n",
      "Epoch 153: Train Loss: 0.06942483286062877, Validation Loss: 0.5204573273658752\n",
      "Epoch 154: Train Loss: 0.07766513526439667, Validation Loss: 0.48783430457115173\n",
      "Epoch 155: Train Loss: 0.06374652683734894, Validation Loss: 0.4810582399368286\n",
      "Epoch 156: Train Loss: 0.0598972054819266, Validation Loss: 0.4815077483654022\n",
      "Epoch 157: Train Loss: 0.06527261063456535, Validation Loss: 0.48370763659477234\n",
      "Epoch 158: Train Loss: 0.05925797546903292, Validation Loss: 0.48689770698547363\n",
      "Epoch 159: Train Loss: 0.06173406665523847, Validation Loss: 0.4829166531562805\n",
      "Epoch 160: Train Loss: 0.058088596910238266, Validation Loss: 0.4842562973499298\n",
      "Epoch 161: Train Loss: 0.06327321628729503, Validation Loss: 0.48629170656204224\n",
      "Epoch 162: Train Loss: 0.05887653181950251, Validation Loss: 0.48542559146881104\n",
      "Epoch 163: Train Loss: 0.0669736738006274, Validation Loss: 0.4945763647556305\n",
      "Epoch 164: Train Loss: 0.06489396343628566, Validation Loss: 0.5072788000106812\n",
      "Epoch 165: Train Loss: 0.06668914978702863, Validation Loss: 0.49601417779922485\n",
      "Epoch 166: Train Loss: 0.05521940812468529, Validation Loss: 0.4941093921661377\n",
      "Epoch 167: Train Loss: 0.05867467572291692, Validation Loss: 0.493839293718338\n",
      "Epoch 168: Train Loss: 0.05695578455924988, Validation Loss: 0.49545398354530334\n",
      "Epoch 169: Train Loss: 0.05292758593956629, Validation Loss: 0.49182307720184326\n",
      "Epoch 170: Train Loss: 0.06050105392932892, Validation Loss: 0.5143330097198486\n",
      "Epoch 171: Train Loss: 0.06428374474247296, Validation Loss: 0.501941978931427\n",
      "Epoch 172: Train Loss: 0.05413824071486791, Validation Loss: 0.4923917353153229\n",
      "Epoch 173: Train Loss: 0.06329869478940964, Validation Loss: 0.524075448513031\n",
      "Epoch 174: Train Loss: 0.05821346491575241, Validation Loss: 0.5489425659179688\n",
      "Epoch 175: Train Loss: 0.061022479087114334, Validation Loss: 0.5251412987709045\n",
      "Epoch 176: Train Loss: 0.05395406732956568, Validation Loss: 0.5165256857872009\n",
      "Epoch 177: Train Loss: 0.05262009551127752, Validation Loss: 0.5112181901931763\n",
      "Epoch 178: Train Loss: 0.042532410472631454, Validation Loss: 0.5120894908905029\n",
      "Epoch 179: Train Loss: 0.04383662094672521, Validation Loss: 0.5163144469261169\n",
      "Epoch 180: Train Loss: 0.04450552041331927, Validation Loss: 0.5344528555870056\n",
      "Epoch 181: Train Loss: 0.046192578971385956, Validation Loss: 0.5224176645278931\n",
      "Epoch 182: Train Loss: 0.05284565935532252, Validation Loss: 0.5333857536315918\n",
      "Epoch 183: Train Loss: 0.04235547346373399, Validation Loss: 0.548696756362915\n",
      "Epoch 184: Train Loss: 0.04548598577578863, Validation Loss: 0.5546281933784485\n",
      "Epoch 185: Train Loss: 0.043918135265509285, Validation Loss: 0.5445201992988586\n",
      "Epoch 186: Train Loss: 0.039540845900774, Validation Loss: 0.5381205081939697\n",
      "Epoch 187: Train Loss: 0.04637053174277147, Validation Loss: 0.5323393940925598\n",
      "Epoch 188: Train Loss: 0.040423233062028885, Validation Loss: 0.5339491963386536\n",
      "Epoch 189: Train Loss: 0.039975933730602264, Validation Loss: 0.5359897613525391\n",
      "Epoch 190: Train Loss: 0.037177570164203644, Validation Loss: 0.5441553592681885\n",
      "Epoch 191: Train Loss: 0.04130629077553749, Validation Loss: 0.5367100238800049\n",
      "Epoch 192: Train Loss: 0.04201436912020048, Validation Loss: 0.5121744275093079\n",
      "Epoch 193: Train Loss: 0.043229758739471436, Validation Loss: 0.5035863518714905\n",
      "Epoch 194: Train Loss: 0.038817290837566056, Validation Loss: 0.4961259365081787\n",
      "Epoch 195: Train Loss: 0.03843865295251211, Validation Loss: 0.5113703608512878\n",
      "Epoch 196: Train Loss: 0.047662343829870224, Validation Loss: 0.5234678983688354\n",
      "Epoch 197: Train Loss: 0.042349014431238174, Validation Loss: 0.530899703502655\n",
      "Epoch 198: Train Loss: 0.033407206957538925, Validation Loss: 0.5382005572319031\n",
      "Epoch 199: Train Loss: 0.039563486352562904, Validation Loss: 0.5393838882446289\n",
      "Fold 4\n",
      "Epoch 0: Train Loss: 0.719881514708201, Validation Loss: 0.6905577182769775\n",
      "Epoch 1: Train Loss: 0.6561692754427592, Validation Loss: 0.6908483505249023\n",
      "Epoch 2: Train Loss: 0.6190501848856608, Validation Loss: 0.6904993653297424\n",
      "Epoch 3: Train Loss: 0.6007940173149109, Validation Loss: 0.6884914040565491\n",
      "Epoch 4: Train Loss: 0.5741481184959412, Validation Loss: 0.6859375834465027\n",
      "Epoch 5: Train Loss: 0.567660907904307, Validation Loss: 0.6841257214546204\n",
      "Epoch 6: Train Loss: 0.5513372421264648, Validation Loss: 0.6828646063804626\n",
      "Epoch 7: Train Loss: 0.5409778952598572, Validation Loss: 0.6831130385398865\n",
      "Epoch 8: Train Loss: 0.5522113045056661, Validation Loss: 0.6834242343902588\n",
      "Epoch 9: Train Loss: 0.5455273787180582, Validation Loss: 0.6842496395111084\n",
      "Epoch 10: Train Loss: 0.5450519522031149, Validation Loss: 0.6758494973182678\n",
      "Epoch 11: Train Loss: 0.5109174251556396, Validation Loss: 0.6667810678482056\n",
      "Epoch 12: Train Loss: 0.5015473167101542, Validation Loss: 0.6582606434822083\n",
      "Epoch 13: Train Loss: 0.48805804053942364, Validation Loss: 0.6521408557891846\n",
      "Epoch 14: Train Loss: 0.47116390864054364, Validation Loss: 0.6483577489852905\n",
      "Epoch 15: Train Loss: 0.46194421251614887, Validation Loss: 0.6464830636978149\n",
      "Epoch 16: Train Loss: 0.4526759684085846, Validation Loss: 0.6464853286743164\n",
      "Epoch 17: Train Loss: 0.4503986140092214, Validation Loss: 0.6460710763931274\n",
      "Epoch 18: Train Loss: 0.45719607671101886, Validation Loss: 0.6475403308868408\n",
      "Epoch 19: Train Loss: 0.4577948749065399, Validation Loss: 0.6479303240776062\n",
      "Epoch 20: Train Loss: 0.4456423918406169, Validation Loss: 0.6400117874145508\n",
      "Epoch 21: Train Loss: 0.4220527907212575, Validation Loss: 0.6307802796363831\n",
      "Epoch 22: Train Loss: 0.4095681607723236, Validation Loss: 0.622670590877533\n",
      "Epoch 23: Train Loss: 0.39960792660713196, Validation Loss: 0.6163924336433411\n",
      "Epoch 24: Train Loss: 0.39517075816790265, Validation Loss: 0.6109005808830261\n",
      "Epoch 25: Train Loss: 0.38503575325012207, Validation Loss: 0.6102592945098877\n",
      "Epoch 26: Train Loss: 0.38133397698402405, Validation Loss: 0.6101856827735901\n",
      "Epoch 27: Train Loss: 0.373607595761617, Validation Loss: 0.6112207174301147\n",
      "Epoch 28: Train Loss: 0.3839939435323079, Validation Loss: 0.6122132539749146\n",
      "Epoch 29: Train Loss: 0.3708759347597758, Validation Loss: 0.6136143803596497\n",
      "Epoch 30: Train Loss: 0.3830394148826599, Validation Loss: 0.6011967658996582\n",
      "Epoch 31: Train Loss: 0.35491234064102173, Validation Loss: 0.5949620604515076\n",
      "Epoch 32: Train Loss: 0.35265177488327026, Validation Loss: 0.5904861092567444\n",
      "Epoch 33: Train Loss: 0.3508419692516327, Validation Loss: 0.5907822251319885\n",
      "Epoch 34: Train Loss: 0.34915804862976074, Validation Loss: 0.5888456106185913\n",
      "Epoch 35: Train Loss: 0.3222872018814087, Validation Loss: 0.5873792767524719\n",
      "Epoch 36: Train Loss: 0.3236961563428243, Validation Loss: 0.585093080997467\n",
      "Epoch 37: Train Loss: 0.31970401604970294, Validation Loss: 0.5860286355018616\n",
      "Epoch 38: Train Loss: 0.3178150455156962, Validation Loss: 0.5876801609992981\n",
      "Epoch 39: Train Loss: 0.3118812044461568, Validation Loss: 0.5872406363487244\n",
      "Epoch 40: Train Loss: 0.3288079301516215, Validation Loss: 0.5777753591537476\n",
      "Epoch 41: Train Loss: 0.31455085674921673, Validation Loss: 0.5700163841247559\n",
      "Epoch 42: Train Loss: 0.3097110390663147, Validation Loss: 0.564708948135376\n",
      "Epoch 43: Train Loss: 0.2960738241672516, Validation Loss: 0.5660181641578674\n",
      "Epoch 44: Train Loss: 0.3014470388491948, Validation Loss: 0.5722706913948059\n",
      "Epoch 45: Train Loss: 0.28142644961675006, Validation Loss: 0.5736844539642334\n",
      "Epoch 46: Train Loss: 0.27785853544871014, Validation Loss: 0.5702706575393677\n",
      "Epoch 47: Train Loss: 0.27962808310985565, Validation Loss: 0.5694594979286194\n",
      "Epoch 48: Train Loss: 0.2759202718734741, Validation Loss: 0.566967248916626\n",
      "Epoch 49: Train Loss: 0.27954714000225067, Validation Loss: 0.5685062408447266\n",
      "Epoch 50: Train Loss: 0.2814500828584035, Validation Loss: 0.5590314269065857\n",
      "Epoch 51: Train Loss: 0.3052852650483449, Validation Loss: 0.5616680979728699\n",
      "Epoch 52: Train Loss: 0.26003297170003253, Validation Loss: 0.5656175017356873\n",
      "Epoch 53: Train Loss: 0.27228691180547077, Validation Loss: 0.5627899169921875\n",
      "Epoch 54: Train Loss: 0.26181485255559284, Validation Loss: 0.5521985292434692\n",
      "Epoch 55: Train Loss: 0.24657458066940308, Validation Loss: 0.552367091178894\n",
      "Epoch 56: Train Loss: 0.2566109796365102, Validation Loss: 0.5583965182304382\n",
      "Epoch 57: Train Loss: 0.24263605972131094, Validation Loss: 0.5592343211174011\n",
      "Epoch 58: Train Loss: 0.24302728474140167, Validation Loss: 0.5595455169677734\n",
      "Epoch 59: Train Loss: 0.2515270759661992, Validation Loss: 0.56034916639328\n",
      "Epoch 60: Train Loss: 0.248662402232488, Validation Loss: 0.5526630878448486\n",
      "Epoch 61: Train Loss: 0.25743292768796283, Validation Loss: 0.5655562877655029\n",
      "Epoch 62: Train Loss: 0.25329411029815674, Validation Loss: 0.5642383694648743\n",
      "Epoch 63: Train Loss: 0.22332303722699484, Validation Loss: 0.5579995512962341\n",
      "Epoch 64: Train Loss: 0.2256331592798233, Validation Loss: 0.5442760586738586\n",
      "Epoch 65: Train Loss: 0.21461213131745657, Validation Loss: 0.5395060181617737\n",
      "Epoch 66: Train Loss: 0.21650212506453195, Validation Loss: 0.540149450302124\n",
      "Epoch 67: Train Loss: 0.23626055320103964, Validation Loss: 0.5491520762443542\n",
      "Epoch 68: Train Loss: 0.20900622506936392, Validation Loss: 0.5521766543388367\n",
      "Epoch 69: Train Loss: 0.21473883589108786, Validation Loss: 0.5511926412582397\n",
      "Epoch 70: Train Loss: 0.21514548858006796, Validation Loss: 0.5556690096855164\n",
      "Epoch 71: Train Loss: 0.24816891054312387, Validation Loss: 0.5473439693450928\n",
      "Epoch 72: Train Loss: 0.2149706780910492, Validation Loss: 0.5447753667831421\n",
      "Epoch 73: Train Loss: 0.20445582767327627, Validation Loss: 0.5465263724327087\n",
      "Epoch 74: Train Loss: 0.19996347526709238, Validation Loss: 0.5437958240509033\n",
      "Epoch 75: Train Loss: 0.20566199719905853, Validation Loss: 0.5429633259773254\n",
      "Epoch 76: Train Loss: 0.20331131418546042, Validation Loss: 0.5413833260536194\n",
      "Epoch 77: Train Loss: 0.20419669648011526, Validation Loss: 0.5389813780784607\n",
      "Epoch 78: Train Loss: 0.18848062554995218, Validation Loss: 0.5412657856941223\n",
      "Epoch 79: Train Loss: 0.1902159551779429, Validation Loss: 0.5395457744598389\n",
      "Epoch 80: Train Loss: 0.18871292968591055, Validation Loss: 0.5673580169677734\n",
      "Epoch 81: Train Loss: 0.21853661040465036, Validation Loss: 0.550520122051239\n",
      "Epoch 82: Train Loss: 0.1777854065100352, Validation Loss: 0.5324265956878662\n",
      "Epoch 83: Train Loss: 0.1800270676612854, Validation Loss: 0.5263161659240723\n",
      "Epoch 84: Train Loss: 0.2120731920003891, Validation Loss: 0.542935848236084\n",
      "Epoch 85: Train Loss: 0.17153113087018332, Validation Loss: 0.5577806234359741\n",
      "Epoch 86: Train Loss: 0.17476478219032288, Validation Loss: 0.5643232464790344\n",
      "Epoch 87: Train Loss: 0.17972557743390402, Validation Loss: 0.5586264729499817\n",
      "Epoch 88: Train Loss: 0.16236438353856406, Validation Loss: 0.5576186180114746\n",
      "Epoch 89: Train Loss: 0.16330143560965857, Validation Loss: 0.5603816509246826\n",
      "Epoch 90: Train Loss: 0.16587873796621957, Validation Loss: 0.5485206842422485\n",
      "Epoch 91: Train Loss: 0.1584781507651011, Validation Loss: 0.5542351603507996\n",
      "Epoch 92: Train Loss: 0.16056266675392786, Validation Loss: 0.5750345587730408\n",
      "Epoch 93: Train Loss: 0.1609691729148229, Validation Loss: 0.5651066303253174\n",
      "Epoch 94: Train Loss: 0.15890100598335266, Validation Loss: 0.5522376894950867\n",
      "Epoch 95: Train Loss: 0.14713949958483377, Validation Loss: 0.5511974096298218\n",
      "Epoch 96: Train Loss: 0.14634043971697488, Validation Loss: 0.5598268508911133\n",
      "Epoch 97: Train Loss: 0.14546730120976767, Validation Loss: 0.5680739879608154\n",
      "Epoch 98: Train Loss: 0.15020964046319327, Validation Loss: 0.5686876177787781\n",
      "Epoch 99: Train Loss: 0.1465765635172526, Validation Loss: 0.5738232135772705\n",
      "Epoch 100: Train Loss: 0.15449661016464233, Validation Loss: 0.5776259303092957\n",
      "Epoch 101: Train Loss: 0.14833307762940726, Validation Loss: 0.5763689875602722\n",
      "Epoch 102: Train Loss: 0.1432615021864573, Validation Loss: 0.5666522979736328\n",
      "Epoch 103: Train Loss: 0.15286741654078165, Validation Loss: 0.56244295835495\n",
      "Epoch 104: Train Loss: 0.14154671132564545, Validation Loss: 0.5845576524734497\n",
      "Epoch 105: Train Loss: 0.1309253970781962, Validation Loss: 0.5876865386962891\n",
      "Epoch 106: Train Loss: 0.14061195154984793, Validation Loss: 0.5744127035140991\n",
      "Epoch 107: Train Loss: 0.13069198528925577, Validation Loss: 0.5690563321113586\n",
      "Epoch 108: Train Loss: 0.1434367597103119, Validation Loss: 0.5535415410995483\n",
      "Epoch 109: Train Loss: 0.15350588411092758, Validation Loss: 0.553168535232544\n",
      "Epoch 110: Train Loss: 0.12959410498539606, Validation Loss: 0.5593704581260681\n",
      "Epoch 111: Train Loss: 0.13198120892047882, Validation Loss: 0.5617934465408325\n",
      "Epoch 112: Train Loss: 0.1314991240700086, Validation Loss: 0.6179577112197876\n",
      "Epoch 113: Train Loss: 0.14361468454202017, Validation Loss: 0.5818727016448975\n",
      "Epoch 114: Train Loss: 0.1292219733198484, Validation Loss: 0.5635479092597961\n",
      "Epoch 115: Train Loss: 0.11660934239625931, Validation Loss: 0.5691299438476562\n",
      "Epoch 116: Train Loss: 0.12425406028827031, Validation Loss: 0.5860797762870789\n",
      "Epoch 117: Train Loss: 0.11151679853598277, Validation Loss: 0.5913518071174622\n",
      "Epoch 118: Train Loss: 0.10768368343512218, Validation Loss: 0.5892794728279114\n",
      "Epoch 119: Train Loss: 0.12454905857642491, Validation Loss: 0.5892608761787415\n",
      "Epoch 120: Train Loss: 0.12100951125224431, Validation Loss: 0.5476522445678711\n",
      "Epoch 121: Train Loss: 0.113664910197258, Validation Loss: 0.5433982610702515\n",
      "Epoch 122: Train Loss: 0.12288763870795567, Validation Loss: 0.5911248922348022\n",
      "Epoch 123: Train Loss: 0.10654551287492116, Validation Loss: 0.5934422016143799\n",
      "Epoch 124: Train Loss: 0.10566187898317973, Validation Loss: 0.5625932216644287\n",
      "Epoch 125: Train Loss: 0.09612760941187541, Validation Loss: 0.5604845285415649\n",
      "Epoch 126: Train Loss: 0.1049829622109731, Validation Loss: 0.5627968311309814\n",
      "Epoch 127: Train Loss: 0.097453939417998, Validation Loss: 0.562569797039032\n",
      "Epoch 128: Train Loss: 0.09623164931933086, Validation Loss: 0.5688927173614502\n",
      "Epoch 129: Train Loss: 0.10123445590337117, Validation Loss: 0.5726375579833984\n",
      "Epoch 130: Train Loss: 0.10720658550659816, Validation Loss: 0.5728501677513123\n",
      "Epoch 131: Train Loss: 0.09506168961524963, Validation Loss: 0.5773512721061707\n",
      "Epoch 132: Train Loss: 0.0931452934940656, Validation Loss: 0.5992798805236816\n",
      "Epoch 133: Train Loss: 0.09753099580605824, Validation Loss: 0.5995710492134094\n",
      "Epoch 134: Train Loss: 0.09251252561807632, Validation Loss: 0.5901662111282349\n",
      "Epoch 135: Train Loss: 0.08943069229523341, Validation Loss: 0.5839545130729675\n",
      "Epoch 136: Train Loss: 0.09671130279699962, Validation Loss: 0.5851035118103027\n",
      "Epoch 137: Train Loss: 0.10881085942188899, Validation Loss: 0.5823990702629089\n",
      "Epoch 138: Train Loss: 0.0888151799639066, Validation Loss: 0.5780761241912842\n",
      "Epoch 139: Train Loss: 0.08804913113514583, Validation Loss: 0.5704938173294067\n",
      "Epoch 140: Train Loss: 0.08488033711910248, Validation Loss: 0.5834383368492126\n",
      "Epoch 141: Train Loss: 0.08895683288574219, Validation Loss: 0.5911012887954712\n",
      "Epoch 142: Train Loss: 0.08849432319402695, Validation Loss: 0.5930675268173218\n",
      "Epoch 143: Train Loss: 0.09143135945002238, Validation Loss: 0.5897663831710815\n",
      "Epoch 144: Train Loss: 0.08507321526606877, Validation Loss: 0.5682996511459351\n",
      "Epoch 145: Train Loss: 0.07857647041479747, Validation Loss: 0.558671772480011\n",
      "Epoch 146: Train Loss: 0.0924048845966657, Validation Loss: 0.5680527091026306\n",
      "Epoch 147: Train Loss: 0.08019257585207622, Validation Loss: 0.5674037933349609\n",
      "Epoch 148: Train Loss: 0.07367242127656937, Validation Loss: 0.5670961141586304\n",
      "Epoch 149: Train Loss: 0.07366001357634862, Validation Loss: 0.55865877866745\n",
      "Epoch 150: Train Loss: 0.07722028841574986, Validation Loss: 0.5817575454711914\n",
      "Epoch 151: Train Loss: 0.07645421475172043, Validation Loss: 0.6043863892555237\n",
      "Epoch 152: Train Loss: 0.07158583775162697, Validation Loss: 0.6025584936141968\n",
      "Epoch 153: Train Loss: 0.07580487057566643, Validation Loss: 0.577381432056427\n",
      "Epoch 154: Train Loss: 0.07304208974043529, Validation Loss: 0.5576273202896118\n",
      "Epoch 155: Train Loss: 0.0760716696580251, Validation Loss: 0.5713755488395691\n",
      "Epoch 156: Train Loss: 0.06686028838157654, Validation Loss: 0.5752154588699341\n",
      "Epoch 157: Train Loss: 0.06354896475871404, Validation Loss: 0.5834987163543701\n",
      "Epoch 158: Train Loss: 0.06328871597846349, Validation Loss: 0.5802305340766907\n",
      "Epoch 159: Train Loss: 0.06734491263826688, Validation Loss: 0.5911901593208313\n",
      "Epoch 160: Train Loss: 0.07446793094277382, Validation Loss: 0.5723839402198792\n",
      "Epoch 161: Train Loss: 0.06451814621686935, Validation Loss: 0.6069958209991455\n",
      "Epoch 162: Train Loss: 0.06498164435227712, Validation Loss: 0.6117639541625977\n",
      "Epoch 163: Train Loss: 0.07550498098134995, Validation Loss: 0.5470385551452637\n",
      "Epoch 164: Train Loss: 0.0686321531732877, Validation Loss: 0.5389338135719299\n",
      "Epoch 165: Train Loss: 0.07262036701043446, Validation Loss: 0.5631689429283142\n",
      "Epoch 166: Train Loss: 0.0721869779129823, Validation Loss: 0.5938069224357605\n",
      "Epoch 167: Train Loss: 0.06108438844482104, Validation Loss: 0.5958601832389832\n",
      "Epoch 168: Train Loss: 0.06145375967025757, Validation Loss: 0.5827878713607788\n",
      "Epoch 169: Train Loss: 0.06595343227187793, Validation Loss: 0.5890595316886902\n",
      "Epoch 170: Train Loss: 0.05935578172405561, Validation Loss: 0.5526691675186157\n",
      "Epoch 171: Train Loss: 0.0729571208357811, Validation Loss: 0.5709835290908813\n",
      "Epoch 172: Train Loss: 0.08567884812752406, Validation Loss: 0.6046757102012634\n",
      "Epoch 173: Train Loss: 0.10268787542978923, Validation Loss: 0.4935906231403351\n",
      "Epoch 174: Train Loss: 0.06890472645560901, Validation Loss: 0.5296083092689514\n",
      "Epoch 175: Train Loss: 0.07892680416504542, Validation Loss: 0.5561620593070984\n",
      "Epoch 176: Train Loss: 0.06536852444211642, Validation Loss: 0.5272520780563354\n",
      "Epoch 177: Train Loss: 0.07113358875115712, Validation Loss: 0.5212444067001343\n",
      "Epoch 178: Train Loss: 0.06323877101143201, Validation Loss: 0.5179438591003418\n",
      "Epoch 179: Train Loss: 0.06340958550572395, Validation Loss: 0.5146287083625793\n",
      "Epoch 180: Train Loss: 0.07144349068403244, Validation Loss: 0.5189543962478638\n",
      "Epoch 181: Train Loss: 0.08873948951562245, Validation Loss: 0.548372745513916\n",
      "Epoch 182: Train Loss: 0.09459172189235687, Validation Loss: 0.556075394153595\n",
      "Epoch 183: Train Loss: 0.07572933658957481, Validation Loss: 0.563190221786499\n",
      "Epoch 184: Train Loss: 0.0790293775498867, Validation Loss: 0.5282219052314758\n",
      "Epoch 185: Train Loss: 0.09001966069142024, Validation Loss: 0.5225393772125244\n",
      "Epoch 186: Train Loss: 0.06692451859513919, Validation Loss: 0.5373122096061707\n",
      "Epoch 187: Train Loss: 0.07082519431908925, Validation Loss: 0.5439433455467224\n",
      "Epoch 188: Train Loss: 0.07543177157640457, Validation Loss: 0.5471057295799255\n",
      "Epoch 189: Train Loss: 0.07054284463326137, Validation Loss: 0.5397544503211975\n",
      "Epoch 190: Train Loss: 0.07636146247386932, Validation Loss: 0.5776026844978333\n",
      "Epoch 191: Train Loss: 0.09228237469991048, Validation Loss: 0.5302515625953674\n",
      "Epoch 192: Train Loss: 0.06005061293641726, Validation Loss: 0.4718416631221771\n",
      "Epoch 193: Train Loss: 0.06153846656282743, Validation Loss: 0.47341400384902954\n",
      "Epoch 194: Train Loss: 0.06121562421321869, Validation Loss: 0.5229588747024536\n",
      "Epoch 195: Train Loss: 0.05637808268268903, Validation Loss: 0.5428550839424133\n",
      "Epoch 196: Train Loss: 0.07069296638170879, Validation Loss: 0.5449570417404175\n",
      "Epoch 197: Train Loss: 0.04916120941440264, Validation Loss: 0.5456784963607788\n",
      "Epoch 198: Train Loss: 0.04878835007548332, Validation Loss: 0.5521405339241028\n",
      "Epoch 199: Train Loss: 0.045262984931468964, Validation Loss: 0.5530315041542053\n",
      "Fold 5\n",
      "Epoch 0: Train Loss: 0.6716829140981039, Validation Loss: 0.684134840965271\n",
      "Epoch 1: Train Loss: 0.6097597479820251, Validation Loss: 0.6796385049819946\n",
      "Epoch 2: Train Loss: 0.591478188832601, Validation Loss: 0.6757752299308777\n",
      "Epoch 3: Train Loss: 0.5744261940320333, Validation Loss: 0.6737832427024841\n",
      "Epoch 4: Train Loss: 0.5564259886741638, Validation Loss: 0.6709012985229492\n",
      "Epoch 5: Train Loss: 0.5445204377174377, Validation Loss: 0.6691709160804749\n",
      "Epoch 6: Train Loss: 0.5470544894536337, Validation Loss: 0.6676573753356934\n",
      "Epoch 7: Train Loss: 0.5324900348981222, Validation Loss: 0.6676170825958252\n",
      "Epoch 8: Train Loss: 0.5297563870747884, Validation Loss: 0.669051468372345\n",
      "Epoch 9: Train Loss: 0.5290909806887308, Validation Loss: 0.6702985167503357\n",
      "Epoch 10: Train Loss: 0.5260847608248392, Validation Loss: 0.6578457951545715\n",
      "Epoch 11: Train Loss: 0.5082774062951406, Validation Loss: 0.6457944512367249\n",
      "Epoch 12: Train Loss: 0.47990116477012634, Validation Loss: 0.6388936042785645\n",
      "Epoch 13: Train Loss: 0.4765343964099884, Validation Loss: 0.6288282871246338\n",
      "Epoch 14: Train Loss: 0.46665727098782855, Validation Loss: 0.6215828657150269\n",
      "Epoch 15: Train Loss: 0.44852497180302936, Validation Loss: 0.6204575896263123\n",
      "Epoch 16: Train Loss: 0.44749982158343, Validation Loss: 0.6205338835716248\n",
      "Epoch 17: Train Loss: 0.4342443843682607, Validation Loss: 0.6197814345359802\n",
      "Epoch 18: Train Loss: 0.43244342009226483, Validation Loss: 0.619064211845398\n",
      "Epoch 19: Train Loss: 0.4322069585323334, Validation Loss: 0.6193929314613342\n",
      "Epoch 20: Train Loss: 0.43163933356602985, Validation Loss: 0.5994030833244324\n",
      "Epoch 21: Train Loss: 0.41677698493003845, Validation Loss: 0.5932689309120178\n",
      "Epoch 22: Train Loss: 0.41385159889856976, Validation Loss: 0.5966123342514038\n",
      "Epoch 23: Train Loss: 0.38948262731234234, Validation Loss: 0.6017085313796997\n",
      "Epoch 24: Train Loss: 0.3829640746116638, Validation Loss: 0.6076892614364624\n",
      "Epoch 25: Train Loss: 0.3596113721529643, Validation Loss: 0.6115636825561523\n",
      "Epoch 26: Train Loss: 0.3589661518732707, Validation Loss: 0.6161618232727051\n",
      "Epoch 27: Train Loss: 0.36391056577364606, Validation Loss: 0.6150830388069153\n",
      "Epoch 28: Train Loss: 0.3478527069091797, Validation Loss: 0.6132126450538635\n",
      "Epoch 29: Train Loss: 0.349941223859787, Validation Loss: 0.6121954321861267\n",
      "Epoch 30: Train Loss: 0.3439675768216451, Validation Loss: 0.604206383228302\n",
      "Epoch 31: Train Loss: 0.3287682632605235, Validation Loss: 0.6093249320983887\n",
      "Epoch 32: Train Loss: 0.3111681044101715, Validation Loss: 0.6191974878311157\n",
      "Epoch 33: Train Loss: 0.3071645398934682, Validation Loss: 0.6585223078727722\n",
      "Epoch 34: Train Loss: 0.31470222274462384, Validation Loss: 0.6444195508956909\n",
      "Epoch 35: Train Loss: 0.29362685481707257, Validation Loss: 0.6369636058807373\n",
      "Epoch 36: Train Loss: 0.28980542222658795, Validation Loss: 0.6300360560417175\n",
      "Epoch 37: Train Loss: 0.2891734838485718, Validation Loss: 0.6318672895431519\n",
      "Epoch 38: Train Loss: 0.2859155436356862, Validation Loss: 0.6348839998245239\n",
      "Epoch 39: Train Loss: 0.2759835372368495, Validation Loss: 0.6337119936943054\n",
      "Epoch 40: Train Loss: 0.2877509593963623, Validation Loss: 0.6582229733467102\n",
      "Epoch 41: Train Loss: 0.27722875277201336, Validation Loss: 0.6830122470855713\n",
      "Epoch 42: Train Loss: 0.2759736627340317, Validation Loss: 0.6884676814079285\n",
      "Epoch 43: Train Loss: 0.2573043902715047, Validation Loss: 0.6838266253471375\n",
      "Epoch 44: Train Loss: 0.2537621160348256, Validation Loss: 0.6537238359451294\n",
      "Epoch 45: Train Loss: 0.24934489528338113, Validation Loss: 0.666973352432251\n",
      "Epoch 46: Train Loss: 0.24704144895076752, Validation Loss: 0.6839805841445923\n",
      "Epoch 47: Train Loss: 0.24085423350334167, Validation Loss: 0.6910796761512756\n",
      "Epoch 48: Train Loss: 0.23992228507995605, Validation Loss: 0.6859548091888428\n",
      "Epoch 49: Train Loss: 0.23580769697825113, Validation Loss: 0.6869221329689026\n",
      "Epoch 50: Train Loss: 0.25039831797281903, Validation Loss: 0.6813222169876099\n",
      "Epoch 51: Train Loss: 0.23783897360165915, Validation Loss: 0.7344760894775391\n",
      "Epoch 52: Train Loss: 0.23995826641718546, Validation Loss: 0.7772073745727539\n",
      "Epoch 53: Train Loss: 0.21941223243872324, Validation Loss: 0.7864701747894287\n",
      "Epoch 54: Train Loss: 0.22044598559538522, Validation Loss: 0.7575827240943909\n",
      "Epoch 55: Train Loss: 0.21003155906995138, Validation Loss: 0.7619636654853821\n",
      "Epoch 56: Train Loss: 0.22200262546539307, Validation Loss: 0.7757335305213928\n",
      "Epoch 57: Train Loss: 0.20428081850210825, Validation Loss: 0.7863132357597351\n",
      "Epoch 58: Train Loss: 0.21122792859872183, Validation Loss: 0.7909039258956909\n",
      "Epoch 59: Train Loss: 0.20264170070489249, Validation Loss: 0.7938781976699829\n",
      "Epoch 60: Train Loss: 0.20815353592236838, Validation Loss: 0.8654486536979675\n",
      "Epoch 61: Train Loss: 0.2155355860789617, Validation Loss: 0.8702158331871033\n",
      "Epoch 62: Train Loss: 0.22198298076788583, Validation Loss: 0.8216785788536072\n",
      "Epoch 63: Train Loss: 0.1891290247440338, Validation Loss: 0.7946748733520508\n",
      "Epoch 64: Train Loss: 0.19217906395594278, Validation Loss: 0.7750412225723267\n",
      "Epoch 65: Train Loss: 0.1881308058897654, Validation Loss: 0.7778365612030029\n",
      "Epoch 66: Train Loss: 0.1733150084813436, Validation Loss: 0.7958786487579346\n",
      "Epoch 67: Train Loss: 0.18722386161486307, Validation Loss: 0.8055851459503174\n",
      "Epoch 68: Train Loss: 0.17747552196184793, Validation Loss: 0.8051411509513855\n",
      "Epoch 69: Train Loss: 0.17604392766952515, Validation Loss: 0.807599663734436\n",
      "Epoch 70: Train Loss: 0.16875535746415457, Validation Loss: 0.7911312580108643\n",
      "Epoch 71: Train Loss: 0.18411900103092194, Validation Loss: 0.8105480074882507\n",
      "Epoch 72: Train Loss: 0.17591283222039542, Validation Loss: 0.8524382710456848\n",
      "Epoch 73: Train Loss: 0.16305583715438843, Validation Loss: 0.9122819900512695\n",
      "Epoch 74: Train Loss: 0.1586068719625473, Validation Loss: 0.8972265124320984\n",
      "Epoch 75: Train Loss: 0.15421450634797415, Validation Loss: 0.8378992080688477\n",
      "Epoch 76: Train Loss: 0.15866519510746002, Validation Loss: 0.7925593256950378\n",
      "Epoch 77: Train Loss: 0.14917607605457306, Validation Loss: 0.7814072370529175\n",
      "Epoch 78: Train Loss: 0.14438810447851816, Validation Loss: 0.7847855687141418\n",
      "Epoch 79: Train Loss: 0.15085603296756744, Validation Loss: 0.7867849469184875\n",
      "Epoch 80: Train Loss: 0.14564639826615652, Validation Loss: 0.8719597458839417\n",
      "Epoch 81: Train Loss: 0.15276633699735007, Validation Loss: 0.8675941228866577\n",
      "Epoch 82: Train Loss: 0.14430798093477884, Validation Loss: 0.8405168056488037\n",
      "Epoch 83: Train Loss: 0.1591263860464096, Validation Loss: 0.8168705105781555\n",
      "Epoch 84: Train Loss: 0.16279971599578857, Validation Loss: 0.808506965637207\n",
      "Epoch 85: Train Loss: 0.1448991745710373, Validation Loss: 0.7744544148445129\n",
      "Epoch 86: Train Loss: 0.15052859485149384, Validation Loss: 0.7703850269317627\n",
      "Epoch 87: Train Loss: 0.14017197489738464, Validation Loss: 0.7673995494842529\n",
      "Epoch 88: Train Loss: 0.14419982333978018, Validation Loss: 0.7788355946540833\n",
      "Epoch 89: Train Loss: 0.1393889238437017, Validation Loss: 0.7882484197616577\n",
      "Epoch 90: Train Loss: 0.13056650757789612, Validation Loss: 0.8898460268974304\n",
      "Epoch 91: Train Loss: 0.141472856203715, Validation Loss: 0.8860529661178589\n",
      "Epoch 92: Train Loss: 0.1390726019938787, Validation Loss: 0.8361647129058838\n",
      "Epoch 93: Train Loss: 0.13993844389915466, Validation Loss: 0.8421797156333923\n",
      "Epoch 94: Train Loss: 0.11981713523467381, Validation Loss: 0.8648476600646973\n",
      "Epoch 95: Train Loss: 0.12436091403166454, Validation Loss: 0.8711991310119629\n",
      "Epoch 96: Train Loss: 0.13220682243506113, Validation Loss: 0.8720902800559998\n",
      "Epoch 97: Train Loss: 0.11471471935510635, Validation Loss: 0.8717401623725891\n",
      "Epoch 98: Train Loss: 0.11782281349102657, Validation Loss: 0.8549759984016418\n",
      "Epoch 99: Train Loss: 0.11756768822669983, Validation Loss: 0.8525451421737671\n",
      "Epoch 100: Train Loss: 0.11966031789779663, Validation Loss: 0.8811140656471252\n",
      "Epoch 101: Train Loss: 0.12698251008987427, Validation Loss: 0.8600518107414246\n",
      "Epoch 102: Train Loss: 0.11226277550061543, Validation Loss: 0.8930546045303345\n",
      "Epoch 103: Train Loss: 0.11983779072761536, Validation Loss: 0.9365981221199036\n",
      "Epoch 104: Train Loss: 0.12562300016482672, Validation Loss: 0.9688107371330261\n",
      "Epoch 105: Train Loss: 0.10880747685829799, Validation Loss: 0.9532931447029114\n",
      "Epoch 106: Train Loss: 0.1034836396574974, Validation Loss: 0.9024227261543274\n",
      "Epoch 107: Train Loss: 0.11547182997067769, Validation Loss: 0.8883634805679321\n",
      "Epoch 108: Train Loss: 0.10543848325808843, Validation Loss: 0.8805037140846252\n",
      "Epoch 109: Train Loss: 0.1229287584622701, Validation Loss: 0.8827527165412903\n",
      "Epoch 110: Train Loss: 0.10289610922336578, Validation Loss: 0.8545697927474976\n",
      "Epoch 111: Train Loss: 0.1291793410976728, Validation Loss: 0.8762160539627075\n",
      "Epoch 112: Train Loss: 0.1173551579316457, Validation Loss: 0.9079349040985107\n",
      "Epoch 113: Train Loss: 0.10779094696044922, Validation Loss: 0.8889132142066956\n",
      "Epoch 114: Train Loss: 0.10935183117787044, Validation Loss: 0.8821085095405579\n",
      "Epoch 115: Train Loss: 0.10553856442372005, Validation Loss: 0.8949720859527588\n",
      "Epoch 116: Train Loss: 0.09919583052396774, Validation Loss: 0.8958671689033508\n",
      "Epoch 117: Train Loss: 0.10737477615475655, Validation Loss: 0.9009457230567932\n",
      "Epoch 118: Train Loss: 0.08884940544764201, Validation Loss: 0.890915036201477\n",
      "Epoch 119: Train Loss: 0.09431892136732738, Validation Loss: 0.8943043351173401\n",
      "Epoch 120: Train Loss: 0.0922747328877449, Validation Loss: 0.9021299481391907\n",
      "Epoch 121: Train Loss: 0.11380659292141597, Validation Loss: 0.9391602277755737\n",
      "Epoch 122: Train Loss: 0.10746351629495621, Validation Loss: 1.0565991401672363\n",
      "Epoch 123: Train Loss: 0.09978362669547398, Validation Loss: 1.048901081085205\n",
      "Epoch 124: Train Loss: 0.08580520997444789, Validation Loss: 0.9702242016792297\n",
      "Epoch 125: Train Loss: 0.08240563422441483, Validation Loss: 0.9285462498664856\n",
      "Epoch 126: Train Loss: 0.09145327905813853, Validation Loss: 0.929377555847168\n",
      "Epoch 127: Train Loss: 0.08207424854238828, Validation Loss: 0.9456037282943726\n",
      "Epoch 128: Train Loss: 0.07880000025033951, Validation Loss: 0.9526196122169495\n",
      "Epoch 129: Train Loss: 0.08038754264513652, Validation Loss: 0.9512253403663635\n",
      "Epoch 130: Train Loss: 0.08053842683633168, Validation Loss: 1.0289292335510254\n",
      "Epoch 131: Train Loss: 0.08364463100830714, Validation Loss: 1.0336034297943115\n",
      "Epoch 132: Train Loss: 0.10131118446588516, Validation Loss: 0.9936497807502747\n",
      "Epoch 133: Train Loss: 0.07792926331361134, Validation Loss: 0.9738206267356873\n",
      "Epoch 134: Train Loss: 0.07613997782270114, Validation Loss: 0.9499722719192505\n",
      "Epoch 135: Train Loss: 0.07793273404240608, Validation Loss: 0.9497060775756836\n",
      "Epoch 136: Train Loss: 0.07507066801190376, Validation Loss: 0.9661585092544556\n",
      "Epoch 137: Train Loss: 0.0731125958263874, Validation Loss: 0.9818016886711121\n",
      "Epoch 138: Train Loss: 0.06887611001729965, Validation Loss: 0.9851492047309875\n",
      "Epoch 139: Train Loss: 0.07231773932774861, Validation Loss: 0.989139199256897\n",
      "Epoch 140: Train Loss: 0.0727026214202245, Validation Loss: 1.0155820846557617\n",
      "Epoch 141: Train Loss: 0.09174475570519765, Validation Loss: 1.0451480150222778\n",
      "Epoch 142: Train Loss: 0.07397623856862386, Validation Loss: 1.0467854738235474\n",
      "Epoch 143: Train Loss: 0.08244106670220692, Validation Loss: 1.0470515489578247\n",
      "Epoch 144: Train Loss: 0.0717906504869461, Validation Loss: 1.0426204204559326\n",
      "Epoch 145: Train Loss: 0.07425840323170026, Validation Loss: 1.058341383934021\n",
      "Epoch 146: Train Loss: 0.07616520548860232, Validation Loss: 1.0612215995788574\n",
      "Epoch 147: Train Loss: 0.0770167609055837, Validation Loss: 1.0576411485671997\n",
      "Epoch 148: Train Loss: 0.07307778547207515, Validation Loss: 1.06378173828125\n",
      "Epoch 149: Train Loss: 0.0689894196887811, Validation Loss: 1.0602636337280273\n",
      "Epoch 150: Train Loss: 0.06526976078748703, Validation Loss: 1.0581223964691162\n",
      "Epoch 151: Train Loss: 0.07692716519037883, Validation Loss: 1.053029179573059\n",
      "Epoch 152: Train Loss: 0.07013463228940964, Validation Loss: 1.0521963834762573\n",
      "Epoch 153: Train Loss: 0.06452015787363052, Validation Loss: 1.0490541458129883\n",
      "Epoch 154: Train Loss: 0.06360880161325137, Validation Loss: 1.017594814300537\n",
      "Epoch 155: Train Loss: 0.057721152901649475, Validation Loss: 0.9933304786682129\n",
      "Epoch 156: Train Loss: 0.05412976195414861, Validation Loss: 0.9925419688224792\n",
      "Epoch 157: Train Loss: 0.05447752897938093, Validation Loss: 1.0070064067840576\n",
      "Epoch 158: Train Loss: 0.05602937936782837, Validation Loss: 1.0211037397384644\n",
      "Epoch 159: Train Loss: 0.08282021557291348, Validation Loss: 1.029639482498169\n",
      "Epoch 160: Train Loss: 0.06206560507416725, Validation Loss: 1.12388277053833\n",
      "Epoch 161: Train Loss: 0.0798981562256813, Validation Loss: 1.0926090478897095\n",
      "Epoch 162: Train Loss: 0.07167780647675197, Validation Loss: 1.0715943574905396\n",
      "Epoch 163: Train Loss: 0.06332270428538322, Validation Loss: 1.0713016986846924\n",
      "Epoch 164: Train Loss: 0.06610247616966565, Validation Loss: 1.0955719947814941\n",
      "Epoch 165: Train Loss: 0.061205215752124786, Validation Loss: 1.1452661752700806\n",
      "Epoch 166: Train Loss: 0.05419972042242686, Validation Loss: 1.1596424579620361\n",
      "Epoch 167: Train Loss: 0.05718318745493889, Validation Loss: 1.1521714925765991\n",
      "Epoch 168: Train Loss: 0.06134335199991862, Validation Loss: 1.1177394390106201\n",
      "Epoch 169: Train Loss: 0.05764203021923701, Validation Loss: 1.1240835189819336\n",
      "Epoch 170: Train Loss: 0.05298940340677897, Validation Loss: 1.0190601348876953\n",
      "Epoch 171: Train Loss: 0.05756523211797079, Validation Loss: 0.9987765550613403\n",
      "Epoch 172: Train Loss: 0.05441309387485186, Validation Loss: 1.0383307933807373\n",
      "Epoch 173: Train Loss: 0.05579188962777456, Validation Loss: 1.0687536001205444\n",
      "Epoch 174: Train Loss: 0.06095037112633387, Validation Loss: 1.0434260368347168\n",
      "Epoch 175: Train Loss: 0.0590779185295105, Validation Loss: 1.0306308269500732\n",
      "Epoch 176: Train Loss: 0.057386863976716995, Validation Loss: 1.0232619047164917\n",
      "Epoch 177: Train Loss: 0.05214901578923067, Validation Loss: 1.0309686660766602\n",
      "Epoch 178: Train Loss: 0.05009713893135389, Validation Loss: 1.0439164638519287\n",
      "Epoch 179: Train Loss: 0.05805462971329689, Validation Loss: 1.0565509796142578\n",
      "Epoch 180: Train Loss: 0.04907647892832756, Validation Loss: 1.1476259231567383\n",
      "Epoch 181: Train Loss: 0.05237986768285433, Validation Loss: 1.1847730875015259\n",
      "Epoch 182: Train Loss: 0.04894972965121269, Validation Loss: 1.1628385782241821\n",
      "Epoch 183: Train Loss: 0.04892113928993543, Validation Loss: 1.1287908554077148\n",
      "Epoch 184: Train Loss: 0.05268083388606707, Validation Loss: 1.1357364654541016\n",
      "Epoch 185: Train Loss: 0.053120411932468414, Validation Loss: 1.1383719444274902\n",
      "Epoch 186: Train Loss: 0.04434068500995636, Validation Loss: 1.118962287902832\n",
      "Epoch 187: Train Loss: 0.0453032578031222, Validation Loss: 1.0881354808807373\n",
      "Epoch 188: Train Loss: 0.039682358503341675, Validation Loss: 1.0901724100112915\n",
      "Epoch 189: Train Loss: 0.05089822163184484, Validation Loss: 1.0840998888015747\n",
      "Epoch 190: Train Loss: 0.05430762593944868, Validation Loss: 1.095603585243225\n",
      "Epoch 191: Train Loss: 0.05037829031546911, Validation Loss: 1.095929503440857\n",
      "Epoch 192: Train Loss: 0.04433454076449076, Validation Loss: 1.072970986366272\n",
      "Epoch 193: Train Loss: 0.04883171493808428, Validation Loss: 1.1114338636398315\n",
      "Epoch 194: Train Loss: 0.05173629770676295, Validation Loss: 1.173667550086975\n",
      "Epoch 195: Train Loss: 0.044622521847486496, Validation Loss: 1.171121597290039\n",
      "Epoch 196: Train Loss: 0.04011175036430359, Validation Loss: 1.1465849876403809\n",
      "Epoch 197: Train Loss: 0.038051276157299675, Validation Loss: 1.1231378316879272\n",
      "Epoch 198: Train Loss: 0.04471983201801777, Validation Loss: 1.1179183721542358\n",
      "Epoch 199: Train Loss: 0.04470585100352764, Validation Loss: 1.1271198987960815\n",
      "Accuracy: 0.7416666666666667,Precision: 0.7636363636363637, Recall: 0.7, F1-score: 0.7304347826086957, AUC: 0.7416666666666666\n",
      "Confusion Matrix:\n",
      "[[47 13]\n",
      " [18 42]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nweight decay = 1e-7\\nlearning rate = 0.0001\\nepoch = 100\\nbatch size = 32\\nearly stopping patience = 10\\nstandard scaler\\nReLU\\ncross entropy loss\\ndrop out = 0.8\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "model_save_dir = r'C:\\Users\\User\\Documents\\Lie detect data\\Model'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Define a function to load and pad data\n",
    "def load_data(data_dir, max_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    file_list = os.listdir(data_dir)\n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        label = 1 if 'truth' in file else 0\n",
    "        padded_data = np.zeros((65, max_length))\n",
    "        length = min(data.shape[1], max_length)\n",
    "        padded_data[:, :length] = data[:, :length]\n",
    "        X.append(padded_data)\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load dataset and pad the data\n",
    "data_dir = \"C:\\\\Users\\\\User\\\\Documents\\\\Lie detect data\\\\56M_DWTEEGData\"\n",
    "max_length = 1400  # Define maximum length for padding\n",
    "X, y = load_data(data_dir, max_length)\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# Define EEGNet model\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(65, 32, kernel_size=63, padding=31)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "        self.depthwiseConv1d = nn.Conv1d(32, 64, kernel_size=65, groups=32, padding=32)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)  # Additional convolutional layer\n",
    "        self.batchnorm3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.pooling = nn.AvgPool1d(kernel_size=4)\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        \n",
    "        self._calculate_num_features()\n",
    "        self.fc = nn.Linear(self.num_features, num_classes)\n",
    "\n",
    "    def _calculate_num_features(self):\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.zeros(1, 65, 1400)\n",
    "            sample_output = self._forward_features(sample_input)\n",
    "            self.num_features = sample_output.shape[1]\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.depthwiseConv1d(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.conv2(x)  # Additional convolutional layer\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.pooling(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.global_pool(x)  # Global average pooling layer\n",
    "        x = torch.flatten(x, 1)  # Flatten the output for the fully connected layer\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_and_evaluate(train_loader, val_loader, y_train):\n",
    "    model = EEGNet(num_classes=2).to(device)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-7)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "    num_epochs = 200\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 200\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "            fold_model_path = os.path.join(model_save_dir, f'fold3_model_fold_{fold_idx}.pth')\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': best_val_loss,\n",
    "            }, fold_model_path)\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_idx = 0\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "for train_index, val_index in kf.split(X, y):\n",
    "    print(f'Fold {fold_idx + 1}')\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Normalize data using scaler fitted on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_val = scaler.transform(X_val.reshape(X_val.shape[0], -1))\n",
    "    X_train = X_train.reshape(-1, 65, max_length)\n",
    "    X_val = X_val.reshape(-1, 65, max_length)\n",
    "\n",
    "    # Save the scaler to a file\n",
    "    with open(r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\simpleEEGNet_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = EEGDataset(X_train, y_train)\n",
    "    val_dataset = EEGDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = train_and_evaluate(train_loader, val_loader, y_train)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    fold_idx += 1\n",
    "\n",
    "\n",
    "# Calculate additional metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "auc = roc_auc_score(all_labels, all_predictions)\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy},Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "weight decay = 1e-7\n",
    "learning rate = 0.0001\n",
    "epoch = 100\n",
    "batch size = 32\n",
    "early stopping patience = 10\n",
    "standard scaler\n",
    "ReLU\n",
    "cross entropy loss\n",
    "drop out = 0.8\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66654d63-c450-4d9f-a3e3-63701fd1d41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934cb89-5ea1-4eb0-a152-00c71e49e06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
