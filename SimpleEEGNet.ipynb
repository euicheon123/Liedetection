{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "656cdbc2-da54-45ba-97fc-195b59bd7820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 0: Train Loss: 0.6644076903661092, Validation Loss: 0.684191882610321\n",
      "Epoch 1: Train Loss: 0.6088514924049377, Validation Loss: 0.6744728684425354\n",
      "Epoch 2: Train Loss: 0.5866365234057108, Validation Loss: 0.6626353859901428\n",
      "Epoch 3: Train Loss: 0.5457232594490051, Validation Loss: 0.6514562368392944\n",
      "Epoch 4: Train Loss: 0.5226145188013712, Validation Loss: 0.640117347240448\n",
      "Epoch 5: Train Loss: 0.5137258569399515, Validation Loss: 0.6278354525566101\n",
      "Epoch 6: Train Loss: 0.5012251337369283, Validation Loss: 0.6146366000175476\n",
      "Epoch 7: Train Loss: 0.4854312737782796, Validation Loss: 0.6025906205177307\n",
      "Epoch 8: Train Loss: 0.4880199134349823, Validation Loss: 0.591772198677063\n",
      "Epoch 9: Train Loss: 0.4784955481688182, Validation Loss: 0.5830016732215881\n",
      "Epoch 10: Train Loss: 0.4664299388726552, Validation Loss: 0.5636698007583618\n",
      "Epoch 11: Train Loss: 0.44912777344385785, Validation Loss: 0.5516870617866516\n",
      "Epoch 12: Train Loss: 0.424031267563502, Validation Loss: 0.5367060899734497\n",
      "Epoch 13: Train Loss: 0.4027314782142639, Validation Loss: 0.5233138203620911\n",
      "Epoch 14: Train Loss: 0.38333262999852497, Validation Loss: 0.5169394016265869\n",
      "Epoch 15: Train Loss: 0.3778400520483653, Validation Loss: 0.5130124688148499\n",
      "Epoch 16: Train Loss: 0.3695346613725026, Validation Loss: 0.509492039680481\n",
      "Epoch 17: Train Loss: 0.35985267162323, Validation Loss: 0.5092480778694153\n",
      "Epoch 18: Train Loss: 0.3567839165528615, Validation Loss: 0.5102719664573669\n",
      "Epoch 19: Train Loss: 0.35231425364812213, Validation Loss: 0.5108323097229004\n",
      "Epoch 20: Train Loss: 0.35567737619082135, Validation Loss: 0.5252914428710938\n",
      "Epoch 21: Train Loss: 0.32737456758817035, Validation Loss: 0.535667896270752\n",
      "Epoch 22: Train Loss: 0.3025813698768616, Validation Loss: 0.5337870121002197\n",
      "Epoch 23: Train Loss: 0.33206111192703247, Validation Loss: 0.5327486991882324\n",
      "Epoch 24: Train Loss: 0.30448902646700543, Validation Loss: 0.5350937843322754\n",
      "Epoch 25: Train Loss: 0.27789265910784405, Validation Loss: 0.536626398563385\n",
      "Epoch 26: Train Loss: 0.26271896560986835, Validation Loss: 0.5413791537284851\n",
      "Epoch 27: Train Loss: 0.2874322334925334, Validation Loss: 0.5450748801231384\n",
      "Epoch 28: Train Loss: 0.3020490308602651, Validation Loss: 0.5432314276695251\n",
      "Epoch 29: Train Loss: 0.27095943689346313, Validation Loss: 0.5419698357582092\n",
      "Epoch 30: Train Loss: 0.29047780732313794, Validation Loss: 0.5328905582427979\n",
      "Epoch 31: Train Loss: 0.25377512474854785, Validation Loss: 0.5531660318374634\n",
      "Epoch 32: Train Loss: 0.2798032561937968, Validation Loss: 0.5601269602775574\n",
      "Epoch 33: Train Loss: 0.22355266412099203, Validation Loss: 0.5764070749282837\n",
      "Epoch 34: Train Loss: 0.25376996397972107, Validation Loss: 0.5798434615135193\n",
      "Epoch 35: Train Loss: 0.2248509873946508, Validation Loss: 0.5690655708312988\n",
      "Epoch 36: Train Loss: 0.2259277900060018, Validation Loss: 0.5617730021476746\n",
      "Epoch 37: Train Loss: 0.2142225702603658, Validation Loss: 0.5632101893424988\n",
      "Epoch 38: Train Loss: 0.20459282398223877, Validation Loss: 0.5633972883224487\n",
      "Epoch 39: Train Loss: 0.20363481839497885, Validation Loss: 0.5640447735786438\n",
      "Epoch 40: Train Loss: 0.21286852657794952, Validation Loss: 0.5804218649864197\n",
      "Epoch 41: Train Loss: 0.21611193319161734, Validation Loss: 0.5736588835716248\n",
      "Epoch 42: Train Loss: 0.21606704592704773, Validation Loss: 0.5938167572021484\n",
      "Epoch 43: Train Loss: 0.18336832523345947, Validation Loss: 0.5941272377967834\n",
      "Epoch 44: Train Loss: 0.2009310523668925, Validation Loss: 0.6017091870307922\n",
      "Epoch 45: Train Loss: 0.1795185754696528, Validation Loss: 0.6093940138816833\n",
      "Epoch 46: Train Loss: 0.18613759179910025, Validation Loss: 0.6102559566497803\n",
      "Epoch 47: Train Loss: 0.2028009593486786, Validation Loss: 0.610604465007782\n",
      "Epoch 48: Train Loss: 0.17211714883645376, Validation Loss: 0.6088959574699402\n",
      "Epoch 49: Train Loss: 0.1669217050075531, Validation Loss: 0.6100354194641113\n",
      "Epoch 50: Train Loss: 0.17782142261664072, Validation Loss: 0.6086157560348511\n",
      "Epoch 51: Train Loss: 0.16941856841246286, Validation Loss: 0.603115439414978\n",
      "Epoch 52: Train Loss: 0.16297810773054758, Validation Loss: 0.6126221418380737\n",
      "Epoch 53: Train Loss: 0.16610094904899597, Validation Loss: 0.6550379991531372\n",
      "Epoch 54: Train Loss: 0.16442784170309702, Validation Loss: 0.6583654284477234\n",
      "Epoch 55: Train Loss: 0.1518128514289856, Validation Loss: 0.6681222319602966\n",
      "Epoch 56: Train Loss: 0.13988536596298218, Validation Loss: 0.6696215867996216\n",
      "Early stopping at epoch 57\n",
      "Fold 2\n",
      "Epoch 0: Train Loss: 0.7014636198679606, Validation Loss: 0.6873542070388794\n",
      "Epoch 1: Train Loss: 0.6211787660916647, Validation Loss: 0.678389310836792\n",
      "Epoch 2: Train Loss: 0.5730925997098287, Validation Loss: 0.6656688451766968\n",
      "Epoch 3: Train Loss: 0.562442938486735, Validation Loss: 0.6503801345825195\n",
      "Epoch 4: Train Loss: 0.5325323343276978, Validation Loss: 0.6358183026313782\n",
      "Epoch 5: Train Loss: 0.5257504383722941, Validation Loss: 0.6228489875793457\n",
      "Epoch 6: Train Loss: 0.5166322787602743, Validation Loss: 0.6113986372947693\n",
      "Epoch 7: Train Loss: 0.4983795980612437, Validation Loss: 0.600281834602356\n",
      "Epoch 8: Train Loss: 0.49180511633555096, Validation Loss: 0.5906597971916199\n",
      "Epoch 9: Train Loss: 0.48947229981422424, Validation Loss: 0.58304363489151\n",
      "Epoch 10: Train Loss: 0.4784042139848073, Validation Loss: 0.5619557499885559\n",
      "Epoch 11: Train Loss: 0.4619701604048411, Validation Loss: 0.5437305569648743\n",
      "Epoch 12: Train Loss: 0.4487341145674388, Validation Loss: 0.5252496600151062\n",
      "Epoch 13: Train Loss: 0.4123269021511078, Validation Loss: 0.5068708658218384\n",
      "Epoch 14: Train Loss: 0.39998819430669147, Validation Loss: 0.4936797618865967\n",
      "Epoch 15: Train Loss: 0.38628077507019043, Validation Loss: 0.48419472575187683\n",
      "Epoch 16: Train Loss: 0.3833996852238973, Validation Loss: 0.4748609662055969\n",
      "Epoch 17: Train Loss: 0.37052688995997113, Validation Loss: 0.4693773686885834\n",
      "Epoch 18: Train Loss: 0.3600730796655019, Validation Loss: 0.46629175543785095\n",
      "Epoch 19: Train Loss: 0.37610597411791485, Validation Loss: 0.4648272693157196\n",
      "Epoch 20: Train Loss: 0.38001713156700134, Validation Loss: 0.44779884815216064\n",
      "Epoch 21: Train Loss: 0.3464832802613576, Validation Loss: 0.4266739785671234\n",
      "Epoch 22: Train Loss: 0.33449233571688336, Validation Loss: 0.4329565167427063\n",
      "Epoch 23: Train Loss: 0.3186235229174296, Validation Loss: 0.42187994718551636\n",
      "Epoch 24: Train Loss: 0.29960771401723224, Validation Loss: 0.4163403809070587\n",
      "Epoch 25: Train Loss: 0.3028595248858134, Validation Loss: 0.4147447645664215\n",
      "Epoch 26: Train Loss: 0.28502726554870605, Validation Loss: 0.41084086894989014\n",
      "Epoch 27: Train Loss: 0.26978054642677307, Validation Loss: 0.40560588240623474\n",
      "Epoch 28: Train Loss: 0.2713458140691121, Validation Loss: 0.4021969735622406\n",
      "Epoch 29: Train Loss: 0.2685279001792272, Validation Loss: 0.401104211807251\n",
      "Epoch 30: Train Loss: 0.2642398277918498, Validation Loss: 0.39446258544921875\n",
      "Epoch 31: Train Loss: 0.24770853420098624, Validation Loss: 0.3923954367637634\n",
      "Epoch 32: Train Loss: 0.2399431119362513, Validation Loss: 0.39215099811553955\n",
      "Epoch 33: Train Loss: 0.23156930009524027, Validation Loss: 0.38399556279182434\n",
      "Epoch 34: Train Loss: 0.2125370850165685, Validation Loss: 0.3857506811618805\n",
      "Epoch 35: Train Loss: 0.22149798274040222, Validation Loss: 0.37996265292167664\n",
      "Epoch 36: Train Loss: 0.20953569809595743, Validation Loss: 0.3794066607952118\n",
      "Epoch 37: Train Loss: 0.2028334935506185, Validation Loss: 0.38065826892852783\n",
      "Epoch 38: Train Loss: 0.20199407637119293, Validation Loss: 0.38067811727523804\n",
      "Epoch 39: Train Loss: 0.20748352507750192, Validation Loss: 0.38063427805900574\n",
      "Epoch 40: Train Loss: 0.19670539597670236, Validation Loss: 0.37698572874069214\n",
      "Epoch 41: Train Loss: 0.24355760713418326, Validation Loss: 0.3795015513896942\n",
      "Epoch 42: Train Loss: 0.2049530198176702, Validation Loss: 0.38762906193733215\n",
      "Epoch 43: Train Loss: 0.18255195518334708, Validation Loss: 0.37781140208244324\n",
      "Epoch 44: Train Loss: 0.17151403923829397, Validation Loss: 0.3901318311691284\n",
      "Epoch 45: Train Loss: 0.190214604139328, Validation Loss: 0.3961024284362793\n",
      "Epoch 46: Train Loss: 0.17497219642003378, Validation Loss: 0.3876846134662628\n",
      "Epoch 47: Train Loss: 0.16554919878641763, Validation Loss: 0.38574808835983276\n",
      "Epoch 48: Train Loss: 0.1758605589469274, Validation Loss: 0.38534343242645264\n",
      "Epoch 49: Train Loss: 0.14737518876791, Validation Loss: 0.3864411413669586\n",
      "Epoch 50: Train Loss: 0.18054130425055823, Validation Loss: 0.39414647221565247\n",
      "Epoch 51: Train Loss: 0.15196684002876282, Validation Loss: 0.3837524950504303\n",
      "Epoch 52: Train Loss: 0.143323947985967, Validation Loss: 0.3735288083553314\n",
      "Epoch 53: Train Loss: 0.14481167743603388, Validation Loss: 0.3900007903575897\n",
      "Epoch 54: Train Loss: 0.13906581699848175, Validation Loss: 0.41761112213134766\n",
      "Epoch 55: Train Loss: 0.14618084083000818, Validation Loss: 0.4061587452888489\n",
      "Epoch 56: Train Loss: 0.14650980631510416, Validation Loss: 0.4010845124721527\n",
      "Epoch 57: Train Loss: 0.12975587447484335, Validation Loss: 0.40171974897384644\n",
      "Epoch 58: Train Loss: 0.14653298755486807, Validation Loss: 0.4048287272453308\n",
      "Epoch 59: Train Loss: 0.12753926465908685, Validation Loss: 0.40532323718070984\n",
      "Epoch 60: Train Loss: 0.12637051194906235, Validation Loss: 0.4178573489189148\n",
      "Epoch 61: Train Loss: 0.11553329477707545, Validation Loss: 0.40113621950149536\n",
      "Epoch 62: Train Loss: 0.12740293641885123, Validation Loss: 0.3877251148223877\n",
      "Epoch 63: Train Loss: 0.1201139564315478, Validation Loss: 0.4016476273536682\n",
      "Epoch 64: Train Loss: 0.11550735930601756, Validation Loss: 0.4115599989891052\n",
      "Epoch 65: Train Loss: 0.10454895347356796, Validation Loss: 0.42098549008369446\n",
      "Epoch 66: Train Loss: 0.10223931819200516, Validation Loss: 0.43039119243621826\n",
      "Epoch 67: Train Loss: 0.10262762506802876, Validation Loss: 0.4319251775741577\n",
      "Epoch 68: Train Loss: 0.14661974956591925, Validation Loss: 0.4320792555809021\n",
      "Epoch 69: Train Loss: 0.10551826159159343, Validation Loss: 0.4303574860095978\n",
      "Epoch 70: Train Loss: 0.12579122185707092, Validation Loss: 0.39225777983665466\n",
      "Epoch 71: Train Loss: 0.10967111587524414, Validation Loss: 0.4036285877227783\n",
      "Epoch 72: Train Loss: 0.10571126143137614, Validation Loss: 0.3944268822669983\n",
      "Epoch 73: Train Loss: 0.09682964781920116, Validation Loss: 0.4095286428928375\n",
      "Epoch 74: Train Loss: 0.10346779475609462, Validation Loss: 0.42761605978012085\n",
      "Epoch 75: Train Loss: 0.09200705836216609, Validation Loss: 0.42599308490753174\n",
      "Epoch 76: Train Loss: 0.0882462610801061, Validation Loss: 0.42585158348083496\n",
      "Epoch 77: Train Loss: 0.08608425160249074, Validation Loss: 0.42929351329803467\n",
      "Epoch 78: Train Loss: 0.12255424509445827, Validation Loss: 0.4289427697658539\n",
      "Epoch 79: Train Loss: 0.09649101396401723, Validation Loss: 0.42930540442466736\n",
      "Epoch 80: Train Loss: 0.08978452285130818, Validation Loss: 0.4654330313205719\n",
      "Epoch 81: Train Loss: 0.09693759431441624, Validation Loss: 0.44940027594566345\n",
      "Epoch 82: Train Loss: 0.08098624646663666, Validation Loss: 0.42713800072669983\n",
      "Epoch 83: Train Loss: 0.09039084364970525, Validation Loss: 0.4165922701358795\n",
      "Epoch 84: Train Loss: 0.08268495152393977, Validation Loss: 0.4318557381629944\n",
      "Epoch 85: Train Loss: 0.08075805753469467, Validation Loss: 0.4463661313056946\n",
      "Epoch 86: Train Loss: 0.0774294709165891, Validation Loss: 0.46872586011886597\n",
      "Epoch 87: Train Loss: 0.0812150997420152, Validation Loss: 0.47829899191856384\n",
      "Epoch 88: Train Loss: 0.09083368132511775, Validation Loss: 0.4819914400577545\n",
      "Epoch 89: Train Loss: 0.07244446128606796, Validation Loss: 0.48192229866981506\n",
      "Epoch 90: Train Loss: 0.07342228293418884, Validation Loss: 0.4901140034198761\n",
      "Epoch 91: Train Loss: 0.08915768067042033, Validation Loss: 0.46565455198287964\n",
      "Early stopping at epoch 92\n",
      "Fold 3\n",
      "Epoch 0: Train Loss: 0.6621808409690857, Validation Loss: 0.6896344423294067\n",
      "Epoch 1: Train Loss: 0.5740149219830831, Validation Loss: 0.6774923205375671\n",
      "Epoch 2: Train Loss: 0.5416326324144999, Validation Loss: 0.6644979119300842\n",
      "Epoch 3: Train Loss: 0.5211563209692637, Validation Loss: 0.6509414911270142\n",
      "Epoch 4: Train Loss: 0.49917341272036236, Validation Loss: 0.6360383629798889\n",
      "Epoch 5: Train Loss: 0.48605620861053467, Validation Loss: 0.6203399300575256\n",
      "Epoch 6: Train Loss: 0.474309374888738, Validation Loss: 0.6062806844711304\n",
      "Epoch 7: Train Loss: 0.46013035376866657, Validation Loss: 0.5941965579986572\n",
      "Epoch 8: Train Loss: 0.46924243370691937, Validation Loss: 0.5848496556282043\n",
      "Epoch 9: Train Loss: 0.46670323610305786, Validation Loss: 0.5773419737815857\n",
      "Epoch 10: Train Loss: 0.4598267078399658, Validation Loss: 0.5556268095970154\n",
      "Epoch 11: Train Loss: 0.43099700411160785, Validation Loss: 0.5387259721755981\n",
      "Epoch 12: Train Loss: 0.405855397383372, Validation Loss: 0.5247405171394348\n",
      "Epoch 13: Train Loss: 0.38710688551266986, Validation Loss: 0.5114623308181763\n",
      "Epoch 14: Train Loss: 0.3789810339609782, Validation Loss: 0.5005722641944885\n",
      "Epoch 15: Train Loss: 0.36338289578755695, Validation Loss: 0.49268943071365356\n",
      "Epoch 16: Train Loss: 0.34486769636472064, Validation Loss: 0.4866449236869812\n",
      "Epoch 17: Train Loss: 0.33615297079086304, Validation Loss: 0.4820308983325958\n",
      "Epoch 18: Train Loss: 0.3463109532992045, Validation Loss: 0.4787404239177704\n",
      "Epoch 19: Train Loss: 0.3440152108669281, Validation Loss: 0.4771990180015564\n",
      "Epoch 20: Train Loss: 0.33409424622853595, Validation Loss: 0.47355031967163086\n",
      "Epoch 21: Train Loss: 0.32204143206278485, Validation Loss: 0.47147101163864136\n",
      "Epoch 22: Train Loss: 0.31604909896850586, Validation Loss: 0.4710131883621216\n",
      "Epoch 23: Train Loss: 0.2844742139180501, Validation Loss: 0.4695618450641632\n",
      "Epoch 24: Train Loss: 0.2865840097268422, Validation Loss: 0.46493735909461975\n",
      "Epoch 25: Train Loss: 0.27346443633238476, Validation Loss: 0.4627472758293152\n",
      "Epoch 26: Train Loss: 0.26488346854845685, Validation Loss: 0.4600972831249237\n",
      "Epoch 27: Train Loss: 0.2566254238287608, Validation Loss: 0.45949849486351013\n",
      "Epoch 28: Train Loss: 0.29042347768942517, Validation Loss: 0.45944181084632874\n",
      "Epoch 29: Train Loss: 0.24387291570504507, Validation Loss: 0.45891180634498596\n",
      "Epoch 30: Train Loss: 0.26161139210065204, Validation Loss: 0.45995253324508667\n",
      "Epoch 31: Train Loss: 0.2409505844116211, Validation Loss: 0.458162784576416\n",
      "Epoch 32: Train Loss: 0.22814346353212991, Validation Loss: 0.4563595652580261\n",
      "Epoch 33: Train Loss: 0.24139303465684256, Validation Loss: 0.4457225799560547\n",
      "Epoch 34: Train Loss: 0.21924200157324472, Validation Loss: 0.4465501606464386\n",
      "Epoch 35: Train Loss: 0.22762135167916617, Validation Loss: 0.44979122281074524\n",
      "Epoch 36: Train Loss: 0.20083565513292947, Validation Loss: 0.45062389969825745\n",
      "Epoch 37: Train Loss: 0.22718307872613272, Validation Loss: 0.450117290019989\n",
      "Epoch 38: Train Loss: 0.19462803999582926, Validation Loss: 0.4502159357070923\n",
      "Epoch 39: Train Loss: 0.20029384394486746, Validation Loss: 0.4505522847175598\n",
      "Epoch 40: Train Loss: 0.19574594497680664, Validation Loss: 0.45167937874794006\n",
      "Epoch 41: Train Loss: 0.18191383282343546, Validation Loss: 0.4497379660606384\n",
      "Epoch 42: Train Loss: 0.1863612780968348, Validation Loss: 0.45892128348350525\n",
      "Epoch 43: Train Loss: 0.17152948677539825, Validation Loss: 0.4586356580257416\n",
      "Epoch 44: Train Loss: 0.19240160783131918, Validation Loss: 0.45884114503860474\n",
      "Epoch 45: Train Loss: 0.1726682335138321, Validation Loss: 0.4592449963092804\n",
      "Epoch 46: Train Loss: 0.1595997909704844, Validation Loss: 0.4566047191619873\n",
      "Epoch 47: Train Loss: 0.15807765225569406, Validation Loss: 0.45660069584846497\n",
      "Epoch 48: Train Loss: 0.1496086965004603, Validation Loss: 0.4570362865924835\n",
      "Epoch 49: Train Loss: 0.1548284242550532, Validation Loss: 0.4575585424900055\n",
      "Epoch 50: Train Loss: 0.160558412472407, Validation Loss: 0.45787039399147034\n",
      "Epoch 51: Train Loss: 0.17907186845938364, Validation Loss: 0.4704585671424866\n",
      "Epoch 52: Train Loss: 0.17437991003195444, Validation Loss: 0.47318193316459656\n",
      "Epoch 53: Train Loss: 0.1492854505777359, Validation Loss: 0.47039374709129333\n",
      "Epoch 54: Train Loss: 0.14023900032043457, Validation Loss: 0.459595263004303\n",
      "Epoch 55: Train Loss: 0.17148564010858536, Validation Loss: 0.4482491612434387\n",
      "Epoch 56: Train Loss: 0.13341774543126425, Validation Loss: 0.44383540749549866\n",
      "Epoch 57: Train Loss: 0.18027943869431814, Validation Loss: 0.43891581892967224\n",
      "Epoch 58: Train Loss: 0.1297660768032074, Validation Loss: 0.43857747316360474\n",
      "Epoch 59: Train Loss: 0.1335753599802653, Validation Loss: 0.4397083520889282\n",
      "Epoch 60: Train Loss: 0.12144578744967778, Validation Loss: 0.45117583870887756\n",
      "Epoch 61: Train Loss: 0.13955804208914438, Validation Loss: 0.4615863859653473\n",
      "Epoch 62: Train Loss: 0.1183577577273051, Validation Loss: 0.4653518497943878\n",
      "Epoch 63: Train Loss: 0.12377752860387166, Validation Loss: 0.46438875794410706\n",
      "Epoch 64: Train Loss: 0.12407311052083969, Validation Loss: 0.46125173568725586\n",
      "Epoch 65: Train Loss: 0.12052051723003387, Validation Loss: 0.46770885586738586\n",
      "Epoch 66: Train Loss: 0.12490278730789821, Validation Loss: 0.47470352053642273\n",
      "Epoch 67: Train Loss: 0.10974597930908203, Validation Loss: 0.4786035716533661\n",
      "Epoch 68: Train Loss: 0.10091129690408707, Validation Loss: 0.4794609248638153\n",
      "Epoch 69: Train Loss: 0.1240397368868192, Validation Loss: 0.47973349690437317\n",
      "Epoch 70: Train Loss: 0.14791536331176758, Validation Loss: 0.4771631360054016\n",
      "Epoch 71: Train Loss: 0.1266655003031095, Validation Loss: 0.461764395236969\n",
      "Epoch 72: Train Loss: 0.13842536260684332, Validation Loss: 0.49533674120903015\n",
      "Epoch 73: Train Loss: 0.13207411020994186, Validation Loss: 0.4882218539714813\n",
      "Epoch 74: Train Loss: 0.12125655015309651, Validation Loss: 0.4720849096775055\n",
      "Epoch 75: Train Loss: 0.10403001805146535, Validation Loss: 0.47702935338020325\n",
      "Epoch 76: Train Loss: 0.1197229673465093, Validation Loss: 0.4844593107700348\n",
      "Epoch 77: Train Loss: 0.12597781916459402, Validation Loss: 0.4879780113697052\n",
      "Epoch 78: Train Loss: 0.09446943302949269, Validation Loss: 0.4883887767791748\n",
      "Epoch 79: Train Loss: 0.09632061173518498, Validation Loss: 0.4887697398662567\n",
      "Epoch 80: Train Loss: 0.10709011554718018, Validation Loss: 0.46683526039123535\n",
      "Epoch 81: Train Loss: 0.09373619904120763, Validation Loss: 0.4576363265514374\n",
      "Epoch 82: Train Loss: 0.1404674549897512, Validation Loss: 0.46792837977409363\n",
      "Epoch 83: Train Loss: 0.09705166021982829, Validation Loss: 0.49116817116737366\n",
      "Epoch 84: Train Loss: 0.10182917366425197, Validation Loss: 0.48307886719703674\n",
      "Epoch 85: Train Loss: 0.08160625398159027, Validation Loss: 0.4771708548069\n",
      "Epoch 86: Train Loss: 0.08420870577295621, Validation Loss: 0.47227978706359863\n",
      "Epoch 87: Train Loss: 0.08118012299140294, Validation Loss: 0.46925637125968933\n",
      "Epoch 88: Train Loss: 0.09583153575658798, Validation Loss: 0.46887803077697754\n",
      "Epoch 89: Train Loss: 0.08332992841800053, Validation Loss: 0.4692683517932892\n",
      "Epoch 90: Train Loss: 0.10128428041934967, Validation Loss: 0.48535385727882385\n",
      "Epoch 91: Train Loss: 0.07355591654777527, Validation Loss: 0.48502013087272644\n",
      "Epoch 92: Train Loss: 0.07182577749093373, Validation Loss: 0.49167028069496155\n",
      "Epoch 93: Train Loss: 0.0788447658220927, Validation Loss: 0.5024941563606262\n",
      "Epoch 94: Train Loss: 0.07635230074326198, Validation Loss: 0.5046992301940918\n",
      "Epoch 95: Train Loss: 0.06950946648915608, Validation Loss: 0.508598804473877\n",
      "Epoch 96: Train Loss: 0.06541488816340764, Validation Loss: 0.5096319913864136\n",
      "Epoch 97: Train Loss: 0.06811691572268803, Validation Loss: 0.5108048915863037\n",
      "Early stopping at epoch 98\n",
      "Fold 4\n",
      "Epoch 0: Train Loss: 0.737060527006785, Validation Loss: 0.6942369937896729\n",
      "Epoch 1: Train Loss: 0.6287458737691244, Validation Loss: 0.6919918656349182\n",
      "Epoch 2: Train Loss: 0.5744050145149231, Validation Loss: 0.6894780397415161\n",
      "Epoch 3: Train Loss: 0.5518985589345297, Validation Loss: 0.6865114569664001\n",
      "Epoch 4: Train Loss: 0.5296211043993632, Validation Loss: 0.6828912496566772\n",
      "Epoch 5: Train Loss: 0.4999379913012187, Validation Loss: 0.6788439154624939\n",
      "Epoch 6: Train Loss: 0.4947890043258667, Validation Loss: 0.6756564974784851\n",
      "Epoch 7: Train Loss: 0.4801553984483083, Validation Loss: 0.6730586290359497\n",
      "Epoch 8: Train Loss: 0.47828142841657, Validation Loss: 0.6721952557563782\n",
      "Epoch 9: Train Loss: 0.4812847673892975, Validation Loss: 0.6716380715370178\n",
      "Epoch 10: Train Loss: 0.47054870923360187, Validation Loss: 0.6529730558395386\n",
      "Epoch 11: Train Loss: 0.44533881545066833, Validation Loss: 0.6329756379127502\n",
      "Epoch 12: Train Loss: 0.4307641585667928, Validation Loss: 0.614677906036377\n",
      "Epoch 13: Train Loss: 0.4105086624622345, Validation Loss: 0.6007472276687622\n",
      "Epoch 14: Train Loss: 0.3863023618857066, Validation Loss: 0.590254545211792\n",
      "Epoch 15: Train Loss: 0.38016348083813983, Validation Loss: 0.5836514830589294\n",
      "Epoch 16: Train Loss: 0.37786081433296204, Validation Loss: 0.5781887173652649\n",
      "Epoch 17: Train Loss: 0.35938068230946857, Validation Loss: 0.5751019716262817\n",
      "Epoch 18: Train Loss: 0.36339004834493, Validation Loss: 0.5747371912002563\n",
      "Epoch 19: Train Loss: 0.3683370252450307, Validation Loss: 0.5739716291427612\n",
      "Epoch 20: Train Loss: 0.34715988238652545, Validation Loss: 0.5536700487136841\n",
      "Epoch 21: Train Loss: 0.33606642484664917, Validation Loss: 0.5421563386917114\n",
      "Epoch 22: Train Loss: 0.32565616567929584, Validation Loss: 0.534559428691864\n",
      "Epoch 23: Train Loss: 0.31258203585942584, Validation Loss: 0.5236130952835083\n",
      "Epoch 24: Train Loss: 0.3030237853527069, Validation Loss: 0.5175724029541016\n",
      "Epoch 25: Train Loss: 0.3008788526058197, Validation Loss: 0.5144071578979492\n",
      "Epoch 26: Train Loss: 0.29350237051645917, Validation Loss: 0.5114298462867737\n",
      "Epoch 27: Train Loss: 0.2857031524181366, Validation Loss: 0.5109991431236267\n",
      "Epoch 28: Train Loss: 0.28143590688705444, Validation Loss: 0.5122268199920654\n",
      "Epoch 29: Train Loss: 0.2965337137381236, Validation Loss: 0.512458324432373\n",
      "Epoch 30: Train Loss: 0.3121691842873891, Validation Loss: 0.5059494376182556\n",
      "Epoch 31: Train Loss: 0.27518582840760547, Validation Loss: 0.5095239281654358\n",
      "Epoch 32: Train Loss: 0.2653685559829076, Validation Loss: 0.49884864687919617\n",
      "Epoch 33: Train Loss: 0.2469408263762792, Validation Loss: 0.4931049644947052\n",
      "Epoch 34: Train Loss: 0.24834018448988596, Validation Loss: 0.48397326469421387\n",
      "Epoch 35: Train Loss: 0.23533064126968384, Validation Loss: 0.47228747606277466\n",
      "Epoch 36: Train Loss: 0.2479247103134791, Validation Loss: 0.4676840305328369\n",
      "Epoch 37: Train Loss: 0.2217996964852015, Validation Loss: 0.469180703163147\n",
      "Epoch 38: Train Loss: 0.25175462166468304, Validation Loss: 0.4705505669116974\n",
      "Epoch 39: Train Loss: 0.25191933413346607, Validation Loss: 0.46818724274635315\n",
      "Epoch 40: Train Loss: 0.22400764127572378, Validation Loss: 0.4718960225582123\n",
      "Epoch 41: Train Loss: 0.2534397045771281, Validation Loss: 0.45677030086517334\n",
      "Epoch 42: Train Loss: 0.23198528091112772, Validation Loss: 0.45057517290115356\n",
      "Epoch 43: Train Loss: 0.23985821505387625, Validation Loss: 0.44791680574417114\n",
      "Epoch 44: Train Loss: 0.2009652554988861, Validation Loss: 0.43838176131248474\n",
      "Epoch 45: Train Loss: 0.19698068996270499, Validation Loss: 0.43672093749046326\n",
      "Epoch 46: Train Loss: 0.20003805557886759, Validation Loss: 0.43820711970329285\n",
      "Epoch 47: Train Loss: 0.19503491123517355, Validation Loss: 0.43648040294647217\n",
      "Epoch 48: Train Loss: 0.18176084756851196, Validation Loss: 0.43706658482551575\n",
      "Epoch 49: Train Loss: 0.19733060399691263, Validation Loss: 0.43880221247673035\n",
      "Epoch 50: Train Loss: 0.1909429430961609, Validation Loss: 0.4323156476020813\n",
      "Epoch 51: Train Loss: 0.19294478495915732, Validation Loss: 0.42684364318847656\n",
      "Epoch 52: Train Loss: 0.17465962966283163, Validation Loss: 0.436418741941452\n",
      "Epoch 53: Train Loss: 0.17452436685562134, Validation Loss: 0.4382946491241455\n",
      "Epoch 54: Train Loss: 0.20132475097974142, Validation Loss: 0.4343511164188385\n",
      "Epoch 55: Train Loss: 0.15801589687665304, Validation Loss: 0.43611612915992737\n",
      "Epoch 56: Train Loss: 0.16372051338354746, Validation Loss: 0.42592087388038635\n",
      "Epoch 57: Train Loss: 0.15887549022833505, Validation Loss: 0.4210631251335144\n",
      "Epoch 58: Train Loss: 0.1437743604183197, Validation Loss: 0.4217880964279175\n",
      "Epoch 59: Train Loss: 0.14607602109511694, Validation Loss: 0.421590656042099\n",
      "Epoch 60: Train Loss: 0.14458623031775156, Validation Loss: 0.41625818610191345\n",
      "Epoch 61: Train Loss: 0.14358043173948923, Validation Loss: 0.40449240803718567\n",
      "Epoch 62: Train Loss: 0.16069018840789795, Validation Loss: 0.40253356099128723\n",
      "Epoch 63: Train Loss: 0.14027909686168036, Validation Loss: 0.3994062840938568\n",
      "Epoch 64: Train Loss: 0.14525845646858215, Validation Loss: 0.4070662558078766\n",
      "Epoch 65: Train Loss: 0.146220030883948, Validation Loss: 0.41252049803733826\n",
      "Epoch 66: Train Loss: 0.1300725316007932, Validation Loss: 0.428958922624588\n",
      "Epoch 67: Train Loss: 0.12314560512701671, Validation Loss: 0.4319336414337158\n",
      "Epoch 68: Train Loss: 0.1267427330215772, Validation Loss: 0.433980256319046\n",
      "Epoch 69: Train Loss: 0.14249804615974426, Validation Loss: 0.4336403012275696\n",
      "Epoch 70: Train Loss: 0.153905987739563, Validation Loss: 0.41515040397644043\n",
      "Epoch 71: Train Loss: 0.12887767950693765, Validation Loss: 0.39332953095436096\n",
      "Epoch 72: Train Loss: 0.13052875796953836, Validation Loss: 0.3916783630847931\n",
      "Epoch 73: Train Loss: 0.13105755299329758, Validation Loss: 0.4236496686935425\n",
      "Epoch 74: Train Loss: 0.1429812212785085, Validation Loss: 0.4195307195186615\n",
      "Epoch 75: Train Loss: 0.10918937871853511, Validation Loss: 0.3958539664745331\n",
      "Epoch 76: Train Loss: 0.10709279278914134, Validation Loss: 0.39218026399612427\n",
      "Epoch 77: Train Loss: 0.1014372060696284, Validation Loss: 0.39895591139793396\n",
      "Epoch 78: Train Loss: 0.11502823730309804, Validation Loss: 0.39782512187957764\n",
      "Epoch 79: Train Loss: 0.113780344525973, Validation Loss: 0.3996426463127136\n",
      "Epoch 80: Train Loss: 0.11680134137471516, Validation Loss: 0.41363129019737244\n",
      "Epoch 81: Train Loss: 0.11656586080789566, Validation Loss: 0.38571998476982117\n",
      "Epoch 82: Train Loss: 0.1056625892718633, Validation Loss: 0.35164329409599304\n",
      "Epoch 83: Train Loss: 0.12452586988608043, Validation Loss: 0.3576618432998657\n",
      "Epoch 84: Train Loss: 0.09750757614771526, Validation Loss: 0.4203678071498871\n",
      "Epoch 85: Train Loss: 0.09726000328858693, Validation Loss: 0.4470416307449341\n",
      "Epoch 86: Train Loss: 0.09389449159304301, Validation Loss: 0.44939765334129333\n",
      "Epoch 87: Train Loss: 0.09541977693637212, Validation Loss: 0.4360048472881317\n",
      "Epoch 88: Train Loss: 0.0934290091196696, Validation Loss: 0.42708054184913635\n",
      "Epoch 89: Train Loss: 0.08162186791499455, Validation Loss: 0.42649057507514954\n",
      "Epoch 90: Train Loss: 0.09124598652124405, Validation Loss: 0.375929057598114\n",
      "Epoch 91: Train Loss: 0.08986781040827434, Validation Loss: 0.37776681780815125\n",
      "Epoch 92: Train Loss: 0.1673563520113627, Validation Loss: 0.39921244978904724\n",
      "Epoch 93: Train Loss: 0.08420617133378983, Validation Loss: 0.3861946761608124\n",
      "Epoch 94: Train Loss: 0.09415632734696071, Validation Loss: 0.4057742953300476\n",
      "Epoch 95: Train Loss: 0.093630351126194, Validation Loss: 0.4047757089138031\n",
      "Epoch 96: Train Loss: 0.09857082863648732, Validation Loss: 0.3981655240058899\n",
      "Epoch 97: Train Loss: 0.08164957538247108, Validation Loss: 0.39876440167427063\n",
      "Epoch 98: Train Loss: 0.0779888778924942, Validation Loss: 0.3943009674549103\n",
      "Epoch 99: Train Loss: 0.07392921298742294, Validation Loss: 0.39629605412483215\n",
      "Epoch 100: Train Loss: 0.07985839992761612, Validation Loss: 0.39789506793022156\n",
      "Epoch 101: Train Loss: 0.08328026284774144, Validation Loss: 0.39903998374938965\n",
      "Epoch 102: Train Loss: 0.13018937408924103, Validation Loss: 0.4395166337490082\n",
      "Epoch 103: Train Loss: 0.11028081675370534, Validation Loss: 0.4501796364784241\n",
      "Epoch 104: Train Loss: 0.09468858316540718, Validation Loss: 0.4105006158351898\n",
      "Epoch 105: Train Loss: 0.08767835299173991, Validation Loss: 0.42628368735313416\n",
      "Epoch 106: Train Loss: 0.08355207492907842, Validation Loss: 0.4236814081668854\n",
      "Epoch 107: Train Loss: 0.08425999184449513, Validation Loss: 0.4201628565788269\n",
      "Epoch 108: Train Loss: 0.0965223287542661, Validation Loss: 0.4193281829357147\n",
      "Epoch 109: Train Loss: 0.07436745365460713, Validation Loss: 0.4214337468147278\n",
      "Epoch 110: Train Loss: 0.06854791194200516, Validation Loss: 0.4183029532432556\n",
      "Epoch 111: Train Loss: 0.07353423535823822, Validation Loss: 0.3940398097038269\n",
      "Epoch 112: Train Loss: 0.07832003757357597, Validation Loss: 0.35996881127357483\n",
      "Epoch 113: Train Loss: 0.06679268678029378, Validation Loss: 0.36071568727493286\n",
      "Epoch 114: Train Loss: 0.06119993204871813, Validation Loss: 0.39572396874427795\n",
      "Epoch 115: Train Loss: 0.07652342443664868, Validation Loss: 0.4256853759288788\n",
      "Epoch 116: Train Loss: 0.06341356163223584, Validation Loss: 0.4403359591960907\n",
      "Epoch 117: Train Loss: 0.05607781559228897, Validation Loss: 0.43345361948013306\n",
      "Epoch 118: Train Loss: 0.05969936400651932, Validation Loss: 0.4256371259689331\n",
      "Epoch 119: Train Loss: 0.06373539070288341, Validation Loss: 0.4226561486721039\n",
      "Epoch 120: Train Loss: 0.06764291599392891, Validation Loss: 0.39380142092704773\n",
      "Epoch 121: Train Loss: 0.055093586444854736, Validation Loss: 0.4146314263343811\n",
      "Early stopping at epoch 122\n",
      "Fold 5\n",
      "Epoch 0: Train Loss: 0.6722520589828491, Validation Loss: 0.6924296021461487\n",
      "Epoch 1: Train Loss: 0.5828383167584738, Validation Loss: 0.6885971426963806\n",
      "Epoch 2: Train Loss: 0.5455421606699625, Validation Loss: 0.6868571043014526\n",
      "Epoch 3: Train Loss: 0.514111171166102, Validation Loss: 0.684047281742096\n",
      "Epoch 4: Train Loss: 0.49519797166188556, Validation Loss: 0.6796481013298035\n",
      "Epoch 5: Train Loss: 0.47329504291216534, Validation Loss: 0.675030529499054\n",
      "Epoch 6: Train Loss: 0.46649761994679767, Validation Loss: 0.6706841588020325\n",
      "Epoch 7: Train Loss: 0.4623882671197255, Validation Loss: 0.666606605052948\n",
      "Epoch 8: Train Loss: 0.44847631454467773, Validation Loss: 0.6653015613555908\n",
      "Epoch 9: Train Loss: 0.4525935649871826, Validation Loss: 0.6661341190338135\n",
      "Epoch 10: Train Loss: 0.4423714578151703, Validation Loss: 0.6410181522369385\n",
      "Epoch 11: Train Loss: 0.4274679819742839, Validation Loss: 0.6144217252731323\n",
      "Epoch 12: Train Loss: 0.39956120649973553, Validation Loss: 0.5952197313308716\n",
      "Epoch 13: Train Loss: 0.3809870382150014, Validation Loss: 0.5883245468139648\n",
      "Epoch 14: Train Loss: 0.3699900209903717, Validation Loss: 0.5870748162269592\n",
      "Epoch 15: Train Loss: 0.36180878678957623, Validation Loss: 0.5851664543151855\n",
      "Epoch 16: Train Loss: 0.3522159159183502, Validation Loss: 0.5817431807518005\n",
      "Epoch 17: Train Loss: 0.3449549476305644, Validation Loss: 0.5766651034355164\n",
      "Epoch 18: Train Loss: 0.3455893297990163, Validation Loss: 0.5749797821044922\n",
      "Epoch 19: Train Loss: 0.33576518297195435, Validation Loss: 0.5758705735206604\n",
      "Epoch 20: Train Loss: 0.33527488509813946, Validation Loss: 0.5566193461418152\n",
      "Epoch 21: Train Loss: 0.31365178028742474, Validation Loss: 0.5327214002609253\n",
      "Epoch 22: Train Loss: 0.3065585692723592, Validation Loss: 0.5313043594360352\n",
      "Epoch 23: Train Loss: 0.29343677560488385, Validation Loss: 0.5415195226669312\n",
      "Epoch 24: Train Loss: 0.2856660683949788, Validation Loss: 0.5495373010635376\n",
      "Epoch 25: Train Loss: 0.2750783960024516, Validation Loss: 0.5394092202186584\n",
      "Epoch 26: Train Loss: 0.2768187274535497, Validation Loss: 0.5264524221420288\n",
      "Epoch 27: Train Loss: 0.26715172330538434, Validation Loss: 0.5224055647850037\n",
      "Epoch 28: Train Loss: 0.2698420584201813, Validation Loss: 0.5201790928840637\n",
      "Epoch 29: Train Loss: 0.2704353481531143, Validation Loss: 0.5187195539474487\n",
      "Epoch 30: Train Loss: 0.2651330232620239, Validation Loss: 0.534771203994751\n",
      "Epoch 31: Train Loss: 0.27321507533391315, Validation Loss: 0.5180188417434692\n",
      "Epoch 32: Train Loss: 0.24966411292552948, Validation Loss: 0.5099888443946838\n",
      "Epoch 33: Train Loss: 0.23902742067972818, Validation Loss: 0.4736526608467102\n",
      "Epoch 34: Train Loss: 0.22444613774617514, Validation Loss: 0.4866938889026642\n",
      "Epoch 35: Train Loss: 0.22342494130134583, Validation Loss: 0.49817803502082825\n",
      "Epoch 36: Train Loss: 0.21616555253664652, Validation Loss: 0.5019891858100891\n",
      "Epoch 37: Train Loss: 0.21097680429617563, Validation Loss: 0.500817596912384\n",
      "Epoch 38: Train Loss: 0.23865788181622824, Validation Loss: 0.5057986378669739\n",
      "Epoch 39: Train Loss: 0.21310892701148987, Validation Loss: 0.5018385648727417\n",
      "Epoch 40: Train Loss: 0.21427019437154135, Validation Loss: 0.5346335768699646\n",
      "Epoch 41: Train Loss: 0.24129745364189148, Validation Loss: 0.5311050415039062\n",
      "Epoch 42: Train Loss: 0.2053152322769165, Validation Loss: 0.49630364775657654\n",
      "Epoch 43: Train Loss: 0.18726646900177002, Validation Loss: 0.5025792121887207\n",
      "Epoch 44: Train Loss: 0.19348193208376566, Validation Loss: 0.5034053325653076\n",
      "Epoch 45: Train Loss: 0.18837039669354758, Validation Loss: 0.539451539516449\n",
      "Epoch 46: Train Loss: 0.17652675012747446, Validation Loss: 0.543255090713501\n",
      "Epoch 47: Train Loss: 0.18208051721254984, Validation Loss: 0.5375163555145264\n",
      "Epoch 48: Train Loss: 0.18463675677776337, Validation Loss: 0.5280519723892212\n",
      "Epoch 49: Train Loss: 0.16997366150220236, Validation Loss: 0.5252288579940796\n",
      "Epoch 50: Train Loss: 0.1735814760128657, Validation Loss: 0.4603254795074463\n",
      "Epoch 51: Train Loss: 0.20070055623849234, Validation Loss: 0.5057690739631653\n",
      "Epoch 52: Train Loss: 0.17784230907758078, Validation Loss: 0.5629346966743469\n",
      "Epoch 53: Train Loss: 0.16077499091625214, Validation Loss: 0.5498374104499817\n",
      "Epoch 54: Train Loss: 0.16885455449422201, Validation Loss: 0.519952654838562\n",
      "Epoch 55: Train Loss: 0.14969045917193094, Validation Loss: 0.4865507185459137\n",
      "Epoch 56: Train Loss: 0.15421281506617865, Validation Loss: 0.477716326713562\n",
      "Epoch 57: Train Loss: 0.14263049761454263, Validation Loss: 0.48131948709487915\n",
      "Epoch 58: Train Loss: 0.15779739618301392, Validation Loss: 0.48275285959243774\n",
      "Epoch 59: Train Loss: 0.1833320607741674, Validation Loss: 0.4838358163833618\n",
      "Epoch 60: Train Loss: 0.16018094619115195, Validation Loss: 0.5283541679382324\n",
      "Epoch 61: Train Loss: 0.15779074529806772, Validation Loss: 0.540977954864502\n",
      "Epoch 62: Train Loss: 0.14125734815994898, Validation Loss: 0.5194877982139587\n",
      "Epoch 63: Train Loss: 0.13785018026828766, Validation Loss: 0.46840500831604004\n",
      "Epoch 64: Train Loss: 0.1330434357126554, Validation Loss: 0.48645493388175964\n",
      "Epoch 65: Train Loss: 0.15291517227888107, Validation Loss: 0.4935014545917511\n",
      "Epoch 66: Train Loss: 0.12467397749423981, Validation Loss: 0.4890682101249695\n",
      "Epoch 67: Train Loss: 0.14068935811519623, Validation Loss: 0.48123207688331604\n",
      "Epoch 68: Train Loss: 0.1330119619766871, Validation Loss: 0.4771701395511627\n",
      "Epoch 69: Train Loss: 0.12649681915839514, Validation Loss: 0.47529637813568115\n",
      "Epoch 70: Train Loss: 0.11627311259508133, Validation Loss: 0.47417235374450684\n",
      "Epoch 71: Train Loss: 0.12580571820338568, Validation Loss: 0.4870504140853882\n",
      "Epoch 72: Train Loss: 0.120212122797966, Validation Loss: 0.5993541479110718\n",
      "Epoch 73: Train Loss: 0.1433719793955485, Validation Loss: 0.577922523021698\n",
      "Epoch 74: Train Loss: 0.11107156177361806, Validation Loss: 0.5536840558052063\n",
      "Epoch 75: Train Loss: 0.11632427821556728, Validation Loss: 0.5493189096450806\n",
      "Epoch 76: Train Loss: 0.12105085204044978, Validation Loss: 0.5579225420951843\n",
      "Epoch 77: Train Loss: 0.11338462183872859, Validation Loss: 0.5666536688804626\n",
      "Epoch 78: Train Loss: 0.10201798379421234, Validation Loss: 0.5683600306510925\n",
      "Epoch 79: Train Loss: 0.1024407943089803, Validation Loss: 0.5674774050712585\n",
      "Epoch 80: Train Loss: 0.10067172845204671, Validation Loss: 0.5317298769950867\n",
      "Epoch 81: Train Loss: 0.09870408723751704, Validation Loss: 0.48159435391426086\n",
      "Epoch 82: Train Loss: 0.10288890451192856, Validation Loss: 0.4955112934112549\n",
      "Epoch 83: Train Loss: 0.107490008076032, Validation Loss: 0.5252925753593445\n",
      "Epoch 84: Train Loss: 0.0997101937731107, Validation Loss: 0.5308049917221069\n",
      "Epoch 85: Train Loss: 0.10398625830809276, Validation Loss: 0.5198084712028503\n",
      "Epoch 86: Train Loss: 0.08424955606460571, Validation Loss: 0.5157814025878906\n",
      "Epoch 87: Train Loss: 0.09345599760611852, Validation Loss: 0.5093743801116943\n",
      "Epoch 88: Train Loss: 0.08589740345875423, Validation Loss: 0.5056529641151428\n",
      "Epoch 89: Train Loss: 0.1262337565422058, Validation Loss: 0.5086117386817932\n",
      "Early stopping at epoch 90\n",
      "Accuracy: 0.75,Precision: 0.7777777777777778, Recall: 0.7, F1-score: 0.7368421052631579, AUC: 0.75\n",
      "Confusion Matrix:\n",
      "[[48 12]\n",
      " [18 42]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nweight decay = 1e-7\\nlearning rate = 0.0001\\nepoch = 100\\nbatch size = 32\\nearly stopping patience = 10\\nstandard scaler\\nReLU\\ncross entropy loss\\ndrop out = 0.8\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "model_save_dir = r'C:\\Users\\User\\Documents\\Lie detect data\\Model'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Define a function to load and pad data\n",
    "def load_data(data_dir, max_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    file_list = os.listdir(data_dir)\n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        label = 1 if 'truth' in file else 0\n",
    "        padded_data = np.zeros((65, max_length))\n",
    "        length = min(data.shape[1], max_length)\n",
    "        padded_data[:, :length] = data[:, :length]\n",
    "        X.append(padded_data)\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load dataset and pad the data\n",
    "data_dir = \"C:\\\\Users\\\\User\\\\Documents\\\\Lie detect data\\\\56M_DWTEEGData\"\n",
    "max_length = 500  # Define maximum length for padding\n",
    "X, y = load_data(data_dir, max_length)\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# Define EEGNet model\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(65, 32, kernel_size=63, padding=31)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "        self.depthwiseConv1d = nn.Conv1d(32, 64, kernel_size=65, groups=32, padding=32)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)  # Additional convolutional layer\n",
    "        self.batchnorm3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.pooling = nn.AvgPool1d(kernel_size=4)\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        \n",
    "        self._calculate_num_features()\n",
    "        self.fc = nn.Linear(self.num_features, num_classes)\n",
    "\n",
    "    def _calculate_num_features(self):\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.zeros(1, 65, 1400)\n",
    "            sample_output = self._forward_features(sample_input)\n",
    "            self.num_features = sample_output.shape[1]\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.depthwiseConv1d(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.conv2(x)  # Additional convolutional layer\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.pooling(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.global_pool(x)  # Global average pooling layer\n",
    "        x = torch.flatten(x, 1)  # Flatten the output for the fully connected layer\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_and_evaluate(train_loader, val_loader, y_train):\n",
    "    model = EEGNet(num_classes=2).to(device)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-3)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "    num_epochs = 200\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 40\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "            fold_model_path = os.path.join(model_save_dir, f'fold3_model_fold_{fold_idx}.pth')\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': best_val_loss,\n",
    "            }, fold_model_path)\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_idx = 0\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "for train_index, val_index in kf.split(X, y):\n",
    "    print(f'Fold {fold_idx + 1}')\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Normalize data using scaler fitted on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_val = scaler.transform(X_val.reshape(X_val.shape[0], -1))\n",
    "    X_train = X_train.reshape(-1, 65, max_length)\n",
    "    X_val = X_val.reshape(-1, 65, max_length)\n",
    "\n",
    "    # Save the scaler to a file\n",
    "    with open(r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\simpleEEGNet_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = EEGDataset(X_train, y_train)\n",
    "    val_dataset = EEGDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = train_and_evaluate(train_loader, val_loader, y_train)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    fold_idx += 1\n",
    "\n",
    "\n",
    "# Calculate additional metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "auc = roc_auc_score(all_labels, all_predictions)\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy},Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "weight decay = 1e-7\n",
    "learning rate = 0.0001\n",
    "epoch = 100\n",
    "batch size = 32\n",
    "early stopping patience = 10\n",
    "standard scaler\n",
    "ReLU\n",
    "cross entropy loss\n",
    "drop out = 0.8\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66654d63-c450-4d9f-a3e3-63701fd1d41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934cb89-5ea1-4eb0-a152-00c71e49e06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
