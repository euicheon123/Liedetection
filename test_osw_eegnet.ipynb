{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7da84a4a-fdad-4369-b05d-76e9b1bd85f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape: torch.Size([32, 1, 65, 500])\n",
      "batch shape: torch.Size([32, 1, 65, 500])\n",
      "batch shape: torch.Size([20, 1, 65, 500])\n",
      "Accuracy: 0.4405\n",
      "Precision: 0.5777777777777777, Recall: 0.48148148148148145, F1-score: 0.5252525252525253, AUC: 0.42407407407407405\n",
      "Confusion Matrix:\n",
      "[[11 19]\n",
      " [28 26]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, accuracy_score\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Constants\n",
    "EEG_DATA_DIR = r'C:\\Users\\User\\Documents\\Lie detect data\\TestData-1'\n",
    "max_length = 500  # Define maximum length for padding\n",
    "\n",
    "# Define EEGNet model\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, nb_classes, Chans=65, Samples=500, dropoutRate=0.5, \n",
    "                 kernLength=125, F1=8, D=2, F2=None, norm_rate=0.25, dropoutType='Dropout'):\n",
    "        super(EEGNet, self).__init__()\n",
    "        if F2 is None:\n",
    "            F2 = F1 * D\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, F1, (1, kernLength), padding='same', bias=False)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(F1)\n",
    "        \n",
    "        self.depthwiseConv = nn.Conv2d(F1, F1 * D, (Chans, 1), groups=F1, bias=False)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(F1 * D)\n",
    "        \n",
    "        self.averagePool1 = nn.AvgPool2d((1, 4))\n",
    "        \n",
    "        if dropoutType == 'SpatialDropout2D':\n",
    "            self.dropout1 = nn.Dropout2d(dropoutRate)\n",
    "        elif dropoutType == 'Dropout':\n",
    "            self.dropout1 = nn.Dropout(dropoutRate)\n",
    "        else:\n",
    "            raise ValueError('dropoutType must be one of SpatialDropout2D or Dropout')\n",
    "        \n",
    "        self.separableConv1 = nn.Conv2d(F1 * D, F2, (1, 16), padding='same', bias=False)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(F2)\n",
    "        \n",
    "        self.averagePool2 = nn.AvgPool2d((1, 8))\n",
    "        \n",
    "        if dropoutType == 'SpatialDropout2D':\n",
    "            self.dropout2 = nn.Dropout2d(dropoutRate)\n",
    "        elif dropoutType == 'Dropout':\n",
    "            self.dropout2 = nn.Dropout(dropoutRate)\n",
    "        \n",
    "        # Add more depth by adding another separable convolutional block\n",
    "        self.separableConv2 = nn.Conv2d(F2, F2, (1, 16), padding='same', bias=False)\n",
    "        self.batchnorm4 = nn.BatchNorm2d(F2)\n",
    "        \n",
    "        self.averagePool3 = nn.AvgPool2d((1, 8))\n",
    "        \n",
    "        if dropoutType == 'SpatialDropout2D':\n",
    "            self.dropout3 = nn.Dropout2d(dropoutRate)\n",
    "        elif dropoutType == 'Dropout':\n",
    "            self.dropout3 = nn.Dropout(dropoutRate)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Linear(F2 * (Samples // 256), nb_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.depthwiseConv(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.averagePool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.separableConv1(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.averagePool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Pass through the additional depth layers\n",
    "        x = self.separableConv2(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.averagePool3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "# Function to load and label data (same as in your training script)\n",
    "def load_data(data_dir, max_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    file_list = os.listdir(data_dir)\n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        label = 1 if 'truth' in file else 0\n",
    "        if data.shape[1] > max_length:\n",
    "            processed_data = data[:, :max_length]  # Cut data if it exceeds max_length\n",
    "        else:\n",
    "            processed_data = np.zeros((data.shape[0], max_length))\n",
    "            processed_data[:, :data.shape[1]] = data  # Pad data if it is shorter than max_length\n",
    "        X.append(processed_data)\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure the data is reshaped to [1, Chans, Samples]\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32).unsqueeze(0), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load the saved model\n",
    "    model_path = r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\fold3_model_fold_4.pth'\n",
    "    nb_classes = 2\n",
    "    model = EEGNet(nb_classes=nb_classes).to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval\n",
    "\n",
    "    # Move the model to the appropriate device\n",
    "    model.to(device)\n",
    "\n",
    "    # Load and preprocess the training data to create the scaler\n",
    "    X, y = load_data(EEG_DATA_DIR, max_length)\n",
    "\n",
    "    # Load the scaler\n",
    "    with open(r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\simpleEEGNet_scaler.pkl', 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "    X = scaler.transform(X.reshape(X.shape[0], -1))\n",
    "    X = X.reshape(-1, 65, max_length)\n",
    "    \n",
    "\n",
    "    test_dataset = EEGDataset(X, y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        print(f\"batch shape: {X_batch.shape}\")\n",
    "        \n",
    "        X_batch = X_batch.to(device)\n",
    "        labels = y_batch.to(device)\n",
    "        \n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays for metric calculations\n",
    "all_labels = np.array(all_labels)\n",
    "all_predictions = np.array(all_predictions)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "auc = roc_auc_score(all_labels, all_predictions)\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f'Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf969e6f-a1aa-460c-95df-dddb9b77f486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea5dbca-54c6-4a17-aa0b-ebe1f71e4aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
