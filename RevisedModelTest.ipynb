{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b170e6c-139c-436a-bf23-da8f138cb825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3467\n",
      "Precision: 1.0, Recall: 0.0297029702970297, F1-score: 0.057692307692307696, AUC: 0.5148514851485149\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [98  3]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, accuracy_score\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Constants\n",
    "max_length = 3750  # Define maximum length for padding\n",
    "\n",
    "# Define EEGNet model\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int, kernel_size: tuple, padding: tuple = 0):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.depthwise_conv = nn.Conv2d(self.c_in, self.c_in, kernel_size=self.kernel_size,\n",
    "                                        padding=self.padding, groups=self.c_in)\n",
    "        self.conv2d_1x1 = nn.Conv2d(self.c_in, self.c_out, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y = self.depthwise_conv(x)\n",
    "        y = self.conv2d_1x1(y)\n",
    "        return y\n",
    "\n",
    "class SeparableConv1d(nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int, kernel_size: tuple, padding: tuple = 0):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.depthwise_conv = nn.Conv1d(self.c_in, self.c_in, kernel_size=self.kernel_size,\n",
    "                                        padding=self.padding, groups=self.c_in)\n",
    "        self.conv1d_1x1 = nn.Conv1d(self.c_in, self.c_out, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y = self.depthwise_conv(x)\n",
    "        y = self.conv1d_1x1(y)\n",
    "        return y\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, nb_classes: int = 2, Chans: int = 65, Samples: int = 3750,\n",
    "                 dropoutRate: float = 0.5, kernLength: int = 125,\n",
    "                 F1:int = 8, D:int = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        F2 = F1 * D\n",
    "\n",
    "        # Make kernel size and odd number\n",
    "        try:\n",
    "            assert kernLength % 2 != 0\n",
    "        except AssertionError:\n",
    "            raise ValueError(\"ERROR: kernLength must be odd number\")\n",
    "\n",
    "        # In: (B, Chans, Samples, 1)\n",
    "        # Out: (B, F1, Samples, 1)\n",
    "        self.conv1 = nn.Conv1d(Chans, F1, kernLength, padding=(kernLength // 2))\n",
    "        self.bn1 = nn.BatchNorm1d(F1) # (B, F1, Samples, 1)\n",
    "        # In: (B, F1, Samples, 1)\n",
    "        # Out: (B, F2, Samples - Chans + 1, 1)\n",
    "        self.conv2 = nn.Conv1d(F1, F2, Chans, groups=F1)\n",
    "        self.bn2 = nn.BatchNorm1d(F2) # (B, F2, Samples - Chans + 1, 1)\n",
    "        # In: (B, F2, Samples - Chans + 1, 1)\n",
    "        # Out: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        self.avg_pool = nn.AvgPool1d(4)\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "\n",
    "        # In: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        # Out: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        self.conv3 = SeparableConv1d(F2, F2, kernel_size=31, padding=15)\n",
    "        self.bn3 = nn.BatchNorm1d(F2)\n",
    "        # In: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        # Out: (B, F2, (Samples - Chans + 1) / 32, 1)\n",
    "        self.avg_pool2 = nn.AvgPool1d(8)\n",
    "        # In: (B, F2 *  (Samples - Chans + 1) / 32)\n",
    "        self.fc = nn.Linear(F2 * ((Samples - Chans + 1) // 32), nb_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Block 1\n",
    "        y1 = self.conv1(x)\n",
    "        #print(\"conv1: \", y1.shape)\n",
    "        y1 = self.bn1(y1)\n",
    "        #print(\"bn1: \", y1.shape)\n",
    "        y1 = self.conv2(y1)\n",
    "        #print(\"conv2\", y1.shape)\n",
    "        y1 = F.relu(self.bn2(y1))\n",
    "        #print(\"bn2\", y1.shape)\n",
    "        y1 = self.avg_pool(y1)\n",
    "        #print(\"avg_pool\", y1.shape)\n",
    "        y1 = self.dropout(y1)\n",
    "        #print(\"dropout\", y1.shape)\n",
    "\n",
    "        # Block 2\n",
    "        y2 = self.conv3(y1)\n",
    "        #print(\"conv3\", y2.shape)\n",
    "        y2 = F.relu(self.bn3(y2))\n",
    "        #print(\"bn3\", y2.shape)\n",
    "        y2 = self.avg_pool2(y2)\n",
    "        #print(\"avg_pool2\", y2.shape)\n",
    "        y2 = self.dropout(y2)\n",
    "        #print(\"dropout\", y2.shape)\n",
    "        y2 = torch.flatten(y2, 1)\n",
    "        #print(\"flatten\", y2.shape)\n",
    "        y2 = self.fc(y2)\n",
    "        #print(\"fc\", y2.shape)\n",
    "\n",
    "        return y2\n",
    "        \n",
    "# Function to load and label data (same as in your training script)\n",
    "def load_data(data_dir, max_length):\n",
    "    subject_data = {'lie': {}, 'truth': {}}\n",
    "    \n",
    "    file_list = os.listdir(data_dir)\n",
    "    \n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Determine if the file is 'truth' or 'lie'\n",
    "        label_type = 'truth' if 'truth' in file else 'lie'\n",
    "        subj_id = int(file.split('_')[-1].split('.')[0])\n",
    "        \n",
    "        # Grouping logic\n",
    "        if label_type == 'lie':\n",
    "            # Mapping each 5 lie samples to one subject\n",
    "            subject_key = (subj_id - 1) // 5 + 1\n",
    "        else:  # 'truth'\n",
    "            # Mapping each 6 truth samples to one subject\n",
    "            subject_key = (subj_id - 1) // 6 + 1\n",
    "        \n",
    "        # Initialize the subject's list if it doesn't exist\n",
    "        if subject_key not in subject_data[label_type]:\n",
    "            subject_data[label_type][subject_key] = []\n",
    "        \n",
    "        # Pad or truncate the data to match max_length\n",
    "        if data.shape[1] > max_length:\n",
    "            processed_data = data[:, :max_length]  # Truncate if it exceeds max_length\n",
    "        else:\n",
    "            processed_data = np.zeros((data.shape[0], max_length))\n",
    "            processed_data[:, :data.shape[1]] = data  # Pad if it is shorter than max_length\n",
    "        \n",
    "        # Add the processed data to the appropriate list\n",
    "        subject_data[label_type][subject_key].append(processed_data)\n",
    "    \n",
    "    return subject_data\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure the data is reshaped to [1, Chans, Samples]\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load the saved model\n",
    "    model_path = r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\revise_model_fold_0.pth'\n",
    "    model = EEGNet().to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()  # Note the parentheses to invoke eval mode\n",
    "\n",
    "    # Load the scaler\n",
    "    scaler_path = r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\RevisedEEGNet_scaler_0.pkl'\n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "    # Load the data\n",
    "    data_dir = r'C:\\Users\\User\\Documents\\Lie detect data\\6M_EEGData'\n",
    "    max_length = 3750\n",
    "    subject_data = load_data(data_dir, max_length)\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    # Iterate over each subject and their corresponding data\n",
    "    for label_type in ['truth', 'lie']:\n",
    "        for subject_id, data_list in subject_data[label_type].items():\n",
    "            # Combine the data for each subject\n",
    "            subject_data_combined = np.stack(data_list, axis=0)  # Shape: (n_samples, Chans, Samples)\n",
    "\n",
    "            # Apply scaling (flatten the data, scale, and then reshape)\n",
    "            n_samples, chans, samples = subject_data_combined.shape\n",
    "            subject_data_flat = subject_data_combined.reshape(n_samples, -1)  # Flatten the data\n",
    "            subject_data_scaled = scaler.transform(subject_data_flat)  # Scale\n",
    "            subject_data_scaled = subject_data_scaled.reshape(n_samples, chans, samples)  # Reshape back to original shape\n",
    "\n",
    "            # Create dataset and dataloader\n",
    "            labels = [0 if label_type == 'lie' else 1] * n_samples\n",
    "            dataset = EEGDataset(subject_data_scaled, labels)\n",
    "            dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "            # Inference\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in dataloader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    outputs = model(X_batch)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                    all_labels.extend(y_batch.cpu().numpy())\n",
    "                    all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Convert lists to numpy arrays for metric calculations\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    auc = roc_auc_score(all_labels, all_predictions)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "    # Print the metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f'Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee996a90-92a7-469b-9987-4017a71a518d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
