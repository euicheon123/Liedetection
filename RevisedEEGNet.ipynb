{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29d5ee57-cf0b-4330-985e-ada385863b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 0: Train Loss: 0.8048087159792582, Validation Loss: 0.7078487277030945\n",
      "Epoch 1: Train Loss: 0.7429340680440267, Validation Loss: 0.7033565640449524\n",
      "Epoch 2: Train Loss: 0.6761723558108012, Validation Loss: 0.700888454914093\n",
      "Epoch 3: Train Loss: 0.7112066547075907, Validation Loss: 0.6970864534378052\n",
      "Epoch 4: Train Loss: 0.7132417559623718, Validation Loss: 0.6917615532875061\n",
      "Epoch 5: Train Loss: 0.6490134994188944, Validation Loss: 0.6890152096748352\n",
      "Epoch 6: Train Loss: 0.628275732199351, Validation Loss: 0.6886417865753174\n",
      "Epoch 7: Train Loss: 0.65802530447642, Validation Loss: 0.6890084743499756\n",
      "Epoch 8: Train Loss: 0.6425181825955709, Validation Loss: 0.690150797367096\n",
      "Epoch 9: Train Loss: 0.6589213212331136, Validation Loss: 0.6915438175201416\n",
      "Epoch 10: Train Loss: 0.6226653456687927, Validation Loss: 0.689104437828064\n",
      "Epoch 11: Train Loss: 0.6521492004394531, Validation Loss: 0.6891731023788452\n",
      "Epoch 12: Train Loss: 0.6559923688570658, Validation Loss: 0.6915083527565002\n",
      "Epoch 13: Train Loss: 0.6297967235247294, Validation Loss: 0.6953940987586975\n",
      "Epoch 14: Train Loss: 0.6345678766568502, Validation Loss: 0.697721004486084\n",
      "Epoch 15: Train Loss: 0.6259732842445374, Validation Loss: 0.699429988861084\n",
      "Epoch 16: Train Loss: 0.5833539962768555, Validation Loss: 0.7001932263374329\n",
      "Epoch 17: Train Loss: 0.5775802731513977, Validation Loss: 0.7016246914863586\n",
      "Epoch 18: Train Loss: 0.6348594427108765, Validation Loss: 0.7018836736679077\n",
      "Epoch 19: Train Loss: 0.6168958942095438, Validation Loss: 0.7023999094963074\n",
      "Epoch 20: Train Loss: 0.5866310000419617, Validation Loss: 0.7014428377151489\n",
      "Epoch 21: Train Loss: 0.607617457707723, Validation Loss: 0.7023496031761169\n",
      "Epoch 22: Train Loss: 0.6032729347546896, Validation Loss: 0.701400101184845\n",
      "Epoch 23: Train Loss: 0.5912284255027771, Validation Loss: 0.698545515537262\n",
      "Epoch 24: Train Loss: 0.5453371604283651, Validation Loss: 0.6962180733680725\n",
      "Epoch 25: Train Loss: 0.5354551275571188, Validation Loss: 0.6939009428024292\n",
      "Epoch 26: Train Loss: 0.5543849468231201, Validation Loss: 0.6904516816139221\n",
      "Epoch 27: Train Loss: 0.5614652434984843, Validation Loss: 0.6875472068786621\n",
      "Epoch 28: Train Loss: 0.5474881728490194, Validation Loss: 0.6852906346321106\n",
      "Epoch 29: Train Loss: 0.5635803540547689, Validation Loss: 0.6848267912864685\n",
      "Epoch 30: Train Loss: 0.5642683307329813, Validation Loss: 0.6743971705436707\n",
      "Epoch 31: Train Loss: 0.5273188650608063, Validation Loss: 0.6754744648933411\n",
      "Epoch 32: Train Loss: 0.5773952007293701, Validation Loss: 0.6694276332855225\n",
      "Epoch 33: Train Loss: 0.538009762763977, Validation Loss: 0.6505889296531677\n",
      "Epoch 34: Train Loss: 0.5332769056161245, Validation Loss: 0.6375840902328491\n",
      "Epoch 35: Train Loss: 0.5357083876927694, Validation Loss: 0.6304469108581543\n",
      "Epoch 36: Train Loss: 0.5106722315152487, Validation Loss: 0.6214592456817627\n",
      "Epoch 37: Train Loss: 0.5379792253176371, Validation Loss: 0.6148346662521362\n",
      "Epoch 38: Train Loss: 0.5180009206136068, Validation Loss: 0.6117845773696899\n",
      "Epoch 39: Train Loss: 0.5449897547562917, Validation Loss: 0.610722005367279\n",
      "Epoch 40: Train Loss: 0.5108741025129954, Validation Loss: 0.5948441028594971\n",
      "Epoch 41: Train Loss: 0.5407989621162415, Validation Loss: 0.5901617407798767\n",
      "Epoch 42: Train Loss: 0.5031865537166595, Validation Loss: 0.5886648893356323\n",
      "Epoch 43: Train Loss: 0.4912624458471934, Validation Loss: 0.5802649855613708\n",
      "Epoch 44: Train Loss: 0.5119837919871012, Validation Loss: 0.5773686766624451\n",
      "Epoch 45: Train Loss: 0.4664495488007863, Validation Loss: 0.5753450393676758\n",
      "Epoch 46: Train Loss: 0.4793235659599304, Validation Loss: 0.5724166631698608\n",
      "Epoch 47: Train Loss: 0.4709711968898773, Validation Loss: 0.5701581835746765\n",
      "Epoch 48: Train Loss: 0.4663036564985911, Validation Loss: 0.5692377090454102\n",
      "Epoch 49: Train Loss: 0.4630931218465169, Validation Loss: 0.5677781701087952\n",
      "Epoch 50: Train Loss: 0.5044486323992411, Validation Loss: 0.5743709802627563\n",
      "Epoch 51: Train Loss: 0.4780258735020955, Validation Loss: 0.575632631778717\n",
      "Epoch 52: Train Loss: 0.4935771922270457, Validation Loss: 0.5655472874641418\n",
      "Epoch 53: Train Loss: 0.44426192839940387, Validation Loss: 0.5668630003929138\n",
      "Epoch 54: Train Loss: 0.4618321657180786, Validation Loss: 0.570192277431488\n",
      "Epoch 55: Train Loss: 0.46225659052530926, Validation Loss: 0.5721662044525146\n",
      "Epoch 56: Train Loss: 0.4538029233614604, Validation Loss: 0.5765244364738464\n",
      "Epoch 57: Train Loss: 0.4700288772583008, Validation Loss: 0.5780693888664246\n",
      "Epoch 58: Train Loss: 0.45185568928718567, Validation Loss: 0.5771795511245728\n",
      "Epoch 59: Train Loss: 0.45251322786013287, Validation Loss: 0.5770425200462341\n",
      "Epoch 60: Train Loss: 0.45142218470573425, Validation Loss: 0.5582377910614014\n",
      "Epoch 61: Train Loss: 0.4462146560351054, Validation Loss: 0.5545890927314758\n",
      "Epoch 62: Train Loss: 0.4534602363904317, Validation Loss: 0.5557060241699219\n",
      "Epoch 63: Train Loss: 0.458903710047404, Validation Loss: 0.5602414011955261\n",
      "Epoch 64: Train Loss: 0.42903729279836017, Validation Loss: 0.5605340600013733\n",
      "Epoch 65: Train Loss: 0.416201780239741, Validation Loss: 0.5620956420898438\n",
      "Epoch 66: Train Loss: 0.43706320722897846, Validation Loss: 0.5534844398498535\n",
      "Epoch 67: Train Loss: 0.42536266644795734, Validation Loss: 0.5501503348350525\n",
      "Epoch 68: Train Loss: 0.41359501083691913, Validation Loss: 0.5482330918312073\n",
      "Epoch 69: Train Loss: 0.39725719889005023, Validation Loss: 0.5480750799179077\n",
      "Epoch 70: Train Loss: 0.39249784747759503, Validation Loss: 0.5490615367889404\n",
      "Epoch 71: Train Loss: 0.4271799425284068, Validation Loss: 0.5546342730522156\n",
      "Epoch 72: Train Loss: 0.43052510420481366, Validation Loss: 0.549548327922821\n",
      "Epoch 73: Train Loss: 0.38350585103034973, Validation Loss: 0.5408614873886108\n",
      "Epoch 74: Train Loss: 0.4261883894602458, Validation Loss: 0.5420297384262085\n",
      "Epoch 75: Train Loss: 0.39556222160657245, Validation Loss: 0.5472389459609985\n",
      "Epoch 76: Train Loss: 0.38526809215545654, Validation Loss: 0.5482161045074463\n",
      "Epoch 77: Train Loss: 0.3543988565603892, Validation Loss: 0.5478886365890503\n",
      "Epoch 78: Train Loss: 0.40220452348391217, Validation Loss: 0.5477193593978882\n",
      "Epoch 79: Train Loss: 0.39763449629147846, Validation Loss: 0.5469986200332642\n",
      "Epoch 80: Train Loss: 0.39043401678403217, Validation Loss: 0.5346628427505493\n",
      "Epoch 81: Train Loss: 0.38340820868810016, Validation Loss: 0.5402599573135376\n",
      "Epoch 82: Train Loss: 0.3997197250525157, Validation Loss: 0.529736340045929\n",
      "Epoch 83: Train Loss: 0.37562450766563416, Validation Loss: 0.5211732983589172\n",
      "Epoch 84: Train Loss: 0.3779745101928711, Validation Loss: 0.5217176079750061\n",
      "Epoch 85: Train Loss: 0.38827235500017804, Validation Loss: 0.5241392254829407\n",
      "Epoch 86: Train Loss: 0.3487326701482137, Validation Loss: 0.5294930338859558\n",
      "Epoch 87: Train Loss: 0.3678419589996338, Validation Loss: 0.532237708568573\n",
      "Epoch 88: Train Loss: 0.33957068125406903, Validation Loss: 0.5329449772834778\n",
      "Epoch 89: Train Loss: 0.3527866105238597, Validation Loss: 0.533505380153656\n",
      "Epoch 90: Train Loss: 0.3636487027009328, Validation Loss: 0.5358109474182129\n",
      "Epoch 91: Train Loss: 0.3539803822835286, Validation Loss: 0.5371730327606201\n",
      "Epoch 92: Train Loss: 0.31498610973358154, Validation Loss: 0.5274357199668884\n",
      "Epoch 93: Train Loss: 0.34203653534253436, Validation Loss: 0.5175330638885498\n",
      "Epoch 94: Train Loss: 0.32819082339604694, Validation Loss: 0.5197879076004028\n",
      "Epoch 95: Train Loss: 0.31062160929044086, Validation Loss: 0.5194349884986877\n",
      "Epoch 96: Train Loss: 0.33258322874705, Validation Loss: 0.518836259841919\n",
      "Epoch 97: Train Loss: 0.3277210493882497, Validation Loss: 0.5209295153617859\n",
      "Epoch 98: Train Loss: 0.3243798017501831, Validation Loss: 0.5237065553665161\n",
      "Epoch 99: Train Loss: 0.3377884427706401, Validation Loss: 0.5237195491790771\n",
      "Epoch 100: Train Loss: 0.3621507485707601, Validation Loss: 0.5303997993469238\n",
      "Epoch 101: Train Loss: 0.3283005754152934, Validation Loss: 0.5211448669433594\n",
      "Epoch 102: Train Loss: 0.3372693161169688, Validation Loss: 0.5023694038391113\n",
      "Epoch 103: Train Loss: 0.3111019531885783, Validation Loss: 0.49603506922721863\n",
      "Epoch 104: Train Loss: 0.3003976543744405, Validation Loss: 0.49890998005867004\n",
      "Epoch 105: Train Loss: 0.287159264087677, Validation Loss: 0.5040215253829956\n",
      "Epoch 106: Train Loss: 0.3085901737213135, Validation Loss: 0.5099316835403442\n",
      "Epoch 107: Train Loss: 0.3290833532810211, Validation Loss: 0.5136951804161072\n",
      "Epoch 108: Train Loss: 0.30167799194653827, Validation Loss: 0.5151455998420715\n",
      "Epoch 109: Train Loss: 0.305756409962972, Validation Loss: 0.5151456594467163\n",
      "Epoch 110: Train Loss: 0.292176753282547, Validation Loss: 0.5408099889755249\n",
      "Epoch 111: Train Loss: 0.28634533286094666, Validation Loss: 0.5516546368598938\n",
      "Epoch 112: Train Loss: 0.31575483083724976, Validation Loss: 0.5303910374641418\n",
      "Epoch 113: Train Loss: 0.31910862525304157, Validation Loss: 0.5079289078712463\n",
      "Epoch 114: Train Loss: 0.30334872007369995, Validation Loss: 0.4943755269050598\n",
      "Epoch 115: Train Loss: 0.2820767859617869, Validation Loss: 0.4930455982685089\n",
      "Epoch 116: Train Loss: 0.2902202109495799, Validation Loss: 0.495414674282074\n",
      "Epoch 117: Train Loss: 0.2582113693157832, Validation Loss: 0.4997410476207733\n",
      "Epoch 118: Train Loss: 0.27807220816612244, Validation Loss: 0.5014909505844116\n",
      "Epoch 119: Train Loss: 0.28219107786814374, Validation Loss: 0.5010054707527161\n",
      "Epoch 120: Train Loss: 0.2821866571903229, Validation Loss: 0.5006720423698425\n",
      "Epoch 121: Train Loss: 0.28165019551912945, Validation Loss: 0.4945189654827118\n",
      "Epoch 122: Train Loss: 0.28699929515520733, Validation Loss: 0.49359697103500366\n",
      "Epoch 123: Train Loss: 0.28646586338679, Validation Loss: 0.4942450225353241\n",
      "Epoch 124: Train Loss: 0.27537522713343304, Validation Loss: 0.4952581226825714\n",
      "Epoch 125: Train Loss: 0.259822020928065, Validation Loss: 0.4944694936275482\n",
      "Epoch 126: Train Loss: 0.25101826588312787, Validation Loss: 0.4967876076698303\n",
      "Epoch 127: Train Loss: 0.25209376215934753, Validation Loss: 0.4973214566707611\n",
      "Epoch 128: Train Loss: 0.27689990897973377, Validation Loss: 0.49573057889938354\n",
      "Epoch 129: Train Loss: 0.2707776327927907, Validation Loss: 0.4947429597377777\n",
      "Epoch 130: Train Loss: 0.248585378130277, Validation Loss: 0.5061864256858826\n",
      "Epoch 131: Train Loss: 0.24658918380737305, Validation Loss: 0.505114734172821\n",
      "Epoch 132: Train Loss: 0.24441036581993103, Validation Loss: 0.49046850204467773\n",
      "Epoch 133: Train Loss: 0.2751981317996979, Validation Loss: 0.48597899079322815\n",
      "Epoch 134: Train Loss: 0.23196669419606528, Validation Loss: 0.4893988072872162\n",
      "Epoch 135: Train Loss: 0.2534569501876831, Validation Loss: 0.49517589807510376\n",
      "Epoch 136: Train Loss: 0.24357885122299194, Validation Loss: 0.4918838441371918\n",
      "Epoch 137: Train Loss: 0.23561470210552216, Validation Loss: 0.4893803298473358\n",
      "Epoch 138: Train Loss: 0.23239349325497946, Validation Loss: 0.48632335662841797\n",
      "Epoch 139: Train Loss: 0.24044927954673767, Validation Loss: 0.48714175820350647\n",
      "Epoch 140: Train Loss: 0.23382051289081573, Validation Loss: 0.48649945855140686\n",
      "Epoch 141: Train Loss: 0.2266507645448049, Validation Loss: 0.4795458912849426\n",
      "Epoch 142: Train Loss: 0.22861380378405252, Validation Loss: 0.48810672760009766\n",
      "Epoch 143: Train Loss: 0.23192079365253448, Validation Loss: 0.4987903833389282\n",
      "Epoch 144: Train Loss: 0.22961374123891196, Validation Loss: 0.5168193578720093\n",
      "Epoch 145: Train Loss: 0.24256757895151773, Validation Loss: 0.5107242465019226\n",
      "Epoch 146: Train Loss: 0.23661516110102335, Validation Loss: 0.49716514348983765\n",
      "Epoch 147: Train Loss: 0.2552626033624013, Validation Loss: 0.488693505525589\n",
      "Epoch 148: Train Loss: 0.22044606506824493, Validation Loss: 0.4876425564289093\n",
      "Epoch 149: Train Loss: 0.2098475843667984, Validation Loss: 0.486760675907135\n",
      "Epoch 150: Train Loss: 0.22566012541453043, Validation Loss: 0.48152774572372437\n",
      "Epoch 151: Train Loss: 0.22597257296244302, Validation Loss: 0.4607483744621277\n",
      "Epoch 152: Train Loss: 0.2274584323167801, Validation Loss: 0.48780062794685364\n",
      "Epoch 153: Train Loss: 0.24732442200183868, Validation Loss: 0.5228543281555176\n",
      "Epoch 154: Train Loss: 0.21728792786598206, Validation Loss: 0.5168233513832092\n",
      "Epoch 155: Train Loss: 0.22497033576170603, Validation Loss: 0.5117068290710449\n",
      "Epoch 156: Train Loss: 0.2183444599310557, Validation Loss: 0.5120058655738831\n",
      "Epoch 157: Train Loss: 0.20778929690519968, Validation Loss: 0.5073553919792175\n",
      "Epoch 158: Train Loss: 0.1834187557299932, Validation Loss: 0.5054303407669067\n",
      "Epoch 159: Train Loss: 0.1719433069229126, Validation Loss: 0.5051568746566772\n",
      "Epoch 160: Train Loss: 0.2111094743013382, Validation Loss: 0.469450443983078\n",
      "Epoch 161: Train Loss: 0.2026691883802414, Validation Loss: 0.47839653491973877\n",
      "Epoch 162: Train Loss: 0.20367901027202606, Validation Loss: 0.49420440196990967\n",
      "Epoch 163: Train Loss: 0.20953468481699625, Validation Loss: 0.48674869537353516\n",
      "Epoch 164: Train Loss: 0.2101394236087799, Validation Loss: 0.47535470128059387\n",
      "Epoch 165: Train Loss: 0.21069070200125375, Validation Loss: 0.47376424074172974\n",
      "Epoch 166: Train Loss: 0.20404890676339468, Validation Loss: 0.4708324372768402\n",
      "Epoch 167: Train Loss: 0.2072166899840037, Validation Loss: 0.4755241870880127\n",
      "Epoch 168: Train Loss: 0.17103194693724313, Validation Loss: 0.47739359736442566\n",
      "Epoch 169: Train Loss: 0.19114911556243896, Validation Loss: 0.4772477447986603\n",
      "Epoch 170: Train Loss: 0.2153028647104899, Validation Loss: 0.5108062028884888\n",
      "Epoch 171: Train Loss: 0.19233488539854685, Validation Loss: 0.5135936737060547\n",
      "Epoch 172: Train Loss: 0.1934062441190084, Validation Loss: 0.47807255387306213\n",
      "Epoch 173: Train Loss: 0.17656013866265616, Validation Loss: 0.4638741612434387\n",
      "Epoch 174: Train Loss: 0.18030613164107004, Validation Loss: 0.45040231943130493\n",
      "Epoch 175: Train Loss: 0.2054817130168279, Validation Loss: 0.44691577553749084\n",
      "Epoch 176: Train Loss: 0.19131500025590262, Validation Loss: 0.46726736426353455\n",
      "Epoch 177: Train Loss: 0.1846287945906321, Validation Loss: 0.47724834084510803\n",
      "Epoch 178: Train Loss: 0.18466933071613312, Validation Loss: 0.47959810495376587\n",
      "Epoch 179: Train Loss: 0.18145101269086203, Validation Loss: 0.4801349341869354\n",
      "Epoch 180: Train Loss: 0.18027527630329132, Validation Loss: 0.4677683115005493\n",
      "Epoch 181: Train Loss: 0.1776188462972641, Validation Loss: 0.47091829776763916\n",
      "Epoch 182: Train Loss: 0.18417158226172128, Validation Loss: 0.4687533378601074\n",
      "Epoch 183: Train Loss: 0.16349031031131744, Validation Loss: 0.47798657417297363\n",
      "Epoch 184: Train Loss: 0.18400070567925772, Validation Loss: 0.469480961561203\n",
      "Epoch 185: Train Loss: 0.18950174252192178, Validation Loss: 0.4640728235244751\n",
      "Epoch 186: Train Loss: 0.18831171095371246, Validation Loss: 0.46066129207611084\n",
      "Epoch 187: Train Loss: 0.16646007200082144, Validation Loss: 0.46572935581207275\n",
      "Epoch 188: Train Loss: 0.17436969776948294, Validation Loss: 0.4688869118690491\n",
      "Epoch 189: Train Loss: 0.16474301119645438, Validation Loss: 0.4696739912033081\n",
      "Epoch 190: Train Loss: 0.16018503407637277, Validation Loss: 0.5037353038787842\n",
      "Epoch 191: Train Loss: 0.1733821431795756, Validation Loss: 0.47380778193473816\n",
      "Epoch 192: Train Loss: 0.15884877244631448, Validation Loss: 0.4311040937900543\n",
      "Epoch 193: Train Loss: 0.15279450515906015, Validation Loss: 0.43367916345596313\n",
      "Epoch 194: Train Loss: 0.15857340892155966, Validation Loss: 0.45787882804870605\n",
      "Epoch 195: Train Loss: 0.16613497336705527, Validation Loss: 0.47564658522605896\n",
      "Epoch 196: Train Loss: 0.16141132016976675, Validation Loss: 0.47703486680984497\n",
      "Epoch 197: Train Loss: 0.1644562433163325, Validation Loss: 0.477301687002182\n",
      "Epoch 198: Train Loss: 0.15370563666025797, Validation Loss: 0.47709986567497253\n",
      "Epoch 199: Train Loss: 0.1485199828942617, Validation Loss: 0.47692254185676575\n",
      "Epoch 200: Train Loss: 0.14900421599547067, Validation Loss: 0.4799080193042755\n",
      "Epoch 201: Train Loss: 0.18050354222456613, Validation Loss: 0.49825116991996765\n",
      "Epoch 202: Train Loss: 0.1469613860050837, Validation Loss: 0.5411729216575623\n",
      "Epoch 203: Train Loss: 0.16315233707427979, Validation Loss: 0.5044186115264893\n",
      "Epoch 204: Train Loss: 0.17251664896806082, Validation Loss: 0.48172661662101746\n",
      "Epoch 205: Train Loss: 0.15155482292175293, Validation Loss: 0.4701448976993561\n",
      "Epoch 206: Train Loss: 0.16496209800243378, Validation Loss: 0.476846843957901\n",
      "Epoch 207: Train Loss: 0.15374593436717987, Validation Loss: 0.4844452440738678\n",
      "Epoch 208: Train Loss: 0.13412541647752127, Validation Loss: 0.48456767201423645\n",
      "Epoch 209: Train Loss: 0.14114842812220255, Validation Loss: 0.48393514752388\n",
      "Epoch 210: Train Loss: 0.15960315863291422, Validation Loss: 0.4709271192550659\n",
      "Epoch 211: Train Loss: 0.13759506245454153, Validation Loss: 0.4408072233200073\n",
      "Epoch 212: Train Loss: 0.15437328815460205, Validation Loss: 0.4393095076084137\n",
      "Epoch 213: Train Loss: 0.1490579148133596, Validation Loss: 0.4482746124267578\n",
      "Epoch 214: Train Loss: 0.13882422695557275, Validation Loss: 0.45993438363075256\n",
      "Epoch 215: Train Loss: 0.14280461271603903, Validation Loss: 0.4586423337459564\n",
      "Epoch 216: Train Loss: 0.1309338410695394, Validation Loss: 0.45492857694625854\n",
      "Epoch 217: Train Loss: 0.14994054536024728, Validation Loss: 0.44846418499946594\n",
      "Epoch 218: Train Loss: 0.11283987512191136, Validation Loss: 0.44973358511924744\n",
      "Epoch 219: Train Loss: 0.14173093438148499, Validation Loss: 0.4520154297351837\n",
      "Epoch 220: Train Loss: 0.1331883246699969, Validation Loss: 0.4855005145072937\n",
      "Epoch 221: Train Loss: 0.11406491945187251, Validation Loss: 0.48519423604011536\n",
      "Epoch 222: Train Loss: 0.13600820302963257, Validation Loss: 0.4872707724571228\n",
      "Epoch 223: Train Loss: 0.13177372763554254, Validation Loss: 0.48001837730407715\n",
      "Epoch 224: Train Loss: 0.13542318840821585, Validation Loss: 0.47608286142349243\n",
      "Epoch 225: Train Loss: 0.12331729133923848, Validation Loss: 0.4777143597602844\n",
      "Epoch 226: Train Loss: 0.12535922477642694, Validation Loss: 0.483156681060791\n",
      "Epoch 227: Train Loss: 0.12159598618745804, Validation Loss: 0.4886038601398468\n",
      "Epoch 228: Train Loss: 0.11314683159192403, Validation Loss: 0.49116846919059753\n",
      "Epoch 229: Train Loss: 0.12837684899568558, Validation Loss: 0.49226662516593933\n",
      "Epoch 230: Train Loss: 0.10986268520355225, Validation Loss: 0.48826032876968384\n",
      "Epoch 231: Train Loss: 0.12862325956424078, Validation Loss: 0.5015740990638733\n",
      "Epoch 232: Train Loss: 0.14729085564613342, Validation Loss: 0.5250429511070251\n",
      "Epoch 233: Train Loss: 0.15646527707576752, Validation Loss: 0.5159063339233398\n",
      "Epoch 234: Train Loss: 0.13534253587325415, Validation Loss: 0.4808577597141266\n",
      "Epoch 235: Train Loss: 0.12955707808335623, Validation Loss: 0.4872005581855774\n",
      "Epoch 236: Train Loss: 0.1429879143834114, Validation Loss: 0.4869619309902191\n",
      "Epoch 237: Train Loss: 0.12302325417598088, Validation Loss: 0.48639795184135437\n",
      "Epoch 238: Train Loss: 0.12205114215612411, Validation Loss: 0.48571836948394775\n",
      "Epoch 239: Train Loss: 0.11787469685077667, Validation Loss: 0.4864693284034729\n",
      "Epoch 240: Train Loss: 0.12244476626316707, Validation Loss: 0.46941256523132324\n",
      "Epoch 241: Train Loss: 0.14385653535525003, Validation Loss: 0.4764074385166168\n",
      "Epoch 242: Train Loss: 0.14989607532819113, Validation Loss: 0.5039771795272827\n",
      "Epoch 243: Train Loss: 0.12819168716669083, Validation Loss: 0.5397826433181763\n",
      "Epoch 244: Train Loss: 0.1251156950990359, Validation Loss: 0.5361833572387695\n",
      "Epoch 245: Train Loss: 0.11916529138882954, Validation Loss: 0.5424262881278992\n",
      "Epoch 246: Train Loss: 0.1108166476090749, Validation Loss: 0.5377737283706665\n",
      "Epoch 247: Train Loss: 0.13361381242672601, Validation Loss: 0.5339074730873108\n",
      "Epoch 248: Train Loss: 0.14965606232484183, Validation Loss: 0.5290287137031555\n",
      "Epoch 249: Train Loss: 0.1315358653664589, Validation Loss: 0.5293925404548645\n",
      "Epoch 250: Train Loss: 0.12093768765528996, Validation Loss: 0.4879802465438843\n",
      "Epoch 251: Train Loss: 0.11558301498492558, Validation Loss: 0.4635321795940399\n",
      "Epoch 252: Train Loss: 0.12293260792891185, Validation Loss: 0.46833616495132446\n",
      "Epoch 253: Train Loss: 0.1461192642649015, Validation Loss: 0.47264227271080017\n",
      "Epoch 254: Train Loss: 0.11730975657701492, Validation Loss: 0.5042828917503357\n",
      "Epoch 255: Train Loss: 0.1238701765735944, Validation Loss: 0.5099097490310669\n",
      "Epoch 256: Train Loss: 0.09833577771981557, Validation Loss: 0.49493589997291565\n",
      "Epoch 257: Train Loss: 0.11123622705539067, Validation Loss: 0.4840623736381531\n",
      "Epoch 258: Train Loss: 0.11002576847871144, Validation Loss: 0.48431771993637085\n",
      "Epoch 259: Train Loss: 0.12318900972604752, Validation Loss: 0.4838100075721741\n",
      "Epoch 260: Train Loss: 0.11712491512298584, Validation Loss: 0.49829530715942383\n",
      "Epoch 261: Train Loss: 0.10178537915150325, Validation Loss: 0.5203055143356323\n",
      "Epoch 262: Train Loss: 0.12171896298726399, Validation Loss: 0.518819272518158\n",
      "Epoch 263: Train Loss: 0.12410027037064235, Validation Loss: 0.4916819930076599\n",
      "Epoch 264: Train Loss: 0.10263520975907643, Validation Loss: 0.4778330624103546\n",
      "Epoch 265: Train Loss: 0.10063113023837407, Validation Loss: 0.46964776515960693\n",
      "Epoch 266: Train Loss: 0.10261146972576778, Validation Loss: 0.471640944480896\n",
      "Epoch 267: Train Loss: 0.12444934248924255, Validation Loss: 0.4732508361339569\n",
      "Epoch 268: Train Loss: 0.08654135713974635, Validation Loss: 0.4755050539970398\n",
      "Epoch 269: Train Loss: 0.12223351498444875, Validation Loss: 0.47526806592941284\n",
      "Epoch 270: Train Loss: 0.09965386738379796, Validation Loss: 0.5205145478248596\n",
      "Epoch 271: Train Loss: 0.11796954025824864, Validation Loss: 0.5508736371994019\n",
      "Epoch 272: Train Loss: 0.106196624537309, Validation Loss: 0.5168988704681396\n",
      "Epoch 273: Train Loss: 0.11301617572704951, Validation Loss: 0.49759528040885925\n",
      "Epoch 274: Train Loss: 0.0978608950972557, Validation Loss: 0.4869217276573181\n",
      "Epoch 275: Train Loss: 0.09078294783830643, Validation Loss: 0.47791483998298645\n",
      "Epoch 276: Train Loss: 0.10897935430208842, Validation Loss: 0.4674215316772461\n",
      "Epoch 277: Train Loss: 0.10572224855422974, Validation Loss: 0.4630482494831085\n",
      "Epoch 278: Train Loss: 0.09641430526971817, Validation Loss: 0.46468496322631836\n",
      "Epoch 279: Train Loss: 0.08600840841730435, Validation Loss: 0.46564796566963196\n",
      "Epoch 280: Train Loss: 0.09444436430931091, Validation Loss: 0.47274261713027954\n",
      "Epoch 281: Train Loss: 0.09177468965450923, Validation Loss: 0.516626238822937\n",
      "Epoch 282: Train Loss: 0.10623345772425334, Validation Loss: 0.5164522528648376\n",
      "Epoch 283: Train Loss: 0.09073261668284734, Validation Loss: 0.5209153890609741\n",
      "Epoch 284: Train Loss: 0.09734108050664265, Validation Loss: 0.5150492787361145\n",
      "Epoch 285: Train Loss: 0.10671137024958928, Validation Loss: 0.5222974419593811\n",
      "Epoch 286: Train Loss: 0.10763097057739894, Validation Loss: 0.5040157437324524\n",
      "Epoch 287: Train Loss: 0.10954366127649943, Validation Loss: 0.4915607273578644\n",
      "Epoch 288: Train Loss: 0.07336784899234772, Validation Loss: 0.4894252419471741\n",
      "Epoch 289: Train Loss: 0.09182151903708775, Validation Loss: 0.4905893802642822\n",
      "Epoch 290: Train Loss: 0.08677781124909718, Validation Loss: 0.49027693271636963\n",
      "Epoch 291: Train Loss: 0.08940828343232472, Validation Loss: 0.504714846611023\n",
      "Early stopping at epoch 292\n",
      "Fold 2\n",
      "Epoch 0: Train Loss: 0.752949039141337, Validation Loss: 0.6909727454185486\n",
      "Epoch 1: Train Loss: 0.7177070180575053, Validation Loss: 0.6890607476234436\n",
      "Epoch 2: Train Loss: 0.6942967772483826, Validation Loss: 0.6868117451667786\n",
      "Epoch 3: Train Loss: 0.6641812523206075, Validation Loss: 0.682650625705719\n",
      "Epoch 4: Train Loss: 0.676253100236257, Validation Loss: 0.6774757504463196\n",
      "Epoch 5: Train Loss: 0.6360585888226827, Validation Loss: 0.6739075779914856\n",
      "Epoch 6: Train Loss: 0.6227825284004211, Validation Loss: 0.6695916056632996\n",
      "Epoch 7: Train Loss: 0.686916450659434, Validation Loss: 0.6656282544136047\n",
      "Epoch 8: Train Loss: 0.6632902423540751, Validation Loss: 0.6622411012649536\n",
      "Epoch 9: Train Loss: 0.6987742384274801, Validation Loss: 0.6596327424049377\n",
      "Epoch 10: Train Loss: 0.644659141699473, Validation Loss: 0.653262197971344\n",
      "Epoch 11: Train Loss: 0.6808350880940756, Validation Loss: 0.647438645362854\n",
      "Epoch 12: Train Loss: 0.6417090892791748, Validation Loss: 0.6431772112846375\n",
      "Epoch 13: Train Loss: 0.6474136312802633, Validation Loss: 0.6377025842666626\n",
      "Epoch 14: Train Loss: 0.6378825108210245, Validation Loss: 0.6353198885917664\n",
      "Epoch 15: Train Loss: 0.6125563581784567, Validation Loss: 0.634093701839447\n",
      "Epoch 16: Train Loss: 0.611448069413503, Validation Loss: 0.6327282786369324\n",
      "Epoch 17: Train Loss: 0.60747891664505, Validation Loss: 0.6312945485115051\n",
      "Epoch 18: Train Loss: 0.6280720432599386, Validation Loss: 0.6297600865364075\n",
      "Epoch 19: Train Loss: 0.5888805786768595, Validation Loss: 0.6286740303039551\n",
      "Epoch 20: Train Loss: 0.5944726268450419, Validation Loss: 0.6178040504455566\n",
      "Epoch 21: Train Loss: 0.6326810916264852, Validation Loss: 0.6099611520767212\n",
      "Epoch 22: Train Loss: 0.6354884505271912, Validation Loss: 0.6066620945930481\n",
      "Epoch 23: Train Loss: 0.5769381523132324, Validation Loss: 0.6049934029579163\n",
      "Epoch 24: Train Loss: 0.5713405013084412, Validation Loss: 0.6020851135253906\n",
      "Epoch 25: Train Loss: 0.5554593801498413, Validation Loss: 0.6006007790565491\n",
      "Epoch 26: Train Loss: 0.5795636177062988, Validation Loss: 0.5992038249969482\n",
      "Epoch 27: Train Loss: 0.5798055330912272, Validation Loss: 0.5980390310287476\n",
      "Epoch 28: Train Loss: 0.5746789375940958, Validation Loss: 0.5970630049705505\n",
      "Epoch 29: Train Loss: 0.5813408692677816, Validation Loss: 0.59636390209198\n",
      "Epoch 30: Train Loss: 0.584865391254425, Validation Loss: 0.5951868891716003\n",
      "Epoch 31: Train Loss: 0.5532588164011637, Validation Loss: 0.5987608432769775\n",
      "Epoch 32: Train Loss: 0.5642993847529093, Validation Loss: 0.6020768284797668\n",
      "Epoch 33: Train Loss: 0.5682340065638224, Validation Loss: 0.598041296005249\n",
      "Epoch 34: Train Loss: 0.5380066235860189, Validation Loss: 0.5909860134124756\n",
      "Epoch 35: Train Loss: 0.515974203745524, Validation Loss: 0.586406946182251\n",
      "Epoch 36: Train Loss: 0.5625446836153666, Validation Loss: 0.5848268270492554\n",
      "Epoch 37: Train Loss: 0.5221634407838186, Validation Loss: 0.5851402878761292\n",
      "Epoch 38: Train Loss: 0.5363693634668986, Validation Loss: 0.5851674675941467\n",
      "Epoch 39: Train Loss: 0.5712099273999532, Validation Loss: 0.5849342942237854\n",
      "Epoch 40: Train Loss: 0.5193501810232798, Validation Loss: 0.5935098528862\n",
      "Epoch 41: Train Loss: 0.5175393720467886, Validation Loss: 0.5919299125671387\n",
      "Epoch 42: Train Loss: 0.5479912559191386, Validation Loss: 0.5719724893569946\n",
      "Epoch 43: Train Loss: 0.49829615155855816, Validation Loss: 0.5660247206687927\n",
      "Epoch 44: Train Loss: 0.556172102689743, Validation Loss: 0.5645812153816223\n",
      "Epoch 45: Train Loss: 0.5118923385938009, Validation Loss: 0.5630220770835876\n",
      "Epoch 46: Train Loss: 0.5040618280569712, Validation Loss: 0.5643605589866638\n",
      "Epoch 47: Train Loss: 0.4989716708660126, Validation Loss: 0.5655041933059692\n",
      "Epoch 48: Train Loss: 0.5272888839244843, Validation Loss: 0.5660218596458435\n",
      "Epoch 49: Train Loss: 0.4941066304842631, Validation Loss: 0.5656625032424927\n",
      "Epoch 50: Train Loss: 0.49996663133303326, Validation Loss: 0.5710469484329224\n",
      "Epoch 51: Train Loss: 0.4574111004670461, Validation Loss: 0.580660343170166\n",
      "Epoch 52: Train Loss: 0.5262079437573751, Validation Loss: 0.5761898756027222\n",
      "Epoch 53: Train Loss: 0.4751257797082265, Validation Loss: 0.5701447129249573\n",
      "Epoch 54: Train Loss: 0.4700388212998708, Validation Loss: 0.5726073384284973\n",
      "Epoch 55: Train Loss: 0.4550711214542389, Validation Loss: 0.5733031034469604\n",
      "Epoch 56: Train Loss: 0.47294920682907104, Validation Loss: 0.5673803091049194\n",
      "Epoch 57: Train Loss: 0.4807327091693878, Validation Loss: 0.5630359053611755\n",
      "Epoch 58: Train Loss: 0.4642561376094818, Validation Loss: 0.5614730715751648\n",
      "Epoch 59: Train Loss: 0.4686587353547414, Validation Loss: 0.5610796809196472\n",
      "Epoch 60: Train Loss: 0.4976399441560109, Validation Loss: 0.5540180802345276\n",
      "Epoch 61: Train Loss: 0.4834521512190501, Validation Loss: 0.5582076907157898\n",
      "Epoch 62: Train Loss: 0.4401274124781291, Validation Loss: 0.5565196871757507\n",
      "Epoch 63: Train Loss: 0.4697477916876475, Validation Loss: 0.5515690445899963\n",
      "Epoch 64: Train Loss: 0.4564722279707591, Validation Loss: 0.5487224459648132\n",
      "Epoch 65: Train Loss: 0.4503994683424632, Validation Loss: 0.5461037755012512\n",
      "Epoch 66: Train Loss: 0.4310406943162282, Validation Loss: 0.5435726642608643\n",
      "Epoch 67: Train Loss: 0.44387457768122357, Validation Loss: 0.5430113673210144\n",
      "Epoch 68: Train Loss: 0.4448626935482025, Validation Loss: 0.5428088903427124\n",
      "Epoch 69: Train Loss: 0.45268527666727704, Validation Loss: 0.5426982045173645\n",
      "Epoch 70: Train Loss: 0.4293995201587677, Validation Loss: 0.54325932264328\n",
      "Epoch 71: Train Loss: 0.4207407534122467, Validation Loss: 0.5462981462478638\n",
      "Epoch 72: Train Loss: 0.4355691075325012, Validation Loss: 0.5463294386863708\n",
      "Epoch 73: Train Loss: 0.3988668421904246, Validation Loss: 0.5392506122589111\n",
      "Epoch 74: Train Loss: 0.4443742632865906, Validation Loss: 0.5357881188392639\n",
      "Epoch 75: Train Loss: 0.41324809193611145, Validation Loss: 0.5345377922058105\n",
      "Epoch 76: Train Loss: 0.4156532684961955, Validation Loss: 0.5379365086555481\n",
      "Epoch 77: Train Loss: 0.39619649449984234, Validation Loss: 0.5389676690101624\n",
      "Epoch 78: Train Loss: 0.39754510919253033, Validation Loss: 0.5388547778129578\n",
      "Epoch 79: Train Loss: 0.39550406734148663, Validation Loss: 0.5382699370384216\n",
      "Epoch 80: Train Loss: 0.41307781140009564, Validation Loss: 0.5256980657577515\n",
      "Epoch 81: Train Loss: 0.41453875104586285, Validation Loss: 0.5207685232162476\n",
      "Epoch 82: Train Loss: 0.38713380694389343, Validation Loss: 0.5397288203239441\n",
      "Epoch 83: Train Loss: 0.3909955422083537, Validation Loss: 0.5479696989059448\n",
      "Epoch 84: Train Loss: 0.3815866907437642, Validation Loss: 0.5325923562049866\n",
      "Epoch 85: Train Loss: 0.37333611647288006, Validation Loss: 0.5190593600273132\n",
      "Epoch 86: Train Loss: 0.41617825627326965, Validation Loss: 0.5129243731498718\n",
      "Epoch 87: Train Loss: 0.3973669409751892, Validation Loss: 0.5091288685798645\n",
      "Epoch 88: Train Loss: 0.3914020260175069, Validation Loss: 0.5088909268379211\n",
      "Epoch 89: Train Loss: 0.35944612820943195, Validation Loss: 0.5089781284332275\n",
      "Epoch 90: Train Loss: 0.3771919409434001, Validation Loss: 0.5175713896751404\n",
      "Epoch 91: Train Loss: 0.38621991872787476, Validation Loss: 0.5278605818748474\n",
      "Epoch 92: Train Loss: 0.3909585475921631, Validation Loss: 0.5320454835891724\n",
      "Epoch 93: Train Loss: 0.3629033863544464, Validation Loss: 0.5212085247039795\n",
      "Epoch 94: Train Loss: 0.3745484451452891, Validation Loss: 0.5126293897628784\n",
      "Epoch 95: Train Loss: 0.347345103820165, Validation Loss: 0.5190644264221191\n",
      "Epoch 96: Train Loss: 0.3623256981372833, Validation Loss: 0.520128071308136\n",
      "Epoch 97: Train Loss: 0.3294951021671295, Validation Loss: 0.5207611918449402\n",
      "Epoch 98: Train Loss: 0.3532814681529999, Validation Loss: 0.5213043093681335\n",
      "Epoch 99: Train Loss: 0.36402225494384766, Validation Loss: 0.5209318399429321\n",
      "Epoch 100: Train Loss: 0.3390444020430247, Validation Loss: 0.5240014791488647\n",
      "Epoch 101: Train Loss: 0.3398275276025136, Validation Loss: 0.56084805727005\n",
      "Epoch 102: Train Loss: 0.3475797772407532, Validation Loss: 0.522077202796936\n",
      "Epoch 103: Train Loss: 0.3344777723153432, Validation Loss: 0.5137518644332886\n",
      "Epoch 104: Train Loss: 0.32799839973449707, Validation Loss: 0.5157619118690491\n",
      "Epoch 105: Train Loss: 0.3240068256855011, Validation Loss: 0.5175820589065552\n",
      "Epoch 106: Train Loss: 0.3327132562796275, Validation Loss: 0.5190814733505249\n",
      "Epoch 107: Train Loss: 0.3185497621695201, Validation Loss: 0.5164358615875244\n",
      "Epoch 108: Train Loss: 0.31739314397176105, Validation Loss: 0.5154021978378296\n",
      "Epoch 109: Train Loss: 0.32850022117296857, Validation Loss: 0.5145046710968018\n",
      "Epoch 110: Train Loss: 0.3140912850697835, Validation Loss: 0.4969608783721924\n",
      "Epoch 111: Train Loss: 0.3030274311701457, Validation Loss: 0.5048314929008484\n",
      "Epoch 112: Train Loss: 0.31154144803682965, Validation Loss: 0.5201613306999207\n",
      "Epoch 113: Train Loss: 0.3136497636636098, Validation Loss: 0.5198173522949219\n",
      "Epoch 114: Train Loss: 0.28138813376426697, Validation Loss: 0.5128472447395325\n",
      "Epoch 115: Train Loss: 0.29585280021031696, Validation Loss: 0.521705150604248\n",
      "Epoch 116: Train Loss: 0.29780561725298565, Validation Loss: 0.5256940126419067\n",
      "Epoch 117: Train Loss: 0.28852370381355286, Validation Loss: 0.5225222110748291\n",
      "Epoch 118: Train Loss: 0.310511976480484, Validation Loss: 0.5198995471000671\n",
      "Epoch 119: Train Loss: 0.30798447132110596, Validation Loss: 0.5193461179733276\n",
      "Epoch 120: Train Loss: 0.2886400620142619, Validation Loss: 0.5037550330162048\n",
      "Epoch 121: Train Loss: 0.2985379199186961, Validation Loss: 0.5032384991645813\n",
      "Epoch 122: Train Loss: 0.2903042435646057, Validation Loss: 0.5250810980796814\n",
      "Epoch 123: Train Loss: 0.2929065426190694, Validation Loss: 0.533036470413208\n",
      "Epoch 124: Train Loss: 0.2841443717479706, Validation Loss: 0.5219675898551941\n",
      "Epoch 125: Train Loss: 0.29811017711957294, Validation Loss: 0.5274611711502075\n",
      "Epoch 126: Train Loss: 0.2909371554851532, Validation Loss: 0.5272432565689087\n",
      "Epoch 127: Train Loss: 0.2632463773091634, Validation Loss: 0.5222333669662476\n",
      "Epoch 128: Train Loss: 0.27393772204717, Validation Loss: 0.5187488794326782\n",
      "Epoch 129: Train Loss: 0.2853936751683553, Validation Loss: 0.5176451206207275\n",
      "Epoch 130: Train Loss: 0.252399206161499, Validation Loss: 0.514488697052002\n",
      "Epoch 131: Train Loss: 0.2592365543047587, Validation Loss: 0.5052548050880432\n",
      "Epoch 132: Train Loss: 0.2748303910096486, Validation Loss: 0.5076463222503662\n",
      "Epoch 133: Train Loss: 0.2668909927209218, Validation Loss: 0.5013696551322937\n",
      "Epoch 134: Train Loss: 0.24290777742862701, Validation Loss: 0.496033251285553\n",
      "Epoch 135: Train Loss: 0.24043017129103342, Validation Loss: 0.5118330717086792\n",
      "Epoch 136: Train Loss: 0.24370714028676352, Validation Loss: 0.512363612651825\n",
      "Epoch 137: Train Loss: 0.23777355750401816, Validation Loss: 0.5075646042823792\n",
      "Epoch 138: Train Loss: 0.23636366426944733, Validation Loss: 0.5052729249000549\n",
      "Epoch 139: Train Loss: 0.2504126826922099, Validation Loss: 0.5043956637382507\n",
      "Epoch 140: Train Loss: 0.2277331401904424, Validation Loss: 0.4856158494949341\n",
      "Epoch 141: Train Loss: 0.2448857973019282, Validation Loss: 0.488201767206192\n",
      "Epoch 142: Train Loss: 0.24260162313779196, Validation Loss: 0.5154956579208374\n",
      "Epoch 143: Train Loss: 0.2468565454085668, Validation Loss: 0.5107844471931458\n",
      "Epoch 144: Train Loss: 0.23009430368741354, Validation Loss: 0.5159571766853333\n",
      "Epoch 145: Train Loss: 0.20878322422504425, Validation Loss: 0.5210489630699158\n",
      "Epoch 146: Train Loss: 0.21882860362529755, Validation Loss: 0.5238613486289978\n",
      "Epoch 147: Train Loss: 0.22313782572746277, Validation Loss: 0.5197378396987915\n",
      "Epoch 148: Train Loss: 0.2631028393904368, Validation Loss: 0.5207890272140503\n",
      "Epoch 149: Train Loss: 0.20047184328238168, Validation Loss: 0.5204916000366211\n",
      "Epoch 150: Train Loss: 0.2139944980541865, Validation Loss: 0.50757896900177\n",
      "Epoch 151: Train Loss: 0.2438043753306071, Validation Loss: 0.49832454323768616\n",
      "Epoch 152: Train Loss: 0.24325390656789145, Validation Loss: 0.5017215013504028\n",
      "Epoch 153: Train Loss: 0.2241234133640925, Validation Loss: 0.5189637541770935\n",
      "Epoch 154: Train Loss: 0.22526434560616812, Validation Loss: 0.5305286049842834\n",
      "Epoch 155: Train Loss: 0.2115850547949473, Validation Loss: 0.5210627317428589\n",
      "Epoch 156: Train Loss: 0.20758083959420523, Validation Loss: 0.5047624111175537\n",
      "Epoch 157: Train Loss: 0.19705077509085336, Validation Loss: 0.4917912781238556\n",
      "Epoch 158: Train Loss: 0.18130933245023093, Validation Loss: 0.4897170662879944\n",
      "Epoch 159: Train Loss: 0.20625289777914682, Validation Loss: 0.49036166071891785\n",
      "Epoch 160: Train Loss: 0.19046810766061148, Validation Loss: 0.5236522555351257\n",
      "Epoch 161: Train Loss: 0.20046954353650412, Validation Loss: 0.5314496755599976\n",
      "Epoch 162: Train Loss: 0.19040518999099731, Validation Loss: 0.5395148396492004\n",
      "Epoch 163: Train Loss: 0.20636541148026785, Validation Loss: 0.5398158431053162\n",
      "Epoch 164: Train Loss: 0.19454662998517355, Validation Loss: 0.5185900330543518\n",
      "Epoch 165: Train Loss: 0.1815444976091385, Validation Loss: 0.4989742338657379\n",
      "Epoch 166: Train Loss: 0.17975658675034842, Validation Loss: 0.5031938552856445\n",
      "Epoch 167: Train Loss: 0.19200638930002847, Validation Loss: 0.5085257887840271\n",
      "Epoch 168: Train Loss: 0.17918453365564346, Validation Loss: 0.5108850598335266\n",
      "Epoch 169: Train Loss: 0.16538805266221365, Validation Loss: 0.5117453336715698\n",
      "Epoch 170: Train Loss: 0.17530447244644165, Validation Loss: 0.5587093830108643\n",
      "Epoch 171: Train Loss: 0.20081960161527, Validation Loss: 0.5094320774078369\n",
      "Epoch 172: Train Loss: 0.18879676361878714, Validation Loss: 0.5145466327667236\n",
      "Epoch 173: Train Loss: 0.17057367165883383, Validation Loss: 0.541864812374115\n",
      "Epoch 174: Train Loss: 0.17280172308286032, Validation Loss: 0.5409682393074036\n",
      "Epoch 175: Train Loss: 0.1914836217959722, Validation Loss: 0.5305489897727966\n",
      "Epoch 176: Train Loss: 0.18062524994214377, Validation Loss: 0.5426454544067383\n",
      "Epoch 177: Train Loss: 0.15670031805833182, Validation Loss: 0.5510391592979431\n",
      "Epoch 178: Train Loss: 0.15889685352643332, Validation Loss: 0.5500256419181824\n",
      "Epoch 179: Train Loss: 0.17043898502985635, Validation Loss: 0.5485270023345947\n",
      "Epoch 180: Train Loss: 0.1756610224644343, Validation Loss: 0.49805185198783875\n",
      "Epoch 181: Train Loss: 0.16030766566594443, Validation Loss: 0.49340689182281494\n",
      "Epoch 182: Train Loss: 0.17895387609799704, Validation Loss: 0.5310498476028442\n",
      "Epoch 183: Train Loss: 0.17464004953702292, Validation Loss: 0.5273430347442627\n",
      "Epoch 184: Train Loss: 0.1524023562669754, Validation Loss: 0.5402431488037109\n",
      "Epoch 185: Train Loss: 0.15063494443893433, Validation Loss: 0.5440658330917358\n",
      "Epoch 186: Train Loss: 0.16410352289676666, Validation Loss: 0.5346117615699768\n",
      "Epoch 187: Train Loss: 0.15959172695875168, Validation Loss: 0.5307878851890564\n",
      "Epoch 188: Train Loss: 0.17627584437529245, Validation Loss: 0.5300120711326599\n",
      "Epoch 189: Train Loss: 0.1358520562450091, Validation Loss: 0.5291749835014343\n",
      "Epoch 190: Train Loss: 0.16303343574206033, Validation Loss: 0.5358263254165649\n",
      "Epoch 191: Train Loss: 0.16291002680857977, Validation Loss: 0.563343346118927\n",
      "Epoch 192: Train Loss: 0.16680642465750375, Validation Loss: 0.5349847674369812\n",
      "Epoch 193: Train Loss: 0.17272829512755075, Validation Loss: 0.4864925444126129\n",
      "Epoch 194: Train Loss: 0.1720708062251409, Validation Loss: 0.4930605888366699\n",
      "Epoch 195: Train Loss: 0.14896958818038306, Validation Loss: 0.5234370231628418\n",
      "Epoch 196: Train Loss: 0.13819358746210733, Validation Loss: 0.5147847533226013\n",
      "Epoch 197: Train Loss: 0.14073317497968674, Validation Loss: 0.5169450044631958\n",
      "Epoch 198: Train Loss: 0.14088902870814005, Validation Loss: 0.515600323677063\n",
      "Epoch 199: Train Loss: 0.15782687067985535, Validation Loss: 0.515665590763092\n",
      "Epoch 200: Train Loss: 0.1253210554520289, Validation Loss: 0.5610345602035522\n",
      "Epoch 201: Train Loss: 0.16500679651896158, Validation Loss: 0.5433578491210938\n",
      "Epoch 202: Train Loss: 0.13900193323691687, Validation Loss: 0.5344443917274475\n",
      "Epoch 203: Train Loss: 0.1506375645597776, Validation Loss: 0.5232399106025696\n",
      "Epoch 204: Train Loss: 0.12275729328393936, Validation Loss: 0.5123409628868103\n",
      "Epoch 205: Train Loss: 0.15253225713968277, Validation Loss: 0.5174301266670227\n",
      "Epoch 206: Train Loss: 0.14492194851239523, Validation Loss: 0.5267674922943115\n",
      "Epoch 207: Train Loss: 0.12162974973519643, Validation Loss: 0.5259746313095093\n",
      "Epoch 208: Train Loss: 0.12877369920412698, Validation Loss: 0.5249462723731995\n",
      "Epoch 209: Train Loss: 0.14703367153803507, Validation Loss: 0.5246385931968689\n",
      "Epoch 210: Train Loss: 0.12966431180636087, Validation Loss: 0.5203104019165039\n",
      "Epoch 211: Train Loss: 0.12122959891955058, Validation Loss: 0.5174123644828796\n",
      "Epoch 212: Train Loss: 0.14406271278858185, Validation Loss: 0.5280070304870605\n",
      "Epoch 213: Train Loss: 0.13710897415876389, Validation Loss: 0.5455922484397888\n",
      "Epoch 214: Train Loss: 0.13863306740919748, Validation Loss: 0.5591405630111694\n",
      "Epoch 215: Train Loss: 0.12306166191895802, Validation Loss: 0.5806964039802551\n",
      "Epoch 216: Train Loss: 0.15037225931882858, Validation Loss: 0.5728661417961121\n",
      "Epoch 217: Train Loss: 0.13185022523005804, Validation Loss: 0.5528475642204285\n",
      "Epoch 218: Train Loss: 0.1122928112745285, Validation Loss: 0.5449329018592834\n",
      "Epoch 219: Train Loss: 0.12105831255515416, Validation Loss: 0.5435779094696045\n",
      "Epoch 220: Train Loss: 0.12554925680160522, Validation Loss: 0.5146389603614807\n",
      "Epoch 221: Train Loss: 0.13870307554801306, Validation Loss: 0.5214023590087891\n",
      "Epoch 222: Train Loss: 0.13334041833877563, Validation Loss: 0.5140124559402466\n",
      "Epoch 223: Train Loss: 0.11669954657554626, Validation Loss: 0.49476388096809387\n",
      "Epoch 224: Train Loss: 0.13101489345232645, Validation Loss: 0.49698761105537415\n",
      "Epoch 225: Train Loss: 0.11685459812482198, Validation Loss: 0.49816274642944336\n",
      "Epoch 226: Train Loss: 0.10378474245468776, Validation Loss: 0.5144216418266296\n",
      "Epoch 227: Train Loss: 0.11986971149841945, Validation Loss: 0.5214158296585083\n",
      "Epoch 228: Train Loss: 0.10560608158508937, Validation Loss: 0.5254589915275574\n",
      "Epoch 229: Train Loss: 0.09984593093395233, Validation Loss: 0.5260979533195496\n",
      "Epoch 230: Train Loss: 0.12509676069021225, Validation Loss: 0.5578911304473877\n",
      "Epoch 231: Train Loss: 0.12180218597253163, Validation Loss: 0.5462639331817627\n",
      "Epoch 232: Train Loss: 0.11496579150358836, Validation Loss: 0.52286297082901\n",
      "Epoch 233: Train Loss: 0.12537374099095663, Validation Loss: 0.5283218026161194\n",
      "Epoch 234: Train Loss: 0.11814888070027034, Validation Loss: 0.5393773913383484\n",
      "Epoch 235: Train Loss: 0.11181063205003738, Validation Loss: 0.5468003153800964\n",
      "Epoch 236: Train Loss: 0.09731254975001018, Validation Loss: 0.5587460994720459\n",
      "Epoch 237: Train Loss: 0.10524727404117584, Validation Loss: 0.5594354271888733\n",
      "Epoch 238: Train Loss: 0.10999150574207306, Validation Loss: 0.5549418926239014\n",
      "Epoch 239: Train Loss: 0.09877792994181316, Validation Loss: 0.5547767281532288\n",
      "Early stopping at epoch 240\n",
      "Fold 3\n",
      "Epoch 0: Train Loss: 0.6958615779876709, Validation Loss: 0.684863269329071\n",
      "Epoch 1: Train Loss: 0.6680459578831991, Validation Loss: 0.6841748356819153\n",
      "Epoch 2: Train Loss: 0.6633757948875427, Validation Loss: 0.68385910987854\n",
      "Epoch 3: Train Loss: 0.6363974610964457, Validation Loss: 0.6836438179016113\n",
      "Epoch 4: Train Loss: 0.6456120610237122, Validation Loss: 0.6832202672958374\n",
      "Epoch 5: Train Loss: 0.6487100919087728, Validation Loss: 0.6829298138618469\n",
      "Epoch 6: Train Loss: 0.620453397432963, Validation Loss: 0.682233452796936\n",
      "Epoch 7: Train Loss: 0.6338046987851461, Validation Loss: 0.6814831495285034\n",
      "Epoch 8: Train Loss: 0.6090241471926371, Validation Loss: 0.6810314655303955\n",
      "Epoch 9: Train Loss: 0.6319439808527628, Validation Loss: 0.6807312369346619\n",
      "Epoch 10: Train Loss: 0.6264151732126871, Validation Loss: 0.6775198578834534\n",
      "Epoch 11: Train Loss: 0.5938929518063863, Validation Loss: 0.6743533611297607\n",
      "Epoch 12: Train Loss: 0.6368563175201416, Validation Loss: 0.6705722808837891\n",
      "Epoch 13: Train Loss: 0.5866648157437643, Validation Loss: 0.6679907441139221\n",
      "Epoch 14: Train Loss: 0.5830797553062439, Validation Loss: 0.6671863198280334\n",
      "Epoch 15: Train Loss: 0.5770996809005737, Validation Loss: 0.6677337884902954\n",
      "Epoch 16: Train Loss: 0.5838898420333862, Validation Loss: 0.6680880784988403\n",
      "Epoch 17: Train Loss: 0.5918179949124655, Validation Loss: 0.6684718132019043\n",
      "Epoch 18: Train Loss: 0.6139247417449951, Validation Loss: 0.6686322689056396\n",
      "Epoch 19: Train Loss: 0.5790937940279642, Validation Loss: 0.6686853766441345\n",
      "Epoch 20: Train Loss: 0.5449018478393555, Validation Loss: 0.6699009537696838\n",
      "Epoch 21: Train Loss: 0.5688368876775106, Validation Loss: 0.669816792011261\n",
      "Epoch 22: Train Loss: 0.5510373512903849, Validation Loss: 0.6689308881759644\n",
      "Epoch 23: Train Loss: 0.5733141104380289, Validation Loss: 0.6677848696708679\n",
      "Epoch 24: Train Loss: 0.5698820352554321, Validation Loss: 0.6680489182472229\n",
      "Epoch 25: Train Loss: 0.5540850559870402, Validation Loss: 0.6670546531677246\n",
      "Epoch 26: Train Loss: 0.5782618920008341, Validation Loss: 0.6667067408561707\n",
      "Epoch 27: Train Loss: 0.5555468797683716, Validation Loss: 0.6660598516464233\n",
      "Epoch 28: Train Loss: 0.5803907116254171, Validation Loss: 0.665902316570282\n",
      "Epoch 29: Train Loss: 0.5791048804918925, Validation Loss: 0.6656648516654968\n",
      "Epoch 30: Train Loss: 0.5495635072390238, Validation Loss: 0.6597601771354675\n",
      "Epoch 31: Train Loss: 0.5518743395805359, Validation Loss: 0.6555536985397339\n",
      "Epoch 32: Train Loss: 0.5561249454816183, Validation Loss: 0.6526183485984802\n",
      "Epoch 33: Train Loss: 0.540784219900767, Validation Loss: 0.6500101685523987\n",
      "Epoch 34: Train Loss: 0.5152740677197775, Validation Loss: 0.6500772833824158\n",
      "Epoch 35: Train Loss: 0.5262131690979004, Validation Loss: 0.6509385704994202\n",
      "Epoch 36: Train Loss: 0.5491270025571188, Validation Loss: 0.6519192457199097\n",
      "Epoch 37: Train Loss: 0.5102410912513733, Validation Loss: 0.6512985229492188\n",
      "Epoch 38: Train Loss: 0.5299681822458903, Validation Loss: 0.6508105397224426\n",
      "Epoch 39: Train Loss: 0.5173325637976328, Validation Loss: 0.6507980227470398\n",
      "Epoch 40: Train Loss: 0.5085067351659139, Validation Loss: 0.6459314227104187\n",
      "Epoch 41: Train Loss: 0.5346535245577494, Validation Loss: 0.6443079710006714\n",
      "Epoch 42: Train Loss: 0.5208091338475546, Validation Loss: 0.6431363224983215\n",
      "Epoch 43: Train Loss: 0.49097644289334613, Validation Loss: 0.6423042416572571\n",
      "Epoch 44: Train Loss: 0.488147368033727, Validation Loss: 0.6410717964172363\n",
      "Epoch 45: Train Loss: 0.49541818102200824, Validation Loss: 0.6407276391983032\n",
      "Epoch 46: Train Loss: 0.48836567004521686, Validation Loss: 0.6412811875343323\n",
      "Epoch 47: Train Loss: 0.47948144872983295, Validation Loss: 0.6413404941558838\n",
      "Epoch 48: Train Loss: 0.48968133330345154, Validation Loss: 0.6411153078079224\n",
      "Epoch 49: Train Loss: 0.47407299280166626, Validation Loss: 0.6406750082969666\n",
      "Epoch 50: Train Loss: 0.4872627953688304, Validation Loss: 0.6403607726097107\n",
      "Epoch 51: Train Loss: 0.4853893220424652, Validation Loss: 0.6384789347648621\n",
      "Epoch 52: Train Loss: 0.4816807210445404, Validation Loss: 0.6379731893539429\n",
      "Epoch 53: Train Loss: 0.4950773815313975, Validation Loss: 0.6393696069717407\n",
      "Epoch 54: Train Loss: 0.4694900115331014, Validation Loss: 0.6390557289123535\n",
      "Epoch 55: Train Loss: 0.490656981865565, Validation Loss: 0.6365633606910706\n",
      "Epoch 56: Train Loss: 0.4450539747873942, Validation Loss: 0.6342812776565552\n",
      "Epoch 57: Train Loss: 0.48002171516418457, Validation Loss: 0.6334239840507507\n",
      "Epoch 58: Train Loss: 0.46393752098083496, Validation Loss: 0.6327787041664124\n",
      "Epoch 59: Train Loss: 0.4503822426001231, Validation Loss: 0.632203221321106\n",
      "Epoch 60: Train Loss: 0.4628944198290507, Validation Loss: 0.629834771156311\n",
      "Epoch 61: Train Loss: 0.4540050228436788, Validation Loss: 0.6277395486831665\n",
      "Epoch 62: Train Loss: 0.48342392841974896, Validation Loss: 0.623528778553009\n",
      "Epoch 63: Train Loss: 0.46077196796735126, Validation Loss: 0.6203163266181946\n",
      "Epoch 64: Train Loss: 0.4528083900610606, Validation Loss: 0.6191304326057434\n",
      "Epoch 65: Train Loss: 0.44764989614486694, Validation Loss: 0.6169611811637878\n",
      "Epoch 66: Train Loss: 0.4231003522872925, Validation Loss: 0.6164581775665283\n",
      "Epoch 67: Train Loss: 0.4194641311963399, Validation Loss: 0.6161621809005737\n",
      "Epoch 68: Train Loss: 0.42353540658950806, Validation Loss: 0.6164005994796753\n",
      "Epoch 69: Train Loss: 0.41350018978118896, Validation Loss: 0.6163402795791626\n",
      "Epoch 70: Train Loss: 0.41403137644131977, Validation Loss: 0.6169496178627014\n",
      "Epoch 71: Train Loss: 0.4234306812286377, Validation Loss: 0.6134095191955566\n",
      "Epoch 72: Train Loss: 0.41545377175013226, Validation Loss: 0.6083576679229736\n",
      "Epoch 73: Train Loss: 0.41020997365315753, Validation Loss: 0.6062483787536621\n",
      "Epoch 74: Train Loss: 0.418930063645045, Validation Loss: 0.6046838164329529\n",
      "Epoch 75: Train Loss: 0.39519508679707843, Validation Loss: 0.6042503118515015\n",
      "Epoch 76: Train Loss: 0.3982087771097819, Validation Loss: 0.6043788194656372\n",
      "Epoch 77: Train Loss: 0.39760497212409973, Validation Loss: 0.6041868925094604\n",
      "Epoch 78: Train Loss: 0.4001389543215434, Validation Loss: 0.6046487092971802\n",
      "Epoch 79: Train Loss: 0.3715714017550151, Validation Loss: 0.6047579050064087\n",
      "Epoch 80: Train Loss: 0.38609790802001953, Validation Loss: 0.601539134979248\n",
      "Epoch 81: Train Loss: 0.39086611072222394, Validation Loss: 0.6000195741653442\n",
      "Epoch 82: Train Loss: 0.3848983347415924, Validation Loss: 0.5946658253669739\n",
      "Epoch 83: Train Loss: 0.38783709208170575, Validation Loss: 0.5910507440567017\n",
      "Epoch 84: Train Loss: 0.36218945185343426, Validation Loss: 0.5919812917709351\n",
      "Epoch 85: Train Loss: 0.36005117495854694, Validation Loss: 0.5926531553268433\n",
      "Epoch 86: Train Loss: 0.4131096303462982, Validation Loss: 0.5922035574913025\n",
      "Epoch 87: Train Loss: 0.35134469469388324, Validation Loss: 0.5925209522247314\n",
      "Epoch 88: Train Loss: 0.38121763865152997, Validation Loss: 0.5919262170791626\n",
      "Epoch 89: Train Loss: 0.3680979708830516, Validation Loss: 0.591668963432312\n",
      "Epoch 90: Train Loss: 0.3999893168608348, Validation Loss: 0.5873895287513733\n",
      "Epoch 91: Train Loss: 0.38586416840553284, Validation Loss: 0.5797727704048157\n",
      "Epoch 92: Train Loss: 0.3651581605275472, Validation Loss: 0.5812575221061707\n",
      "Epoch 93: Train Loss: 0.3853355050086975, Validation Loss: 0.5774346590042114\n",
      "Epoch 94: Train Loss: 0.34795578320821124, Validation Loss: 0.5727259516716003\n",
      "Epoch 95: Train Loss: 0.33997196952501935, Validation Loss: 0.5718096494674683\n",
      "Epoch 96: Train Loss: 0.33467091123263043, Validation Loss: 0.5705636143684387\n",
      "Epoch 97: Train Loss: 0.34082141518592834, Validation Loss: 0.5703297853469849\n",
      "Epoch 98: Train Loss: 0.33629263440767926, Validation Loss: 0.5706555247306824\n",
      "Epoch 99: Train Loss: 0.32549413045247394, Validation Loss: 0.5703031420707703\n",
      "Epoch 100: Train Loss: 0.35063299536705017, Validation Loss: 0.568040132522583\n",
      "Epoch 101: Train Loss: 0.36554329593976337, Validation Loss: 0.5706814527511597\n",
      "Epoch 102: Train Loss: 0.3511187632878621, Validation Loss: 0.5742220878601074\n",
      "Epoch 103: Train Loss: 0.32631709178288776, Validation Loss: 0.5772266983985901\n",
      "Epoch 104: Train Loss: 0.3461027542750041, Validation Loss: 0.5713385939598083\n",
      "Epoch 105: Train Loss: 0.3173004984855652, Validation Loss: 0.5661832690238953\n",
      "Epoch 106: Train Loss: 0.3199172616004944, Validation Loss: 0.5627233386039734\n",
      "Epoch 107: Train Loss: 0.3194273014863332, Validation Loss: 0.5604788661003113\n",
      "Epoch 108: Train Loss: 0.3427845736344655, Validation Loss: 0.5585030317306519\n",
      "Epoch 109: Train Loss: 0.3129927416642507, Validation Loss: 0.5580459833145142\n",
      "Epoch 110: Train Loss: 0.3102000057697296, Validation Loss: 0.561680018901825\n",
      "Epoch 111: Train Loss: 0.33048466841379803, Validation Loss: 0.5615915060043335\n",
      "Epoch 112: Train Loss: 0.30973222851753235, Validation Loss: 0.5581721067428589\n",
      "Epoch 113: Train Loss: 0.30156824986139935, Validation Loss: 0.5505695343017578\n",
      "Epoch 114: Train Loss: 0.3000054160753886, Validation Loss: 0.5535570383071899\n",
      "Epoch 115: Train Loss: 0.2991478244463603, Validation Loss: 0.5545352101325989\n",
      "Epoch 116: Train Loss: 0.2862266997496287, Validation Loss: 0.5553458333015442\n",
      "Epoch 117: Train Loss: 0.2937888006369273, Validation Loss: 0.5553073883056641\n",
      "Epoch 118: Train Loss: 0.27817713220914203, Validation Loss: 0.555105447769165\n",
      "Epoch 119: Train Loss: 0.2920931975046794, Validation Loss: 0.5542967319488525\n",
      "Epoch 120: Train Loss: 0.28619080781936646, Validation Loss: 0.5511975884437561\n",
      "Epoch 121: Train Loss: 0.29075570901234943, Validation Loss: 0.5453516244888306\n",
      "Epoch 122: Train Loss: 0.2957438925902049, Validation Loss: 0.5458915829658508\n",
      "Epoch 123: Train Loss: 0.28546929359436035, Validation Loss: 0.5411587953567505\n",
      "Epoch 124: Train Loss: 0.2846889595190684, Validation Loss: 0.5401334762573242\n",
      "Epoch 125: Train Loss: 0.2870516876379649, Validation Loss: 0.5427278876304626\n",
      "Epoch 126: Train Loss: 0.24617600937684378, Validation Loss: 0.5458015203475952\n",
      "Epoch 127: Train Loss: 0.29820765058199566, Validation Loss: 0.5475659370422363\n",
      "Epoch 128: Train Loss: 0.28842860956986743, Validation Loss: 0.5482638478279114\n",
      "Epoch 129: Train Loss: 0.28113378087679547, Validation Loss: 0.5480072498321533\n",
      "Epoch 130: Train Loss: 0.2628239244222641, Validation Loss: 0.5437669157981873\n",
      "Epoch 131: Train Loss: 0.2849751065174739, Validation Loss: 0.5385214686393738\n",
      "Epoch 132: Train Loss: 0.27104512850443524, Validation Loss: 0.5474236011505127\n",
      "Epoch 133: Train Loss: 0.2569202383359273, Validation Loss: 0.5509267449378967\n",
      "Epoch 134: Train Loss: 0.24644331634044647, Validation Loss: 0.5480839014053345\n",
      "Epoch 135: Train Loss: 0.2496753136316935, Validation Loss: 0.5472095012664795\n",
      "Epoch 136: Train Loss: 0.2609009991089503, Validation Loss: 0.543958842754364\n",
      "Epoch 137: Train Loss: 0.22590787212053934, Validation Loss: 0.542483925819397\n",
      "Epoch 138: Train Loss: 0.24500766396522522, Validation Loss: 0.5415051579475403\n",
      "Epoch 139: Train Loss: 0.24371670186519623, Validation Loss: 0.54136723279953\n",
      "Epoch 140: Train Loss: 0.22822992503643036, Validation Loss: 0.5311530232429504\n",
      "Epoch 141: Train Loss: 0.2597823292016983, Validation Loss: 0.5330122709274292\n",
      "Epoch 142: Train Loss: 0.22843452791372934, Validation Loss: 0.5379273891448975\n",
      "Epoch 143: Train Loss: 0.21973875164985657, Validation Loss: 0.5397643446922302\n",
      "Epoch 144: Train Loss: 0.23224491874376932, Validation Loss: 0.5309798121452332\n",
      "Epoch 145: Train Loss: 0.22532719870408377, Validation Loss: 0.5302247405052185\n",
      "Epoch 146: Train Loss: 0.22387709220250449, Validation Loss: 0.5302319526672363\n",
      "Epoch 147: Train Loss: 0.2277655949195226, Validation Loss: 0.532046914100647\n",
      "Epoch 148: Train Loss: 0.2029816855986913, Validation Loss: 0.5333064198493958\n",
      "Epoch 149: Train Loss: 0.22273767491181692, Validation Loss: 0.5335332751274109\n",
      "Epoch 150: Train Loss: 0.2304301063219706, Validation Loss: 0.5325547456741333\n",
      "Epoch 151: Train Loss: 0.20392623047033945, Validation Loss: 0.5353579521179199\n",
      "Epoch 152: Train Loss: 0.2352866530418396, Validation Loss: 0.5410003066062927\n",
      "Epoch 153: Train Loss: 0.22732038795948029, Validation Loss: 0.5371297001838684\n",
      "Epoch 154: Train Loss: 0.23393716911474863, Validation Loss: 0.5389083623886108\n",
      "Epoch 155: Train Loss: 0.21284617483615875, Validation Loss: 0.5407971143722534\n",
      "Epoch 156: Train Loss: 0.2101439287265142, Validation Loss: 0.5404750108718872\n",
      "Epoch 157: Train Loss: 0.23333801329135895, Validation Loss: 0.537303626537323\n",
      "Epoch 158: Train Loss: 0.2141087551911672, Validation Loss: 0.5362569689750671\n",
      "Epoch 159: Train Loss: 0.2197423279285431, Validation Loss: 0.5350532531738281\n",
      "Epoch 160: Train Loss: 0.21141206721464792, Validation Loss: 0.5377969145774841\n",
      "Epoch 161: Train Loss: 0.20982089638710022, Validation Loss: 0.5378610491752625\n",
      "Epoch 162: Train Loss: 0.21416235466798147, Validation Loss: 0.5400646924972534\n",
      "Epoch 163: Train Loss: 0.2105384568373362, Validation Loss: 0.5416840314865112\n",
      "Epoch 164: Train Loss: 0.224306990702947, Validation Loss: 0.5322284698486328\n",
      "Epoch 165: Train Loss: 0.2080472707748413, Validation Loss: 0.5325201749801636\n",
      "Epoch 166: Train Loss: 0.2016502171754837, Validation Loss: 0.5330530405044556\n",
      "Epoch 167: Train Loss: 0.1907700002193451, Validation Loss: 0.5316442251205444\n",
      "Epoch 168: Train Loss: 0.18505008270343146, Validation Loss: 0.5304408073425293\n",
      "Epoch 169: Train Loss: 0.2044826199611028, Validation Loss: 0.5300285816192627\n",
      "Epoch 170: Train Loss: 0.19948051373163858, Validation Loss: 0.5251361727714539\n",
      "Epoch 171: Train Loss: 0.18414369225502014, Validation Loss: 0.5311875939369202\n",
      "Epoch 172: Train Loss: 0.1979813277721405, Validation Loss: 0.5330208539962769\n",
      "Epoch 173: Train Loss: 0.19251050055027008, Validation Loss: 0.533632755279541\n",
      "Epoch 174: Train Loss: 0.1743532419204712, Validation Loss: 0.5403284430503845\n",
      "Epoch 175: Train Loss: 0.2220675895611445, Validation Loss: 0.5327379703521729\n",
      "Epoch 176: Train Loss: 0.16260292629400888, Validation Loss: 0.5241296291351318\n",
      "Epoch 177: Train Loss: 0.17218113938967386, Validation Loss: 0.5235697627067566\n",
      "Epoch 178: Train Loss: 0.17473909258842468, Validation Loss: 0.5221916437149048\n",
      "Epoch 179: Train Loss: 0.17780007918675741, Validation Loss: 0.5221619606018066\n",
      "Epoch 180: Train Loss: 0.18731159965197244, Validation Loss: 0.553816020488739\n",
      "Epoch 181: Train Loss: 0.24487684667110443, Validation Loss: 0.5579233169555664\n",
      "Epoch 182: Train Loss: 0.17370588580767313, Validation Loss: 0.549920380115509\n",
      "Epoch 183: Train Loss: 0.19136931002140045, Validation Loss: 0.5447587370872498\n",
      "Epoch 184: Train Loss: 0.1667009542385737, Validation Loss: 0.5431919693946838\n",
      "Epoch 185: Train Loss: 0.17030123869578043, Validation Loss: 0.5399657487869263\n",
      "Epoch 186: Train Loss: 0.16725591321786246, Validation Loss: 0.5407036542892456\n",
      "Epoch 187: Train Loss: 0.1456821709871292, Validation Loss: 0.5397405028343201\n",
      "Epoch 188: Train Loss: 0.160769651333491, Validation Loss: 0.5381144285202026\n",
      "Epoch 189: Train Loss: 0.15493866304556528, Validation Loss: 0.537982702255249\n",
      "Epoch 190: Train Loss: 0.16102638840675354, Validation Loss: 0.5339176654815674\n",
      "Epoch 191: Train Loss: 0.17383243143558502, Validation Loss: 0.5424383282661438\n",
      "Epoch 192: Train Loss: 0.1845162957906723, Validation Loss: 0.5628864169120789\n",
      "Epoch 193: Train Loss: 0.17426731685797373, Validation Loss: 0.5727741718292236\n",
      "Epoch 194: Train Loss: 0.16934527456760406, Validation Loss: 0.5675873756408691\n",
      "Epoch 195: Train Loss: 0.17014401157697043, Validation Loss: 0.5509141087532043\n",
      "Epoch 196: Train Loss: 0.14330110947291055, Validation Loss: 0.5450140237808228\n",
      "Epoch 197: Train Loss: 0.14576083421707153, Validation Loss: 0.5419262051582336\n",
      "Epoch 198: Train Loss: 0.15835983057816824, Validation Loss: 0.5420820713043213\n",
      "Epoch 199: Train Loss: 0.14479157825311026, Validation Loss: 0.543225109577179\n",
      "Epoch 200: Train Loss: 0.15195640921592712, Validation Loss: 0.5612457990646362\n",
      "Epoch 201: Train Loss: 0.14925481379032135, Validation Loss: 0.567349910736084\n",
      "Epoch 202: Train Loss: 0.14794562011957169, Validation Loss: 0.5691871047019958\n",
      "Epoch 203: Train Loss: 0.1462571049729983, Validation Loss: 0.5649822950363159\n",
      "Epoch 204: Train Loss: 0.12625529120365778, Validation Loss: 0.5640062093734741\n",
      "Epoch 205: Train Loss: 0.12726474553346634, Validation Loss: 0.5582125782966614\n",
      "Epoch 206: Train Loss: 0.1349525029460589, Validation Loss: 0.5531606674194336\n",
      "Epoch 207: Train Loss: 0.146002267797788, Validation Loss: 0.5503095984458923\n",
      "Epoch 208: Train Loss: 0.13400554656982422, Validation Loss: 0.5493279099464417\n",
      "Epoch 209: Train Loss: 0.13522679110368094, Validation Loss: 0.548988938331604\n",
      "Epoch 210: Train Loss: 0.12227840224901836, Validation Loss: 0.5528786182403564\n",
      "Epoch 211: Train Loss: 0.14485042542219162, Validation Loss: 0.54549241065979\n",
      "Epoch 212: Train Loss: 0.14161551495393118, Validation Loss: 0.5613805055618286\n",
      "Epoch 213: Train Loss: 0.14426707724730173, Validation Loss: 0.5954596996307373\n",
      "Epoch 214: Train Loss: 0.16401696701844534, Validation Loss: 0.5928261280059814\n",
      "Epoch 215: Train Loss: 0.14092924197514853, Validation Loss: 0.5808336734771729\n",
      "Epoch 216: Train Loss: 0.13182602326075235, Validation Loss: 0.571542501449585\n",
      "Epoch 217: Train Loss: 0.1437741219997406, Validation Loss: 0.5686720013618469\n",
      "Epoch 218: Train Loss: 0.14253410696983337, Validation Loss: 0.5670807361602783\n",
      "Epoch 219: Train Loss: 0.1490813990434011, Validation Loss: 0.5679486989974976\n",
      "Epoch 220: Train Loss: 0.1394273986419042, Validation Loss: 0.5581787824630737\n",
      "Epoch 221: Train Loss: 0.12889949480692545, Validation Loss: 0.5504059195518494\n",
      "Epoch 222: Train Loss: 0.13750249395767847, Validation Loss: 0.5562868714332581\n",
      "Epoch 223: Train Loss: 0.12609516580899557, Validation Loss: 0.5466768741607666\n",
      "Epoch 224: Train Loss: 0.12372827033201854, Validation Loss: 0.5472835302352905\n",
      "Epoch 225: Train Loss: 0.12481616934140523, Validation Loss: 0.5555229187011719\n",
      "Epoch 226: Train Loss: 0.11961748699347179, Validation Loss: 0.555875837802887\n",
      "Epoch 227: Train Loss: 0.11969732741514842, Validation Loss: 0.5467612743377686\n",
      "Epoch 228: Train Loss: 0.09975704550743103, Validation Loss: 0.5390092134475708\n",
      "Epoch 229: Train Loss: 0.11173685143391292, Validation Loss: 0.5372318625450134\n",
      "Epoch 230: Train Loss: 0.11973048249880473, Validation Loss: 0.4951443076133728\n",
      "Epoch 231: Train Loss: 0.11446954061587651, Validation Loss: 0.5034164786338806\n",
      "Epoch 232: Train Loss: 0.12421871225039165, Validation Loss: 0.5381839275360107\n",
      "Epoch 233: Train Loss: 0.14863735934098563, Validation Loss: 0.5754231810569763\n",
      "Epoch 234: Train Loss: 0.1267723316947619, Validation Loss: 0.5893633365631104\n",
      "Epoch 235: Train Loss: 0.1171799898147583, Validation Loss: 0.5719959735870361\n",
      "Epoch 236: Train Loss: 0.10301319261391957, Validation Loss: 0.5506023168563843\n",
      "Epoch 237: Train Loss: 0.1282591000199318, Validation Loss: 0.5385316610336304\n",
      "Epoch 238: Train Loss: 0.10038509716590245, Validation Loss: 0.5351414084434509\n",
      "Epoch 239: Train Loss: 0.10802826037009557, Validation Loss: 0.5334306955337524\n",
      "Epoch 240: Train Loss: 0.10403974850972493, Validation Loss: 0.524219810962677\n",
      "Epoch 241: Train Loss: 0.11875242739915848, Validation Loss: 0.5497143864631653\n",
      "Epoch 242: Train Loss: 0.10097561528285344, Validation Loss: 0.5684374570846558\n",
      "Epoch 243: Train Loss: 0.10482340802748998, Validation Loss: 0.5495964884757996\n",
      "Epoch 244: Train Loss: 0.0968850702047348, Validation Loss: 0.5363483428955078\n",
      "Epoch 245: Train Loss: 0.10794244706630707, Validation Loss: 0.5402302742004395\n",
      "Epoch 246: Train Loss: 0.10270562519629796, Validation Loss: 0.5474578738212585\n",
      "Epoch 247: Train Loss: 0.09630048523346584, Validation Loss: 0.5493356585502625\n",
      "Epoch 248: Train Loss: 0.10814017802476883, Validation Loss: 0.5500858426094055\n",
      "Epoch 249: Train Loss: 0.0978210096557935, Validation Loss: 0.5508748888969421\n",
      "Epoch 250: Train Loss: 0.09878255426883698, Validation Loss: 0.5657075047492981\n",
      "Epoch 251: Train Loss: 0.10738790035247803, Validation Loss: 0.5445982813835144\n",
      "Epoch 252: Train Loss: 0.11151388784249623, Validation Loss: 0.531928539276123\n",
      "Epoch 253: Train Loss: 0.08546765645345052, Validation Loss: 0.5219790935516357\n",
      "Epoch 254: Train Loss: 0.11225716769695282, Validation Loss: 0.5201317071914673\n",
      "Epoch 255: Train Loss: 0.10719880213340123, Validation Loss: 0.526715099811554\n",
      "Epoch 256: Train Loss: 0.10628464321295421, Validation Loss: 0.5269871950149536\n",
      "Epoch 257: Train Loss: 0.10256121307611465, Validation Loss: 0.5255324244499207\n",
      "Epoch 258: Train Loss: 0.08454143752654393, Validation Loss: 0.5244330167770386\n",
      "Epoch 259: Train Loss: 0.09402733792861302, Validation Loss: 0.5232847332954407\n",
      "Epoch 260: Train Loss: 0.09450974812110265, Validation Loss: 0.5195844173431396\n",
      "Epoch 261: Train Loss: 0.10039983689785004, Validation Loss: 0.5161752104759216\n",
      "Epoch 262: Train Loss: 0.09764044731855392, Validation Loss: 0.5240564942359924\n",
      "Epoch 263: Train Loss: 0.08362359926104546, Validation Loss: 0.5194188356399536\n",
      "Epoch 264: Train Loss: 0.09037366757790248, Validation Loss: 0.5176911354064941\n",
      "Epoch 265: Train Loss: 0.11406837900479634, Validation Loss: 0.535132646560669\n",
      "Epoch 266: Train Loss: 0.09074539442857106, Validation Loss: 0.5482586622238159\n",
      "Epoch 267: Train Loss: 0.09096963206926982, Validation Loss: 0.5485326647758484\n",
      "Epoch 268: Train Loss: 0.0952210674683253, Validation Loss: 0.5432042479515076\n",
      "Epoch 269: Train Loss: 0.09990810354550679, Validation Loss: 0.5415256023406982\n",
      "Epoch 270: Train Loss: 0.07906356950600942, Validation Loss: 0.5022856593132019\n",
      "Epoch 271: Train Loss: 0.08665252476930618, Validation Loss: 0.5104265213012695\n",
      "Epoch 272: Train Loss: 0.09036341557900111, Validation Loss: 0.5631088614463806\n",
      "Epoch 273: Train Loss: 0.08612185219923656, Validation Loss: 0.5752306580543518\n",
      "Epoch 274: Train Loss: 0.09569671005010605, Validation Loss: 0.5634801983833313\n",
      "Epoch 275: Train Loss: 0.11987885336081187, Validation Loss: 0.5465056300163269\n",
      "Epoch 276: Train Loss: 0.07546942432721455, Validation Loss: 0.5287243723869324\n",
      "Epoch 277: Train Loss: 0.08910725762446721, Validation Loss: 0.5235397815704346\n",
      "Epoch 278: Train Loss: 0.10147275278965633, Validation Loss: 0.5249781608581543\n",
      "Epoch 279: Train Loss: 0.07282770425081253, Validation Loss: 0.5257177948951721\n",
      "Epoch 280: Train Loss: 0.08712891489267349, Validation Loss: 0.572021484375\n",
      "Epoch 281: Train Loss: 0.11688965807358424, Validation Loss: 0.5577610731124878\n",
      "Epoch 282: Train Loss: 0.09053108592828114, Validation Loss: 0.5399947166442871\n",
      "Epoch 283: Train Loss: 0.10142260044813156, Validation Loss: 0.543587863445282\n",
      "Epoch 284: Train Loss: 0.104457621773084, Validation Loss: 0.5569349527359009\n",
      "Epoch 285: Train Loss: 0.08893787364164989, Validation Loss: 0.5416120886802673\n",
      "Epoch 286: Train Loss: 0.08289021750291188, Validation Loss: 0.53343266248703\n",
      "Epoch 287: Train Loss: 0.09834410746892293, Validation Loss: 0.5360801815986633\n",
      "Epoch 288: Train Loss: 0.08287915339072545, Validation Loss: 0.5391413569450378\n",
      "Epoch 289: Train Loss: 0.0832649643222491, Validation Loss: 0.5395953059196472\n",
      "Epoch 290: Train Loss: 0.06742228443423907, Validation Loss: 0.5771992802619934\n",
      "Epoch 291: Train Loss: 0.09965053697427113, Validation Loss: 0.5867293477058411\n",
      "Epoch 292: Train Loss: 0.11177461097637813, Validation Loss: 0.6608448624610901\n",
      "Epoch 293: Train Loss: 0.10118622581164043, Validation Loss: 0.653545618057251\n",
      "Epoch 294: Train Loss: 0.08594395716985066, Validation Loss: 0.6227973103523254\n",
      "Epoch 295: Train Loss: 0.08347948143879573, Validation Loss: 0.6150364875793457\n",
      "Epoch 296: Train Loss: 0.1026666909456253, Validation Loss: 0.6136531233787537\n",
      "Epoch 297: Train Loss: 0.08685171852509181, Validation Loss: 0.608888566493988\n",
      "Epoch 298: Train Loss: 0.08458040654659271, Validation Loss: 0.6067817807197571\n",
      "Epoch 299: Train Loss: 0.0864768996834755, Validation Loss: 0.6070865988731384\n",
      "Epoch 300: Train Loss: 0.0929443488518397, Validation Loss: 0.608540415763855\n",
      "Epoch 301: Train Loss: 0.09684035181999207, Validation Loss: 0.5763070583343506\n",
      "Epoch 302: Train Loss: 0.12834726522366205, Validation Loss: 0.5895861387252808\n",
      "Epoch 303: Train Loss: 0.14027115205923715, Validation Loss: 0.5853797793388367\n",
      "Epoch 304: Train Loss: 0.08738056073586146, Validation Loss: 0.6266655325889587\n",
      "Epoch 305: Train Loss: 0.10439745088418324, Validation Loss: 0.6748274564743042\n",
      "Epoch 306: Train Loss: 0.09351352353890736, Validation Loss: 0.6649101972579956\n",
      "Epoch 307: Train Loss: 0.09195496141910553, Validation Loss: 0.648189127445221\n",
      "Epoch 308: Train Loss: 0.09150834133227666, Validation Loss: 0.63332599401474\n",
      "Epoch 309: Train Loss: 0.09482018897930782, Validation Loss: 0.628265380859375\n",
      "Epoch 310: Train Loss: 0.11067226032416026, Validation Loss: 0.6275673508644104\n",
      "Epoch 311: Train Loss: 0.11428052683671315, Validation Loss: 0.6705439686775208\n",
      "Epoch 312: Train Loss: 0.08188461760679881, Validation Loss: 0.6374363899230957\n",
      "Epoch 313: Train Loss: 0.08168840159972508, Validation Loss: 0.6074152588844299\n",
      "Epoch 314: Train Loss: 0.07279376933972041, Validation Loss: 0.5791144967079163\n",
      "Epoch 315: Train Loss: 0.06938465560475986, Validation Loss: 0.5704012513160706\n",
      "Epoch 316: Train Loss: 0.07246602823336919, Validation Loss: 0.5691897869110107\n",
      "Epoch 317: Train Loss: 0.07827295114596684, Validation Loss: 0.5701345801353455\n",
      "Epoch 318: Train Loss: 0.06467011570930481, Validation Loss: 0.5733610987663269\n",
      "Epoch 319: Train Loss: 0.07483187566200893, Validation Loss: 0.5759997963905334\n",
      "Epoch 320: Train Loss: 0.07853378728032112, Validation Loss: 0.5890401005744934\n",
      "Epoch 321: Train Loss: 0.07744145393371582, Validation Loss: 0.5879173874855042\n",
      "Epoch 322: Train Loss: 0.0557435043156147, Validation Loss: 0.5735751986503601\n",
      "Epoch 323: Train Loss: 0.07635381569465001, Validation Loss: 0.5799161195755005\n",
      "Epoch 324: Train Loss: 0.06265147899587949, Validation Loss: 0.5833384394645691\n",
      "Epoch 325: Train Loss: 0.05766158799330393, Validation Loss: 0.5814061760902405\n",
      "Epoch 326: Train Loss: 0.05560712516307831, Validation Loss: 0.5827598571777344\n",
      "Epoch 327: Train Loss: 0.07125790044665337, Validation Loss: 0.5810819268226624\n",
      "Epoch 328: Train Loss: 0.052759418884913124, Validation Loss: 0.5801517963409424\n",
      "Epoch 329: Train Loss: 0.07592999438444774, Validation Loss: 0.5792752504348755\n",
      "Early stopping at epoch 330\n",
      "Fold 4\n",
      "Epoch 0: Train Loss: 0.703055759270986, Validation Loss: 0.6883521676063538\n",
      "Epoch 1: Train Loss: 0.626213550567627, Validation Loss: 0.6889902353286743\n",
      "Epoch 2: Train Loss: 0.6173809170722961, Validation Loss: 0.6892971396446228\n",
      "Epoch 3: Train Loss: 0.621810257434845, Validation Loss: 0.6884929537773132\n",
      "Epoch 4: Train Loss: 0.6477305690447489, Validation Loss: 0.6870521306991577\n",
      "Epoch 5: Train Loss: 0.5993565122286478, Validation Loss: 0.6854149103164673\n",
      "Epoch 6: Train Loss: 0.6326760450998942, Validation Loss: 0.6833220720291138\n",
      "Epoch 7: Train Loss: 0.6248630086580912, Validation Loss: 0.6810572743415833\n",
      "Epoch 8: Train Loss: 0.6358600854873657, Validation Loss: 0.6788825392723083\n",
      "Epoch 9: Train Loss: 0.5878318945566813, Validation Loss: 0.6771650314331055\n",
      "Epoch 10: Train Loss: 0.6185538570086161, Validation Loss: 0.6747206449508667\n",
      "Epoch 11: Train Loss: 0.5922544797261556, Validation Loss: 0.6715207695960999\n",
      "Epoch 12: Train Loss: 0.5513554215431213, Validation Loss: 0.667191743850708\n",
      "Epoch 13: Train Loss: 0.6068354646364847, Validation Loss: 0.6628483533859253\n",
      "Epoch 14: Train Loss: 0.5950355529785156, Validation Loss: 0.6593775153160095\n",
      "Epoch 15: Train Loss: 0.5547966559727987, Validation Loss: 0.656175434589386\n",
      "Epoch 16: Train Loss: 0.5594733456770579, Validation Loss: 0.6540243029594421\n",
      "Epoch 17: Train Loss: 0.5340262552102407, Validation Loss: 0.652797281742096\n",
      "Epoch 18: Train Loss: 0.5442248781522115, Validation Loss: 0.6514186859130859\n",
      "Epoch 19: Train Loss: 0.5836451649665833, Validation Loss: 0.6512079834938049\n",
      "Epoch 20: Train Loss: 0.55333012342453, Validation Loss: 0.6501485705375671\n",
      "Epoch 21: Train Loss: 0.5599105954170227, Validation Loss: 0.6462136507034302\n",
      "Epoch 22: Train Loss: 0.5181807974974314, Validation Loss: 0.6389831900596619\n",
      "Epoch 23: Train Loss: 0.548217386007309, Validation Loss: 0.6357950568199158\n",
      "Epoch 24: Train Loss: 0.5245532592137655, Validation Loss: 0.6333560347557068\n",
      "Epoch 25: Train Loss: 0.5194100141525269, Validation Loss: 0.6327022910118103\n",
      "Epoch 26: Train Loss: 0.5243900914986929, Validation Loss: 0.633358895778656\n",
      "Epoch 27: Train Loss: 0.5463352203369141, Validation Loss: 0.6334680914878845\n",
      "Epoch 28: Train Loss: 0.5072216888268789, Validation Loss: 0.6341655254364014\n",
      "Epoch 29: Train Loss: 0.5585019389788309, Validation Loss: 0.6335919499397278\n",
      "Epoch 30: Train Loss: 0.5052178502082825, Validation Loss: 0.6332438588142395\n",
      "Epoch 31: Train Loss: 0.48173291484514874, Validation Loss: 0.6295761466026306\n",
      "Epoch 32: Train Loss: 0.49490270018577576, Validation Loss: 0.6254412531852722\n",
      "Epoch 33: Train Loss: 0.4876575767993927, Validation Loss: 0.6232184171676636\n",
      "Epoch 34: Train Loss: 0.48486219843228656, Validation Loss: 0.6233198046684265\n",
      "Epoch 35: Train Loss: 0.49552345275878906, Validation Loss: 0.6225639581680298\n",
      "Epoch 36: Train Loss: 0.465633084376653, Validation Loss: 0.621391773223877\n",
      "Epoch 37: Train Loss: 0.45665813485781354, Validation Loss: 0.6207863092422485\n",
      "Epoch 38: Train Loss: 0.4655233522256215, Validation Loss: 0.620464563369751\n",
      "Epoch 39: Train Loss: 0.4437088668346405, Validation Loss: 0.6202973127365112\n",
      "Epoch 40: Train Loss: 0.45926084121068317, Validation Loss: 0.6168141961097717\n",
      "Epoch 41: Train Loss: 0.46416837970415753, Validation Loss: 0.6145355105400085\n",
      "Epoch 42: Train Loss: 0.4278238316377004, Validation Loss: 0.6114701628684998\n",
      "Epoch 43: Train Loss: 0.4522486428419749, Validation Loss: 0.6108273267745972\n",
      "Epoch 44: Train Loss: 0.41959888736406964, Validation Loss: 0.6102660298347473\n",
      "Epoch 45: Train Loss: 0.4461087981859843, Validation Loss: 0.6121782064437866\n",
      "Epoch 46: Train Loss: 0.39585503935813904, Validation Loss: 0.613554060459137\n",
      "Epoch 47: Train Loss: 0.41929545998573303, Validation Loss: 0.6141234636306763\n",
      "Epoch 48: Train Loss: 0.434736708799998, Validation Loss: 0.613743245601654\n",
      "Epoch 49: Train Loss: 0.41877485315004986, Validation Loss: 0.6130895018577576\n",
      "Epoch 50: Train Loss: 0.4112919867038727, Validation Loss: 0.601047694683075\n",
      "Epoch 51: Train Loss: 0.411547730366389, Validation Loss: 0.594581663608551\n",
      "Epoch 52: Train Loss: 0.41059647997220355, Validation Loss: 0.5923131108283997\n",
      "Epoch 53: Train Loss: 0.391244242588679, Validation Loss: 0.5915506482124329\n",
      "Epoch 54: Train Loss: 0.4026513894399007, Validation Loss: 0.5952067971229553\n",
      "Epoch 55: Train Loss: 0.38295893867810565, Validation Loss: 0.5956754684448242\n",
      "Epoch 56: Train Loss: 0.3931890030701955, Validation Loss: 0.5956570506095886\n",
      "Epoch 57: Train Loss: 0.3682352304458618, Validation Loss: 0.5936963558197021\n",
      "Epoch 58: Train Loss: 0.36035825808842975, Validation Loss: 0.5911188721656799\n",
      "Epoch 59: Train Loss: 0.3831108311812083, Validation Loss: 0.5912808775901794\n",
      "Epoch 60: Train Loss: 0.3700026273727417, Validation Loss: 0.575268030166626\n",
      "Epoch 61: Train Loss: 0.37548142671585083, Validation Loss: 0.5688341856002808\n",
      "Epoch 62: Train Loss: 0.3726113239924113, Validation Loss: 0.5551886558532715\n",
      "Epoch 63: Train Loss: 0.37892253200213116, Validation Loss: 0.548457145690918\n",
      "Epoch 64: Train Loss: 0.3566501239935557, Validation Loss: 0.5483753085136414\n",
      "Epoch 65: Train Loss: 0.36730356017748517, Validation Loss: 0.5552889704704285\n",
      "Epoch 66: Train Loss: 0.35039695103963214, Validation Loss: 0.5560581088066101\n",
      "Epoch 67: Train Loss: 0.34934304157892865, Validation Loss: 0.5546658635139465\n",
      "Epoch 68: Train Loss: 0.342841496070226, Validation Loss: 0.5553461313247681\n",
      "Epoch 69: Train Loss: 0.36392544706662494, Validation Loss: 0.5556594729423523\n",
      "Epoch 70: Train Loss: 0.3474361101786296, Validation Loss: 0.55352783203125\n",
      "Epoch 71: Train Loss: 0.3347115417321523, Validation Loss: 0.5554770827293396\n",
      "Epoch 72: Train Loss: 0.3544653256734212, Validation Loss: 0.5577074289321899\n",
      "Epoch 73: Train Loss: 0.3174588978290558, Validation Loss: 0.5590237379074097\n",
      "Epoch 74: Train Loss: 0.33658312757809955, Validation Loss: 0.5624846816062927\n",
      "Epoch 75: Train Loss: 0.3371177911758423, Validation Loss: 0.5679275393486023\n",
      "Epoch 76: Train Loss: 0.3198279043038686, Validation Loss: 0.5715147256851196\n",
      "Epoch 77: Train Loss: 0.3271409074465434, Validation Loss: 0.5714902877807617\n",
      "Epoch 78: Train Loss: 0.3074149986108144, Validation Loss: 0.5684972405433655\n",
      "Epoch 79: Train Loss: 0.32603926459948224, Validation Loss: 0.5674354434013367\n",
      "Epoch 80: Train Loss: 0.28584611415863037, Validation Loss: 0.5528817772865295\n",
      "Epoch 81: Train Loss: 0.3394785026709239, Validation Loss: 0.5533141493797302\n",
      "Epoch 82: Train Loss: 0.3304065664609273, Validation Loss: 0.536193311214447\n",
      "Epoch 83: Train Loss: 0.3237033188343048, Validation Loss: 0.5061379075050354\n",
      "Epoch 84: Train Loss: 0.3250175217787425, Validation Loss: 0.5049280524253845\n",
      "Epoch 85: Train Loss: 0.3003344237804413, Validation Loss: 0.5213986039161682\n",
      "Epoch 86: Train Loss: 0.3026999781529109, Validation Loss: 0.5353068709373474\n",
      "Epoch 87: Train Loss: 0.2734048267205556, Validation Loss: 0.5392580628395081\n",
      "Epoch 88: Train Loss: 0.2798940936724345, Validation Loss: 0.5397660136222839\n",
      "Epoch 89: Train Loss: 0.28074302275975543, Validation Loss: 0.5370916724205017\n",
      "Epoch 90: Train Loss: 0.29338498910268146, Validation Loss: 0.5080479383468628\n",
      "Epoch 91: Train Loss: 0.310134619474411, Validation Loss: 0.5145349502563477\n",
      "Epoch 92: Train Loss: 0.2879256308078766, Validation Loss: 0.5221483111381531\n",
      "Epoch 93: Train Loss: 0.30076803763707477, Validation Loss: 0.5255053043365479\n",
      "Epoch 94: Train Loss: 0.2636040101448695, Validation Loss: 0.5189012289047241\n",
      "Epoch 95: Train Loss: 0.276615450779597, Validation Loss: 0.5175822377204895\n",
      "Epoch 96: Train Loss: 0.2837338348229726, Validation Loss: 0.5136598944664001\n",
      "Epoch 97: Train Loss: 0.27040991683801013, Validation Loss: 0.5143833756446838\n",
      "Epoch 98: Train Loss: 0.24297533929347992, Validation Loss: 0.5161482095718384\n",
      "Epoch 99: Train Loss: 0.276127889752388, Validation Loss: 0.5175081491470337\n",
      "Epoch 100: Train Loss: 0.26877983411153156, Validation Loss: 0.5364159941673279\n",
      "Epoch 101: Train Loss: 0.2910270094871521, Validation Loss: 0.5279603004455566\n",
      "Epoch 102: Train Loss: 0.2661982427040736, Validation Loss: 0.4926730990409851\n",
      "Epoch 103: Train Loss: 0.2549733718236287, Validation Loss: 0.4834861159324646\n",
      "Epoch 104: Train Loss: 0.2574026087919871, Validation Loss: 0.4886751174926758\n",
      "Epoch 105: Train Loss: 0.26806584000587463, Validation Loss: 0.49267613887786865\n",
      "Epoch 106: Train Loss: 0.24649702509244284, Validation Loss: 0.49275773763656616\n",
      "Epoch 107: Train Loss: 0.24435646831989288, Validation Loss: 0.489656925201416\n",
      "Epoch 108: Train Loss: 0.2579238911469777, Validation Loss: 0.49029186367988586\n",
      "Epoch 109: Train Loss: 0.2386846641699473, Validation Loss: 0.49109843373298645\n",
      "Epoch 110: Train Loss: 0.23423880338668823, Validation Loss: 0.5172730684280396\n",
      "Epoch 111: Train Loss: 0.2451555828253428, Validation Loss: 0.5250150561332703\n",
      "Epoch 112: Train Loss: 0.24037156502405801, Validation Loss: 0.49817410111427307\n",
      "Epoch 113: Train Loss: 0.22528777519861856, Validation Loss: 0.4886937737464905\n",
      "Epoch 114: Train Loss: 0.23338683446248373, Validation Loss: 0.48595935106277466\n",
      "Epoch 115: Train Loss: 0.21760953466097513, Validation Loss: 0.4907185137271881\n",
      "Epoch 116: Train Loss: 0.23800904055436453, Validation Loss: 0.49907761812210083\n",
      "Epoch 117: Train Loss: 0.2346195230881373, Validation Loss: 0.4959542453289032\n",
      "Epoch 118: Train Loss: 0.2217800368865331, Validation Loss: 0.49362605810165405\n",
      "Epoch 119: Train Loss: 0.2134236047665278, Validation Loss: 0.4918156564235687\n",
      "Epoch 120: Train Loss: 0.22532309095064798, Validation Loss: 0.47735971212387085\n",
      "Epoch 121: Train Loss: 0.2415356586376826, Validation Loss: 0.5036344528198242\n",
      "Epoch 122: Train Loss: 0.20794331034024557, Validation Loss: 0.5375794172286987\n",
      "Epoch 123: Train Loss: 0.2626403371493022, Validation Loss: 0.45586636662483215\n",
      "Epoch 124: Train Loss: 0.21366951366265616, Validation Loss: 0.4452655613422394\n",
      "Epoch 125: Train Loss: 0.22155889372030893, Validation Loss: 0.4638579189777374\n",
      "Epoch 126: Train Loss: 0.21566449602444968, Validation Loss: 0.48236557841300964\n",
      "Epoch 127: Train Loss: 0.2133062481880188, Validation Loss: 0.4907481372356415\n",
      "Epoch 128: Train Loss: 0.21240211526552835, Validation Loss: 0.4934072494506836\n",
      "Epoch 129: Train Loss: 0.1943072279294332, Validation Loss: 0.4933771789073944\n",
      "Epoch 130: Train Loss: 0.19074949622154236, Validation Loss: 0.526733934879303\n",
      "Epoch 131: Train Loss: 0.20030681292215982, Validation Loss: 0.5237759947776794\n",
      "Epoch 132: Train Loss: 0.20575093726317087, Validation Loss: 0.4922257363796234\n",
      "Epoch 133: Train Loss: 0.18216008941332498, Validation Loss: 0.4727221727371216\n",
      "Epoch 134: Train Loss: 0.2017351041237513, Validation Loss: 0.4493616819381714\n",
      "Epoch 135: Train Loss: 0.18817691008249918, Validation Loss: 0.4476863443851471\n",
      "Epoch 136: Train Loss: 0.19773479799429575, Validation Loss: 0.47323688864707947\n",
      "Epoch 137: Train Loss: 0.2025818129380544, Validation Loss: 0.4823524057865143\n",
      "Epoch 138: Train Loss: 0.19458470245202383, Validation Loss: 0.48689472675323486\n",
      "Epoch 139: Train Loss: 0.19454563160737356, Validation Loss: 0.4855790436267853\n",
      "Epoch 140: Train Loss: 0.21843715012073517, Validation Loss: 0.443724662065506\n",
      "Epoch 141: Train Loss: 0.2026122361421585, Validation Loss: 0.4688919186592102\n",
      "Epoch 142: Train Loss: 0.20170767605304718, Validation Loss: 0.49220794439315796\n",
      "Epoch 143: Train Loss: 0.20327238738536835, Validation Loss: 0.4845616817474365\n",
      "Epoch 144: Train Loss: 0.19558359682559967, Validation Loss: 0.45764902234077454\n",
      "Epoch 145: Train Loss: 0.20642091830571493, Validation Loss: 0.4613349437713623\n",
      "Epoch 146: Train Loss: 0.19966056446234384, Validation Loss: 0.4939783811569214\n",
      "Epoch 147: Train Loss: 0.18523586293061575, Validation Loss: 0.49424511194229126\n",
      "Epoch 148: Train Loss: 0.17549249033133188, Validation Loss: 0.49287310242652893\n",
      "Epoch 149: Train Loss: 0.1978945384422938, Validation Loss: 0.49270957708358765\n",
      "Epoch 150: Train Loss: 0.16462376713752747, Validation Loss: 0.5070849061012268\n",
      "Epoch 151: Train Loss: 0.1936876724163691, Validation Loss: 0.45877283811569214\n",
      "Epoch 152: Train Loss: 0.18290377656618753, Validation Loss: 0.48366183042526245\n",
      "Epoch 153: Train Loss: 0.23546778162320456, Validation Loss: 0.46316754817962646\n",
      "Epoch 154: Train Loss: 0.1721647083759308, Validation Loss: 0.44025206565856934\n",
      "Epoch 155: Train Loss: 0.1728175232807795, Validation Loss: 0.4517131447792053\n",
      "Epoch 156: Train Loss: 0.2014873077472051, Validation Loss: 0.46784359216690063\n",
      "Epoch 157: Train Loss: 0.1709925432999929, Validation Loss: 0.4879160523414612\n",
      "Epoch 158: Train Loss: 0.19755085309346518, Validation Loss: 0.49768364429473877\n",
      "Epoch 159: Train Loss: 0.16796923677126566, Validation Loss: 0.49670693278312683\n",
      "Epoch 160: Train Loss: 0.19686060150464377, Validation Loss: 0.4018927216529846\n",
      "Epoch 161: Train Loss: 0.20470091700553894, Validation Loss: 0.40041837096214294\n",
      "Epoch 162: Train Loss: 0.18248272438844046, Validation Loss: 0.4416360855102539\n",
      "Epoch 163: Train Loss: 0.1932969739039739, Validation Loss: 0.4998021125793457\n",
      "Epoch 164: Train Loss: 0.1751569708188375, Validation Loss: 0.5175161957740784\n",
      "Epoch 165: Train Loss: 0.17031045258045197, Validation Loss: 0.501473069190979\n",
      "Epoch 166: Train Loss: 0.1726285070180893, Validation Loss: 0.4884893000125885\n",
      "Epoch 167: Train Loss: 0.16534653306007385, Validation Loss: 0.48575514554977417\n",
      "Epoch 168: Train Loss: 0.1690902834137281, Validation Loss: 0.47872671484947205\n",
      "Epoch 169: Train Loss: 0.16067166129748026, Validation Loss: 0.4758864939212799\n",
      "Epoch 170: Train Loss: 0.18723812202612558, Validation Loss: 0.4840664863586426\n",
      "Epoch 171: Train Loss: 0.1445004716515541, Validation Loss: 0.4636691212654114\n",
      "Epoch 172: Train Loss: 0.17019104460875192, Validation Loss: 0.43628251552581787\n",
      "Epoch 173: Train Loss: 0.1745202292998632, Validation Loss: 0.4382520914077759\n",
      "Epoch 174: Train Loss: 0.1617338458697001, Validation Loss: 0.4667673707008362\n",
      "Epoch 175: Train Loss: 0.1454444924990336, Validation Loss: 0.49837327003479004\n",
      "Epoch 176: Train Loss: 0.16579749683539072, Validation Loss: 0.4706512987613678\n",
      "Epoch 177: Train Loss: 0.13752355923255286, Validation Loss: 0.4725937247276306\n",
      "Epoch 178: Train Loss: 0.14224723974863687, Validation Loss: 0.47508704662323\n",
      "Epoch 179: Train Loss: 0.1537312219540278, Validation Loss: 0.47514697909355164\n",
      "Epoch 180: Train Loss: 0.17667310684919357, Validation Loss: 0.5127477645874023\n",
      "Epoch 181: Train Loss: 0.1382183333237966, Validation Loss: 0.45225951075553894\n",
      "Epoch 182: Train Loss: 0.1542795697848002, Validation Loss: 0.4268965423107147\n",
      "Epoch 183: Train Loss: 0.14756265779336294, Validation Loss: 0.4366007447242737\n",
      "Epoch 184: Train Loss: 0.14855578045050302, Validation Loss: 0.48587867617607117\n",
      "Epoch 185: Train Loss: 0.13476250072320303, Validation Loss: 0.5270872116088867\n",
      "Epoch 186: Train Loss: 0.12485229223966599, Validation Loss: 0.5246871709823608\n",
      "Epoch 187: Train Loss: 0.13333814839522043, Validation Loss: 0.5064201951026917\n",
      "Epoch 188: Train Loss: 0.1194373145699501, Validation Loss: 0.4997636377811432\n",
      "Epoch 189: Train Loss: 0.1302129328250885, Validation Loss: 0.4987903833389282\n",
      "Epoch 190: Train Loss: 0.1271405021349589, Validation Loss: 0.4738788306713104\n",
      "Epoch 191: Train Loss: 0.1295721357067426, Validation Loss: 0.4851914346218109\n",
      "Epoch 192: Train Loss: 0.13173292577266693, Validation Loss: 0.4651370048522949\n",
      "Epoch 193: Train Loss: 0.16002394258975983, Validation Loss: 0.448073148727417\n",
      "Epoch 194: Train Loss: 0.15472975621620813, Validation Loss: 0.4154825508594513\n",
      "Epoch 195: Train Loss: 0.13243602712949118, Validation Loss: 0.40047159790992737\n",
      "Epoch 196: Train Loss: 0.12559832632541656, Validation Loss: 0.4210670292377472\n",
      "Epoch 197: Train Loss: 0.13354657342036566, Validation Loss: 0.44247981905937195\n",
      "Epoch 198: Train Loss: 0.13321765512228012, Validation Loss: 0.4606664180755615\n",
      "Epoch 199: Train Loss: 0.11808788776397705, Validation Loss: 0.4618426263332367\n",
      "Epoch 200: Train Loss: 0.12230035662651062, Validation Loss: 0.557312548160553\n",
      "Epoch 201: Train Loss: 0.11221393446127574, Validation Loss: 0.43496689200401306\n",
      "Epoch 202: Train Loss: 0.1268898049990336, Validation Loss: 0.4097864031791687\n",
      "Epoch 203: Train Loss: 0.1329709862669309, Validation Loss: 0.424748957157135\n",
      "Epoch 204: Train Loss: 0.15615465740362802, Validation Loss: 0.4088476896286011\n",
      "Epoch 205: Train Loss: 0.12799105544885, Validation Loss: 0.392997682094574\n",
      "Epoch 206: Train Loss: 0.1307049592336019, Validation Loss: 0.40006664395332336\n",
      "Epoch 207: Train Loss: 0.14240959535042444, Validation Loss: 0.41125595569610596\n",
      "Epoch 208: Train Loss: 0.14189023772875467, Validation Loss: 0.4169495105743408\n",
      "Epoch 209: Train Loss: 0.12291244665781657, Validation Loss: 0.42136350274086\n",
      "Epoch 210: Train Loss: 0.12835579613844553, Validation Loss: 0.4874437153339386\n",
      "Epoch 211: Train Loss: 0.1357854257027308, Validation Loss: 0.47797060012817383\n",
      "Epoch 212: Train Loss: 0.11415341248114903, Validation Loss: 0.449022501707077\n",
      "Epoch 213: Train Loss: 0.1226423978805542, Validation Loss: 0.4712432026863098\n",
      "Epoch 214: Train Loss: 0.1210829143722852, Validation Loss: 0.4769197106361389\n",
      "Epoch 215: Train Loss: 0.12874621401230493, Validation Loss: 0.4606035351753235\n",
      "Epoch 216: Train Loss: 0.14573805530865988, Validation Loss: 0.42990583181381226\n",
      "Epoch 217: Train Loss: 0.11633257319529851, Validation Loss: 0.43445733189582825\n",
      "Epoch 218: Train Loss: 0.10309771200021108, Validation Loss: 0.43571144342422485\n",
      "Epoch 219: Train Loss: 0.1030214528242747, Validation Loss: 0.43848952651023865\n",
      "Epoch 220: Train Loss: 0.1135642280181249, Validation Loss: 0.4418189525604248\n",
      "Epoch 221: Train Loss: 0.12093609323104222, Validation Loss: 0.4740874469280243\n",
      "Epoch 222: Train Loss: 0.10732726256052653, Validation Loss: 0.5078818202018738\n",
      "Epoch 223: Train Loss: 0.12928133209546408, Validation Loss: 0.47997403144836426\n",
      "Epoch 224: Train Loss: 0.12638569623231888, Validation Loss: 0.4492787718772888\n",
      "Epoch 225: Train Loss: 0.11919918407996495, Validation Loss: 0.4546048939228058\n",
      "Epoch 226: Train Loss: 0.10180976986885071, Validation Loss: 0.46795421838760376\n",
      "Epoch 227: Train Loss: 0.11965131759643555, Validation Loss: 0.47320908308029175\n",
      "Epoch 228: Train Loss: 0.12094064305226009, Validation Loss: 0.4610145092010498\n",
      "Epoch 229: Train Loss: 0.10027291874090831, Validation Loss: 0.4570627212524414\n",
      "Epoch 230: Train Loss: 0.09214820216099422, Validation Loss: 0.45457038283348083\n",
      "Epoch 231: Train Loss: 0.12242792298396428, Validation Loss: 0.5347946882247925\n",
      "Epoch 232: Train Loss: 0.11618652194738388, Validation Loss: 0.541718065738678\n",
      "Epoch 233: Train Loss: 0.11117528627316157, Validation Loss: 0.39230841398239136\n",
      "Epoch 234: Train Loss: 0.10263519485791524, Validation Loss: 0.3598470687866211\n",
      "Epoch 235: Train Loss: 0.11247271796067555, Validation Loss: 0.3938824236392975\n",
      "Epoch 236: Train Loss: 0.10453883806864421, Validation Loss: 0.42971059679985046\n",
      "Epoch 237: Train Loss: 0.09746391077836354, Validation Loss: 0.4597925841808319\n",
      "Epoch 238: Train Loss: 0.11160716166098912, Validation Loss: 0.4638201594352722\n",
      "Epoch 239: Train Loss: 0.09460181494553883, Validation Loss: 0.4591042995452881\n",
      "Epoch 240: Train Loss: 0.11149479448795319, Validation Loss: 0.39378032088279724\n",
      "Epoch 241: Train Loss: 0.10207261145114899, Validation Loss: 0.4092206358909607\n",
      "Epoch 242: Train Loss: 0.11673722167809804, Validation Loss: 0.4201618731021881\n",
      "Epoch 243: Train Loss: 0.09504569073518117, Validation Loss: 0.4715658128261566\n",
      "Epoch 244: Train Loss: 0.08411935965220134, Validation Loss: 0.5169031023979187\n",
      "Epoch 245: Train Loss: 0.10687461743752162, Validation Loss: 0.547544002532959\n",
      "Epoch 246: Train Loss: 0.09541554749011993, Validation Loss: 0.5077470541000366\n",
      "Epoch 247: Train Loss: 0.09692493081092834, Validation Loss: 0.470348984003067\n",
      "Epoch 248: Train Loss: 0.08333496004343033, Validation Loss: 0.45722612738609314\n",
      "Epoch 249: Train Loss: 0.08416102329889934, Validation Loss: 0.45514538884162903\n",
      "Epoch 250: Train Loss: 0.10404964536428452, Validation Loss: 0.46617022156715393\n",
      "Epoch 251: Train Loss: 0.11291531721750896, Validation Loss: 0.45195430517196655\n",
      "Epoch 252: Train Loss: 0.08804600934187572, Validation Loss: 0.4485934376716614\n",
      "Epoch 253: Train Loss: 0.10699914892514546, Validation Loss: 0.43939098715782166\n",
      "Epoch 254: Train Loss: 0.1020855779449145, Validation Loss: 0.44141677021980286\n",
      "Epoch 255: Train Loss: 0.11281845221916835, Validation Loss: 0.4414914548397064\n",
      "Epoch 256: Train Loss: 0.11126948148012161, Validation Loss: 0.4348814785480499\n",
      "Epoch 257: Train Loss: 0.08962690581878026, Validation Loss: 0.44389447569847107\n",
      "Epoch 258: Train Loss: 0.1012172003587087, Validation Loss: 0.4485907256603241\n",
      "Epoch 259: Train Loss: 0.08970580001672109, Validation Loss: 0.45095211267471313\n",
      "Epoch 260: Train Loss: 0.07556850711504619, Validation Loss: 0.4854489266872406\n",
      "Epoch 261: Train Loss: 0.09680431336164474, Validation Loss: 0.4874100387096405\n",
      "Epoch 262: Train Loss: 0.08185413231452306, Validation Loss: 0.5274649262428284\n",
      "Epoch 263: Train Loss: 0.08000044027964275, Validation Loss: 0.5144426226615906\n",
      "Epoch 264: Train Loss: 0.10802117735147476, Validation Loss: 0.5113704800605774\n",
      "Epoch 265: Train Loss: 0.08597156405448914, Validation Loss: 0.484747976064682\n",
      "Epoch 266: Train Loss: 0.08681654681762059, Validation Loss: 0.4761379063129425\n",
      "Epoch 267: Train Loss: 0.08377168079217275, Validation Loss: 0.4663178026676178\n",
      "Epoch 268: Train Loss: 0.07941014071305592, Validation Loss: 0.4591634273529053\n",
      "Epoch 269: Train Loss: 0.08671671400467555, Validation Loss: 0.4575265944004059\n",
      "Epoch 270: Train Loss: 0.08753776177763939, Validation Loss: 0.5036442279815674\n",
      "Epoch 271: Train Loss: 0.0804970512787501, Validation Loss: 0.5140783786773682\n",
      "Epoch 272: Train Loss: 0.08196739852428436, Validation Loss: 0.564363420009613\n",
      "Epoch 273: Train Loss: 0.08534109344085057, Validation Loss: 0.5543515086174011\n",
      "Epoch 274: Train Loss: 0.07075419028600057, Validation Loss: 0.47505563497543335\n",
      "Epoch 275: Train Loss: 0.06692450866103172, Validation Loss: 0.4358968138694763\n",
      "Epoch 276: Train Loss: 0.12017787247896194, Validation Loss: 0.4215644896030426\n",
      "Epoch 277: Train Loss: 0.08081974585851033, Validation Loss: 0.44576916098594666\n",
      "Epoch 278: Train Loss: 0.09481098502874374, Validation Loss: 0.46567922830581665\n",
      "Epoch 279: Train Loss: 0.05977808063228925, Validation Loss: 0.4746098220348358\n",
      "Epoch 280: Train Loss: 0.09634686509768169, Validation Loss: 0.6372702717781067\n",
      "Epoch 281: Train Loss: 0.10042486091454823, Validation Loss: 0.42889758944511414\n",
      "Epoch 282: Train Loss: 0.08707285175720851, Validation Loss: 0.36614900827407837\n",
      "Epoch 283: Train Loss: 0.08632834255695343, Validation Loss: 0.41458016633987427\n",
      "Epoch 284: Train Loss: 0.09352333843708038, Validation Loss: 0.4626714587211609\n",
      "Epoch 285: Train Loss: 0.079014057914416, Validation Loss: 0.47958117723464966\n",
      "Epoch 286: Train Loss: 0.08723602692286174, Validation Loss: 0.47123444080352783\n",
      "Epoch 287: Train Loss: 0.08231882005929947, Validation Loss: 0.47656047344207764\n",
      "Epoch 288: Train Loss: 0.0724827175339063, Validation Loss: 0.47165048122406006\n",
      "Epoch 289: Train Loss: 0.07449068625768025, Validation Loss: 0.470974862575531\n",
      "Epoch 290: Train Loss: 0.12774754812320074, Validation Loss: 0.50156569480896\n",
      "Epoch 291: Train Loss: 0.09657424688339233, Validation Loss: 0.6207923293113708\n",
      "Epoch 292: Train Loss: 0.08496012290318807, Validation Loss: 0.5049760341644287\n",
      "Epoch 293: Train Loss: 0.0774535524348418, Validation Loss: 0.43968746066093445\n",
      "Epoch 294: Train Loss: 0.08289639900128047, Validation Loss: 0.4389980733394623\n",
      "Epoch 295: Train Loss: 0.06777021661400795, Validation Loss: 0.44627121090888977\n",
      "Epoch 296: Train Loss: 0.07657770812511444, Validation Loss: 0.4565117657184601\n",
      "Epoch 297: Train Loss: 0.06215879072745641, Validation Loss: 0.47672411799430847\n",
      "Epoch 298: Train Loss: 0.06952132036288579, Validation Loss: 0.49448513984680176\n",
      "Epoch 299: Train Loss: 0.06523566817243893, Validation Loss: 0.49422189593315125\n",
      "Epoch 300: Train Loss: 0.06313027441501617, Validation Loss: 0.5583330988883972\n",
      "Epoch 301: Train Loss: 0.06505775451660156, Validation Loss: 0.5188561677932739\n",
      "Epoch 302: Train Loss: 0.0633965643743674, Validation Loss: 0.4673781991004944\n",
      "Epoch 303: Train Loss: 0.05704168602824211, Validation Loss: 0.449383944272995\n",
      "Epoch 304: Train Loss: 0.0660707267622153, Validation Loss: 0.4798016846179962\n",
      "Epoch 305: Train Loss: 0.06784212216734886, Validation Loss: 0.4429290294647217\n",
      "Epoch 306: Train Loss: 0.09121997654438019, Validation Loss: 0.44989946484565735\n",
      "Epoch 307: Train Loss: 0.05069518213470777, Validation Loss: 0.44637051224708557\n",
      "Epoch 308: Train Loss: 0.07476856062809627, Validation Loss: 0.4473288059234619\n",
      "Epoch 309: Train Loss: 0.06182176123062769, Validation Loss: 0.4494091272354126\n",
      "Epoch 310: Train Loss: 0.05807878946264585, Validation Loss: 0.49504712224006653\n",
      "Epoch 311: Train Loss: 0.06943399707476298, Validation Loss: 0.42486345767974854\n",
      "Epoch 312: Train Loss: 0.07023834685484569, Validation Loss: 0.40485045313835144\n",
      "Epoch 313: Train Loss: 0.05013053740064303, Validation Loss: 0.4465729594230652\n",
      "Epoch 314: Train Loss: 0.06037015716234843, Validation Loss: 0.4650290310382843\n",
      "Epoch 315: Train Loss: 0.06183869888385137, Validation Loss: 0.4754025340080261\n",
      "Epoch 316: Train Loss: 0.07208492855230968, Validation Loss: 0.5036038756370544\n",
      "Epoch 317: Train Loss: 0.055132550497849785, Validation Loss: 0.49905481934547424\n",
      "Epoch 318: Train Loss: 0.04728835076093674, Validation Loss: 0.49613919854164124\n",
      "Epoch 319: Train Loss: 0.053838601956764855, Validation Loss: 0.4944523572921753\n",
      "Epoch 320: Train Loss: 0.05785234272480011, Validation Loss: 0.49072352051734924\n",
      "Epoch 321: Train Loss: 0.05667759974797567, Validation Loss: 0.6623024940490723\n",
      "Epoch 322: Train Loss: 0.06755212197701137, Validation Loss: 0.49110904335975647\n",
      "Epoch 323: Train Loss: 0.05804847925901413, Validation Loss: 0.4410335421562195\n",
      "Epoch 324: Train Loss: 0.05379284049073855, Validation Loss: 0.42343851923942566\n",
      "Epoch 325: Train Loss: 0.054171349853277206, Validation Loss: 0.4506286382675171\n",
      "Epoch 326: Train Loss: 0.06194452941417694, Validation Loss: 0.46712347865104675\n",
      "Epoch 327: Train Loss: 0.04749557996789614, Validation Loss: 0.4722760021686554\n",
      "Epoch 328: Train Loss: 0.06620251759886742, Validation Loss: 0.4681391417980194\n",
      "Epoch 329: Train Loss: 0.04697651291886965, Validation Loss: 0.4623693525791168\n",
      "Epoch 330: Train Loss: 0.04622464378674825, Validation Loss: 0.43713515996932983\n",
      "Epoch 331: Train Loss: 0.050616505245367684, Validation Loss: 0.4083179235458374\n",
      "Epoch 332: Train Loss: 0.064114843805631, Validation Loss: 0.5437878966331482\n",
      "Epoch 333: Train Loss: 0.05128460253278414, Validation Loss: 0.6365870237350464\n",
      "Early stopping at epoch 334\n",
      "Fold 5\n",
      "Epoch 0: Train Loss: 0.7031778494517008, Validation Loss: 0.680680513381958\n",
      "Epoch 1: Train Loss: 0.6597114404042562, Validation Loss: 0.6810713410377502\n",
      "Epoch 2: Train Loss: 0.633353610833486, Validation Loss: 0.6778197288513184\n",
      "Epoch 3: Train Loss: 0.6273063818613688, Validation Loss: 0.6705563068389893\n",
      "Epoch 4: Train Loss: 0.5959759950637817, Validation Loss: 0.6607818603515625\n",
      "Epoch 5: Train Loss: 0.6155271331469218, Validation Loss: 0.6548725962638855\n",
      "Epoch 6: Train Loss: 0.5662505825360616, Validation Loss: 0.650610625743866\n",
      "Epoch 7: Train Loss: 0.5855803489685059, Validation Loss: 0.648012638092041\n",
      "Epoch 8: Train Loss: 0.5951297084490458, Validation Loss: 0.6466226577758789\n",
      "Epoch 9: Train Loss: 0.6173543334007263, Validation Loss: 0.6452457308769226\n",
      "Epoch 10: Train Loss: 0.5887259840965271, Validation Loss: 0.637082576751709\n",
      "Epoch 11: Train Loss: 0.6081436077753702, Validation Loss: 0.6297696828842163\n",
      "Epoch 12: Train Loss: 0.5739181439081827, Validation Loss: 0.6224846839904785\n",
      "Epoch 13: Train Loss: 0.5628325939178467, Validation Loss: 0.6143976449966431\n",
      "Epoch 14: Train Loss: 0.5468348264694214, Validation Loss: 0.6092546582221985\n",
      "Epoch 15: Train Loss: 0.5517727335294088, Validation Loss: 0.5998669266700745\n",
      "Epoch 16: Train Loss: 0.5473496317863464, Validation Loss: 0.5906583666801453\n",
      "Epoch 17: Train Loss: 0.5731086333592733, Validation Loss: 0.584937334060669\n",
      "Epoch 18: Train Loss: 0.5384282469749451, Validation Loss: 0.5825461149215698\n",
      "Epoch 19: Train Loss: 0.5357412497202555, Validation Loss: 0.581231415271759\n",
      "Epoch 20: Train Loss: 0.5408456325531006, Validation Loss: 0.5781697630882263\n",
      "Epoch 21: Train Loss: 0.5104497869809469, Validation Loss: 0.5744444727897644\n",
      "Epoch 22: Train Loss: 0.5302955309549967, Validation Loss: 0.5686289668083191\n",
      "Epoch 23: Train Loss: 0.5413862566153208, Validation Loss: 0.5535767078399658\n",
      "Epoch 24: Train Loss: 0.5260651310284933, Validation Loss: 0.5454694628715515\n",
      "Epoch 25: Train Loss: 0.5312581857045492, Validation Loss: 0.5429657697677612\n",
      "Epoch 26: Train Loss: 0.5371920466423035, Validation Loss: 0.5413476228713989\n",
      "Epoch 27: Train Loss: 0.481076975663503, Validation Loss: 0.5408893823623657\n",
      "Epoch 28: Train Loss: 0.5078888535499573, Validation Loss: 0.539330244064331\n",
      "Epoch 29: Train Loss: 0.4826955894629161, Validation Loss: 0.5387318134307861\n",
      "Epoch 30: Train Loss: 0.5044638713200887, Validation Loss: 0.5467734336853027\n",
      "Epoch 31: Train Loss: 0.4700133204460144, Validation Loss: 0.5565987229347229\n",
      "Epoch 32: Train Loss: 0.4717680712540944, Validation Loss: 0.5474318265914917\n",
      "Epoch 33: Train Loss: 0.4648892283439636, Validation Loss: 0.5208674669265747\n",
      "Epoch 34: Train Loss: 0.4558350344498952, Validation Loss: 0.5045823454856873\n",
      "Epoch 35: Train Loss: 0.4373394548892975, Validation Loss: 0.4970320165157318\n",
      "Epoch 36: Train Loss: 0.4461335837841034, Validation Loss: 0.49663516879081726\n",
      "Epoch 37: Train Loss: 0.4402489463488261, Validation Loss: 0.49737510085105896\n",
      "Epoch 38: Train Loss: 0.445626825094223, Validation Loss: 0.496686726808548\n",
      "Epoch 39: Train Loss: 0.42549700538317364, Validation Loss: 0.496587872505188\n",
      "Epoch 40: Train Loss: 0.46897979577382404, Validation Loss: 0.5161433815956116\n",
      "Epoch 41: Train Loss: 0.43254541357358295, Validation Loss: 0.5192350745201111\n",
      "Epoch 42: Train Loss: 0.4227455159028371, Validation Loss: 0.5073878765106201\n",
      "Epoch 43: Train Loss: 0.41604067881902057, Validation Loss: 0.4991006851196289\n",
      "Epoch 44: Train Loss: 0.40609030922253925, Validation Loss: 0.48583972454071045\n",
      "Epoch 45: Train Loss: 0.420668621857961, Validation Loss: 0.4795992970466614\n",
      "Epoch 46: Train Loss: 0.41045408447583515, Validation Loss: 0.47673898935317993\n",
      "Epoch 47: Train Loss: 0.39304104447364807, Validation Loss: 0.47540923953056335\n",
      "Epoch 48: Train Loss: 0.39887680610020954, Validation Loss: 0.47462013363838196\n",
      "Epoch 49: Train Loss: 0.39286089936892193, Validation Loss: 0.474318265914917\n",
      "Epoch 50: Train Loss: 0.3834925790627797, Validation Loss: 0.4856918156147003\n",
      "Epoch 51: Train Loss: 0.38277096549669903, Validation Loss: 0.4703798294067383\n",
      "Epoch 52: Train Loss: 0.4210846821467082, Validation Loss: 0.4715959429740906\n",
      "Epoch 53: Train Loss: 0.39452343185742694, Validation Loss: 0.4774351418018341\n",
      "Epoch 54: Train Loss: 0.3873082101345062, Validation Loss: 0.47956347465515137\n",
      "Epoch 55: Train Loss: 0.37678031126658124, Validation Loss: 0.4728441834449768\n",
      "Epoch 56: Train Loss: 0.39448196689287823, Validation Loss: 0.455624520778656\n",
      "Epoch 57: Train Loss: 0.37136022249857586, Validation Loss: 0.4528147578239441\n",
      "Epoch 58: Train Loss: 0.36529257893562317, Validation Loss: 0.45473968982696533\n",
      "Epoch 59: Train Loss: 0.35333571831385296, Validation Loss: 0.4547038972377777\n",
      "Epoch 60: Train Loss: 0.35749181111653644, Validation Loss: 0.4646252691745758\n",
      "Epoch 61: Train Loss: 0.3637812038262685, Validation Loss: 0.4593682885169983\n",
      "Epoch 62: Train Loss: 0.3551472028096517, Validation Loss: 0.4548097550868988\n",
      "Epoch 63: Train Loss: 0.34520284334818524, Validation Loss: 0.45089173316955566\n",
      "Epoch 64: Train Loss: 0.34934544563293457, Validation Loss: 0.44847336411476135\n",
      "Epoch 65: Train Loss: 0.35103652874628705, Validation Loss: 0.4337952435016632\n",
      "Epoch 66: Train Loss: 0.3317674696445465, Validation Loss: 0.42889392375946045\n",
      "Epoch 67: Train Loss: 0.3219363788763682, Validation Loss: 0.42723116278648376\n",
      "Epoch 68: Train Loss: 0.3298322757085164, Validation Loss: 0.4283907413482666\n",
      "Epoch 69: Train Loss: 0.3210157851378123, Validation Loss: 0.42815065383911133\n",
      "Epoch 70: Train Loss: 0.3081352214018504, Validation Loss: 0.4347606301307678\n",
      "Epoch 71: Train Loss: 0.33002279202143353, Validation Loss: 0.4376394748687744\n",
      "Epoch 72: Train Loss: 0.3378194371859233, Validation Loss: 0.45472252368927\n",
      "Epoch 73: Train Loss: 0.3335897922515869, Validation Loss: 0.4526798725128174\n",
      "Epoch 74: Train Loss: 0.313754270474116, Validation Loss: 0.4462728798389435\n",
      "Epoch 75: Train Loss: 0.3211967547734578, Validation Loss: 0.443814218044281\n",
      "Epoch 76: Train Loss: 0.3177233537038167, Validation Loss: 0.43969497084617615\n",
      "Epoch 77: Train Loss: 0.29115431507428485, Validation Loss: 0.43815913796424866\n",
      "Epoch 78: Train Loss: 0.29514026641845703, Validation Loss: 0.437012255191803\n",
      "Epoch 79: Train Loss: 0.29591773947079975, Validation Loss: 0.4366719722747803\n",
      "Epoch 80: Train Loss: 0.3000791569550832, Validation Loss: 0.4114587604999542\n",
      "Epoch 81: Train Loss: 0.28853971759478253, Validation Loss: 0.425615131855011\n",
      "Epoch 82: Train Loss: 0.31870804230372113, Validation Loss: 0.44034668803215027\n",
      "Epoch 83: Train Loss: 0.2890299956003825, Validation Loss: 0.4154696762561798\n",
      "Epoch 84: Train Loss: 0.2875249783198039, Validation Loss: 0.4030359983444214\n",
      "Epoch 85: Train Loss: 0.2950578232606252, Validation Loss: 0.4063622057437897\n",
      "Epoch 86: Train Loss: 0.27768463393052417, Validation Loss: 0.4161633551120758\n",
      "Epoch 87: Train Loss: 0.2656636933485667, Validation Loss: 0.4224056303501129\n",
      "Epoch 88: Train Loss: 0.2612157066663106, Validation Loss: 0.4212079346179962\n",
      "Epoch 89: Train Loss: 0.27285489439964294, Validation Loss: 0.42121729254722595\n",
      "Epoch 90: Train Loss: 0.28926469882329303, Validation Loss: 0.39815324544906616\n",
      "Epoch 91: Train Loss: 0.2685070385535558, Validation Loss: 0.3911190927028656\n",
      "Epoch 92: Train Loss: 0.2923859159151713, Validation Loss: 0.4053225517272949\n",
      "Epoch 93: Train Loss: 0.2841385006904602, Validation Loss: 0.40257641673088074\n",
      "Epoch 94: Train Loss: 0.25518156091372174, Validation Loss: 0.39269906282424927\n",
      "Epoch 95: Train Loss: 0.26354725658893585, Validation Loss: 0.3984799385070801\n",
      "Epoch 96: Train Loss: 0.26316294570763904, Validation Loss: 0.39412882924079895\n",
      "Epoch 97: Train Loss: 0.2486736923456192, Validation Loss: 0.39124077558517456\n",
      "Epoch 98: Train Loss: 0.26525027056535083, Validation Loss: 0.3917076885700226\n",
      "Epoch 99: Train Loss: 0.2468565305074056, Validation Loss: 0.3914198577404022\n",
      "Epoch 100: Train Loss: 0.24708465735117593, Validation Loss: 0.3882870078086853\n",
      "Epoch 101: Train Loss: 0.261986901362737, Validation Loss: 0.40896227955818176\n",
      "Epoch 102: Train Loss: 0.25010285278161365, Validation Loss: 0.41080760955810547\n",
      "Epoch 103: Train Loss: 0.2371608962615331, Validation Loss: 0.3919326663017273\n",
      "Epoch 104: Train Loss: 0.24232406417528787, Validation Loss: 0.3628917634487152\n",
      "Epoch 105: Train Loss: 0.23846113681793213, Validation Loss: 0.36485591530799866\n",
      "Epoch 106: Train Loss: 0.2375577688217163, Validation Loss: 0.38150832056999207\n",
      "Epoch 107: Train Loss: 0.24276483058929443, Validation Loss: 0.3923540711402893\n",
      "Epoch 108: Train Loss: 0.21675839523474374, Validation Loss: 0.39706364274024963\n",
      "Epoch 109: Train Loss: 0.2222166657447815, Validation Loss: 0.3987158536911011\n",
      "Epoch 110: Train Loss: 0.2098086029291153, Validation Loss: 0.4158952832221985\n",
      "Epoch 111: Train Loss: 0.2492570479710897, Validation Loss: 0.38693299889564514\n",
      "Epoch 112: Train Loss: 0.24353401362895966, Validation Loss: 0.39351722598075867\n",
      "Epoch 113: Train Loss: 0.23163222273190817, Validation Loss: 0.4192019999027252\n",
      "Epoch 114: Train Loss: 0.22053120533625284, Validation Loss: 0.4214022755622864\n",
      "Epoch 115: Train Loss: 0.2233943392833074, Validation Loss: 0.41015127301216125\n",
      "Epoch 116: Train Loss: 0.22877920170625052, Validation Loss: 0.39530646800994873\n",
      "Epoch 117: Train Loss: 0.203791543841362, Validation Loss: 0.38495388627052307\n",
      "Epoch 118: Train Loss: 0.1844349503517151, Validation Loss: 0.37988513708114624\n",
      "Epoch 119: Train Loss: 0.21781444549560547, Validation Loss: 0.3791028559207916\n",
      "Epoch 120: Train Loss: 0.197189932068189, Validation Loss: 0.36148884892463684\n",
      "Epoch 121: Train Loss: 0.23331578075885773, Validation Loss: 0.38177594542503357\n",
      "Epoch 122: Train Loss: 0.2286924123764038, Validation Loss: 0.40002185106277466\n",
      "Epoch 123: Train Loss: 0.21091736356417337, Validation Loss: 0.40052032470703125\n",
      "Epoch 124: Train Loss: 0.18715677658716837, Validation Loss: 0.4033316373825073\n",
      "Epoch 125: Train Loss: 0.18737431863943735, Validation Loss: 0.3925468623638153\n",
      "Epoch 126: Train Loss: 0.19850337505340576, Validation Loss: 0.38255077600479126\n",
      "Epoch 127: Train Loss: 0.18666826685269675, Validation Loss: 0.3716054856777191\n",
      "Epoch 128: Train Loss: 0.21475504338741302, Validation Loss: 0.3717992305755615\n",
      "Epoch 129: Train Loss: 0.18769617875417074, Validation Loss: 0.3714228570461273\n",
      "Epoch 130: Train Loss: 0.1918317824602127, Validation Loss: 0.36563512682914734\n",
      "Epoch 131: Train Loss: 0.17012994488080344, Validation Loss: 0.35462725162506104\n",
      "Epoch 132: Train Loss: 0.1989105890194575, Validation Loss: 0.3430556654930115\n",
      "Epoch 133: Train Loss: 0.20257762571175894, Validation Loss: 0.3477898836135864\n",
      "Epoch 134: Train Loss: 0.17711911598841348, Validation Loss: 0.3675684630870819\n",
      "Epoch 135: Train Loss: 0.1841162145137787, Validation Loss: 0.3724534809589386\n",
      "Epoch 136: Train Loss: 0.191801850994428, Validation Loss: 0.36810120940208435\n",
      "Epoch 137: Train Loss: 0.18761232992013296, Validation Loss: 0.36225301027297974\n",
      "Epoch 138: Train Loss: 0.16547847290833792, Validation Loss: 0.3591945767402649\n",
      "Epoch 139: Train Loss: 0.1789217789967855, Validation Loss: 0.35749369859695435\n",
      "Epoch 140: Train Loss: 0.2037456085284551, Validation Loss: 0.3595854341983795\n",
      "Epoch 141: Train Loss: 0.20362656315167746, Validation Loss: 0.40048208832740784\n",
      "Epoch 142: Train Loss: 0.1844918578863144, Validation Loss: 0.38790082931518555\n",
      "Epoch 143: Train Loss: 0.18661150336265564, Validation Loss: 0.3819291889667511\n",
      "Epoch 144: Train Loss: 0.1894626667102178, Validation Loss: 0.3740876019001007\n",
      "Epoch 145: Train Loss: 0.1618089278539022, Validation Loss: 0.37676239013671875\n",
      "Epoch 146: Train Loss: 0.17800149818261465, Validation Loss: 0.3688080906867981\n",
      "Epoch 147: Train Loss: 0.2135749707619349, Validation Loss: 0.37144893407821655\n",
      "Epoch 148: Train Loss: 0.1697748750448227, Validation Loss: 0.36984097957611084\n",
      "Epoch 149: Train Loss: 0.17203348378340402, Validation Loss: 0.37078657746315\n",
      "Epoch 150: Train Loss: 0.16461819410324097, Validation Loss: 0.42859700322151184\n",
      "Epoch 151: Train Loss: 0.15617438157399496, Validation Loss: 0.4287657141685486\n",
      "Epoch 152: Train Loss: 0.1611215720574061, Validation Loss: 0.3844848573207855\n",
      "Epoch 153: Train Loss: 0.15754529337088266, Validation Loss: 0.3668791651725769\n",
      "Epoch 154: Train Loss: 0.19050098458925882, Validation Loss: 0.38229313492774963\n",
      "Epoch 155: Train Loss: 0.1656122754017512, Validation Loss: 0.36192241311073303\n",
      "Epoch 156: Train Loss: 0.18831648429234824, Validation Loss: 0.3698332905769348\n",
      "Epoch 157: Train Loss: 0.15120367209116617, Validation Loss: 0.3782937526702881\n",
      "Epoch 158: Train Loss: 0.1559539089600245, Validation Loss: 0.3868434727191925\n",
      "Epoch 159: Train Loss: 0.1549930969874064, Validation Loss: 0.3889681398868561\n",
      "Epoch 160: Train Loss: 0.16564102470874786, Validation Loss: 0.4835526645183563\n",
      "Epoch 161: Train Loss: 0.1691165715456009, Validation Loss: 0.5031831860542297\n",
      "Epoch 162: Train Loss: 0.16810560723145804, Validation Loss: 0.5694381594657898\n",
      "Epoch 163: Train Loss: 0.1663517008225123, Validation Loss: 0.5596283078193665\n",
      "Epoch 164: Train Loss: 0.15370086828867593, Validation Loss: 0.4826327860355377\n",
      "Epoch 165: Train Loss: 0.15438605348269144, Validation Loss: 0.3956003487110138\n",
      "Epoch 166: Train Loss: 0.14589224259058634, Validation Loss: 0.37157922983169556\n",
      "Epoch 167: Train Loss: 0.14093534151713052, Validation Loss: 0.36970382928848267\n",
      "Epoch 168: Train Loss: 0.14766256511211395, Validation Loss: 0.37022706866264343\n",
      "Epoch 169: Train Loss: 0.1499879558881124, Validation Loss: 0.3710586726665497\n",
      "Epoch 170: Train Loss: 0.14897718528906503, Validation Loss: 0.45869481563568115\n",
      "Epoch 171: Train Loss: 0.15509272118409476, Validation Loss: 0.4765680134296417\n",
      "Epoch 172: Train Loss: 0.1434920926888784, Validation Loss: 0.38281911611557007\n",
      "Epoch 173: Train Loss: 0.1391082927584648, Validation Loss: 0.3353176712989807\n",
      "Epoch 174: Train Loss: 0.14587271213531494, Validation Loss: 0.3579244017601013\n",
      "Epoch 175: Train Loss: 0.14085022111733755, Validation Loss: 0.39497071504592896\n",
      "Epoch 176: Train Loss: 0.12100578844547272, Validation Loss: 0.41547679901123047\n",
      "Epoch 177: Train Loss: 0.17916514972845712, Validation Loss: 0.4101407527923584\n",
      "Epoch 178: Train Loss: 0.1327479307850202, Validation Loss: 0.403241902589798\n",
      "Epoch 179: Train Loss: 0.1261473869283994, Validation Loss: 0.4014951288700104\n",
      "Epoch 180: Train Loss: 0.11797079195578893, Validation Loss: 0.3752756416797638\n",
      "Epoch 181: Train Loss: 0.14897187302509943, Validation Loss: 0.3767237365245819\n",
      "Epoch 182: Train Loss: 0.1332624132434527, Validation Loss: 0.3939242660999298\n",
      "Epoch 183: Train Loss: 0.12424108137687047, Validation Loss: 0.3989550471305847\n",
      "Epoch 184: Train Loss: 0.14720608790715536, Validation Loss: 0.40617331862449646\n",
      "Epoch 185: Train Loss: 0.12258774538834889, Validation Loss: 0.43541252613067627\n",
      "Epoch 186: Train Loss: 0.13457327832778296, Validation Loss: 0.4359450340270996\n",
      "Epoch 187: Train Loss: 0.12381301075220108, Validation Loss: 0.42159250378608704\n",
      "Epoch 188: Train Loss: 0.1336895947655042, Validation Loss: 0.4144488275051117\n",
      "Epoch 189: Train Loss: 0.1250298743446668, Validation Loss: 0.41228342056274414\n",
      "Epoch 190: Train Loss: 0.12756236145893732, Validation Loss: 0.336288720369339\n",
      "Epoch 191: Train Loss: 0.13216387728850046, Validation Loss: 0.3756318986415863\n",
      "Epoch 192: Train Loss: 0.15746984630823135, Validation Loss: 0.39434388279914856\n",
      "Epoch 193: Train Loss: 0.14209647476673126, Validation Loss: 0.4240722358226776\n",
      "Epoch 194: Train Loss: 0.12934416780869165, Validation Loss: 0.4318862557411194\n",
      "Epoch 195: Train Loss: 0.1278415968020757, Validation Loss: 0.44822537899017334\n",
      "Epoch 196: Train Loss: 0.13302494088808695, Validation Loss: 0.4496304392814636\n",
      "Epoch 197: Train Loss: 0.10356963674227397, Validation Loss: 0.44270920753479004\n",
      "Epoch 198: Train Loss: 0.11368325104316075, Validation Loss: 0.4390573799610138\n",
      "Epoch 199: Train Loss: 0.10872889061768849, Validation Loss: 0.4388844668865204\n",
      "Epoch 200: Train Loss: 0.1143638864159584, Validation Loss: 0.3939317464828491\n",
      "Epoch 201: Train Loss: 0.1344736764828364, Validation Loss: 0.44868943095207214\n",
      "Epoch 202: Train Loss: 0.11346691350142162, Validation Loss: 0.4274698495864868\n",
      "Epoch 203: Train Loss: 0.13452511529127756, Validation Loss: 0.4313475489616394\n",
      "Epoch 204: Train Loss: 0.12577388187249502, Validation Loss: 0.4080023169517517\n",
      "Epoch 205: Train Loss: 0.09730785836776097, Validation Loss: 0.4071190357208252\n",
      "Epoch 206: Train Loss: 0.10420206189155579, Validation Loss: 0.41436782479286194\n",
      "Epoch 207: Train Loss: 0.12153662492831548, Validation Loss: 0.4198426604270935\n",
      "Epoch 208: Train Loss: 0.11876305937767029, Validation Loss: 0.4220813810825348\n",
      "Epoch 209: Train Loss: 0.11391229927539825, Validation Loss: 0.42322519421577454\n",
      "Epoch 210: Train Loss: 0.09974030653635661, Validation Loss: 0.4113667905330658\n",
      "Epoch 211: Train Loss: 0.11547578622897466, Validation Loss: 0.3969300389289856\n",
      "Epoch 212: Train Loss: 0.11114365855852763, Validation Loss: 0.40233299136161804\n",
      "Epoch 213: Train Loss: 0.12571943054596582, Validation Loss: 0.3491377830505371\n",
      "Epoch 214: Train Loss: 0.12055126825968425, Validation Loss: 0.3725947439670563\n",
      "Epoch 215: Train Loss: 0.13194063305854797, Validation Loss: 0.39602258801460266\n",
      "Epoch 216: Train Loss: 0.13248792539040247, Validation Loss: 0.41447195410728455\n",
      "Epoch 217: Train Loss: 0.10508549461762111, Validation Loss: 0.4204533100128174\n",
      "Epoch 218: Train Loss: 0.10521141439676285, Validation Loss: 0.4212617874145508\n",
      "Epoch 219: Train Loss: 0.10994209100802739, Validation Loss: 0.420737624168396\n",
      "Epoch 220: Train Loss: 0.10044786582390468, Validation Loss: 0.4284362494945526\n",
      "Epoch 221: Train Loss: 0.11060462892055511, Validation Loss: 0.41632914543151855\n",
      "Epoch 222: Train Loss: 0.11016242454449336, Validation Loss: 0.4273645877838135\n",
      "Epoch 223: Train Loss: 0.13246010740598044, Validation Loss: 0.446088969707489\n",
      "Epoch 224: Train Loss: 0.12482426067193349, Validation Loss: 0.41060101985931396\n",
      "Epoch 225: Train Loss: 0.11711979657411575, Validation Loss: 0.35973936319351196\n",
      "Epoch 226: Train Loss: 0.11261329551537831, Validation Loss: 0.34018009901046753\n",
      "Epoch 227: Train Loss: 0.0962127869327863, Validation Loss: 0.342347115278244\n",
      "Epoch 228: Train Loss: 0.09681001057227452, Validation Loss: 0.3527269959449768\n",
      "Epoch 229: Train Loss: 0.1060113658507665, Validation Loss: 0.35846811532974243\n",
      "Epoch 230: Train Loss: 0.10467655211687088, Validation Loss: 0.6681855916976929\n",
      "Epoch 231: Train Loss: 0.12371198584636052, Validation Loss: 0.561340868473053\n",
      "Epoch 232: Train Loss: 0.11496858050425847, Validation Loss: 0.4128318130970001\n",
      "Epoch 233: Train Loss: 0.11248127122720082, Validation Loss: 0.3886162042617798\n",
      "Epoch 234: Train Loss: 0.11152491470177968, Validation Loss: 0.4310206472873688\n",
      "Epoch 235: Train Loss: 0.09361487875382106, Validation Loss: 0.5187227725982666\n",
      "Epoch 236: Train Loss: 0.08378320187330246, Validation Loss: 0.5608406066894531\n",
      "Epoch 237: Train Loss: 0.08240121603012085, Validation Loss: 0.5489926934242249\n",
      "Epoch 238: Train Loss: 0.111978679895401, Validation Loss: 0.5347825288772583\n",
      "Epoch 239: Train Loss: 0.10652465124924977, Validation Loss: 0.5291378498077393\n",
      "Epoch 240: Train Loss: 0.08827003588279088, Validation Loss: 0.3758646547794342\n",
      "Epoch 241: Train Loss: 0.0912814661860466, Validation Loss: 0.378497451543808\n",
      "Epoch 242: Train Loss: 0.09583209703365962, Validation Loss: 0.44671788811683655\n",
      "Epoch 243: Train Loss: 0.13389863818883896, Validation Loss: 0.4385104179382324\n",
      "Epoch 244: Train Loss: 0.10225384682416916, Validation Loss: 0.41469672322273254\n",
      "Epoch 245: Train Loss: 0.08745943754911423, Validation Loss: 0.44158655405044556\n",
      "Epoch 246: Train Loss: 0.10789541155099869, Validation Loss: 0.45879924297332764\n",
      "Epoch 247: Train Loss: 0.11908802390098572, Validation Loss: 0.4488953948020935\n",
      "Epoch 248: Train Loss: 0.12035740166902542, Validation Loss: 0.4430077373981476\n",
      "Epoch 249: Train Loss: 0.09443947424491246, Validation Loss: 0.4426206052303314\n",
      "Epoch 250: Train Loss: 0.08743862807750702, Validation Loss: 0.4722640812397003\n",
      "Epoch 251: Train Loss: 0.0832303985953331, Validation Loss: 0.49207186698913574\n",
      "Epoch 252: Train Loss: 0.08310559516151746, Validation Loss: 0.5184597969055176\n",
      "Epoch 253: Train Loss: 0.09156730274359386, Validation Loss: 0.5265669822692871\n",
      "Epoch 254: Train Loss: 0.09722906971971194, Validation Loss: 0.5148246884346008\n",
      "Epoch 255: Train Loss: 0.07908093432585399, Validation Loss: 0.5014230608940125\n",
      "Epoch 256: Train Loss: 0.10521711160739262, Validation Loss: 0.49067258834838867\n",
      "Epoch 257: Train Loss: 0.07893695682287216, Validation Loss: 0.4885627329349518\n",
      "Epoch 258: Train Loss: 0.08428256958723068, Validation Loss: 0.48500069975852966\n",
      "Epoch 259: Train Loss: 0.08184824138879776, Validation Loss: 0.48316818475723267\n",
      "Epoch 260: Train Loss: 0.07921070481340091, Validation Loss: 0.37888652086257935\n",
      "Epoch 261: Train Loss: 0.09296880165735881, Validation Loss: 0.41662269830703735\n",
      "Epoch 262: Train Loss: 0.07793383797009786, Validation Loss: 0.5300685167312622\n",
      "Epoch 263: Train Loss: 0.09415565182765324, Validation Loss: 0.5705338716506958\n",
      "Epoch 264: Train Loss: 0.09860633313655853, Validation Loss: 0.5170720219612122\n",
      "Epoch 265: Train Loss: 0.07905403648813565, Validation Loss: 0.463396817445755\n",
      "Epoch 266: Train Loss: 0.0936581442753474, Validation Loss: 0.4328315556049347\n",
      "Epoch 267: Train Loss: 0.08284318695465724, Validation Loss: 0.4307161867618561\n",
      "Epoch 268: Train Loss: 0.08844580128788948, Validation Loss: 0.43923717737197876\n",
      "Epoch 269: Train Loss: 0.09113653749227524, Validation Loss: 0.4424852132797241\n",
      "Epoch 270: Train Loss: 0.07094083726406097, Validation Loss: 0.6045399904251099\n",
      "Epoch 271: Train Loss: 0.1046011820435524, Validation Loss: 0.6408334374427795\n",
      "Epoch 272: Train Loss: 0.10042302558819453, Validation Loss: 0.5768679976463318\n",
      "Early stopping at epoch 273\n",
      "Accuracy: 0.7166666666666667,Precision: 0.7166666666666667, Recall: 0.7166666666666667, F1-score: 0.7166666666666667, AUC: 0.7166666666666667\n",
      "Confusion Matrix:\n",
      "[[43 17]\n",
      " [17 43]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "\n",
    "model_save_dir = r'C:\\Users\\User\\Documents\\Lie detect data\\Model'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Define a function to load and pad data\n",
    "def load_data(data_dir, max_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    file_list = os.listdir(data_dir)\n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        label = 1 if 'truth' in file else 0\n",
    "        if data.shape[1] > max_length:\n",
    "            processed_data = data[:, :max_length]  # Cut data if it exceeds max_length\n",
    "        else:\n",
    "            processed_data = np.zeros((data.shape[0], max_length))\n",
    "            processed_data[:, :data.shape[1]] = data  # Pad data if it is shorter than max_length\n",
    "        X.append(processed_data)\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load dataset and pad the data\n",
    "data_dir = \"C:\\\\Users\\\\User\\\\Documents\\\\Lie detect data\\\\56M_DWTEEGData\"\n",
    "max_length = 500 # Define maximum length for padding\n",
    "X, y = load_data(data_dir, max_length)\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure the data is reshaped to [1, Chans, Samples]\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# Define EEGNet model\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int, kernel_size: tuple, padding: tuple = 0):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.depthwise_conv = nn.Conv2d(self.c_in, self.c_in, kernel_size=self.kernel_size,\n",
    "                                        padding=self.padding, groups=self.c_in)\n",
    "        self.conv2d_1x1 = nn.Conv2d(self.c_in, self.c_out, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y = self.depthwise_conv(x)\n",
    "        y = self.conv2d_1x1(y)\n",
    "        return y\n",
    "\n",
    "class SeparableConv1d(nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int, kernel_size: tuple, padding: tuple = 0):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.depthwise_conv = nn.Conv1d(self.c_in, self.c_in, kernel_size=self.kernel_size,\n",
    "                                        padding=self.padding, groups=self.c_in)\n",
    "        self.conv1d_1x1 = nn.Conv1d(self.c_in, self.c_out, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y = self.depthwise_conv(x)\n",
    "        y = self.conv1d_1x1(y)\n",
    "        return y\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, nb_classes: int = 2, Chans: int = 65, Samples: int = 500,\n",
    "                 dropoutRate: float = 0.5, kernLength: int = 127,\n",
    "                 F1:int = 8, D:int = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        F2 = F1 * D\n",
    "\n",
    "        # Make kernel size and odd number\n",
    "        try:\n",
    "            assert kernLength % 2 != 0\n",
    "        except AssertionError:\n",
    "            raise ValueError(\"ERROR: kernLength must be odd number\")\n",
    "\n",
    "        # In: (B, Chans, Samples, 1)\n",
    "        # Out: (B, F1, Samples, 1)\n",
    "        self.conv1 = nn.Conv1d(Chans, F1, kernLength, padding=(kernLength // 2))\n",
    "        self.bn1 = nn.BatchNorm1d(F1) # (B, F1, Samples, 1)\n",
    "        # In: (B, F1, Samples, 1)\n",
    "        # Out: (B, F2, Samples - Chans + 1, 1)\n",
    "        self.conv2 = nn.Conv1d(F1, F2, Chans, groups=F1)\n",
    "        self.bn2 = nn.BatchNorm1d(F2) # (B, F2, Samples - Chans + 1, 1)\n",
    "        # In: (B, F2, Samples - Chans + 1, 1)\n",
    "        # Out: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        self.avg_pool = nn.AvgPool1d(4)\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "\n",
    "        # In: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        # Out: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        self.conv3 = SeparableConv1d(F2, F2, kernel_size=15, padding=7)\n",
    "        self.bn3 = nn.BatchNorm1d(F2)\n",
    "        # In: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        # Out: (B, F2, (Samples - Chans + 1) / 32, 1)\n",
    "        self.avg_pool2 = nn.AvgPool1d(8)\n",
    "        # In: (B, F2 *  (Samples - Chans + 1) / 32)\n",
    "        self.fc = nn.Linear(F2 * ((Samples - Chans + 1) // 32), nb_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Block 1\n",
    "        y1 = self.conv1(x)\n",
    "        #print(\"conv1: \", y1.shape)\n",
    "        y1 = self.bn1(y1)\n",
    "        #print(\"bn1: \", y1.shape)\n",
    "        y1 = self.conv2(y1)\n",
    "        #print(\"conv2\", y1.shape)\n",
    "        y1 = F.relu(self.bn2(y1))\n",
    "        #print(\"bn2\", y1.shape)\n",
    "        y1 = self.avg_pool(y1)\n",
    "        #print(\"avg_pool\", y1.shape)\n",
    "        y1 = self.dropout(y1)\n",
    "        #print(\"dropout\", y1.shape)\n",
    "\n",
    "        # Block 2\n",
    "        y2 = self.conv3(y1)\n",
    "        #print(\"conv3\", y2.shape)\n",
    "        y2 = F.relu(self.bn3(y2))\n",
    "        #print(\"bn3\", y2.shape)\n",
    "        y2 = self.avg_pool2(y2)\n",
    "        #print(\"avg_pool2\", y2.shape)\n",
    "        y2 = self.dropout(y2)\n",
    "        #print(\"dropout\", y2.shape)\n",
    "        y2 = torch.flatten(y2, 1)\n",
    "        #print(\"flatten\", y2.shape)\n",
    "        y2 = self.fc(y2)\n",
    "        #print(\"fc\", y2.shape)\n",
    "\n",
    "        return y2\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_and_evaluate(train_loader, val_loader, y_train):\n",
    "    model = EEGNet().to(device)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-3)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "    num_epochs = 500\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 100\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "            fold_model_path = os.path.join(model_save_dir, f'revise_model_fold_{fold_idx}.pth')\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': best_val_loss,\n",
    "            }, fold_model_path)\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_idx = 0\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "for train_index, val_index in kf.split(X, y):\n",
    "    print(f'Fold {fold_idx + 1}')\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Normalize data using scaler fitted on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_val = scaler.transform(X_val.reshape(X_val.shape[0], -1))\n",
    "    X_train = X_train.reshape(-1, 65, max_length)\n",
    "    X_val = X_val.reshape(-1, 65, max_length)\n",
    "\n",
    "    # Save the scaler to a file\n",
    "    with open(r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\RevisedEEGNet_scaler_{}.pkl'.format(fold_idx), 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = EEGDataset(X_train, y_train)\n",
    "    val_dataset = EEGDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = train_and_evaluate(train_loader, val_loader, y_train)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    fold_idx += 1\n",
    "\n",
    "\n",
    "# Calculate additional metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "auc = roc_auc_score(all_labels, all_predictions)\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy},Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae639325-1800-4763-ad37-b5931e4391c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abbb309-50a0-43fa-bcc0-05e6d62e6cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
