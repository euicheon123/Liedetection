{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d5ee57-cf0b-4330-985e-ada385863b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 0: Train Loss: 0.7058594624201456, Validation Loss: 0.678370475769043\n",
      "Epoch 1: Train Loss: 0.6759293079376221, Validation Loss: 0.6754753589630127\n",
      "Epoch 2: Train Loss: 0.6751305063565572, Validation Loss: 0.6742416024208069\n",
      "Epoch 3: Train Loss: 0.6718753774960836, Validation Loss: 0.6718498468399048\n",
      "Epoch 4: Train Loss: 0.6453804175059, Validation Loss: 0.6696836948394775\n",
      "Epoch 5: Train Loss: 0.6497670412063599, Validation Loss: 0.6675712466239929\n",
      "Epoch 6: Train Loss: 0.6384867429733276, Validation Loss: 0.6662997007369995\n",
      "Epoch 7: Train Loss: 0.6114775538444519, Validation Loss: 0.6651366949081421\n",
      "Epoch 8: Train Loss: 0.6280161539713541, Validation Loss: 0.6643182635307312\n",
      "Epoch 9: Train Loss: 0.6165091792742411, Validation Loss: 0.663619875907898\n",
      "Epoch 10: Train Loss: 0.616681714852651, Validation Loss: 0.6601712107658386\n",
      "Epoch 11: Train Loss: 0.6040835777918497, Validation Loss: 0.6575169563293457\n",
      "Epoch 12: Train Loss: 0.6010390122731527, Validation Loss: 0.6529468297958374\n",
      "Epoch 13: Train Loss: 0.594450056552887, Validation Loss: 0.6468972563743591\n",
      "Epoch 14: Train Loss: 0.5837632417678833, Validation Loss: 0.6415234208106995\n",
      "Epoch 15: Train Loss: 0.5768715937932333, Validation Loss: 0.6360942721366882\n",
      "Epoch 16: Train Loss: 0.5620134075482687, Validation Loss: 0.6321438550949097\n",
      "Epoch 17: Train Loss: 0.5689720511436462, Validation Loss: 0.6291738152503967\n",
      "Epoch 18: Train Loss: 0.5703882376352946, Validation Loss: 0.627122700214386\n",
      "Epoch 19: Train Loss: 0.5636777877807617, Validation Loss: 0.6258003115653992\n",
      "Epoch 20: Train Loss: 0.5593046545982361, Validation Loss: 0.6177804470062256\n",
      "Epoch 21: Train Loss: 0.5313784678777059, Validation Loss: 0.6109834909439087\n",
      "Epoch 22: Train Loss: 0.5501308639844259, Validation Loss: 0.6043298244476318\n",
      "Epoch 23: Train Loss: 0.5170873800913492, Validation Loss: 0.5963171124458313\n",
      "Epoch 24: Train Loss: 0.513293037811915, Validation Loss: 0.5913709998130798\n",
      "Epoch 25: Train Loss: 0.528633048137029, Validation Loss: 0.5889486074447632\n",
      "Epoch 26: Train Loss: 0.5009064078330994, Validation Loss: 0.5874166488647461\n",
      "Epoch 27: Train Loss: 0.5083177983760834, Validation Loss: 0.5864458680152893\n",
      "Epoch 28: Train Loss: 0.504910965760549, Validation Loss: 0.5851945877075195\n",
      "Epoch 29: Train Loss: 0.4877253472805023, Validation Loss: 0.5840733647346497\n",
      "Epoch 30: Train Loss: 0.4777369201183319, Validation Loss: 0.5842927098274231\n",
      "Epoch 31: Train Loss: 0.4829976161321004, Validation Loss: 0.5804191827774048\n",
      "Epoch 32: Train Loss: 0.4741390844186147, Validation Loss: 0.5747783184051514\n",
      "Epoch 33: Train Loss: 0.45125595728556317, Validation Loss: 0.5685372352600098\n",
      "Epoch 34: Train Loss: 0.4543694257736206, Validation Loss: 0.5646108984947205\n",
      "Epoch 35: Train Loss: 0.4583253860473633, Validation Loss: 0.5594668984413147\n",
      "Epoch 36: Train Loss: 0.4356306791305542, Validation Loss: 0.5565540790557861\n",
      "Epoch 37: Train Loss: 0.4303302566210429, Validation Loss: 0.5549435615539551\n",
      "Epoch 38: Train Loss: 0.43843334913253784, Validation Loss: 0.5537036657333374\n",
      "Epoch 39: Train Loss: 0.43014220396677655, Validation Loss: 0.5527867078781128\n",
      "Epoch 40: Train Loss: 0.4423074821631114, Validation Loss: 0.5446563959121704\n",
      "Epoch 41: Train Loss: 0.42865467071533203, Validation Loss: 0.5403547286987305\n",
      "Epoch 42: Train Loss: 0.43887468179066974, Validation Loss: 0.5408722758293152\n",
      "Epoch 43: Train Loss: 0.43091481924057007, Validation Loss: 0.5433719754219055\n",
      "Epoch 44: Train Loss: 0.4242934584617615, Validation Loss: 0.5427084565162659\n",
      "Epoch 45: Train Loss: 0.4107723633448283, Validation Loss: 0.5423453450202942\n",
      "Epoch 46: Train Loss: 0.42781471212704975, Validation Loss: 0.5401931405067444\n",
      "Epoch 47: Train Loss: 0.409273495276769, Validation Loss: 0.5396439433097839\n",
      "Epoch 48: Train Loss: 0.42079923550287884, Validation Loss: 0.5395187735557556\n",
      "Epoch 49: Train Loss: 0.4131535490353902, Validation Loss: 0.5392584800720215\n",
      "Epoch 50: Train Loss: 0.4092872440814972, Validation Loss: 0.534236490726471\n",
      "Epoch 51: Train Loss: 0.3991342584292094, Validation Loss: 0.5325366854667664\n",
      "Epoch 52: Train Loss: 0.3984072208404541, Validation Loss: 0.5236274003982544\n",
      "Epoch 53: Train Loss: 0.4189728895823161, Validation Loss: 0.5256504416465759\n",
      "Epoch 54: Train Loss: 0.3808947304884593, Validation Loss: 0.5279254913330078\n",
      "Epoch 55: Train Loss: 0.3956495026747386, Validation Loss: 0.5320092439651489\n",
      "Epoch 56: Train Loss: 0.39592976371447247, Validation Loss: 0.5317822694778442\n",
      "Epoch 57: Train Loss: 0.3936636547247569, Validation Loss: 0.5329583883285522\n",
      "Epoch 58: Train Loss: 0.39216770728429157, Validation Loss: 0.532713770866394\n",
      "Epoch 59: Train Loss: 0.39774495363235474, Validation Loss: 0.5325474143028259\n",
      "Epoch 60: Train Loss: 0.3970395227273305, Validation Loss: 0.5306897163391113\n",
      "Epoch 61: Train Loss: 0.39008328318595886, Validation Loss: 0.532355785369873\n",
      "Epoch 62: Train Loss: 0.39172978202501935, Validation Loss: 0.5277707576751709\n",
      "Epoch 63: Train Loss: 0.39096034566561383, Validation Loss: 0.5294860005378723\n",
      "Epoch 64: Train Loss: 0.38975879549980164, Validation Loss: 0.5289463996887207\n",
      "Epoch 65: Train Loss: 0.39521656433741253, Validation Loss: 0.5265970826148987\n",
      "Epoch 66: Train Loss: 0.40319905678431195, Validation Loss: 0.5217463374137878\n",
      "Epoch 67: Train Loss: 0.403034766515096, Validation Loss: 0.5198816657066345\n",
      "Epoch 68: Train Loss: 0.39492009083429974, Validation Loss: 0.5192998647689819\n",
      "Epoch 69: Train Loss: 0.41331880291302997, Validation Loss: 0.5189310312271118\n",
      "Epoch 70: Train Loss: 0.38529308636983234, Validation Loss: 0.5166323781013489\n",
      "Epoch 71: Train Loss: 0.37298019727071124, Validation Loss: 0.5175787806510925\n",
      "Epoch 72: Train Loss: 0.38863347967465717, Validation Loss: 0.5189549326896667\n",
      "Epoch 73: Train Loss: 0.38170626759529114, Validation Loss: 0.5187746286392212\n",
      "Epoch 74: Train Loss: 0.3811452587445577, Validation Loss: 0.5160059332847595\n",
      "Epoch 75: Train Loss: 0.3640816807746887, Validation Loss: 0.5159374475479126\n",
      "Epoch 76: Train Loss: 0.38136528929074603, Validation Loss: 0.5164387822151184\n",
      "Epoch 77: Train Loss: 0.3693636457125346, Validation Loss: 0.5174641013145447\n",
      "Epoch 78: Train Loss: 0.3642776807149251, Validation Loss: 0.5180749297142029\n",
      "Epoch 79: Train Loss: 0.387961079676946, Validation Loss: 0.5182532072067261\n",
      "Epoch 80: Train Loss: 0.3745020627975464, Validation Loss: 0.5124586224555969\n",
      "Epoch 81: Train Loss: 0.3852004011472066, Validation Loss: 0.5158922076225281\n",
      "Epoch 82: Train Loss: 0.3608011305332184, Validation Loss: 0.5120484232902527\n",
      "Epoch 83: Train Loss: 0.370995153983434, Validation Loss: 0.5102942585945129\n",
      "Epoch 84: Train Loss: 0.3776695628960927, Validation Loss: 0.5107400417327881\n",
      "Epoch 85: Train Loss: 0.36173726121584576, Validation Loss: 0.512195348739624\n",
      "Epoch 86: Train Loss: 0.36622606714566547, Validation Loss: 0.5135305523872375\n",
      "Epoch 87: Train Loss: 0.3671632607777913, Validation Loss: 0.5148680210113525\n",
      "Epoch 88: Train Loss: 0.3600481649239858, Validation Loss: 0.5155343413352966\n",
      "Epoch 89: Train Loss: 0.3632117013136546, Validation Loss: 0.5156067609786987\n",
      "Epoch 90: Train Loss: 0.35750136772791546, Validation Loss: 0.5199421644210815\n",
      "Epoch 91: Train Loss: 0.3690791726112366, Validation Loss: 0.5130124092102051\n",
      "Epoch 92: Train Loss: 0.35643667976061505, Validation Loss: 0.4947446882724762\n",
      "Epoch 93: Train Loss: 0.3696383734544118, Validation Loss: 0.49024155735969543\n",
      "Epoch 94: Train Loss: 0.3504393994808197, Validation Loss: 0.4908061921596527\n",
      "Epoch 95: Train Loss: 0.3615267872810364, Validation Loss: 0.4952680766582489\n",
      "Epoch 96: Train Loss: 0.36676883697509766, Validation Loss: 0.4972623288631439\n",
      "Epoch 97: Train Loss: 0.3587341606616974, Validation Loss: 0.49687665700912476\n",
      "Epoch 98: Train Loss: 0.3719496428966522, Validation Loss: 0.4950905740261078\n",
      "Epoch 99: Train Loss: 0.3528798222541809, Validation Loss: 0.4955078959465027\n",
      "Epoch 100: Train Loss: 0.3672902484734853, Validation Loss: 0.5021246671676636\n",
      "Epoch 101: Train Loss: 0.35745513439178467, Validation Loss: 0.5159470438957214\n",
      "Epoch 102: Train Loss: 0.3638331393400828, Validation Loss: 0.5257275104522705\n",
      "Epoch 103: Train Loss: 0.35048896074295044, Validation Loss: 0.5227017998695374\n",
      "Epoch 104: Train Loss: 0.3538747231165568, Validation Loss: 0.5140841603279114\n",
      "Epoch 105: Train Loss: 0.3580244580904643, Validation Loss: 0.5038820505142212\n",
      "Epoch 106: Train Loss: 0.3487277030944824, Validation Loss: 0.5020567178726196\n",
      "Epoch 107: Train Loss: 0.35697637995084125, Validation Loss: 0.5031338334083557\n",
      "Epoch 108: Train Loss: 0.3434855341911316, Validation Loss: 0.5044714212417603\n",
      "Epoch 109: Train Loss: 0.35159297784169513, Validation Loss: 0.5051582455635071\n",
      "Epoch 110: Train Loss: 0.3452324370543162, Validation Loss: 0.5094917416572571\n",
      "Epoch 111: Train Loss: 0.34793931245803833, Validation Loss: 0.5107904076576233\n",
      "Epoch 112: Train Loss: 0.3626302083333333, Validation Loss: 0.512586236000061\n",
      "Epoch 113: Train Loss: 0.34868647654851276, Validation Loss: 0.51422518491745\n",
      "Epoch 114: Train Loss: 0.35878615578015643, Validation Loss: 0.5137539505958557\n",
      "Epoch 115: Train Loss: 0.3392485976219177, Validation Loss: 0.5127360224723816\n",
      "Epoch 116: Train Loss: 0.34299617012341815, Validation Loss: 0.5134381055831909\n",
      "Epoch 117: Train Loss: 0.3514148195584615, Validation Loss: 0.513735830783844\n",
      "Epoch 118: Train Loss: 0.3527081608772278, Validation Loss: 0.5141409635543823\n",
      "Epoch 119: Train Loss: 0.34726189573605853, Validation Loss: 0.514397382736206\n",
      "Epoch 120: Train Loss: 0.3514280617237091, Validation Loss: 0.5195213556289673\n",
      "Epoch 121: Train Loss: 0.344987412293752, Validation Loss: 0.5216131806373596\n",
      "Epoch 122: Train Loss: 0.346265971660614, Validation Loss: 0.5200173854827881\n",
      "Epoch 123: Train Loss: 0.34369807442029315, Validation Loss: 0.5169958472251892\n",
      "Epoch 124: Train Loss: 0.3437691330909729, Validation Loss: 0.5121414065361023\n",
      "Epoch 125: Train Loss: 0.35464481512705487, Validation Loss: 0.511246919631958\n",
      "Epoch 126: Train Loss: 0.33416993419329327, Validation Loss: 0.5122087597846985\n",
      "Epoch 127: Train Loss: 0.33769555886586505, Validation Loss: 0.5128653049468994\n",
      "Epoch 128: Train Loss: 0.35407520333925885, Validation Loss: 0.5127962231636047\n",
      "Epoch 129: Train Loss: 0.33521151542663574, Validation Loss: 0.5128903388977051\n",
      "Epoch 130: Train Loss: 0.3457906444867452, Validation Loss: 0.5110634565353394\n",
      "Epoch 131: Train Loss: 0.33369279901186627, Validation Loss: 0.5093321204185486\n",
      "Epoch 132: Train Loss: 0.34890908002853394, Validation Loss: 0.5156976580619812\n",
      "Epoch 133: Train Loss: 0.3398803969224294, Validation Loss: 0.5213668346405029\n",
      "Epoch 134: Train Loss: 0.33739809195200604, Validation Loss: 0.5218499898910522\n",
      "Epoch 135: Train Loss: 0.3418562710285187, Validation Loss: 0.5216864347457886\n",
      "Epoch 136: Train Loss: 0.3588353991508484, Validation Loss: 0.5225127339363098\n",
      "Epoch 137: Train Loss: 0.3327493170897166, Validation Loss: 0.522461473941803\n",
      "Epoch 138: Train Loss: 0.33877554535865784, Validation Loss: 0.5226342082023621\n",
      "Epoch 139: Train Loss: 0.3519427676995595, Validation Loss: 0.5227359533309937\n",
      "Epoch 140: Train Loss: 0.34617189566294354, Validation Loss: 0.5269368886947632\n",
      "Epoch 141: Train Loss: 0.34289470314979553, Validation Loss: 0.5346106886863708\n",
      "Epoch 142: Train Loss: 0.33830054601033527, Validation Loss: 0.5364370942115784\n",
      "Epoch 143: Train Loss: 0.3366917669773102, Validation Loss: 0.5335648059844971\n",
      "Epoch 144: Train Loss: 0.3379621009031932, Validation Loss: 0.5315757989883423\n",
      "Epoch 145: Train Loss: 0.34203160802523297, Validation Loss: 0.5276686549186707\n",
      "Epoch 146: Train Loss: 0.3309169312318166, Validation Loss: 0.5261193513870239\n",
      "Epoch 147: Train Loss: 0.33128153284390766, Validation Loss: 0.5252934694290161\n",
      "Epoch 148: Train Loss: 0.33407734831174213, Validation Loss: 0.5252280235290527\n",
      "Epoch 149: Train Loss: 0.3328312238057454, Validation Loss: 0.5252776741981506\n",
      "Epoch 150: Train Loss: 0.33406491080919903, Validation Loss: 0.5250265598297119\n",
      "Epoch 151: Train Loss: 0.3391492962837219, Validation Loss: 0.5241245031356812\n",
      "Epoch 152: Train Loss: 0.34308281540870667, Validation Loss: 0.5243069529533386\n",
      "Epoch 153: Train Loss: 0.3500492572784424, Validation Loss: 0.5257521271705627\n",
      "Epoch 154: Train Loss: 0.3325521449247996, Validation Loss: 0.5283225178718567\n",
      "Epoch 155: Train Loss: 0.3314794699350993, Validation Loss: 0.5299910306930542\n",
      "Epoch 156: Train Loss: 0.32897311449050903, Validation Loss: 0.5308188796043396\n",
      "Epoch 157: Train Loss: 0.34319931268692017, Validation Loss: 0.5300494432449341\n",
      "Epoch 158: Train Loss: 0.33239981532096863, Validation Loss: 0.5296032428741455\n",
      "Epoch 159: Train Loss: 0.3329106668631236, Validation Loss: 0.52959805727005\n",
      "Epoch 160: Train Loss: 0.32929278413454693, Validation Loss: 0.5272459983825684\n",
      "Epoch 161: Train Loss: 0.3376481036345164, Validation Loss: 0.5254288911819458\n",
      "Epoch 162: Train Loss: 0.32701948285102844, Validation Loss: 0.5210995674133301\n",
      "Epoch 163: Train Loss: 0.3339184323946635, Validation Loss: 0.5201581716537476\n",
      "Epoch 164: Train Loss: 0.3299408257007599, Validation Loss: 0.5208927989006042\n",
      "Epoch 165: Train Loss: 0.3316626350084941, Validation Loss: 0.522571325302124\n",
      "Epoch 166: Train Loss: 0.3304479817549388, Validation Loss: 0.5246384739875793\n",
      "Epoch 167: Train Loss: 0.32641656200091046, Validation Loss: 0.5247799754142761\n",
      "Epoch 168: Train Loss: 0.3273615936438243, Validation Loss: 0.5248992443084717\n",
      "Epoch 169: Train Loss: 0.3285494844118754, Validation Loss: 0.5249559879302979\n",
      "Epoch 170: Train Loss: 0.3390663762887319, Validation Loss: 0.5232093930244446\n",
      "Epoch 171: Train Loss: 0.3346625864505768, Validation Loss: 0.5202718377113342\n",
      "Epoch 172: Train Loss: 0.33624976873397827, Validation Loss: 0.5173865556716919\n",
      "Epoch 173: Train Loss: 0.33119410276412964, Validation Loss: 0.5143083333969116\n",
      "Epoch 174: Train Loss: 0.3220538794994354, Validation Loss: 0.5129378437995911\n",
      "Epoch 175: Train Loss: 0.33648186922073364, Validation Loss: 0.5128461718559265\n",
      "Epoch 176: Train Loss: 0.3323404788970947, Validation Loss: 0.5140339732170105\n",
      "Epoch 177: Train Loss: 0.3328065574169159, Validation Loss: 0.5148072838783264\n",
      "Epoch 178: Train Loss: 0.33093059062957764, Validation Loss: 0.515616774559021\n",
      "Epoch 179: Train Loss: 0.33733173211415607, Validation Loss: 0.5160619616508484\n",
      "Epoch 180: Train Loss: 0.34108826518058777, Validation Loss: 0.532329261302948\n",
      "Epoch 181: Train Loss: 0.3283191720644633, Validation Loss: 0.532267689704895\n",
      "Epoch 182: Train Loss: 0.3545110523700714, Validation Loss: 0.5267195105552673\n",
      "Epoch 183: Train Loss: 0.35203586022059125, Validation Loss: 0.5231174826622009\n",
      "Epoch 184: Train Loss: 0.3355875313282013, Validation Loss: 0.520370602607727\n",
      "Epoch 185: Train Loss: 0.3311578631401062, Validation Loss: 0.5212091207504272\n",
      "Epoch 186: Train Loss: 0.33306704958279926, Validation Loss: 0.5228220820426941\n",
      "Epoch 187: Train Loss: 0.32526344060897827, Validation Loss: 0.5245879292488098\n",
      "Epoch 188: Train Loss: 0.33233173688252765, Validation Loss: 0.5253442525863647\n",
      "Epoch 189: Train Loss: 0.33253613114356995, Validation Loss: 0.5256027579307556\n",
      "Epoch 190: Train Loss: 0.32911110917727154, Validation Loss: 0.5319697856903076\n",
      "Epoch 191: Train Loss: 0.32919108867645264, Validation Loss: 0.5330571532249451\n",
      "Epoch 192: Train Loss: 0.3345225950082143, Validation Loss: 0.5365040898323059\n",
      "Early stopping at epoch 193\n",
      "Fold 2\n",
      "Epoch 0: Train Loss: 0.6900848348935446, Validation Loss: 0.6919443607330322\n",
      "Epoch 1: Train Loss: 0.6894014477729797, Validation Loss: 0.690351128578186\n",
      "Epoch 2: Train Loss: 0.6705997188886007, Validation Loss: 0.6891610622406006\n",
      "Epoch 3: Train Loss: 0.6622494459152222, Validation Loss: 0.6897469162940979\n",
      "Epoch 4: Train Loss: 0.6401488184928894, Validation Loss: 0.6913790106773376\n",
      "Epoch 5: Train Loss: 0.64492267370224, Validation Loss: 0.6915844678878784\n",
      "Epoch 6: Train Loss: 0.6395651896794637, Validation Loss: 0.6918855309486389\n",
      "Epoch 7: Train Loss: 0.6395468513170878, Validation Loss: 0.6920473575592041\n",
      "Epoch 8: Train Loss: 0.633195956548055, Validation Loss: 0.6920163631439209\n",
      "Epoch 9: Train Loss: 0.6308713555335999, Validation Loss: 0.6919400095939636\n",
      "Epoch 10: Train Loss: 0.6255359848340353, Validation Loss: 0.6916839480400085\n",
      "Epoch 11: Train Loss: 0.6192541718482971, Validation Loss: 0.6864131093025208\n",
      "Epoch 12: Train Loss: 0.6090942819913229, Validation Loss: 0.6807736754417419\n",
      "Epoch 13: Train Loss: 0.6123532255490621, Validation Loss: 0.6760315299034119\n",
      "Epoch 14: Train Loss: 0.5891065200169882, Validation Loss: 0.6715661287307739\n",
      "Epoch 15: Train Loss: 0.5758920709292094, Validation Loss: 0.6688413023948669\n",
      "Epoch 16: Train Loss: 0.5872652133305868, Validation Loss: 0.6669078469276428\n",
      "Epoch 17: Train Loss: 0.5810344815254211, Validation Loss: 0.6661580801010132\n",
      "Epoch 18: Train Loss: 0.581545094648997, Validation Loss: 0.6654881238937378\n",
      "Epoch 19: Train Loss: 0.5580206314722697, Validation Loss: 0.6648566126823425\n",
      "Epoch 20: Train Loss: 0.5617924928665161, Validation Loss: 0.6623868346214294\n",
      "Epoch 21: Train Loss: 0.563006579875946, Validation Loss: 0.6576812863349915\n",
      "Epoch 22: Train Loss: 0.5263953804969788, Validation Loss: 0.6343463063240051\n",
      "Epoch 23: Train Loss: 0.5196489493052164, Validation Loss: 0.6181306838989258\n",
      "Epoch 24: Train Loss: 0.5133864680926005, Validation Loss: 0.6077753901481628\n",
      "Epoch 25: Train Loss: 0.4972655773162842, Validation Loss: 0.6005275845527649\n",
      "Epoch 26: Train Loss: 0.4981461564699809, Validation Loss: 0.596178412437439\n",
      "Epoch 27: Train Loss: 0.5059491395950317, Validation Loss: 0.5942603945732117\n",
      "Epoch 28: Train Loss: 0.5049506425857544, Validation Loss: 0.5928647518157959\n",
      "Epoch 29: Train Loss: 0.4912838935852051, Validation Loss: 0.5920486450195312\n",
      "Epoch 30: Train Loss: 0.4804563522338867, Validation Loss: 0.5907205939292908\n",
      "Epoch 31: Train Loss: 0.47865432500839233, Validation Loss: 0.5899519920349121\n",
      "Epoch 32: Train Loss: 0.4617041051387787, Validation Loss: 0.5870030522346497\n",
      "Epoch 33: Train Loss: 0.4677748382091522, Validation Loss: 0.5785805583000183\n",
      "Epoch 34: Train Loss: 0.43674663702646893, Validation Loss: 0.5674036741256714\n",
      "Epoch 35: Train Loss: 0.4617745776971181, Validation Loss: 0.561677098274231\n",
      "Epoch 36: Train Loss: 0.43912795186042786, Validation Loss: 0.5595415830612183\n",
      "Epoch 37: Train Loss: 0.4406294624010722, Validation Loss: 0.5577983260154724\n",
      "Epoch 38: Train Loss: 0.4593701461950938, Validation Loss: 0.5569295287132263\n",
      "Epoch 39: Train Loss: 0.43814929326375324, Validation Loss: 0.5565210580825806\n",
      "Epoch 40: Train Loss: 0.4325419068336487, Validation Loss: 0.556037425994873\n",
      "Epoch 41: Train Loss: 0.4396345516045888, Validation Loss: 0.5567959547042847\n",
      "Epoch 42: Train Loss: 0.4133349855740865, Validation Loss: 0.5540165305137634\n",
      "Epoch 43: Train Loss: 0.42470962802569073, Validation Loss: 0.5446591973304749\n",
      "Epoch 44: Train Loss: 0.41989148656527203, Validation Loss: 0.5419920086860657\n",
      "Epoch 45: Train Loss: 0.4071453809738159, Validation Loss: 0.5409387350082397\n",
      "Epoch 46: Train Loss: 0.4014163513978322, Validation Loss: 0.5387680530548096\n",
      "Epoch 47: Train Loss: 0.40843409299850464, Validation Loss: 0.5377426743507385\n",
      "Epoch 48: Train Loss: 0.41506849726041156, Validation Loss: 0.5371625423431396\n",
      "Epoch 49: Train Loss: 0.4060859680175781, Validation Loss: 0.536840558052063\n",
      "Epoch 50: Train Loss: 0.4090901513894399, Validation Loss: 0.5339652299880981\n",
      "Epoch 51: Train Loss: 0.39949046572049457, Validation Loss: 0.5330224633216858\n",
      "Epoch 52: Train Loss: 0.4109827975432078, Validation Loss: 0.5268247127532959\n",
      "Epoch 53: Train Loss: 0.4100164771080017, Validation Loss: 0.5249837636947632\n",
      "Epoch 54: Train Loss: 0.3937886953353882, Validation Loss: 0.5254782438278198\n",
      "Epoch 55: Train Loss: 0.3905347983042399, Validation Loss: 0.527096152305603\n",
      "Epoch 56: Train Loss: 0.39185161391894024, Validation Loss: 0.5282827019691467\n",
      "Epoch 57: Train Loss: 0.39241038759549457, Validation Loss: 0.528613269329071\n",
      "Epoch 58: Train Loss: 0.38981135686238605, Validation Loss: 0.5282971262931824\n",
      "Epoch 59: Train Loss: 0.39053123195966083, Validation Loss: 0.5280227661132812\n",
      "Epoch 60: Train Loss: 0.3806999921798706, Validation Loss: 0.5258934497833252\n",
      "Epoch 61: Train Loss: 0.38519324858983356, Validation Loss: 0.5230435729026794\n",
      "Epoch 62: Train Loss: 0.3980179826418559, Validation Loss: 0.5207257270812988\n",
      "Epoch 63: Train Loss: 0.3850507438182831, Validation Loss: 0.5181862711906433\n",
      "Epoch 64: Train Loss: 0.38794271151224774, Validation Loss: 0.5165948867797852\n",
      "Epoch 65: Train Loss: 0.37348271409670514, Validation Loss: 0.5158511996269226\n",
      "Epoch 66: Train Loss: 0.3846401075522105, Validation Loss: 0.5145127773284912\n",
      "Epoch 67: Train Loss: 0.3819020887215932, Validation Loss: 0.5140827894210815\n",
      "Epoch 68: Train Loss: 0.38415180643399555, Validation Loss: 0.5136023163795471\n",
      "Epoch 69: Train Loss: 0.37874388694763184, Validation Loss: 0.5132836699485779\n",
      "Epoch 70: Train Loss: 0.38670921325683594, Validation Loss: 0.5041682124137878\n",
      "Epoch 71: Train Loss: 0.37352800369262695, Validation Loss: 0.4962928891181946\n",
      "Epoch 72: Train Loss: 0.39033856987953186, Validation Loss: 0.5028024315834045\n",
      "Epoch 73: Train Loss: 0.39562012751897174, Validation Loss: 0.5100425481796265\n",
      "Epoch 74: Train Loss: 0.4041911760965983, Validation Loss: 0.5172534584999084\n",
      "Epoch 75: Train Loss: 0.3704703251520793, Validation Loss: 0.5236167311668396\n",
      "Epoch 76: Train Loss: 0.3712426821390788, Validation Loss: 0.5242475271224976\n",
      "Epoch 77: Train Loss: 0.38155536850293476, Validation Loss: 0.5226078033447266\n",
      "Epoch 78: Train Loss: 0.3690466384092967, Validation Loss: 0.5218237638473511\n",
      "Epoch 79: Train Loss: 0.3750768005847931, Validation Loss: 0.521532416343689\n",
      "Epoch 80: Train Loss: 0.3868154088656108, Validation Loss: 0.5078635215759277\n",
      "Epoch 81: Train Loss: 0.39469509323438007, Validation Loss: 0.4943505823612213\n",
      "Epoch 82: Train Loss: 0.3738008936246236, Validation Loss: 0.4896942377090454\n",
      "Epoch 83: Train Loss: 0.36473946770032245, Validation Loss: 0.5128734707832336\n",
      "Epoch 84: Train Loss: 0.3722543219725291, Validation Loss: 0.5266202688217163\n",
      "Epoch 85: Train Loss: 0.3732193112373352, Validation Loss: 0.529497504234314\n",
      "Epoch 86: Train Loss: 0.3714238107204437, Validation Loss: 0.5233113169670105\n",
      "Epoch 87: Train Loss: 0.3677472571531932, Validation Loss: 0.5177405476570129\n",
      "Epoch 88: Train Loss: 0.37246087193489075, Validation Loss: 0.5148265957832336\n",
      "Epoch 89: Train Loss: 0.36862900853157043, Validation Loss: 0.5135214328765869\n",
      "Epoch 90: Train Loss: 0.36488815148671466, Validation Loss: 0.5108858346939087\n",
      "Epoch 91: Train Loss: 0.3605382939179738, Validation Loss: 0.5144844055175781\n",
      "Epoch 92: Train Loss: 0.3714575668176015, Validation Loss: 0.5134910345077515\n",
      "Epoch 93: Train Loss: 0.3649765948454539, Validation Loss: 0.5188210606575012\n",
      "Epoch 94: Train Loss: 0.3598349293073018, Validation Loss: 0.5230671167373657\n",
      "Epoch 95: Train Loss: 0.3514167368412018, Validation Loss: 0.5252991318702698\n",
      "Epoch 96: Train Loss: 0.3683234751224518, Validation Loss: 0.527262270450592\n",
      "Epoch 97: Train Loss: 0.35274272163709003, Validation Loss: 0.5261569619178772\n",
      "Epoch 98: Train Loss: 0.36199985941251117, Validation Loss: 0.5240705013275146\n",
      "Epoch 99: Train Loss: 0.35626063744227093, Validation Loss: 0.5222195982933044\n",
      "Epoch 100: Train Loss: 0.3769424557685852, Validation Loss: 0.5254156589508057\n",
      "Epoch 101: Train Loss: 0.3529130717118581, Validation Loss: 0.5221735239028931\n",
      "Epoch 102: Train Loss: 0.3522775371869405, Validation Loss: 0.517419695854187\n",
      "Epoch 103: Train Loss: 0.3535596231619517, Validation Loss: 0.5142039656639099\n",
      "Epoch 104: Train Loss: 0.3559379279613495, Validation Loss: 0.5157799124717712\n",
      "Epoch 105: Train Loss: 0.400431493918101, Validation Loss: 0.5229031443595886\n",
      "Epoch 106: Train Loss: 0.3689609070618947, Validation Loss: 0.5229569673538208\n",
      "Epoch 107: Train Loss: 0.3520801067352295, Validation Loss: 0.520134449005127\n",
      "Epoch 108: Train Loss: 0.364200492699941, Validation Loss: 0.5184997320175171\n",
      "Epoch 109: Train Loss: 0.36528972784678143, Validation Loss: 0.5176435708999634\n",
      "Epoch 110: Train Loss: 0.3686733841896057, Validation Loss: 0.5341237187385559\n",
      "Epoch 111: Train Loss: 0.35316382845242816, Validation Loss: 0.5405119061470032\n",
      "Epoch 112: Train Loss: 0.3521583477656047, Validation Loss: 0.5471785068511963\n",
      "Epoch 113: Train Loss: 0.34835177659988403, Validation Loss: 0.5523664951324463\n",
      "Epoch 114: Train Loss: 0.34582658608754474, Validation Loss: 0.5552769303321838\n",
      "Epoch 115: Train Loss: 0.34606772661209106, Validation Loss: 0.5570082068443298\n",
      "Epoch 116: Train Loss: 0.34762773911158246, Validation Loss: 0.5578613877296448\n",
      "Epoch 117: Train Loss: 0.3501817584037781, Validation Loss: 0.5582877397537231\n",
      "Epoch 118: Train Loss: 0.34297794103622437, Validation Loss: 0.5581587553024292\n",
      "Epoch 119: Train Loss: 0.35344059268633526, Validation Loss: 0.5578310489654541\n",
      "Epoch 120: Train Loss: 0.3457796573638916, Validation Loss: 0.5594601631164551\n",
      "Epoch 121: Train Loss: 0.35078126192092896, Validation Loss: 0.5636079907417297\n",
      "Epoch 122: Train Loss: 0.34563636779785156, Validation Loss: 0.5653498768806458\n",
      "Epoch 123: Train Loss: 0.34614689151446026, Validation Loss: 0.5650568604469299\n",
      "Epoch 124: Train Loss: 0.36810463666915894, Validation Loss: 0.5671824812889099\n",
      "Epoch 125: Train Loss: 0.34374986092249554, Validation Loss: 0.5676490664482117\n",
      "Epoch 126: Train Loss: 0.34823044141133624, Validation Loss: 0.565639317035675\n",
      "Epoch 127: Train Loss: 0.34970908363660175, Validation Loss: 0.564603865146637\n",
      "Epoch 128: Train Loss: 0.35971402128537494, Validation Loss: 0.5637240409851074\n",
      "Epoch 129: Train Loss: 0.3448082109292348, Validation Loss: 0.563223123550415\n",
      "Epoch 130: Train Loss: 0.34815700848897296, Validation Loss: 0.5513018369674683\n",
      "Epoch 131: Train Loss: 0.3484075168768565, Validation Loss: 0.5421490669250488\n",
      "Epoch 132: Train Loss: 0.3465714653333028, Validation Loss: 0.5250610709190369\n",
      "Epoch 133: Train Loss: 0.34711114565531415, Validation Loss: 0.4911556839942932\n",
      "Epoch 134: Train Loss: 0.3405802547931671, Validation Loss: 0.49204221367836\n",
      "Epoch 135: Train Loss: 0.33892138799031574, Validation Loss: 0.49832916259765625\n",
      "Epoch 136: Train Loss: 0.34132738908131915, Validation Loss: 0.5050345063209534\n",
      "Epoch 137: Train Loss: 0.34060707688331604, Validation Loss: 0.5069786906242371\n",
      "Epoch 138: Train Loss: 0.35273686051368713, Validation Loss: 0.5085663199424744\n",
      "Epoch 139: Train Loss: 0.3457786540190379, Validation Loss: 0.5085455775260925\n",
      "Epoch 140: Train Loss: 0.3573160668214162, Validation Loss: 0.5014438033103943\n",
      "Epoch 141: Train Loss: 0.35876135031382245, Validation Loss: 0.5173325538635254\n",
      "Epoch 142: Train Loss: 0.34788934389750165, Validation Loss: 0.5083411931991577\n",
      "Epoch 143: Train Loss: 0.35747790336608887, Validation Loss: 0.5034805536270142\n",
      "Epoch 144: Train Loss: 0.35164909561475116, Validation Loss: 0.5060226321220398\n",
      "Epoch 145: Train Loss: 0.34927960236867267, Validation Loss: 0.5061808228492737\n",
      "Epoch 146: Train Loss: 0.3402015467484792, Validation Loss: 0.5073185563087463\n",
      "Epoch 147: Train Loss: 0.3496117889881134, Validation Loss: 0.5062110424041748\n",
      "Epoch 148: Train Loss: 0.3587077558040619, Validation Loss: 0.5052490830421448\n",
      "Epoch 149: Train Loss: 0.3480637272198995, Validation Loss: 0.505101203918457\n",
      "Epoch 150: Train Loss: 0.3455415765444438, Validation Loss: 0.49025455117225647\n",
      "Epoch 151: Train Loss: 0.3521323303381602, Validation Loss: 0.48932671546936035\n",
      "Epoch 152: Train Loss: 0.34896165132522583, Validation Loss: 0.4993804693222046\n",
      "Epoch 153: Train Loss: 0.3479499618212382, Validation Loss: 0.5116549730300903\n",
      "Epoch 154: Train Loss: 0.34268466631571454, Validation Loss: 0.5176635384559631\n",
      "Epoch 155: Train Loss: 0.33983490864435834, Validation Loss: 0.5208819508552551\n",
      "Epoch 156: Train Loss: 0.34842302401860553, Validation Loss: 0.5204684138298035\n",
      "Epoch 157: Train Loss: 0.3491335113843282, Validation Loss: 0.5172420144081116\n",
      "Epoch 158: Train Loss: 0.3484693964322408, Validation Loss: 0.5152477622032166\n",
      "Epoch 159: Train Loss: 0.3425161341826121, Validation Loss: 0.5152922868728638\n",
      "Epoch 160: Train Loss: 0.3533405363559723, Validation Loss: 0.5150439739227295\n",
      "Epoch 161: Train Loss: 0.341130663951238, Validation Loss: 0.5263418555259705\n",
      "Epoch 162: Train Loss: 0.34111477931340534, Validation Loss: 0.5324726700782776\n",
      "Epoch 163: Train Loss: 0.3704001307487488, Validation Loss: 0.5246416330337524\n",
      "Epoch 164: Train Loss: 0.3406563897927602, Validation Loss: 0.5175753831863403\n",
      "Epoch 165: Train Loss: 0.3385698199272156, Validation Loss: 0.5152522921562195\n",
      "Epoch 166: Train Loss: 0.3467535674571991, Validation Loss: 0.519507110118866\n",
      "Epoch 167: Train Loss: 0.33321113387743634, Validation Loss: 0.5222530961036682\n",
      "Epoch 168: Train Loss: 0.338083416223526, Validation Loss: 0.5241462588310242\n",
      "Epoch 169: Train Loss: 0.33973480264345807, Validation Loss: 0.5249841809272766\n",
      "Epoch 170: Train Loss: 0.34201324979464215, Validation Loss: 0.5390560626983643\n",
      "Epoch 171: Train Loss: 0.351497620344162, Validation Loss: 0.5517672896385193\n",
      "Epoch 172: Train Loss: 0.3407447338104248, Validation Loss: 0.5537818074226379\n",
      "Epoch 173: Train Loss: 0.3363405764102936, Validation Loss: 0.5519477725028992\n",
      "Epoch 174: Train Loss: 0.34728850920995075, Validation Loss: 0.5434747338294983\n",
      "Epoch 175: Train Loss: 0.33820220828056335, Validation Loss: 0.5402784943580627\n",
      "Epoch 176: Train Loss: 0.33738115429878235, Validation Loss: 0.5383303761482239\n",
      "Epoch 177: Train Loss: 0.33924450476964313, Validation Loss: 0.5384458303451538\n",
      "Epoch 178: Train Loss: 0.3377384344736735, Validation Loss: 0.5394259095191956\n",
      "Epoch 179: Train Loss: 0.34627145528793335, Validation Loss: 0.5396867990493774\n",
      "Epoch 180: Train Loss: 0.3374633689721425, Validation Loss: 0.552617609500885\n",
      "Epoch 181: Train Loss: 0.3371172547340393, Validation Loss: 0.5614827275276184\n",
      "Epoch 182: Train Loss: 0.34098819891611737, Validation Loss: 0.5615776181221008\n",
      "Epoch 183: Train Loss: 0.3495163818200429, Validation Loss: 0.5537431836128235\n",
      "Epoch 184: Train Loss: 0.3353978097438812, Validation Loss: 0.551880955696106\n",
      "Epoch 185: Train Loss: 0.3455853760242462, Validation Loss: 0.5531318187713623\n",
      "Epoch 186: Train Loss: 0.3356097340583801, Validation Loss: 0.5547452569007874\n",
      "Epoch 187: Train Loss: 0.3465884029865265, Validation Loss: 0.5540047287940979\n",
      "Epoch 188: Train Loss: 0.34410811463991803, Validation Loss: 0.5532893538475037\n",
      "Epoch 189: Train Loss: 0.36546241243680316, Validation Loss: 0.5525829792022705\n",
      "Epoch 190: Train Loss: 0.33565081159273785, Validation Loss: 0.5406824946403503\n",
      "Epoch 191: Train Loss: 0.3369944890340169, Validation Loss: 0.5489276647567749\n",
      "Epoch 192: Train Loss: 0.3522821366786957, Validation Loss: 0.5421890616416931\n",
      "Epoch 193: Train Loss: 0.3351038098335266, Validation Loss: 0.5425166487693787\n",
      "Epoch 194: Train Loss: 0.33977969487508136, Validation Loss: 0.5514069199562073\n",
      "Epoch 195: Train Loss: 0.33970242738723755, Validation Loss: 0.5479453802108765\n",
      "Epoch 196: Train Loss: 0.3443622887134552, Validation Loss: 0.5470104217529297\n",
      "Epoch 197: Train Loss: 0.34428099791208905, Validation Loss: 0.548385500907898\n",
      "Epoch 198: Train Loss: 0.33747806151707965, Validation Loss: 0.5489920377731323\n",
      "Epoch 199: Train Loss: 0.3363100389639537, Validation Loss: 0.5488701462745667\n",
      "Epoch 200: Train Loss: 0.33738527695337933, Validation Loss: 0.5223551392555237\n",
      "Epoch 201: Train Loss: 0.3372547725836436, Validation Loss: 0.5179539322853088\n",
      "Epoch 202: Train Loss: 0.3377648890018463, Validation Loss: 0.5144448280334473\n",
      "Epoch 203: Train Loss: 0.3396662473678589, Validation Loss: 0.5194491744041443\n",
      "Epoch 204: Train Loss: 0.34113919734954834, Validation Loss: 0.5213704109191895\n",
      "Epoch 205: Train Loss: 0.3323436776796977, Validation Loss: 0.5247257351875305\n",
      "Epoch 206: Train Loss: 0.33939364552497864, Validation Loss: 0.5292665958404541\n",
      "Epoch 207: Train Loss: 0.33712631464004517, Validation Loss: 0.5307962894439697\n",
      "Epoch 208: Train Loss: 0.33135224382082623, Validation Loss: 0.5306552052497864\n",
      "Epoch 209: Train Loss: 0.33356840411822003, Validation Loss: 0.5303636193275452\n",
      "Epoch 210: Train Loss: 0.33113570014635724, Validation Loss: 0.5243648290634155\n",
      "Epoch 211: Train Loss: 0.3469453454017639, Validation Loss: 0.5235591530799866\n",
      "Epoch 212: Train Loss: 0.3491256833076477, Validation Loss: 0.5193325281143188\n",
      "Epoch 213: Train Loss: 0.33635759353637695, Validation Loss: 0.5263919234275818\n",
      "Epoch 214: Train Loss: 0.3379454016685486, Validation Loss: 0.5395824909210205\n",
      "Epoch 215: Train Loss: 0.3392014503479004, Validation Loss: 0.5452994704246521\n",
      "Epoch 216: Train Loss: 0.33184624711672467, Validation Loss: 0.5428943037986755\n",
      "Epoch 217: Train Loss: 0.34587297836939496, Validation Loss: 0.5429682731628418\n",
      "Epoch 218: Train Loss: 0.3366280794143677, Validation Loss: 0.5421090126037598\n",
      "Epoch 219: Train Loss: 0.3340868055820465, Validation Loss: 0.5411801934242249\n",
      "Epoch 220: Train Loss: 0.3466203212738037, Validation Loss: 0.5532661080360413\n",
      "Epoch 221: Train Loss: 0.3425053258736928, Validation Loss: 0.5552944540977478\n",
      "Epoch 222: Train Loss: 0.3379820982615153, Validation Loss: 0.5590537190437317\n",
      "Epoch 223: Train Loss: 0.34735995531082153, Validation Loss: 0.5410830974578857\n",
      "Epoch 224: Train Loss: 0.3351660470167796, Validation Loss: 0.5405001044273376\n",
      "Epoch 225: Train Loss: 0.3378753463427226, Validation Loss: 0.5434104204177856\n",
      "Epoch 226: Train Loss: 0.34040310978889465, Validation Loss: 0.543583869934082\n",
      "Epoch 227: Train Loss: 0.33827757835388184, Validation Loss: 0.5414477586746216\n",
      "Epoch 228: Train Loss: 0.3368755280971527, Validation Loss: 0.5403602719306946\n",
      "Epoch 229: Train Loss: 0.3305352032184601, Validation Loss: 0.5400030016899109\n",
      "Epoch 230: Train Loss: 0.3441872199376424, Validation Loss: 0.544045090675354\n",
      "Epoch 231: Train Loss: 0.33614341417948407, Validation Loss: 0.5635693669319153\n",
      "Epoch 232: Train Loss: 0.3352419237295787, Validation Loss: 0.5679258704185486\n",
      "Epoch 233: Train Loss: 0.3572157621383667, Validation Loss: 0.5676004886627197\n",
      "Epoch 234: Train Loss: 0.33414355913798016, Validation Loss: 0.5714054703712463\n",
      "Epoch 235: Train Loss: 0.3387563129266103, Validation Loss: 0.5781784653663635\n",
      "Epoch 236: Train Loss: 0.3323675294717153, Validation Loss: 0.5814582705497742\n",
      "Epoch 237: Train Loss: 0.33571067452430725, Validation Loss: 0.5820625424385071\n",
      "Epoch 238: Train Loss: 0.3380583127339681, Validation Loss: 0.5818659067153931\n",
      "Epoch 239: Train Loss: 0.33703746398289997, Validation Loss: 0.5817577242851257\n",
      "Epoch 240: Train Loss: 0.3350534737110138, Validation Loss: 0.5736554265022278\n",
      "Epoch 241: Train Loss: 0.3371345102787018, Validation Loss: 0.5664247870445251\n",
      "Epoch 242: Train Loss: 0.34858205914497375, Validation Loss: 0.5601391792297363\n",
      "Epoch 243: Train Loss: 0.33011099696159363, Validation Loss: 0.557170033454895\n",
      "Epoch 244: Train Loss: 0.33592191338539124, Validation Loss: 0.5553654432296753\n",
      "Epoch 245: Train Loss: 0.33673928181330365, Validation Loss: 0.5560112595558167\n",
      "Epoch 246: Train Loss: 0.33526421586672467, Validation Loss: 0.5561830997467041\n",
      "Epoch 247: Train Loss: 0.3317798972129822, Validation Loss: 0.5568320155143738\n",
      "Epoch 248: Train Loss: 0.33371224999427795, Validation Loss: 0.557316243648529\n",
      "Epoch 249: Train Loss: 0.3459370533625285, Validation Loss: 0.5574471354484558\n",
      "Epoch 250: Train Loss: 0.33441614111264545, Validation Loss: 0.5674677491188049\n",
      "Early stopping at epoch 251\n",
      "Fold 3\n",
      "Epoch 0: Train Loss: 0.6860458850860596, Validation Loss: 0.691063642501831\n",
      "Epoch 1: Train Loss: 0.6651859283447266, Validation Loss: 0.6887597441673279\n",
      "Epoch 2: Train Loss: 0.6466352939605713, Validation Loss: 0.6860558390617371\n",
      "Epoch 3: Train Loss: 0.634958545366923, Validation Loss: 0.6835993528366089\n",
      "Epoch 4: Train Loss: 0.6175851424535116, Validation Loss: 0.6813668608665466\n",
      "Epoch 5: Train Loss: 0.6295002500216166, Validation Loss: 0.6793343424797058\n",
      "Epoch 6: Train Loss: 0.6185034116109213, Validation Loss: 0.6779152750968933\n",
      "Epoch 7: Train Loss: 0.6090679367383321, Validation Loss: 0.6769078373908997\n",
      "Epoch 8: Train Loss: 0.6078898111979166, Validation Loss: 0.6762437224388123\n",
      "Epoch 9: Train Loss: 0.6049234668413798, Validation Loss: 0.6758071184158325\n",
      "Epoch 10: Train Loss: 0.6140460968017578, Validation Loss: 0.6719421744346619\n",
      "Epoch 11: Train Loss: 0.6061163147290548, Validation Loss: 0.6682869791984558\n",
      "Epoch 12: Train Loss: 0.5893295804659525, Validation Loss: 0.6649158596992493\n",
      "Epoch 13: Train Loss: 0.5738289554913839, Validation Loss: 0.6617141962051392\n",
      "Epoch 14: Train Loss: 0.5679914355278015, Validation Loss: 0.6586964130401611\n",
      "Epoch 15: Train Loss: 0.5641893744468689, Validation Loss: 0.6557149887084961\n",
      "Epoch 16: Train Loss: 0.6081759333610535, Validation Loss: 0.6534212827682495\n",
      "Epoch 17: Train Loss: 0.5651160081227621, Validation Loss: 0.6518314480781555\n",
      "Epoch 18: Train Loss: 0.562346746524175, Validation Loss: 0.6507083773612976\n",
      "Epoch 19: Train Loss: 0.5729320645332336, Validation Loss: 0.6499151587486267\n",
      "Epoch 20: Train Loss: 0.5595248738924662, Validation Loss: 0.64412921667099\n",
      "Epoch 21: Train Loss: 0.5572318037350973, Validation Loss: 0.6380293965339661\n",
      "Epoch 22: Train Loss: 0.5344565113385519, Validation Loss: 0.6320959329605103\n",
      "Epoch 23: Train Loss: 0.5338091055552164, Validation Loss: 0.6253954172134399\n",
      "Epoch 24: Train Loss: 0.5187664031982422, Validation Loss: 0.6196788549423218\n",
      "Epoch 25: Train Loss: 0.5041598180929819, Validation Loss: 0.6155078411102295\n",
      "Epoch 26: Train Loss: 0.5131112734476725, Validation Loss: 0.6123459935188293\n",
      "Epoch 27: Train Loss: 0.5062102675437927, Validation Loss: 0.6102186441421509\n",
      "Epoch 28: Train Loss: 0.49921103318532306, Validation Loss: 0.6088531017303467\n",
      "Epoch 29: Train Loss: 0.5067174633344015, Validation Loss: 0.6079263687133789\n",
      "Epoch 30: Train Loss: 0.4774661958217621, Validation Loss: 0.6016772389411926\n",
      "Epoch 31: Train Loss: 0.4830406606197357, Validation Loss: 0.5956389307975769\n",
      "Epoch 32: Train Loss: 0.46767401695251465, Validation Loss: 0.5888453722000122\n",
      "Epoch 33: Train Loss: 0.4607335726420085, Validation Loss: 0.5844560861587524\n",
      "Epoch 34: Train Loss: 0.4534824689229329, Validation Loss: 0.5814265608787537\n",
      "Epoch 35: Train Loss: 0.4533547858397166, Validation Loss: 0.5781244039535522\n",
      "Epoch 36: Train Loss: 0.44900689522425336, Validation Loss: 0.5767039656639099\n",
      "Epoch 37: Train Loss: 0.45013412833213806, Validation Loss: 0.575805127620697\n",
      "Epoch 38: Train Loss: 0.4562465449174245, Validation Loss: 0.5751479268074036\n",
      "Epoch 39: Train Loss: 0.44457439581553143, Validation Loss: 0.5746415853500366\n",
      "Epoch 40: Train Loss: 0.4456150531768799, Validation Loss: 0.5724303126335144\n",
      "Epoch 41: Train Loss: 0.4390191932519277, Validation Loss: 0.5711902976036072\n",
      "Epoch 42: Train Loss: 0.4335498611132304, Validation Loss: 0.5678532123565674\n",
      "Epoch 43: Train Loss: 0.438467135032018, Validation Loss: 0.56516432762146\n",
      "Epoch 44: Train Loss: 0.44542155663172406, Validation Loss: 0.5626828670501709\n",
      "Epoch 45: Train Loss: 0.42081265648206073, Validation Loss: 0.5601414442062378\n",
      "Epoch 46: Train Loss: 0.41346585750579834, Validation Loss: 0.5592334866523743\n",
      "Epoch 47: Train Loss: 0.4291984836260478, Validation Loss: 0.558881402015686\n",
      "Epoch 48: Train Loss: 0.4219654401143392, Validation Loss: 0.5587582588195801\n",
      "Epoch 49: Train Loss: 0.4269780417283376, Validation Loss: 0.5586000084877014\n",
      "Epoch 50: Train Loss: 0.4152880609035492, Validation Loss: 0.5604560971260071\n",
      "Epoch 51: Train Loss: 0.4056686758995056, Validation Loss: 0.5599427223205566\n",
      "Epoch 52: Train Loss: 0.4078372021516164, Validation Loss: 0.561771035194397\n",
      "Epoch 53: Train Loss: 0.40868910153706867, Validation Loss: 0.5624087452888489\n",
      "Epoch 54: Train Loss: 0.40729599197705585, Validation Loss: 0.5617923140525818\n",
      "Epoch 55: Train Loss: 0.40831998984018963, Validation Loss: 0.5599241256713867\n",
      "Epoch 56: Train Loss: 0.3956001003583272, Validation Loss: 0.5595129728317261\n",
      "Epoch 57: Train Loss: 0.40861014525095624, Validation Loss: 0.5582756400108337\n",
      "Epoch 58: Train Loss: 0.39453572034835815, Validation Loss: 0.5578317642211914\n",
      "Epoch 59: Train Loss: 0.39998047550519306, Validation Loss: 0.5578079223632812\n",
      "Epoch 60: Train Loss: 0.38571550448735553, Validation Loss: 0.5527212023735046\n",
      "Epoch 61: Train Loss: 0.38944361607233685, Validation Loss: 0.5504062175750732\n",
      "Epoch 62: Train Loss: 0.3878728151321411, Validation Loss: 0.5555133819580078\n",
      "Epoch 63: Train Loss: 0.368379145860672, Validation Loss: 0.5592558979988098\n",
      "Epoch 64: Train Loss: 0.3769497076670329, Validation Loss: 0.5630707740783691\n",
      "Epoch 65: Train Loss: 0.3788832227389018, Validation Loss: 0.5662945508956909\n",
      "Epoch 66: Train Loss: 0.36528391639391583, Validation Loss: 0.5687388181686401\n",
      "Epoch 67: Train Loss: 0.38330061237017315, Validation Loss: 0.5694172978401184\n",
      "Epoch 68: Train Loss: 0.3744383752346039, Validation Loss: 0.569678783416748\n",
      "Epoch 69: Train Loss: 0.39637620250384015, Validation Loss: 0.569734513759613\n",
      "Epoch 70: Train Loss: 0.3737682104110718, Validation Loss: 0.5712748765945435\n",
      "Epoch 71: Train Loss: 0.3806087374687195, Validation Loss: 0.5708529353141785\n",
      "Epoch 72: Train Loss: 0.3814584016799927, Validation Loss: 0.5735747814178467\n",
      "Epoch 73: Train Loss: 0.3660098810990651, Validation Loss: 0.5715193152427673\n",
      "Epoch 74: Train Loss: 0.3540392021338145, Validation Loss: 0.5702090263366699\n",
      "Epoch 75: Train Loss: 0.36558036009470624, Validation Loss: 0.5673496723175049\n",
      "Epoch 76: Train Loss: 0.3535911540190379, Validation Loss: 0.5664806962013245\n",
      "Epoch 77: Train Loss: 0.36549808581670123, Validation Loss: 0.5639203786849976\n",
      "Epoch 78: Train Loss: 0.3656497001647949, Validation Loss: 0.5628384947776794\n",
      "Epoch 79: Train Loss: 0.3674251139163971, Validation Loss: 0.5624542236328125\n",
      "Epoch 80: Train Loss: 0.3550735414028168, Validation Loss: 0.5510265827178955\n",
      "Epoch 81: Train Loss: 0.3547649880250295, Validation Loss: 0.5365174412727356\n",
      "Epoch 82: Train Loss: 0.37256405750910443, Validation Loss: 0.528338611125946\n",
      "Epoch 83: Train Loss: 0.3535913328329722, Validation Loss: 0.5327387452125549\n",
      "Epoch 84: Train Loss: 0.35437487562497455, Validation Loss: 0.5432778000831604\n",
      "Epoch 85: Train Loss: 0.3519913951555888, Validation Loss: 0.5566556453704834\n",
      "Epoch 86: Train Loss: 0.3692619999249776, Validation Loss: 0.561644434928894\n",
      "Epoch 87: Train Loss: 0.3527076145013173, Validation Loss: 0.5622497200965881\n",
      "Epoch 88: Train Loss: 0.3515877624352773, Validation Loss: 0.5632990598678589\n",
      "Epoch 89: Train Loss: 0.34708237648010254, Validation Loss: 0.5632299780845642\n",
      "Epoch 90: Train Loss: 0.37898972630500793, Validation Loss: 0.5539513826370239\n",
      "Epoch 91: Train Loss: 0.35732971628506977, Validation Loss: 0.5486157536506653\n",
      "Epoch 92: Train Loss: 0.35711811979611713, Validation Loss: 0.5478034019470215\n",
      "Epoch 93: Train Loss: 0.35406158367792767, Validation Loss: 0.5428271889686584\n",
      "Epoch 94: Train Loss: 0.3489966889222463, Validation Loss: 0.5396643280982971\n",
      "Epoch 95: Train Loss: 0.3768211205800374, Validation Loss: 0.537544846534729\n",
      "Epoch 96: Train Loss: 0.3727295696735382, Validation Loss: 0.5330604910850525\n",
      "Epoch 97: Train Loss: 0.34695332249005634, Validation Loss: 0.531000018119812\n",
      "Epoch 98: Train Loss: 0.3449496626853943, Validation Loss: 0.5306439995765686\n",
      "Epoch 99: Train Loss: 0.34937676787376404, Validation Loss: 0.5307180285453796\n",
      "Epoch 100: Train Loss: 0.3616599937280019, Validation Loss: 0.538653552532196\n",
      "Epoch 101: Train Loss: 0.3440732459227244, Validation Loss: 0.5461622476577759\n",
      "Epoch 102: Train Loss: 0.35999974608421326, Validation Loss: 0.5593671202659607\n",
      "Epoch 103: Train Loss: 0.3440511425336202, Validation Loss: 0.5708832144737244\n",
      "Epoch 104: Train Loss: 0.3460048536459605, Validation Loss: 0.5698766112327576\n",
      "Epoch 105: Train Loss: 0.33547699451446533, Validation Loss: 0.5706882476806641\n",
      "Epoch 106: Train Loss: 0.3373939593633016, Validation Loss: 0.5721521973609924\n",
      "Epoch 107: Train Loss: 0.342415452003479, Validation Loss: 0.5728200674057007\n",
      "Epoch 108: Train Loss: 0.35449857513109845, Validation Loss: 0.5718533396720886\n",
      "Epoch 109: Train Loss: 0.3415612777074178, Validation Loss: 0.571262001991272\n",
      "Epoch 110: Train Loss: 0.3375999132792155, Validation Loss: 0.5551022887229919\n",
      "Epoch 111: Train Loss: 0.3433455427487691, Validation Loss: 0.5466986894607544\n",
      "Epoch 112: Train Loss: 0.3370450834433238, Validation Loss: 0.5410481691360474\n",
      "Epoch 113: Train Loss: 0.3369736075401306, Validation Loss: 0.5393481850624084\n",
      "Epoch 114: Train Loss: 0.34182777007420856, Validation Loss: 0.5398277640342712\n",
      "Epoch 115: Train Loss: 0.34155113498369855, Validation Loss: 0.5406360030174255\n",
      "Epoch 116: Train Loss: 0.3369010090827942, Validation Loss: 0.5428001880645752\n",
      "Epoch 117: Train Loss: 0.3403063615163167, Validation Loss: 0.5429236888885498\n",
      "Epoch 118: Train Loss: 0.34008513887723285, Validation Loss: 0.5433344841003418\n",
      "Epoch 119: Train Loss: 0.33147333065668744, Validation Loss: 0.5433261394500732\n",
      "Epoch 120: Train Loss: 0.3321075737476349, Validation Loss: 0.5529158115386963\n",
      "Epoch 121: Train Loss: 0.33541446924209595, Validation Loss: 0.5577245354652405\n",
      "Epoch 122: Train Loss: 0.35372063517570496, Validation Loss: 0.5678837895393372\n",
      "Epoch 123: Train Loss: 0.3377167483170827, Validation Loss: 0.5710260272026062\n",
      "Epoch 124: Train Loss: 0.34379364053408307, Validation Loss: 0.5678216218948364\n",
      "Epoch 125: Train Loss: 0.34456270933151245, Validation Loss: 0.5632298588752747\n",
      "Epoch 126: Train Loss: 0.33697837591171265, Validation Loss: 0.5614184141159058\n",
      "Epoch 127: Train Loss: 0.33996881047884625, Validation Loss: 0.5599067807197571\n",
      "Epoch 128: Train Loss: 0.34873291850090027, Validation Loss: 0.559505045413971\n",
      "Epoch 129: Train Loss: 0.3392361104488373, Validation Loss: 0.5591526627540588\n",
      "Epoch 130: Train Loss: 0.3545645574728648, Validation Loss: 0.5676635503768921\n",
      "Epoch 131: Train Loss: 0.34063008427619934, Validation Loss: 0.5724769830703735\n",
      "Epoch 132: Train Loss: 0.33974939584732056, Validation Loss: 0.5808122158050537\n",
      "Epoch 133: Train Loss: 0.3630517025788625, Validation Loss: 0.5734707117080688\n",
      "Epoch 134: Train Loss: 0.33358775575955707, Validation Loss: 0.5748167037963867\n",
      "Epoch 135: Train Loss: 0.33848021427790326, Validation Loss: 0.5795959234237671\n",
      "Epoch 136: Train Loss: 0.34104761481285095, Validation Loss: 0.5817882418632507\n",
      "Epoch 137: Train Loss: 0.3401625653107961, Validation Loss: 0.5794311165809631\n",
      "Epoch 138: Train Loss: 0.3341364562511444, Validation Loss: 0.5772284865379333\n",
      "Epoch 139: Train Loss: 0.33270612359046936, Validation Loss: 0.5761772990226746\n",
      "Epoch 140: Train Loss: 0.336293230454127, Validation Loss: 0.5649279952049255\n",
      "Epoch 141: Train Loss: 0.34152306119600934, Validation Loss: 0.5618943572044373\n",
      "Epoch 142: Train Loss: 0.33642106254895526, Validation Loss: 0.5688800811767578\n",
      "Epoch 143: Train Loss: 0.3289201358954112, Validation Loss: 0.575747549533844\n",
      "Epoch 144: Train Loss: 0.33748969435691833, Validation Loss: 0.5801629424095154\n",
      "Epoch 145: Train Loss: 0.33092427253723145, Validation Loss: 0.5805608630180359\n",
      "Epoch 146: Train Loss: 0.3300669590632121, Validation Loss: 0.5805758237838745\n",
      "Epoch 147: Train Loss: 0.3299753765265147, Validation Loss: 0.5802239775657654\n",
      "Epoch 148: Train Loss: 0.339544137318929, Validation Loss: 0.5792639255523682\n",
      "Epoch 149: Train Loss: 0.33069145679473877, Validation Loss: 0.5785635709762573\n",
      "Epoch 150: Train Loss: 0.3495211700598399, Validation Loss: 0.5716161131858826\n",
      "Epoch 151: Train Loss: 0.3330873449643453, Validation Loss: 0.5684443712234497\n",
      "Epoch 152: Train Loss: 0.3351599872112274, Validation Loss: 0.5691133737564087\n",
      "Epoch 153: Train Loss: 0.33092079559961957, Validation Loss: 0.5728790760040283\n",
      "Epoch 154: Train Loss: 0.35452083746592206, Validation Loss: 0.5606409311294556\n",
      "Epoch 155: Train Loss: 0.32931390404701233, Validation Loss: 0.5529075860977173\n",
      "Epoch 156: Train Loss: 0.3326736092567444, Validation Loss: 0.5484442710876465\n",
      "Epoch 157: Train Loss: 0.3374966283639272, Validation Loss: 0.5472043752670288\n",
      "Epoch 158: Train Loss: 0.3393235703309377, Validation Loss: 0.5460734963417053\n",
      "Epoch 159: Train Loss: 0.3324236770470937, Validation Loss: 0.5459067225456238\n",
      "Epoch 160: Train Loss: 0.3352380891640981, Validation Loss: 0.539375901222229\n",
      "Epoch 161: Train Loss: 0.33306098977724713, Validation Loss: 0.5413012504577637\n",
      "Epoch 162: Train Loss: 0.3340785602728526, Validation Loss: 0.542367160320282\n",
      "Epoch 163: Train Loss: 0.33133111397425336, Validation Loss: 0.5449686050415039\n",
      "Epoch 164: Train Loss: 0.337785671154658, Validation Loss: 0.5470672845840454\n",
      "Epoch 165: Train Loss: 0.3359440267086029, Validation Loss: 0.5481277108192444\n",
      "Epoch 166: Train Loss: 0.3317132592201233, Validation Loss: 0.5487744808197021\n",
      "Epoch 167: Train Loss: 0.3323085407416026, Validation Loss: 0.5498629212379456\n",
      "Epoch 168: Train Loss: 0.33783825238545734, Validation Loss: 0.5501161217689514\n",
      "Epoch 169: Train Loss: 0.3332742154598236, Validation Loss: 0.549927830696106\n",
      "Epoch 170: Train Loss: 0.33007139960924786, Validation Loss: 0.5546508431434631\n",
      "Epoch 171: Train Loss: 0.33370786905288696, Validation Loss: 0.5526260137557983\n",
      "Epoch 172: Train Loss: 0.3284544547398885, Validation Loss: 0.5538606643676758\n",
      "Epoch 173: Train Loss: 0.32872869571050006, Validation Loss: 0.5609109401702881\n",
      "Epoch 174: Train Loss: 0.3302575846513112, Validation Loss: 0.5663398504257202\n",
      "Epoch 175: Train Loss: 0.3339666823546092, Validation Loss: 0.567927896976471\n",
      "Epoch 176: Train Loss: 0.3341720402240753, Validation Loss: 0.568135142326355\n",
      "Epoch 177: Train Loss: 0.3318970700105031, Validation Loss: 0.5678830742835999\n",
      "Epoch 178: Train Loss: 0.33073801795641583, Validation Loss: 0.5675415992736816\n",
      "Epoch 179: Train Loss: 0.33153100808461505, Validation Loss: 0.5670480728149414\n",
      "Epoch 180: Train Loss: 0.34199469288190204, Validation Loss: 0.5487889647483826\n",
      "Epoch 181: Train Loss: 0.334799587726593, Validation Loss: 0.537355899810791\n",
      "Early stopping at epoch 182\n",
      "Fold 4\n",
      "Epoch 0: Train Loss: 0.7001885374387106, Validation Loss: 0.6951214075088501\n",
      "Epoch 1: Train Loss: 0.685475488503774, Validation Loss: 0.6938846707344055\n",
      "Epoch 2: Train Loss: 0.6845954259236654, Validation Loss: 0.6925007700920105\n",
      "Epoch 3: Train Loss: 0.6513129870096842, Validation Loss: 0.6911431550979614\n",
      "Epoch 4: Train Loss: 0.656739870707194, Validation Loss: 0.690019428730011\n",
      "Epoch 5: Train Loss: 0.6473962267239889, Validation Loss: 0.6889550685882568\n",
      "Epoch 6: Train Loss: 0.627900222937266, Validation Loss: 0.6882064938545227\n",
      "Epoch 7: Train Loss: 0.6223044395446777, Validation Loss: 0.687818169593811\n",
      "Epoch 8: Train Loss: 0.6119583447774252, Validation Loss: 0.687650740146637\n",
      "Epoch 9: Train Loss: 0.6223591168721517, Validation Loss: 0.6876088976860046\n",
      "Epoch 10: Train Loss: 0.6043015122413635, Validation Loss: 0.6858939528465271\n",
      "Epoch 11: Train Loss: 0.598477840423584, Validation Loss: 0.6840001940727234\n",
      "Epoch 12: Train Loss: 0.5918376247088114, Validation Loss: 0.6817502379417419\n",
      "Epoch 13: Train Loss: 0.5587305625279745, Validation Loss: 0.6797022223472595\n",
      "Epoch 14: Train Loss: 0.5736728111902872, Validation Loss: 0.6778037548065186\n",
      "Epoch 15: Train Loss: 0.5660009185473124, Validation Loss: 0.6761974096298218\n",
      "Epoch 16: Train Loss: 0.582472026348114, Validation Loss: 0.6753126382827759\n",
      "Epoch 17: Train Loss: 0.5682615041732788, Validation Loss: 0.6747204661369324\n",
      "Epoch 18: Train Loss: 0.5534007946650187, Validation Loss: 0.6743994355201721\n",
      "Epoch 19: Train Loss: 0.56748894850413, Validation Loss: 0.6742031574249268\n",
      "Epoch 20: Train Loss: 0.5500480135281881, Validation Loss: 0.6709941625595093\n",
      "Epoch 21: Train Loss: 0.5499566197395325, Validation Loss: 0.6672026515007019\n",
      "Epoch 22: Train Loss: 0.5275538861751556, Validation Loss: 0.6639211773872375\n",
      "Epoch 23: Train Loss: 0.5208134849866232, Validation Loss: 0.6613526940345764\n",
      "Epoch 24: Train Loss: 0.52454141775767, Validation Loss: 0.6603217124938965\n",
      "Epoch 25: Train Loss: 0.5044333835442861, Validation Loss: 0.6598325371742249\n",
      "Epoch 26: Train Loss: 0.5013987024625143, Validation Loss: 0.6592265963554382\n",
      "Epoch 27: Train Loss: 0.5173921982447306, Validation Loss: 0.6587021946907043\n",
      "Epoch 28: Train Loss: 0.5085192124048868, Validation Loss: 0.6581581830978394\n",
      "Epoch 29: Train Loss: 0.5028204917907715, Validation Loss: 0.65767902135849\n",
      "Epoch 30: Train Loss: 0.49640771746635437, Validation Loss: 0.6571088433265686\n",
      "Epoch 31: Train Loss: 0.5100588301817576, Validation Loss: 0.6576468348503113\n",
      "Epoch 32: Train Loss: 0.5055025319258372, Validation Loss: 0.658385694026947\n",
      "Epoch 33: Train Loss: 0.480894277493159, Validation Loss: 0.6584482789039612\n",
      "Epoch 34: Train Loss: 0.5000945627689362, Validation Loss: 0.6587404012680054\n",
      "Epoch 35: Train Loss: 0.4787299434343974, Validation Loss: 0.6580368876457214\n",
      "Epoch 36: Train Loss: 0.47066013018290204, Validation Loss: 0.6569522023200989\n",
      "Epoch 37: Train Loss: 0.5099364022413889, Validation Loss: 0.6563615798950195\n",
      "Epoch 38: Train Loss: 0.4552578429381053, Validation Loss: 0.6556022763252258\n",
      "Epoch 39: Train Loss: 0.46955130497614544, Validation Loss: 0.6548358201980591\n",
      "Epoch 40: Train Loss: 0.46528687079747516, Validation Loss: 0.6538577079772949\n",
      "Epoch 41: Train Loss: 0.4591833750406901, Validation Loss: 0.6549153327941895\n",
      "Epoch 42: Train Loss: 0.47553924719492596, Validation Loss: 0.6554288864135742\n",
      "Epoch 43: Train Loss: 0.4509655435880025, Validation Loss: 0.6535824537277222\n",
      "Epoch 44: Train Loss: 0.45215046405792236, Validation Loss: 0.6495959162712097\n",
      "Epoch 45: Train Loss: 0.4554568827152252, Validation Loss: 0.6488444209098816\n",
      "Epoch 46: Train Loss: 0.4517100552717845, Validation Loss: 0.6486030220985413\n",
      "Epoch 47: Train Loss: 0.44045846660931903, Validation Loss: 0.647279679775238\n",
      "Epoch 48: Train Loss: 0.43818387389183044, Validation Loss: 0.64615797996521\n",
      "Epoch 49: Train Loss: 0.44704413414001465, Validation Loss: 0.6452196836471558\n",
      "Epoch 50: Train Loss: 0.43638502558072406, Validation Loss: 0.6441150307655334\n",
      "Epoch 51: Train Loss: 0.43912917375564575, Validation Loss: 0.6435950398445129\n",
      "Epoch 52: Train Loss: 0.43662293752034503, Validation Loss: 0.6478884220123291\n",
      "Epoch 53: Train Loss: 0.4339866836865743, Validation Loss: 0.6453840136528015\n",
      "Epoch 54: Train Loss: 0.427569180727005, Validation Loss: 0.641618549823761\n",
      "Epoch 55: Train Loss: 0.42921998103459674, Validation Loss: 0.6394361853599548\n",
      "Epoch 56: Train Loss: 0.4271269142627716, Validation Loss: 0.6385337710380554\n",
      "Epoch 57: Train Loss: 0.44071226318677265, Validation Loss: 0.6388024687767029\n",
      "Epoch 58: Train Loss: 0.41765302419662476, Validation Loss: 0.6380218863487244\n",
      "Epoch 59: Train Loss: 0.4187651177247365, Validation Loss: 0.6371996402740479\n",
      "Epoch 60: Train Loss: 0.44971877336502075, Validation Loss: 0.6396368741989136\n",
      "Epoch 61: Train Loss: 0.4121263225873311, Validation Loss: 0.6383286118507385\n",
      "Epoch 62: Train Loss: 0.4143291115760803, Validation Loss: 0.6326251029968262\n",
      "Epoch 63: Train Loss: 0.39820704857508343, Validation Loss: 0.6334759593009949\n",
      "Epoch 64: Train Loss: 0.40885764360427856, Validation Loss: 0.6341931819915771\n",
      "Epoch 65: Train Loss: 0.4120834271113078, Validation Loss: 0.634748637676239\n",
      "Epoch 66: Train Loss: 0.42697200179100037, Validation Loss: 0.6378693580627441\n",
      "Epoch 67: Train Loss: 0.4054631094137828, Validation Loss: 0.6388496160507202\n",
      "Epoch 68: Train Loss: 0.405597448348999, Validation Loss: 0.6376246809959412\n",
      "Epoch 69: Train Loss: 0.4056111176808675, Validation Loss: 0.6368679404258728\n",
      "Epoch 70: Train Loss: 0.40858539938926697, Validation Loss: 0.6268025636672974\n",
      "Epoch 71: Train Loss: 0.3943360348542531, Validation Loss: 0.6241937279701233\n",
      "Epoch 72: Train Loss: 0.39814119537671405, Validation Loss: 0.6213337182998657\n",
      "Epoch 73: Train Loss: 0.41008124748865765, Validation Loss: 0.6261410117149353\n",
      "Epoch 74: Train Loss: 0.39059794942537945, Validation Loss: 0.6249141097068787\n",
      "Epoch 75: Train Loss: 0.40142905712127686, Validation Loss: 0.6214960217475891\n",
      "Epoch 76: Train Loss: 0.38411131501197815, Validation Loss: 0.6180571913719177\n",
      "Epoch 77: Train Loss: 0.3883914550145467, Validation Loss: 0.6157214641571045\n",
      "Epoch 78: Train Loss: 0.3929160535335541, Validation Loss: 0.6146937608718872\n",
      "Epoch 79: Train Loss: 0.40723241368929547, Validation Loss: 0.6143757700920105\n",
      "Epoch 80: Train Loss: 0.39339746038119, Validation Loss: 0.6040689945220947\n",
      "Epoch 81: Train Loss: 0.38989585638046265, Validation Loss: 0.5947539806365967\n",
      "Epoch 82: Train Loss: 0.40942053000132245, Validation Loss: 0.5940349698066711\n",
      "Epoch 83: Train Loss: 0.3970757524172465, Validation Loss: 0.6064959764480591\n",
      "Epoch 84: Train Loss: 0.39006386200586957, Validation Loss: 0.6280511617660522\n",
      "Epoch 85: Train Loss: 0.3935808638731639, Validation Loss: 0.6425229907035828\n",
      "Epoch 86: Train Loss: 0.38787705699602765, Validation Loss: 0.6518587470054626\n",
      "Epoch 87: Train Loss: 0.3746555844942729, Validation Loss: 0.6539198756217957\n",
      "Epoch 88: Train Loss: 0.382303645213445, Validation Loss: 0.6529877781867981\n",
      "Epoch 89: Train Loss: 0.3789695103963216, Validation Loss: 0.6524676084518433\n",
      "Epoch 90: Train Loss: 0.3980924387772878, Validation Loss: 0.6565834879875183\n",
      "Epoch 91: Train Loss: 0.38095208009084064, Validation Loss: 0.6482444405555725\n",
      "Epoch 92: Train Loss: 0.39454705516497296, Validation Loss: 0.6407943367958069\n",
      "Epoch 93: Train Loss: 0.37879521648089093, Validation Loss: 0.6360653042793274\n",
      "Epoch 94: Train Loss: 0.38204291462898254, Validation Loss: 0.6311514973640442\n",
      "Epoch 95: Train Loss: 0.3692043622334798, Validation Loss: 0.6266049742698669\n",
      "Epoch 96: Train Loss: 0.361104816198349, Validation Loss: 0.6232491731643677\n",
      "Epoch 97: Train Loss: 0.3760337730248769, Validation Loss: 0.6214956045150757\n",
      "Epoch 98: Train Loss: 0.37789618968963623, Validation Loss: 0.6208273768424988\n",
      "Epoch 99: Train Loss: 0.3759792745113373, Validation Loss: 0.6205339431762695\n",
      "Epoch 100: Train Loss: 0.3643917441368103, Validation Loss: 0.60602205991745\n",
      "Epoch 101: Train Loss: 0.3771109978357951, Validation Loss: 0.605124831199646\n",
      "Epoch 102: Train Loss: 0.37335464358329773, Validation Loss: 0.6086337566375732\n",
      "Epoch 103: Train Loss: 0.3595568339029948, Validation Loss: 0.6103513240814209\n",
      "Epoch 104: Train Loss: 0.35885384678840637, Validation Loss: 0.6082767844200134\n",
      "Epoch 105: Train Loss: 0.3834834396839142, Validation Loss: 0.6093261241912842\n",
      "Epoch 106: Train Loss: 0.39490483204523724, Validation Loss: 0.6182982325553894\n",
      "Epoch 107: Train Loss: 0.366878867149353, Validation Loss: 0.621051549911499\n",
      "Epoch 108: Train Loss: 0.3774694800376892, Validation Loss: 0.6213618516921997\n",
      "Epoch 109: Train Loss: 0.3656793733437856, Validation Loss: 0.6213323473930359\n",
      "Epoch 110: Train Loss: 0.3728269934654236, Validation Loss: 0.6147516369819641\n",
      "Epoch 111: Train Loss: 0.39917538563410443, Validation Loss: 0.6087627410888672\n",
      "Epoch 112: Train Loss: 0.3603733281294505, Validation Loss: 0.590573787689209\n",
      "Epoch 113: Train Loss: 0.371852308511734, Validation Loss: 0.5834446549415588\n",
      "Epoch 114: Train Loss: 0.3645551602045695, Validation Loss: 0.5818813443183899\n",
      "Epoch 115: Train Loss: 0.3660706381003062, Validation Loss: 0.5837699174880981\n",
      "Epoch 116: Train Loss: 0.37702877322832745, Validation Loss: 0.5845966339111328\n",
      "Epoch 117: Train Loss: 0.3527888258298238, Validation Loss: 0.5859304070472717\n",
      "Epoch 118: Train Loss: 0.374177207549413, Validation Loss: 0.5877501964569092\n",
      "Epoch 119: Train Loss: 0.3537084758281708, Validation Loss: 0.5879533886909485\n",
      "Epoch 120: Train Loss: 0.35289735595385235, Validation Loss: 0.5971508026123047\n",
      "Epoch 121: Train Loss: 0.3586791555086772, Validation Loss: 0.5998961329460144\n",
      "Epoch 122: Train Loss: 0.36064226428667706, Validation Loss: 0.6081151962280273\n",
      "Epoch 123: Train Loss: 0.35877567529678345, Validation Loss: 0.6156998872756958\n",
      "Epoch 124: Train Loss: 0.35972339908281964, Validation Loss: 0.6121534109115601\n",
      "Epoch 125: Train Loss: 0.36330317457516986, Validation Loss: 0.6149938702583313\n",
      "Epoch 126: Train Loss: 0.3621280789375305, Validation Loss: 0.6219600439071655\n",
      "Epoch 127: Train Loss: 0.3537633419036865, Validation Loss: 0.626303493976593\n",
      "Epoch 128: Train Loss: 0.3673875133196513, Validation Loss: 0.6279297471046448\n",
      "Epoch 129: Train Loss: 0.35797767837842304, Validation Loss: 0.627811849117279\n",
      "Epoch 130: Train Loss: 0.35337115327517193, Validation Loss: 0.6287786960601807\n",
      "Epoch 131: Train Loss: 0.35347990194956463, Validation Loss: 0.6155144572257996\n",
      "Epoch 132: Train Loss: 0.35638027389844257, Validation Loss: 0.6022132635116577\n",
      "Epoch 133: Train Loss: 0.3507624367872874, Validation Loss: 0.595971405506134\n",
      "Epoch 134: Train Loss: 0.3478992184003194, Validation Loss: 0.6004221439361572\n",
      "Epoch 135: Train Loss: 0.3720012605190277, Validation Loss: 0.6114040613174438\n",
      "Epoch 136: Train Loss: 0.35928906003634137, Validation Loss: 0.6191064715385437\n",
      "Epoch 137: Train Loss: 0.37132195631663006, Validation Loss: 0.6249657869338989\n",
      "Epoch 138: Train Loss: 0.3594914674758911, Validation Loss: 0.6261308789253235\n",
      "Epoch 139: Train Loss: 0.34737329681714374, Validation Loss: 0.626031756401062\n",
      "Epoch 140: Train Loss: 0.34576401114463806, Validation Loss: 0.6291060447692871\n",
      "Epoch 141: Train Loss: 0.3441225290298462, Validation Loss: 0.6343532800674438\n",
      "Epoch 142: Train Loss: 0.3540397584438324, Validation Loss: 0.6403489112854004\n",
      "Epoch 143: Train Loss: 0.3470213711261749, Validation Loss: 0.640620768070221\n",
      "Epoch 144: Train Loss: 0.35466910401980084, Validation Loss: 0.6391478776931763\n",
      "Epoch 145: Train Loss: 0.3504879872004191, Validation Loss: 0.6371562480926514\n",
      "Epoch 146: Train Loss: 0.3452112078666687, Validation Loss: 0.6360861659049988\n",
      "Epoch 147: Train Loss: 0.34589381019274396, Validation Loss: 0.6347965002059937\n",
      "Epoch 148: Train Loss: 0.3571808735529582, Validation Loss: 0.634671151638031\n",
      "Epoch 149: Train Loss: 0.3488703966140747, Validation Loss: 0.6344132423400879\n",
      "Epoch 150: Train Loss: 0.3486557602882385, Validation Loss: 0.6145675778388977\n",
      "Epoch 151: Train Loss: 0.36443477869033813, Validation Loss: 0.610222339630127\n",
      "Epoch 152: Train Loss: 0.35539456208546955, Validation Loss: 0.6217839121818542\n",
      "Epoch 153: Train Loss: 0.3466867705186208, Validation Loss: 0.6342947483062744\n",
      "Epoch 154: Train Loss: 0.36046138405799866, Validation Loss: 0.6337332725524902\n",
      "Epoch 155: Train Loss: 0.3588773508866628, Validation Loss: 0.6321834325790405\n",
      "Epoch 156: Train Loss: 0.3438415229320526, Validation Loss: 0.6259036064147949\n",
      "Epoch 157: Train Loss: 0.34052231907844543, Validation Loss: 0.6213417649269104\n",
      "Epoch 158: Train Loss: 0.333381066719691, Validation Loss: 0.6196829676628113\n",
      "Epoch 159: Train Loss: 0.3547918101151784, Validation Loss: 0.6194598078727722\n",
      "Epoch 160: Train Loss: 0.370987872282664, Validation Loss: 0.5966389179229736\n",
      "Epoch 161: Train Loss: 0.3589912752310435, Validation Loss: 0.5798198580741882\n",
      "Epoch 162: Train Loss: 0.35681143403053284, Validation Loss: 0.5855987668037415\n",
      "Epoch 163: Train Loss: 0.3429137170314789, Validation Loss: 0.614734411239624\n",
      "Epoch 164: Train Loss: 0.36210742592811584, Validation Loss: 0.6407429575920105\n",
      "Epoch 165: Train Loss: 0.3484988212585449, Validation Loss: 0.6505240201950073\n",
      "Epoch 166: Train Loss: 0.3457689086596171, Validation Loss: 0.6500554084777832\n",
      "Epoch 167: Train Loss: 0.3383885721365611, Validation Loss: 0.6465725898742676\n",
      "Epoch 168: Train Loss: 0.36109360059102374, Validation Loss: 0.6440099477767944\n",
      "Epoch 169: Train Loss: 0.3380107879638672, Validation Loss: 0.6421518325805664\n",
      "Epoch 170: Train Loss: 0.3386465907096863, Validation Loss: 0.6118883490562439\n",
      "Epoch 171: Train Loss: 0.351400484641393, Validation Loss: 0.5991053581237793\n",
      "Epoch 172: Train Loss: 0.35049787163734436, Validation Loss: 0.6102104187011719\n",
      "Epoch 173: Train Loss: 0.3675321539243062, Validation Loss: 0.6209538578987122\n",
      "Epoch 174: Train Loss: 0.34118637442588806, Validation Loss: 0.6228809952735901\n",
      "Epoch 175: Train Loss: 0.3367873728275299, Validation Loss: 0.6270763278007507\n",
      "Epoch 176: Train Loss: 0.34035196900367737, Validation Loss: 0.6299130916595459\n",
      "Epoch 177: Train Loss: 0.34104256828625995, Validation Loss: 0.6309638619422913\n",
      "Epoch 178: Train Loss: 0.33750460545221966, Validation Loss: 0.6304200291633606\n",
      "Epoch 179: Train Loss: 0.34157539407412213, Validation Loss: 0.6301971077919006\n",
      "Epoch 180: Train Loss: 0.3394814034303029, Validation Loss: 0.633196234703064\n",
      "Epoch 181: Train Loss: 0.34363632400830585, Validation Loss: 0.6321913003921509\n",
      "Epoch 182: Train Loss: 0.3532947202523549, Validation Loss: 0.6291692852973938\n",
      "Epoch 183: Train Loss: 0.33677756786346436, Validation Loss: 0.6257007718086243\n",
      "Epoch 184: Train Loss: 0.3379095296065013, Validation Loss: 0.6235074400901794\n",
      "Epoch 185: Train Loss: 0.33369600772857666, Validation Loss: 0.6194244623184204\n",
      "Epoch 186: Train Loss: 0.3378707369168599, Validation Loss: 0.6150018572807312\n",
      "Epoch 187: Train Loss: 0.33380727966626483, Validation Loss: 0.6138848066329956\n",
      "Epoch 188: Train Loss: 0.3388880689938863, Validation Loss: 0.6136278510093689\n",
      "Epoch 189: Train Loss: 0.33595891793568927, Validation Loss: 0.6136981248855591\n",
      "Epoch 190: Train Loss: 0.34553025166193646, Validation Loss: 0.6242803931236267\n",
      "Epoch 191: Train Loss: 0.3394201397895813, Validation Loss: 0.6342683434486389\n",
      "Epoch 192: Train Loss: 0.36183464527130127, Validation Loss: 0.639225959777832\n",
      "Epoch 193: Train Loss: 0.33619868755340576, Validation Loss: 0.6312083005905151\n",
      "Epoch 194: Train Loss: 0.33976127703984577, Validation Loss: 0.6321434378623962\n",
      "Epoch 195: Train Loss: 0.34734707077344257, Validation Loss: 0.6268752217292786\n",
      "Epoch 196: Train Loss: 0.34405531485875446, Validation Loss: 0.6226394176483154\n",
      "Epoch 197: Train Loss: 0.34615403413772583, Validation Loss: 0.6208851337432861\n",
      "Epoch 198: Train Loss: 0.34642812609672546, Validation Loss: 0.6200713515281677\n",
      "Epoch 199: Train Loss: 0.34186341365178424, Validation Loss: 0.6198092103004456\n",
      "Epoch 200: Train Loss: 0.335015207529068, Validation Loss: 0.5986197590827942\n",
      "Epoch 201: Train Loss: 0.3443065285682678, Validation Loss: 0.5767098665237427\n",
      "Epoch 202: Train Loss: 0.3500099678834279, Validation Loss: 0.5656558871269226\n",
      "Epoch 203: Train Loss: 0.33900808294614154, Validation Loss: 0.5646982789039612\n",
      "Epoch 204: Train Loss: 0.3394866983095805, Validation Loss: 0.5650407671928406\n",
      "Epoch 205: Train Loss: 0.3335883915424347, Validation Loss: 0.5632912516593933\n",
      "Epoch 206: Train Loss: 0.3356202046076457, Validation Loss: 0.5621209740638733\n",
      "Epoch 207: Train Loss: 0.3410544196764628, Validation Loss: 0.5621762275695801\n",
      "Epoch 208: Train Loss: 0.33703699707984924, Validation Loss: 0.5617045760154724\n",
      "Epoch 209: Train Loss: 0.3374871214230855, Validation Loss: 0.5621933341026306\n",
      "Epoch 210: Train Loss: 0.33391067385673523, Validation Loss: 0.5679572224617004\n",
      "Epoch 211: Train Loss: 0.34055158495903015, Validation Loss: 0.581618070602417\n",
      "Epoch 212: Train Loss: 0.3411468267440796, Validation Loss: 0.5905857086181641\n",
      "Epoch 213: Train Loss: 0.34500518441200256, Validation Loss: 0.5944529175758362\n",
      "Epoch 214: Train Loss: 0.3731254041194916, Validation Loss: 0.5888106226921082\n",
      "Epoch 215: Train Loss: 0.333411584297816, Validation Loss: 0.5799490213394165\n",
      "Epoch 216: Train Loss: 0.33277300993601483, Validation Loss: 0.5774844884872437\n",
      "Epoch 217: Train Loss: 0.33623047669728595, Validation Loss: 0.5791103839874268\n",
      "Epoch 218: Train Loss: 0.33638908465703327, Validation Loss: 0.5793405175209045\n",
      "Epoch 219: Train Loss: 0.3340636193752289, Validation Loss: 0.5799062848091125\n",
      "Epoch 220: Train Loss: 0.33790649970372516, Validation Loss: 0.583134114742279\n",
      "Epoch 221: Train Loss: 0.3477281630039215, Validation Loss: 0.5975762605667114\n",
      "Epoch 222: Train Loss: 0.33991772929827374, Validation Loss: 0.5954621434211731\n",
      "Epoch 223: Train Loss: 0.33284349242846173, Validation Loss: 0.5950531959533691\n",
      "Epoch 224: Train Loss: 0.33071518937746686, Validation Loss: 0.5903187990188599\n",
      "Epoch 225: Train Loss: 0.3465508123238881, Validation Loss: 0.5830826759338379\n",
      "Epoch 226: Train Loss: 0.3314447303613027, Validation Loss: 0.5842539072036743\n",
      "Epoch 227: Train Loss: 0.34003356099128723, Validation Loss: 0.5864149928092957\n",
      "Epoch 228: Train Loss: 0.334267516930898, Validation Loss: 0.5884814262390137\n",
      "Epoch 229: Train Loss: 0.3325939079125722, Validation Loss: 0.5892020463943481\n",
      "Epoch 230: Train Loss: 0.32993967334429425, Validation Loss: 0.5945767164230347\n",
      "Epoch 231: Train Loss: 0.34140367309252423, Validation Loss: 0.5927165746688843\n",
      "Epoch 232: Train Loss: 0.32825790842374164, Validation Loss: 0.5814030170440674\n",
      "Epoch 233: Train Loss: 0.3417453169822693, Validation Loss: 0.5750337243080139\n",
      "Epoch 234: Train Loss: 0.33067891001701355, Validation Loss: 0.5750331878662109\n",
      "Epoch 235: Train Loss: 0.3367454508940379, Validation Loss: 0.5793745517730713\n",
      "Epoch 236: Train Loss: 0.3318120737870534, Validation Loss: 0.5839254260063171\n",
      "Epoch 237: Train Loss: 0.3353871802488963, Validation Loss: 0.585430383682251\n",
      "Epoch 238: Train Loss: 0.33250876267751056, Validation Loss: 0.5857115983963013\n",
      "Epoch 239: Train Loss: 0.3311297595500946, Validation Loss: 0.5859381556510925\n",
      "Epoch 240: Train Loss: 0.32916804154713947, Validation Loss: 0.579696774482727\n",
      "Epoch 241: Train Loss: 0.33378900090853375, Validation Loss: 0.5673055052757263\n",
      "Epoch 242: Train Loss: 0.3373412589232127, Validation Loss: 0.5688225030899048\n",
      "Epoch 243: Train Loss: 0.34126342336336773, Validation Loss: 0.5756964087486267\n",
      "Epoch 244: Train Loss: 0.3317539095878601, Validation Loss: 0.5724678039550781\n",
      "Epoch 245: Train Loss: 0.3349378804365794, Validation Loss: 0.565682590007782\n",
      "Epoch 246: Train Loss: 0.33774102727572125, Validation Loss: 0.563571035861969\n",
      "Epoch 247: Train Loss: 0.3337110678354899, Validation Loss: 0.5629794001579285\n",
      "Epoch 248: Train Loss: 0.34113000830014545, Validation Loss: 0.5623654127120972\n",
      "Epoch 249: Train Loss: 0.33751508593559265, Validation Loss: 0.5626072883605957\n",
      "Epoch 250: Train Loss: 0.32741551597913104, Validation Loss: 0.5606651306152344\n",
      "Epoch 251: Train Loss: 0.33643941084543866, Validation Loss: 0.5679023861885071\n",
      "Epoch 252: Train Loss: 0.3315143386522929, Validation Loss: 0.5729992389678955\n",
      "Epoch 253: Train Loss: 0.32948187987009686, Validation Loss: 0.556086540222168\n",
      "Epoch 254: Train Loss: 0.3279254237810771, Validation Loss: 0.5575796365737915\n",
      "Epoch 255: Train Loss: 0.32796533902486164, Validation Loss: 0.5775589346885681\n",
      "Epoch 256: Train Loss: 0.33208051323890686, Validation Loss: 0.5853314995765686\n",
      "Epoch 257: Train Loss: 0.32187427083651227, Validation Loss: 0.5819080471992493\n",
      "Epoch 258: Train Loss: 0.3201633592446645, Validation Loss: 0.5790727138519287\n",
      "Epoch 259: Train Loss: 0.32077693939208984, Validation Loss: 0.5785371661186218\n",
      "Epoch 260: Train Loss: 0.3260759711265564, Validation Loss: 0.5427040457725525\n",
      "Epoch 261: Train Loss: 0.33459847172101337, Validation Loss: 0.5323176980018616\n",
      "Epoch 262: Train Loss: 0.325016309817632, Validation Loss: 0.5540847182273865\n",
      "Epoch 263: Train Loss: 0.3216132124265035, Validation Loss: 0.5704019665718079\n",
      "Epoch 264: Train Loss: 0.3218384385108948, Validation Loss: 0.578118085861206\n",
      "Epoch 265: Train Loss: 0.31956298152605694, Validation Loss: 0.5755710005760193\n",
      "Epoch 266: Train Loss: 0.321139136950175, Validation Loss: 0.5730151534080505\n",
      "Epoch 267: Train Loss: 0.3188476363817851, Validation Loss: 0.5716814994812012\n",
      "Epoch 268: Train Loss: 0.3179730375607808, Validation Loss: 0.5714539885520935\n",
      "Epoch 269: Train Loss: 0.3247368137041728, Validation Loss: 0.571398138999939\n",
      "Epoch 270: Train Loss: 0.34414764245351154, Validation Loss: 0.6092866063117981\n",
      "Epoch 271: Train Loss: 0.33188066879908246, Validation Loss: 0.6296637058258057\n",
      "Epoch 272: Train Loss: 0.32454778750737506, Validation Loss: 0.6264920830726624\n",
      "Epoch 273: Train Loss: 0.3218644956747691, Validation Loss: 0.6194376945495605\n",
      "Epoch 274: Train Loss: 0.32331615686416626, Validation Loss: 0.611552894115448\n",
      "Epoch 275: Train Loss: 0.33097177743911743, Validation Loss: 0.5956918597221375\n",
      "Epoch 276: Train Loss: 0.32220985492070514, Validation Loss: 0.5815045833587646\n",
      "Epoch 277: Train Loss: 0.31842875480651855, Validation Loss: 0.5796207189559937\n",
      "Epoch 278: Train Loss: 0.32069406906763714, Validation Loss: 0.5785008668899536\n",
      "Epoch 279: Train Loss: 0.32092007001241046, Validation Loss: 0.5776367783546448\n",
      "Epoch 280: Train Loss: 0.317499836285909, Validation Loss: 0.5881016850471497\n",
      "Epoch 281: Train Loss: 0.319556067387263, Validation Loss: 0.5969981551170349\n",
      "Epoch 282: Train Loss: 0.3192557096481323, Validation Loss: 0.6043450832366943\n",
      "Epoch 283: Train Loss: 0.3197237253189087, Validation Loss: 0.6058080792427063\n",
      "Epoch 284: Train Loss: 0.32107266783714294, Validation Loss: 0.6013131141662598\n",
      "Epoch 285: Train Loss: 0.3170352478822072, Validation Loss: 0.5959067940711975\n",
      "Epoch 286: Train Loss: 0.3157772322495778, Validation Loss: 0.592564046382904\n",
      "Epoch 287: Train Loss: 0.3200840950012207, Validation Loss: 0.5899888277053833\n",
      "Epoch 288: Train Loss: 0.31817572315533954, Validation Loss: 0.5895153284072876\n",
      "Epoch 289: Train Loss: 0.31871533393859863, Validation Loss: 0.5889312028884888\n",
      "Epoch 290: Train Loss: 0.31977739930152893, Validation Loss: 0.57256019115448\n",
      "Epoch 291: Train Loss: 0.31929023067156476, Validation Loss: 0.5610224008560181\n",
      "Epoch 292: Train Loss: 0.3178946276505788, Validation Loss: 0.5488450527191162\n",
      "Epoch 293: Train Loss: 0.31552936633427936, Validation Loss: 0.5437166690826416\n",
      "Epoch 294: Train Loss: 0.3178304036458333, Validation Loss: 0.5443907380104065\n",
      "Epoch 295: Train Loss: 0.32013951738675434, Validation Loss: 0.5444129109382629\n",
      "Epoch 296: Train Loss: 0.3223723868529002, Validation Loss: 0.5452097654342651\n",
      "Epoch 297: Train Loss: 0.3232136368751526, Validation Loss: 0.5477595925331116\n",
      "Epoch 298: Train Loss: 0.31648629903793335, Validation Loss: 0.5481812953948975\n",
      "Epoch 299: Train Loss: 0.3194497923056285, Validation Loss: 0.5482229590415955\n",
      "Epoch 300: Train Loss: 0.31741363803545636, Validation Loss: 0.5476473569869995\n",
      "Epoch 301: Train Loss: 0.31830241282780963, Validation Loss: 0.5471163988113403\n",
      "Epoch 302: Train Loss: 0.3197010358174642, Validation Loss: 0.5494388937950134\n",
      "Epoch 303: Train Loss: 0.31722813844680786, Validation Loss: 0.549970269203186\n",
      "Epoch 304: Train Loss: 0.317134956518809, Validation Loss: 0.5510704517364502\n",
      "Epoch 305: Train Loss: 0.32699092229207355, Validation Loss: 0.5511634945869446\n",
      "Epoch 306: Train Loss: 0.3225705921649933, Validation Loss: 0.5529304146766663\n",
      "Epoch 307: Train Loss: 0.32036782304445904, Validation Loss: 0.5533727407455444\n",
      "Epoch 308: Train Loss: 0.32861924171447754, Validation Loss: 0.5531485080718994\n",
      "Epoch 309: Train Loss: 0.31693538029988605, Validation Loss: 0.5529160499572754\n",
      "Epoch 310: Train Loss: 0.32318878173828125, Validation Loss: 0.5436882376670837\n",
      "Epoch 311: Train Loss: 0.31707606712977093, Validation Loss: 0.5425165295600891\n",
      "Epoch 312: Train Loss: 0.31844154993693036, Validation Loss: 0.5460735559463501\n",
      "Epoch 313: Train Loss: 0.31963186462720233, Validation Loss: 0.5545976758003235\n",
      "Epoch 314: Train Loss: 0.32246164480845135, Validation Loss: 0.5627703070640564\n",
      "Epoch 315: Train Loss: 0.31900250911712646, Validation Loss: 0.56684410572052\n",
      "Epoch 316: Train Loss: 0.31496866544087726, Validation Loss: 0.5675106644630432\n",
      "Epoch 317: Train Loss: 0.3190159797668457, Validation Loss: 0.5663046836853027\n",
      "Epoch 318: Train Loss: 0.32582788666089374, Validation Loss: 0.5655875205993652\n",
      "Epoch 319: Train Loss: 0.3284183740615845, Validation Loss: 0.5655349493026733\n",
      "Epoch 320: Train Loss: 0.3262237111727397, Validation Loss: 0.5378980040550232\n",
      "Epoch 321: Train Loss: 0.32319774230321247, Validation Loss: 0.5281569957733154\n",
      "Epoch 322: Train Loss: 0.32863110303878784, Validation Loss: 0.5288048982620239\n",
      "Epoch 323: Train Loss: 0.3276568154493968, Validation Loss: 0.5386937260627747\n",
      "Epoch 324: Train Loss: 0.3217872480551402, Validation Loss: 0.552865743637085\n",
      "Epoch 325: Train Loss: 0.32426293690999347, Validation Loss: 0.5611563920974731\n",
      "Epoch 326: Train Loss: 0.3168950577576955, Validation Loss: 0.5645923018455505\n",
      "Epoch 327: Train Loss: 0.31681610147158307, Validation Loss: 0.5659928917884827\n",
      "Epoch 328: Train Loss: 0.32659823695818585, Validation Loss: 0.5660576820373535\n",
      "Epoch 329: Train Loss: 0.3176802396774292, Validation Loss: 0.5661305785179138\n",
      "Epoch 330: Train Loss: 0.3158857822418213, Validation Loss: 0.5554437041282654\n",
      "Epoch 331: Train Loss: 0.317183514436086, Validation Loss: 0.5465320348739624\n",
      "Epoch 332: Train Loss: 0.31686534484227497, Validation Loss: 0.5439344048500061\n",
      "Epoch 333: Train Loss: 0.31639930605888367, Validation Loss: 0.5445116758346558\n",
      "Epoch 334: Train Loss: 0.3245229721069336, Validation Loss: 0.5502183437347412\n",
      "Epoch 335: Train Loss: 0.31605557600657147, Validation Loss: 0.5538890361785889\n",
      "Epoch 336: Train Loss: 0.31697675585746765, Validation Loss: 0.5555886030197144\n",
      "Epoch 337: Train Loss: 0.3183557391166687, Validation Loss: 0.5544238090515137\n",
      "Epoch 338: Train Loss: 0.31422051787376404, Validation Loss: 0.5540579557418823\n",
      "Epoch 339: Train Loss: 0.3170427083969116, Validation Loss: 0.5538628697395325\n",
      "Epoch 340: Train Loss: 0.3172728518644969, Validation Loss: 0.548625111579895\n",
      "Epoch 341: Train Loss: 0.3160926401615143, Validation Loss: 0.5440699458122253\n",
      "Epoch 342: Train Loss: 0.31497212251027423, Validation Loss: 0.5442661643028259\n",
      "Epoch 343: Train Loss: 0.315361609061559, Validation Loss: 0.5438303351402283\n",
      "Epoch 344: Train Loss: 0.3158654570579529, Validation Loss: 0.5426272749900818\n",
      "Epoch 345: Train Loss: 0.3188466727733612, Validation Loss: 0.5410355925559998\n",
      "Epoch 346: Train Loss: 0.3208056390285492, Validation Loss: 0.54180508852005\n",
      "Epoch 347: Train Loss: 0.3153650363286336, Validation Loss: 0.5431807041168213\n",
      "Epoch 348: Train Loss: 0.31578920284907025, Validation Loss: 0.5436585545539856\n",
      "Epoch 349: Train Loss: 0.31816112995147705, Validation Loss: 0.5436747074127197\n",
      "Epoch 350: Train Loss: 0.31506332755088806, Validation Loss: 0.544965386390686\n",
      "Epoch 351: Train Loss: 0.3158872922261556, Validation Loss: 0.5485455393791199\n",
      "Epoch 352: Train Loss: 0.31679455439249676, Validation Loss: 0.5519632697105408\n",
      "Epoch 353: Train Loss: 0.3160940607388814, Validation Loss: 0.554921567440033\n",
      "Epoch 354: Train Loss: 0.3174380362033844, Validation Loss: 0.5561894178390503\n",
      "Epoch 355: Train Loss: 0.3163076837857564, Validation Loss: 0.5580052137374878\n",
      "Epoch 356: Train Loss: 0.31686797738075256, Validation Loss: 0.5582471489906311\n",
      "Epoch 357: Train Loss: 0.3158637781937917, Validation Loss: 0.5582997798919678\n",
      "Epoch 358: Train Loss: 0.316354235013326, Validation Loss: 0.5585188865661621\n",
      "Epoch 359: Train Loss: 0.3193984429041545, Validation Loss: 0.5585575103759766\n",
      "Epoch 360: Train Loss: 0.3164154092470805, Validation Loss: 0.5574395060539246\n",
      "Epoch 361: Train Loss: 0.31531665722529095, Validation Loss: 0.5577448606491089\n",
      "Epoch 362: Train Loss: 0.3193698724110921, Validation Loss: 0.5569586157798767\n",
      "Epoch 363: Train Loss: 0.3182881573836009, Validation Loss: 0.5592132806777954\n",
      "Epoch 364: Train Loss: 0.327936311562856, Validation Loss: 0.5651780366897583\n",
      "Epoch 365: Train Loss: 0.3149954477945964, Validation Loss: 0.5676074028015137\n",
      "Epoch 366: Train Loss: 0.31675325830777484, Validation Loss: 0.5642815828323364\n",
      "Epoch 367: Train Loss: 0.3218039671579997, Validation Loss: 0.5599827170372009\n",
      "Epoch 368: Train Loss: 0.3167726198832194, Validation Loss: 0.5574935674667358\n",
      "Epoch 369: Train Loss: 0.31657342116038006, Validation Loss: 0.5570587515830994\n",
      "Epoch 370: Train Loss: 0.31937076648076373, Validation Loss: 0.5536227822303772\n",
      "Epoch 371: Train Loss: 0.3155119816462199, Validation Loss: 0.5519599318504333\n",
      "Epoch 372: Train Loss: 0.3162283698717753, Validation Loss: 0.5571252107620239\n",
      "Epoch 373: Train Loss: 0.3157099783420563, Validation Loss: 0.5645561814308167\n",
      "Epoch 374: Train Loss: 0.3145720561345418, Validation Loss: 0.5696831345558167\n",
      "Epoch 375: Train Loss: 0.31602610150973004, Validation Loss: 0.5713267922401428\n",
      "Epoch 376: Train Loss: 0.3206278284390767, Validation Loss: 0.5709649324417114\n",
      "Epoch 377: Train Loss: 0.3146498501300812, Validation Loss: 0.5697330832481384\n",
      "Epoch 378: Train Loss: 0.3150905867417653, Validation Loss: 0.5692777037620544\n",
      "Epoch 379: Train Loss: 0.3230910897254944, Validation Loss: 0.5691207647323608\n",
      "Epoch 380: Train Loss: 0.31537318229675293, Validation Loss: 0.5758447647094727\n",
      "Epoch 381: Train Loss: 0.3153851628303528, Validation Loss: 0.5820578336715698\n",
      "Epoch 382: Train Loss: 0.3155202865600586, Validation Loss: 0.5909178853034973\n",
      "Epoch 383: Train Loss: 0.316313495238622, Validation Loss: 0.5922998785972595\n",
      "Epoch 384: Train Loss: 0.3162321348985036, Validation Loss: 0.5932427048683167\n",
      "Epoch 385: Train Loss: 0.31618207693099976, Validation Loss: 0.596445620059967\n",
      "Epoch 386: Train Loss: 0.3196326990922292, Validation Loss: 0.5972505211830139\n",
      "Epoch 387: Train Loss: 0.3205984930197398, Validation Loss: 0.5970671772956848\n",
      "Epoch 388: Train Loss: 0.32016393542289734, Validation Loss: 0.5966943502426147\n",
      "Epoch 389: Train Loss: 0.3163561522960663, Validation Loss: 0.5964021682739258\n",
      "Epoch 390: Train Loss: 0.3154846628506978, Validation Loss: 0.6066429615020752\n",
      "Epoch 391: Train Loss: 0.31836650768915814, Validation Loss: 0.609138548374176\n",
      "Epoch 392: Train Loss: 0.31691298882166546, Validation Loss: 0.5998184084892273\n",
      "Epoch 393: Train Loss: 0.31583239634831745, Validation Loss: 0.5919199585914612\n",
      "Epoch 394: Train Loss: 0.31514934698740643, Validation Loss: 0.5903526544570923\n",
      "Epoch 395: Train Loss: 0.3147720694541931, Validation Loss: 0.5904558897018433\n",
      "Epoch 396: Train Loss: 0.31658145785331726, Validation Loss: 0.5915988683700562\n",
      "Epoch 397: Train Loss: 0.32070086399714154, Validation Loss: 0.5923052430152893\n",
      "Epoch 398: Train Loss: 0.31513697902361554, Validation Loss: 0.592361330986023\n",
      "Epoch 399: Train Loss: 0.32467515269915265, Validation Loss: 0.5919382572174072\n",
      "Epoch 400: Train Loss: 0.31476326783498126, Validation Loss: 0.5967901945114136\n",
      "Epoch 401: Train Loss: 0.3208097716172536, Validation Loss: 0.6056938767433167\n",
      "Epoch 402: Train Loss: 0.32201777895291644, Validation Loss: 0.6073412895202637\n",
      "Epoch 403: Train Loss: 0.31458069880803424, Validation Loss: 0.6047829985618591\n",
      "Epoch 404: Train Loss: 0.31758514046669006, Validation Loss: 0.597273051738739\n",
      "Epoch 405: Train Loss: 0.315897395213445, Validation Loss: 0.5824255347251892\n",
      "Epoch 406: Train Loss: 0.31913209954897565, Validation Loss: 0.572667121887207\n",
      "Epoch 407: Train Loss: 0.3167528410752614, Validation Loss: 0.5694531202316284\n",
      "Epoch 408: Train Loss: 0.3156681756178538, Validation Loss: 0.5685057044029236\n",
      "Epoch 409: Train Loss: 0.31508078177769977, Validation Loss: 0.5680190920829773\n",
      "Epoch 410: Train Loss: 0.3147875765959422, Validation Loss: 0.5679519176483154\n",
      "Epoch 411: Train Loss: 0.3185087939103444, Validation Loss: 0.569758951663971\n",
      "Epoch 412: Train Loss: 0.31561797857284546, Validation Loss: 0.5710228681564331\n",
      "Epoch 413: Train Loss: 0.31472642223040265, Validation Loss: 0.5699137449264526\n",
      "Epoch 414: Train Loss: 0.32307852307955426, Validation Loss: 0.5698898434638977\n",
      "Epoch 415: Train Loss: 0.3144776523113251, Validation Loss: 0.5712119340896606\n",
      "Epoch 416: Train Loss: 0.31526193022727966, Validation Loss: 0.5714082717895508\n",
      "Epoch 417: Train Loss: 0.3154686490694682, Validation Loss: 0.5711018443107605\n",
      "Epoch 418: Train Loss: 0.3265049457550049, Validation Loss: 0.5701819062232971\n",
      "Epoch 419: Train Loss: 0.3242314060529073, Validation Loss: 0.569238007068634\n",
      "Epoch 420: Train Loss: 0.3169291019439697, Validation Loss: 0.5625216960906982\n",
      "Early stopping at epoch 421\n",
      "Fold 5\n",
      "Epoch 0: Train Loss: 0.7013685703277588, Validation Loss: 0.6967455744743347\n",
      "Epoch 1: Train Loss: 0.6655969023704529, Validation Loss: 0.6976816654205322\n",
      "Epoch 2: Train Loss: 0.6612112919489542, Validation Loss: 0.6990635395050049\n",
      "Epoch 3: Train Loss: 0.6300906737645467, Validation Loss: 0.6993817687034607\n",
      "Epoch 4: Train Loss: 0.6260133783022562, Validation Loss: 0.6998914480209351\n",
      "Epoch 5: Train Loss: 0.6265971859296163, Validation Loss: 0.7010130882263184\n",
      "Epoch 6: Train Loss: 0.6345479488372803, Validation Loss: 0.7012697458267212\n",
      "Epoch 7: Train Loss: 0.6194042563438416, Validation Loss: 0.7017292380332947\n",
      "Epoch 8: Train Loss: 0.6093356211980184, Validation Loss: 0.7020038962364197\n",
      "Epoch 9: Train Loss: 0.617097536722819, Validation Loss: 0.702200174331665\n",
      "Epoch 10: Train Loss: 0.6288420955340067, Validation Loss: 0.7034672498703003\n",
      "Epoch 11: Train Loss: 0.5902625918388367, Validation Loss: 0.703666090965271\n",
      "Epoch 12: Train Loss: 0.5840516289075216, Validation Loss: 0.7023688554763794\n",
      "Epoch 13: Train Loss: 0.5784610708554586, Validation Loss: 0.7019138336181641\n",
      "Epoch 14: Train Loss: 0.5300717353820801, Validation Loss: 0.700657308101654\n",
      "Epoch 15: Train Loss: 0.5630280176798502, Validation Loss: 0.6993729472160339\n",
      "Epoch 16: Train Loss: 0.549314816792806, Validation Loss: 0.6987901926040649\n",
      "Epoch 17: Train Loss: 0.5458255012830099, Validation Loss: 0.6978438496589661\n",
      "Epoch 18: Train Loss: 0.5297371248404185, Validation Loss: 0.6973133683204651\n",
      "Epoch 19: Train Loss: 0.5526665647824606, Validation Loss: 0.6972483992576599\n",
      "Epoch 20: Train Loss: 0.5273261169592539, Validation Loss: 0.6905766129493713\n",
      "Epoch 21: Train Loss: 0.5290345748265585, Validation Loss: 0.6812554597854614\n",
      "Epoch 22: Train Loss: 0.5150421261787415, Validation Loss: 0.6770302057266235\n",
      "Epoch 23: Train Loss: 0.5200620790322622, Validation Loss: 0.6797556281089783\n",
      "Epoch 24: Train Loss: 0.48263677954673767, Validation Loss: 0.6783101558685303\n",
      "Epoch 25: Train Loss: 0.4964280327161153, Validation Loss: 0.6745910048484802\n",
      "Epoch 26: Train Loss: 0.4976973334948222, Validation Loss: 0.670592188835144\n",
      "Epoch 27: Train Loss: 0.49250168601671857, Validation Loss: 0.6670855283737183\n",
      "Epoch 28: Train Loss: 0.4832762877146403, Validation Loss: 0.6657227873802185\n",
      "Epoch 29: Train Loss: 0.4746585786342621, Validation Loss: 0.665239155292511\n",
      "Epoch 30: Train Loss: 0.4795494278271993, Validation Loss: 0.6537452340126038\n",
      "Epoch 31: Train Loss: 0.49247955282529193, Validation Loss: 0.6471458077430725\n",
      "Epoch 32: Train Loss: 0.48396151264508563, Validation Loss: 0.6404719352722168\n",
      "Epoch 33: Train Loss: 0.46402063965797424, Validation Loss: 0.6352153420448303\n",
      "Epoch 34: Train Loss: 0.47138674060503644, Validation Loss: 0.6282789707183838\n",
      "Epoch 35: Train Loss: 0.45774715145428974, Validation Loss: 0.6203934550285339\n",
      "Epoch 36: Train Loss: 0.44764073689778644, Validation Loss: 0.6155508160591125\n",
      "Epoch 37: Train Loss: 0.44932613770167035, Validation Loss: 0.6126862168312073\n",
      "Epoch 38: Train Loss: 0.4455529550711314, Validation Loss: 0.6105377078056335\n",
      "Epoch 39: Train Loss: 0.44592700401941937, Validation Loss: 0.6092082858085632\n",
      "Epoch 40: Train Loss: 0.44241292277971905, Validation Loss: 0.58760666847229\n",
      "Epoch 41: Train Loss: 0.42513208587964374, Validation Loss: 0.5913186073303223\n",
      "Epoch 42: Train Loss: 0.4286695917447408, Validation Loss: 0.5959740877151489\n",
      "Epoch 43: Train Loss: 0.4167395532131195, Validation Loss: 0.59968501329422\n",
      "Epoch 44: Train Loss: 0.41402315100034076, Validation Loss: 0.5986714363098145\n",
      "Epoch 45: Train Loss: 0.4254192014535268, Validation Loss: 0.5987019538879395\n",
      "Epoch 46: Train Loss: 0.4236226975917816, Validation Loss: 0.597777247428894\n",
      "Epoch 47: Train Loss: 0.4122503399848938, Validation Loss: 0.5972287654876709\n",
      "Epoch 48: Train Loss: 0.42296919226646423, Validation Loss: 0.5969647765159607\n",
      "Epoch 49: Train Loss: 0.4227273066838582, Validation Loss: 0.5962193608283997\n",
      "Epoch 50: Train Loss: 0.43302714824676514, Validation Loss: 0.5927783250808716\n",
      "Epoch 51: Train Loss: 0.420318603515625, Validation Loss: 0.5867926478385925\n",
      "Epoch 52: Train Loss: 0.3988180955251058, Validation Loss: 0.6023566722869873\n",
      "Epoch 53: Train Loss: 0.39661218722661334, Validation Loss: 0.6252220869064331\n",
      "Epoch 54: Train Loss: 0.41166069110234577, Validation Loss: 0.6317850947380066\n",
      "Epoch 55: Train Loss: 0.42810144027074176, Validation Loss: 0.626560389995575\n",
      "Epoch 56: Train Loss: 0.42127159237861633, Validation Loss: 0.6180105805397034\n",
      "Epoch 57: Train Loss: 0.41668304800987244, Validation Loss: 0.6153258681297302\n",
      "Epoch 58: Train Loss: 0.38941965500513714, Validation Loss: 0.6119604110717773\n",
      "Epoch 59: Train Loss: 0.3951847056547801, Validation Loss: 0.6104892492294312\n",
      "Epoch 60: Train Loss: 0.4000437259674072, Validation Loss: 0.5809044241905212\n",
      "Epoch 61: Train Loss: 0.3927995463212331, Validation Loss: 0.5564635396003723\n",
      "Epoch 62: Train Loss: 0.40199484427769977, Validation Loss: 0.5501211285591125\n",
      "Epoch 63: Train Loss: 0.4069703022638957, Validation Loss: 0.5472357273101807\n",
      "Epoch 64: Train Loss: 0.3949674665927887, Validation Loss: 0.5455632209777832\n",
      "Epoch 65: Train Loss: 0.37838777899742126, Validation Loss: 0.5480935573577881\n",
      "Epoch 66: Train Loss: 0.39173271258672077, Validation Loss: 0.5575800538063049\n",
      "Epoch 67: Train Loss: 0.3738684356212616, Validation Loss: 0.561832845211029\n",
      "Epoch 68: Train Loss: 0.38863543669382733, Validation Loss: 0.5628902912139893\n",
      "Epoch 69: Train Loss: 0.3803688685099284, Validation Loss: 0.5627151131629944\n",
      "Epoch 70: Train Loss: 0.3733663360277812, Validation Loss: 0.5508027672767639\n",
      "Epoch 71: Train Loss: 0.3709222376346588, Validation Loss: 0.529761016368866\n",
      "Epoch 72: Train Loss: 0.3727787633736928, Validation Loss: 0.5279914140701294\n",
      "Epoch 73: Train Loss: 0.37753116091092426, Validation Loss: 0.5248276591300964\n",
      "Epoch 74: Train Loss: 0.3707466125488281, Validation Loss: 0.5234125256538391\n",
      "Epoch 75: Train Loss: 0.35892564058303833, Validation Loss: 0.5243971943855286\n",
      "Epoch 76: Train Loss: 0.3751762906710307, Validation Loss: 0.5308720469474792\n",
      "Epoch 77: Train Loss: 0.3938305874665578, Validation Loss: 0.5353842377662659\n",
      "Epoch 78: Train Loss: 0.36682186524073285, Validation Loss: 0.5363996624946594\n",
      "Epoch 79: Train Loss: 0.37295467654863995, Validation Loss: 0.5362746119499207\n",
      "Epoch 80: Train Loss: 0.3655464748541514, Validation Loss: 0.5287308096885681\n",
      "Epoch 81: Train Loss: 0.3786644736925761, Validation Loss: 0.5299128293991089\n",
      "Epoch 82: Train Loss: 0.3745565513769786, Validation Loss: 0.5248613953590393\n",
      "Epoch 83: Train Loss: 0.36026840408643085, Validation Loss: 0.5204635858535767\n",
      "Epoch 84: Train Loss: 0.3764684001604716, Validation Loss: 0.5397208333015442\n",
      "Epoch 85: Train Loss: 0.36466045180956524, Validation Loss: 0.5563317537307739\n",
      "Epoch 86: Train Loss: 0.36310656865437824, Validation Loss: 0.5637112259864807\n",
      "Epoch 87: Train Loss: 0.35156936446825665, Validation Loss: 0.5654301643371582\n",
      "Epoch 88: Train Loss: 0.3632667064666748, Validation Loss: 0.5635890364646912\n",
      "Epoch 89: Train Loss: 0.36278796195983887, Validation Loss: 0.5627584457397461\n",
      "Epoch 90: Train Loss: 0.36317012707392377, Validation Loss: 0.5379135012626648\n",
      "Epoch 91: Train Loss: 0.36014939347902936, Validation Loss: 0.5162054896354675\n",
      "Epoch 92: Train Loss: 0.35581759611765545, Validation Loss: 0.4965144991874695\n",
      "Epoch 93: Train Loss: 0.37571704387664795, Validation Loss: 0.5047388672828674\n",
      "Epoch 94: Train Loss: 0.3663522005081177, Validation Loss: 0.5121290683746338\n",
      "Epoch 95: Train Loss: 0.33751001954078674, Validation Loss: 0.5203357338905334\n",
      "Epoch 96: Train Loss: 0.35412583748499554, Validation Loss: 0.5122074484825134\n",
      "Epoch 97: Train Loss: 0.35315313935279846, Validation Loss: 0.5057553052902222\n",
      "Epoch 98: Train Loss: 0.3595777253309886, Validation Loss: 0.5048589110374451\n",
      "Epoch 99: Train Loss: 0.3559967776139577, Validation Loss: 0.5053589344024658\n",
      "Epoch 100: Train Loss: 0.342137614885966, Validation Loss: 0.5324875712394714\n",
      "Epoch 101: Train Loss: 0.34808069467544556, Validation Loss: 0.5548354387283325\n",
      "Epoch 102: Train Loss: 0.34845802187919617, Validation Loss: 0.5556417107582092\n",
      "Epoch 103: Train Loss: 0.3752460678418477, Validation Loss: 0.5396644473075867\n",
      "Epoch 104: Train Loss: 0.362079381942749, Validation Loss: 0.5090815424919128\n",
      "Epoch 105: Train Loss: 0.3474252720673879, Validation Loss: 0.500391960144043\n",
      "Epoch 106: Train Loss: 0.3644709289073944, Validation Loss: 0.5019312500953674\n",
      "Epoch 107: Train Loss: 0.35010672609011334, Validation Loss: 0.5069738030433655\n",
      "Epoch 108: Train Loss: 0.36863064765930176, Validation Loss: 0.5090661644935608\n",
      "Epoch 109: Train Loss: 0.34484973549842834, Validation Loss: 0.5094342231750488\n",
      "Epoch 110: Train Loss: 0.3470446467399597, Validation Loss: 0.533907949924469\n",
      "Epoch 111: Train Loss: 0.34783779581387836, Validation Loss: 0.5333210229873657\n",
      "Epoch 112: Train Loss: 0.3413822154204051, Validation Loss: 0.5198063254356384\n",
      "Epoch 113: Train Loss: 0.3514099617799123, Validation Loss: 0.5151424407958984\n",
      "Epoch 114: Train Loss: 0.3585933844248454, Validation Loss: 0.5258625745773315\n",
      "Epoch 115: Train Loss: 0.34039992094039917, Validation Loss: 0.5854543447494507\n",
      "Epoch 116: Train Loss: 0.35464775562286377, Validation Loss: 0.5876204967498779\n",
      "Epoch 117: Train Loss: 0.3422224322954814, Validation Loss: 0.5871079564094543\n",
      "Epoch 118: Train Loss: 0.342080016930898, Validation Loss: 0.5864906907081604\n",
      "Epoch 119: Train Loss: 0.35089630881945294, Validation Loss: 0.5859869718551636\n",
      "Epoch 120: Train Loss: 0.34779229760169983, Validation Loss: 0.5385597348213196\n",
      "Epoch 121: Train Loss: 0.33677607774734497, Validation Loss: 0.4958052933216095\n",
      "Epoch 122: Train Loss: 0.3420862356821696, Validation Loss: 0.5328528881072998\n",
      "Epoch 123: Train Loss: 0.3401419421037038, Validation Loss: 0.5784473419189453\n",
      "Epoch 124: Train Loss: 0.3609650830427806, Validation Loss: 0.579066812992096\n",
      "Epoch 125: Train Loss: 0.34983216722806293, Validation Loss: 0.577774167060852\n",
      "Epoch 126: Train Loss: 0.34910082817077637, Validation Loss: 0.5768656730651855\n",
      "Epoch 127: Train Loss: 0.33685680230458576, Validation Loss: 0.5767923593521118\n",
      "Epoch 128: Train Loss: 0.3405698637167613, Validation Loss: 0.5767711997032166\n",
      "Epoch 129: Train Loss: 0.3414980272452037, Validation Loss: 0.576460063457489\n",
      "Epoch 130: Train Loss: 0.3702706495920817, Validation Loss: 0.5027891993522644\n",
      "Epoch 131: Train Loss: 0.34963564078013104, Validation Loss: 0.4899788200855255\n",
      "Epoch 132: Train Loss: 0.3450330098470052, Validation Loss: 0.4870878756046295\n",
      "Epoch 133: Train Loss: 0.33727531631787616, Validation Loss: 0.496814489364624\n",
      "Epoch 134: Train Loss: 0.35272666811943054, Validation Loss: 0.5334835648536682\n",
      "Epoch 135: Train Loss: 0.3411443730195363, Validation Loss: 0.5488004088401794\n",
      "Epoch 136: Train Loss: 0.3369519015153249, Validation Loss: 0.5538199543952942\n",
      "Epoch 137: Train Loss: 0.3406588137149811, Validation Loss: 0.5559654235839844\n",
      "Epoch 138: Train Loss: 0.3307067056496938, Validation Loss: 0.5557227730751038\n",
      "Epoch 139: Train Loss: 0.33615965644518536, Validation Loss: 0.5555764436721802\n",
      "Epoch 140: Train Loss: 0.3387724955876668, Validation Loss: 0.5266348719596863\n",
      "Epoch 141: Train Loss: 0.33580857515335083, Validation Loss: 0.5296356081962585\n",
      "Epoch 142: Train Loss: 0.34861767292022705, Validation Loss: 0.5442115664482117\n",
      "Epoch 143: Train Loss: 0.339842955271403, Validation Loss: 0.5455420613288879\n",
      "Epoch 144: Train Loss: 0.3389509618282318, Validation Loss: 0.5430349111557007\n",
      "Epoch 145: Train Loss: 0.3318943778673808, Validation Loss: 0.5326181054115295\n",
      "Epoch 146: Train Loss: 0.34624863664309186, Validation Loss: 0.536517322063446\n",
      "Epoch 147: Train Loss: 0.3416348099708557, Validation Loss: 0.539412796497345\n",
      "Epoch 148: Train Loss: 0.3356332977612813, Validation Loss: 0.5401715636253357\n",
      "Epoch 149: Train Loss: 0.33230889836947125, Validation Loss: 0.5412362217903137\n",
      "Epoch 150: Train Loss: 0.3389078179995219, Validation Loss: 0.5591156482696533\n",
      "Epoch 151: Train Loss: 0.33753880858421326, Validation Loss: 0.5653173923492432\n",
      "Epoch 152: Train Loss: 0.34064443906148273, Validation Loss: 0.5647689700126648\n",
      "Epoch 153: Train Loss: 0.3579220771789551, Validation Loss: 0.5418912768363953\n",
      "Epoch 154: Train Loss: 0.33044446508089703, Validation Loss: 0.5071771144866943\n",
      "Epoch 155: Train Loss: 0.32891865571339923, Validation Loss: 0.5120288133621216\n",
      "Epoch 156: Train Loss: 0.32943939169247943, Validation Loss: 0.5135374069213867\n",
      "Epoch 157: Train Loss: 0.34000452359517414, Validation Loss: 0.5132460594177246\n",
      "Epoch 158: Train Loss: 0.3504403034845988, Validation Loss: 0.5124779939651489\n",
      "Epoch 159: Train Loss: 0.34772565960884094, Validation Loss: 0.5145006775856018\n",
      "Epoch 160: Train Loss: 0.3412880500157674, Validation Loss: 0.5562012195587158\n",
      "Epoch 161: Train Loss: 0.33425381779670715, Validation Loss: 0.5458177328109741\n",
      "Epoch 162: Train Loss: 0.33724937836329144, Validation Loss: 0.5374413728713989\n",
      "Epoch 163: Train Loss: 0.3409152527650197, Validation Loss: 0.5465651154518127\n",
      "Epoch 164: Train Loss: 0.33752047022183734, Validation Loss: 0.5533896684646606\n",
      "Epoch 165: Train Loss: 0.3419169584910075, Validation Loss: 0.5516483783721924\n",
      "Epoch 166: Train Loss: 0.33211060365041095, Validation Loss: 0.544802188873291\n",
      "Epoch 167: Train Loss: 0.33141260345776874, Validation Loss: 0.5365191698074341\n",
      "Epoch 168: Train Loss: 0.3347679078578949, Validation Loss: 0.5320121645927429\n",
      "Epoch 169: Train Loss: 0.33387941122055054, Validation Loss: 0.5306810140609741\n",
      "Epoch 170: Train Loss: 0.33305931091308594, Validation Loss: 0.3994649648666382\n",
      "Epoch 171: Train Loss: 0.34304924805959064, Validation Loss: 0.38002705574035645\n",
      "Epoch 172: Train Loss: 0.3352540036042531, Validation Loss: 0.4696294069290161\n",
      "Epoch 173: Train Loss: 0.3379550576210022, Validation Loss: 0.5251413583755493\n",
      "Epoch 174: Train Loss: 0.3529813587665558, Validation Loss: 0.5336383581161499\n",
      "Epoch 175: Train Loss: 0.3364155391852061, Validation Loss: 0.5552437901496887\n",
      "Epoch 176: Train Loss: 0.32392563422520954, Validation Loss: 0.5693763494491577\n",
      "Epoch 177: Train Loss: 0.3280826012293498, Validation Loss: 0.574275553226471\n",
      "Epoch 178: Train Loss: 0.33404046297073364, Validation Loss: 0.5746475458145142\n",
      "Epoch 179: Train Loss: 0.3393496771653493, Validation Loss: 0.5741607546806335\n",
      "Epoch 180: Train Loss: 0.3279161949952443, Validation Loss: 0.5726847052574158\n",
      "Epoch 181: Train Loss: 0.32283227642377216, Validation Loss: 0.5667375922203064\n",
      "Epoch 182: Train Loss: 0.3335086206595103, Validation Loss: 0.5692108869552612\n",
      "Epoch 183: Train Loss: 0.33239517609278363, Validation Loss: 0.5689172148704529\n",
      "Epoch 184: Train Loss: 0.33240113655726117, Validation Loss: 0.5670486688613892\n",
      "Epoch 185: Train Loss: 0.3332926134268443, Validation Loss: 0.5648396015167236\n",
      "Epoch 186: Train Loss: 0.33723639448483783, Validation Loss: 0.5627721548080444\n",
      "Epoch 187: Train Loss: 0.3345303535461426, Validation Loss: 0.5598960518836975\n",
      "Epoch 188: Train Loss: 0.33294980724652606, Validation Loss: 0.559851884841919\n",
      "Epoch 189: Train Loss: 0.3323216537634532, Validation Loss: 0.559174656867981\n",
      "Epoch 190: Train Loss: 0.336998184521993, Validation Loss: 0.5679701566696167\n",
      "Epoch 191: Train Loss: 0.33940498034159344, Validation Loss: 0.5682825446128845\n",
      "Epoch 192: Train Loss: 0.32957111795743305, Validation Loss: 0.5692676901817322\n",
      "Epoch 193: Train Loss: 0.3433413803577423, Validation Loss: 0.5720338821411133\n",
      "Epoch 194: Train Loss: 0.35000943144162494, Validation Loss: 0.5674042105674744\n",
      "Epoch 195: Train Loss: 0.335989773273468, Validation Loss: 0.5601732134819031\n",
      "Epoch 196: Train Loss: 0.33646608392397565, Validation Loss: 0.5505364537239075\n",
      "Epoch 197: Train Loss: 0.32383020718892414, Validation Loss: 0.544604480266571\n",
      "Epoch 198: Train Loss: 0.3339908719062805, Validation Loss: 0.5423686504364014\n",
      "Epoch 199: Train Loss: 0.3317982057730357, Validation Loss: 0.5417119860649109\n",
      "Epoch 200: Train Loss: 0.33015551169713336, Validation Loss: 0.5242719054222107\n",
      "Epoch 201: Train Loss: 0.33015843232472736, Validation Loss: 0.44676515460014343\n",
      "Epoch 202: Train Loss: 0.32766122619311017, Validation Loss: 0.4170152544975281\n",
      "Epoch 203: Train Loss: 0.3293246825536092, Validation Loss: 0.40281280875205994\n",
      "Epoch 204: Train Loss: 0.34592119852701825, Validation Loss: 0.42079898715019226\n",
      "Epoch 205: Train Loss: 0.3382537563641866, Validation Loss: 0.46688756346702576\n",
      "Epoch 206: Train Loss: 0.32796205083529156, Validation Loss: 0.47907695174217224\n",
      "Epoch 207: Train Loss: 0.3317207197348277, Validation Loss: 0.4846043884754181\n",
      "Epoch 208: Train Loss: 0.3317292630672455, Validation Loss: 0.48775559663772583\n",
      "Epoch 209: Train Loss: 0.3332589268684387, Validation Loss: 0.4887787699699402\n",
      "Epoch 210: Train Loss: 0.32752593358357746, Validation Loss: 0.5056939721107483\n",
      "Epoch 211: Train Loss: 0.33719272414843243, Validation Loss: 0.5215510129928589\n",
      "Epoch 212: Train Loss: 0.34183340271313983, Validation Loss: 0.5224575400352478\n",
      "Epoch 213: Train Loss: 0.3326251208782196, Validation Loss: 0.5176909565925598\n",
      "Epoch 214: Train Loss: 0.3256237904230754, Validation Loss: 0.5140091180801392\n",
      "Epoch 215: Train Loss: 0.3304524819056193, Validation Loss: 0.5116097927093506\n",
      "Epoch 216: Train Loss: 0.33571772774060565, Validation Loss: 0.5107002854347229\n",
      "Epoch 217: Train Loss: 0.3397717972596486, Validation Loss: 0.5120680928230286\n",
      "Epoch 218: Train Loss: 0.3300179640452067, Validation Loss: 0.5124406218528748\n",
      "Epoch 219: Train Loss: 0.3359916110833486, Validation Loss: 0.5124908685684204\n",
      "Epoch 220: Train Loss: 0.3239569365978241, Validation Loss: 0.5132041573524475\n",
      "Epoch 221: Train Loss: 0.3271738787492116, Validation Loss: 0.5128028392791748\n",
      "Epoch 222: Train Loss: 0.3328856627146403, Validation Loss: 0.5110050439834595\n",
      "Epoch 223: Train Loss: 0.3281741440296173, Validation Loss: 0.5167459845542908\n",
      "Epoch 224: Train Loss: 0.33301108082135517, Validation Loss: 0.5259860754013062\n",
      "Epoch 225: Train Loss: 0.326853483915329, Validation Loss: 0.5292595028877258\n",
      "Epoch 226: Train Loss: 0.3280772666136424, Validation Loss: 0.5305677056312561\n",
      "Epoch 227: Train Loss: 0.3308511773745219, Validation Loss: 0.5308818221092224\n",
      "Epoch 228: Train Loss: 0.3304170072078705, Validation Loss: 0.530849277973175\n",
      "Epoch 229: Train Loss: 0.3203027943770091, Validation Loss: 0.5307729244232178\n",
      "Epoch 230: Train Loss: 0.32789260149002075, Validation Loss: 0.5270888209342957\n",
      "Epoch 231: Train Loss: 0.32920483748118085, Validation Loss: 0.5184077620506287\n",
      "Epoch 232: Train Loss: 0.33036236961682636, Validation Loss: 0.5119527578353882\n",
      "Epoch 233: Train Loss: 0.33530816435813904, Validation Loss: 0.5163617730140686\n",
      "Epoch 234: Train Loss: 0.3307982087135315, Validation Loss: 0.514942467212677\n",
      "Epoch 235: Train Loss: 0.3284856081008911, Validation Loss: 0.5140228271484375\n",
      "Epoch 236: Train Loss: 0.3245406150817871, Validation Loss: 0.5139808654785156\n",
      "Epoch 237: Train Loss: 0.32789196570714313, Validation Loss: 0.5147681832313538\n",
      "Epoch 238: Train Loss: 0.3268984854221344, Validation Loss: 0.5150191783905029\n",
      "Epoch 239: Train Loss: 0.32655229171117145, Validation Loss: 0.5149929523468018\n",
      "Epoch 240: Train Loss: 0.34127453962961835, Validation Loss: 0.5072273015975952\n",
      "Epoch 241: Train Loss: 0.33094944556554157, Validation Loss: 0.501786470413208\n",
      "Epoch 242: Train Loss: 0.32338613271713257, Validation Loss: 0.5031065344810486\n",
      "Epoch 243: Train Loss: 0.33690393964449566, Validation Loss: 0.5109955072402954\n",
      "Epoch 244: Train Loss: 0.32400522629419964, Validation Loss: 0.516764760017395\n",
      "Epoch 245: Train Loss: 0.3301832278569539, Validation Loss: 0.519059956073761\n",
      "Epoch 246: Train Loss: 0.32773781816164654, Validation Loss: 0.5178029537200928\n",
      "Epoch 247: Train Loss: 0.32722708582878113, Validation Loss: 0.5148437023162842\n",
      "Epoch 248: Train Loss: 0.32862042387326557, Validation Loss: 0.5129532217979431\n",
      "Epoch 249: Train Loss: 0.3242197334766388, Validation Loss: 0.5128949284553528\n",
      "Epoch 250: Train Loss: 0.32815630237261456, Validation Loss: 0.5012000799179077\n",
      "Epoch 251: Train Loss: 0.3247233033180237, Validation Loss: 0.4896766245365143\n",
      "Epoch 252: Train Loss: 0.3316279451052348, Validation Loss: 0.4728408753871918\n",
      "Epoch 253: Train Loss: 0.32068584362665814, Validation Loss: 0.4521206021308899\n",
      "Epoch 254: Train Loss: 0.32649755477905273, Validation Loss: 0.44444766640663147\n",
      "Epoch 255: Train Loss: 0.3186193108558655, Validation Loss: 0.44374755024909973\n",
      "Epoch 256: Train Loss: 0.31829607486724854, Validation Loss: 0.4445345997810364\n",
      "Epoch 257: Train Loss: 0.3224472502867381, Validation Loss: 0.45097318291664124\n",
      "Epoch 258: Train Loss: 0.3217300772666931, Validation Loss: 0.45885151624679565\n",
      "Epoch 259: Train Loss: 0.3203827639420827, Validation Loss: 0.46148037910461426\n",
      "Epoch 260: Train Loss: 0.3346798320611318, Validation Loss: 0.49498212337493896\n",
      "Epoch 261: Train Loss: 0.32528329888979596, Validation Loss: 0.5063174962997437\n",
      "Epoch 262: Train Loss: 0.3212999800841014, Validation Loss: 0.5120741724967957\n",
      "Epoch 263: Train Loss: 0.3318639894326528, Validation Loss: 0.5118055939674377\n",
      "Epoch 264: Train Loss: 0.33446450034777325, Validation Loss: 0.5049561262130737\n",
      "Epoch 265: Train Loss: 0.32592351237932843, Validation Loss: 0.5024389028549194\n",
      "Epoch 266: Train Loss: 0.328444500764211, Validation Loss: 0.4997475743293762\n",
      "Epoch 267: Train Loss: 0.3234235942363739, Validation Loss: 0.4979066848754883\n",
      "Epoch 268: Train Loss: 0.3253866136074066, Validation Loss: 0.4970409572124481\n",
      "Epoch 269: Train Loss: 0.3240664800008138, Validation Loss: 0.4971364736557007\n",
      "Epoch 270: Train Loss: 0.32320907711982727, Validation Loss: 0.49781373143196106\n",
      "Early stopping at epoch 271\n",
      "Accuracy: 0.7666666666666667,Precision: 0.7857142857142857, Recall: 0.7333333333333333, F1-score: 0.7586206896551724, AUC: 0.7666666666666667\n",
      "Confusion Matrix:\n",
      "[[48 12]\n",
      " [16 44]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "\n",
    "model_save_dir = r'C:\\Users\\User\\Documents\\Lie detect data\\Model'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Define a function to load and pad data\n",
    "def load_data(data_dir, max_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    file_list = os.listdir(data_dir)\n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        label = 1 if 'truth' in file else 0\n",
    "        if data.shape[1] > max_length:\n",
    "            processed_data = data[:, :max_length]  # Cut data if it exceeds max_length\n",
    "        else:\n",
    "            processed_data = np.zeros((data.shape[0], max_length))\n",
    "            processed_data[:, :data.shape[1]] = data  # Pad data if it is shorter than max_length\n",
    "        X.append(processed_data)\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load dataset and pad the data\n",
    "data_dir = \"C:\\\\Users\\\\User\\\\Documents\\\\Lie detect data\\\\56M_DWTEEGData\"\n",
    "max_length = 500 # Define maximum length for padding\n",
    "X, y = load_data(data_dir, max_length)\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure the data is reshaped to [1, Chans, Samples]\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32).unsqueeze(0), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# Define EEGNet model\n",
    "class Conv2dWithConstraint(nn.Conv2d):\n",
    "    def __init__(self, *args, max_norm=1, **kwargs):\n",
    "        self.max_norm = max_norm\n",
    "        super(Conv2dWithConstraint, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data = torch.renorm(\n",
    "            self.weight.data, p=2, dim=0, maxnorm=self.max_norm\n",
    "        )\n",
    "        return super(Conv2dWithConstraint, self).forward(x)\n",
    "\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def InitialBlocks(self, dropoutRate, *args, **kwargs):\n",
    "        block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, self.F1, (1, self.kernelLength), stride=1, padding=(0, self.kernelLength // 2), bias=False),\n",
    "            nn.BatchNorm2d(self.F1, momentum=0.01, affine=True, eps=1e-3),\n",
    "\n",
    "            # DepthwiseConv2D =======================\n",
    "            Conv2dWithConstraint(self.F1, self.F1 * self.D, (self.channels, 1), max_norm=1, stride=1, padding=(0, 0),\n",
    "                                 groups=self.F1, bias=False),\n",
    "            # ========================================\n",
    "\n",
    "            nn.BatchNorm2d(self.F1 * self.D, momentum=0.01, affine=True, eps=1e-3),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 4), stride=4),\n",
    "            nn.Dropout(p=dropoutRate))\n",
    "        block2 = nn.Sequential(\n",
    "            # SeparableConv2D =======================\n",
    "            nn.Conv2d(self.F1 * self.D, self.F1 * self.D, (1, self.kernelLength2), stride=1,\n",
    "                      padding=(0, self.kernelLength2 // 2), bias=False, groups=self.F1 * self.D),\n",
    "            nn.Conv2d(self.F1 * self.D, self.F2, 1, padding=(0, 0), groups=1, bias=False, stride=1),\n",
    "            # ========================================\n",
    "\n",
    "            nn.BatchNorm2d(self.F2, momentum=0.01, affine=True, eps=1e-3),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 8), stride=8),\n",
    "            nn.Dropout(p=dropoutRate))\n",
    "        return nn.Sequential(block1, block2)\n",
    "\n",
    "\n",
    "    def ClassifierBlock(self, inputSize, n_classes):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(inputSize, n_classes, bias=False),\n",
    "            nn.Softmax(dim=1))\n",
    "\n",
    "    def CalculateOutSize(self, model, channels, samples):\n",
    "        '''\n",
    "        Calculate the output based on input size.\n",
    "        model is from nn.Module and inputSize is a array.\n",
    "        '''\n",
    "        data = torch.rand(1, 1, channels, samples)\n",
    "        model.eval()\n",
    "        out = model(data).shape\n",
    "        return out[2:]\n",
    "\n",
    "    def __init__(self, n_classes=2, channels=65, samples=500,\n",
    "                 dropoutRate=0.5, kernelLength=128, kernelLength2=32, F1=8,\n",
    "                 D=2, F2=16):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.F1 = F1\n",
    "        self.F2 = F2\n",
    "        self.D = D\n",
    "        self.samples = samples\n",
    "        self.n_classes = n_classes\n",
    "        self.channels = channels\n",
    "        self.kernelLength = kernelLength\n",
    "        self.kernelLength2 = kernelLength2\n",
    "        self.dropoutRate = dropoutRate\n",
    "\n",
    "        self.blocks = self.InitialBlocks(dropoutRate)\n",
    "        self.blockOutputSize = self.CalculateOutSize(self.blocks, channels, samples)\n",
    "        self.classifierBlock = self.ClassifierBlock(self.F2 * self.blockOutputSize[1], n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        x = x.view(x.size()[0], -1)  # Flatten\n",
    "        x = self.classifierBlock(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def categorical_cross_entropy(y_pred, y_true):\n",
    "    # y_pred = y_pred.cuda()\n",
    "    # y_true = y_true.cuda()\n",
    "    y_pred = torch.clamp(y_pred, 1e-9, 1 - 1e-9)\n",
    "    return -(y_true * torch.log(y_pred)).sum(dim=1).mean()\n",
    "\n",
    "def torch_summarize(model, show_weights=True, show_parameters=True):\n",
    "    \"\"\"Summarizes torch model by showing trainable parameters and weights.\"\"\"\n",
    "    tmpstr = model.__class__.__name__ + ' (\\n'\n",
    "    for key, module in model._modules.items():\n",
    "        # if it contains layers let call it recursively to get params and weights\n",
    "        if type(module) in [\n",
    "            torch.nn.modules.container.Container,\n",
    "            torch.nn.modules.container.Sequential\n",
    "        ]:\n",
    "            modstr = torch_summarize(module)\n",
    "        else:\n",
    "            modstr = module.__repr__()\n",
    "        modstr = _addindent(modstr, 2)\n",
    "\n",
    "        params = sum([np.prod(p.size()) for p in module.parameters()])\n",
    "        weights = tuple([tuple(p.size()) for p in module.parameters()])\n",
    "\n",
    "        tmpstr += '  (' + key + '): ' + modstr\n",
    "        if show_weights:\n",
    "            tmpstr += ', weights={}'.format(weights)\n",
    "        if show_parameters:\n",
    "            tmpstr +=  ', parameters={}'.format(params)\n",
    "        tmpstr += '\\n'\n",
    "\n",
    "    tmpstr = tmpstr + ')'\n",
    "    return tmpstr\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_and_evaluate(train_loader, val_loader, y_train):\n",
    "    model = EEGNet().to(device)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-8)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "    num_epochs = 500\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 100\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "            fold_model_path = os.path.join(model_save_dir, f'revise_model_fold_{fold_idx}.pth')\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': best_val_loss,\n",
    "            }, fold_model_path)\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_idx = 0\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "for train_index, val_index in kf.split(X, y):\n",
    "    print(f'Fold {fold_idx + 1}')\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Normalize data using scaler fitted on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_val = scaler.transform(X_val.reshape(X_val.shape[0], -1))\n",
    "    X_train = X_train.reshape(-1, 65, max_length)\n",
    "    X_val = X_val.reshape(-1, 65, max_length)\n",
    "\n",
    "    # Save the scaler to a file\n",
    "    with open(r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\RevisedEEGNet_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = EEGDataset(X_train, y_train)\n",
    "    val_dataset = EEGDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = train_and_evaluate(train_loader, val_loader, y_train)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    fold_idx += 1\n",
    "\n",
    "\n",
    "# Calculate additional metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "auc = roc_auc_score(all_labels, all_predictions)\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy},Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae639325-1800-4763-ad37-b5931e4391c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
