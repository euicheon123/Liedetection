{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d5ee57-cf0b-4330-985e-ada385863b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'lie' samples: 65\n",
      "Number of 'truth' samples: 78\n",
      "Total number of samples: 143\n",
      "Adding 5 lie samples from subject 8 to test set\n",
      "Adding 6 truth samples from subject 8 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\detectlie\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Convolution.cpp:1032.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.7130306892924838, Validation Loss: 0.694424033164978\n",
      "Epoch 1: Train Loss: 0.7060549325413175, Validation Loss: 0.6962639689445496\n",
      "Epoch 2: Train Loss: 0.6788271334436204, Validation Loss: 0.6969062089920044\n",
      "Epoch 3: Train Loss: 0.6747550434536405, Validation Loss: 0.6988797187805176\n",
      "Epoch 4: Train Loss: 0.7223426302274069, Validation Loss: 0.701143741607666\n",
      "Epoch 5: Train Loss: 0.6586356626616584, Validation Loss: 0.7011482119560242\n",
      "Epoch 6: Train Loss: 0.6743513743082682, Validation Loss: 0.7017793655395508\n",
      "Epoch 7: Train Loss: 0.6278863747914633, Validation Loss: 0.7024704813957214\n",
      "Epoch 8: Train Loss: 0.6350215276082357, Validation Loss: 0.7028712630271912\n",
      "Epoch 9: Train Loss: 0.6540638274616666, Validation Loss: 0.7031747102737427\n",
      "Epoch 10: Train Loss: 0.6529478563202752, Validation Loss: 0.7019696235656738\n",
      "Epoch 11: Train Loss: 0.6324181093109978, Validation Loss: 0.7012786269187927\n",
      "Epoch 12: Train Loss: 0.6366004347801208, Validation Loss: 0.6994778513908386\n",
      "Epoch 13: Train Loss: 0.6219781637191772, Validation Loss: 0.6985298991203308\n",
      "Epoch 14: Train Loss: 0.6243709193335639, Validation Loss: 0.6948217153549194\n",
      "Epoch 15: Train Loss: 0.6152814825375875, Validation Loss: 0.6922280192375183\n",
      "Epoch 16: Train Loss: 0.6352027323510911, Validation Loss: 0.6917810440063477\n",
      "Epoch 17: Train Loss: 0.6026298933558993, Validation Loss: 0.6923739314079285\n",
      "Epoch 18: Train Loss: 0.6323685778511895, Validation Loss: 0.6926006078720093\n",
      "Epoch 19: Train Loss: 0.5886038641134897, Validation Loss: 0.692202627658844\n",
      "Epoch 20: Train Loss: 0.5931627518600888, Validation Loss: 0.6857731342315674\n",
      "Epoch 21: Train Loss: 0.5814842647976346, Validation Loss: 0.6833875775337219\n",
      "Epoch 22: Train Loss: 0.5765419403711954, Validation Loss: 0.6795546412467957\n",
      "Epoch 23: Train Loss: 0.5667481223742167, Validation Loss: 0.6757588386535645\n",
      "Epoch 24: Train Loss: 0.5429528454939524, Validation Loss: 0.6715454459190369\n",
      "Epoch 25: Train Loss: 0.5563165346781412, Validation Loss: 0.6688348054885864\n",
      "Epoch 26: Train Loss: 0.5718841221597459, Validation Loss: 0.6661936640739441\n",
      "Epoch 27: Train Loss: 0.5217158496379852, Validation Loss: 0.6642429828643799\n",
      "Epoch 28: Train Loss: 0.5657555494043562, Validation Loss: 0.6639209389686584\n",
      "Epoch 29: Train Loss: 0.5508584943082597, Validation Loss: 0.6625756025314331\n",
      "Epoch 30: Train Loss: 0.5863038235240512, Validation Loss: 0.6565617322921753\n",
      "Epoch 31: Train Loss: 0.5882852209938897, Validation Loss: 0.6475255489349365\n",
      "Epoch 32: Train Loss: 0.5275632076793246, Validation Loss: 0.6368929147720337\n",
      "Epoch 33: Train Loss: 0.5353136228190528, Validation Loss: 0.6296679973602295\n",
      "Epoch 34: Train Loss: 0.5053263207276663, Validation Loss: 0.6251415014266968\n",
      "Epoch 35: Train Loss: 0.506544354889128, Validation Loss: 0.6149945855140686\n",
      "Epoch 36: Train Loss: 0.5033698909812503, Validation Loss: 0.6096199750900269\n",
      "Epoch 37: Train Loss: 0.5117382109165192, Validation Loss: 0.6071086525917053\n",
      "Epoch 38: Train Loss: 0.49652276436487836, Validation Loss: 0.6033222675323486\n",
      "Epoch 39: Train Loss: 0.47906490166982013, Validation Loss: 0.602784276008606\n",
      "Epoch 40: Train Loss: 0.47453287574979996, Validation Loss: 0.597639799118042\n",
      "Epoch 41: Train Loss: 0.4642926553885142, Validation Loss: 0.5934224724769592\n",
      "Epoch 42: Train Loss: 0.45277047488424516, Validation Loss: 0.587296724319458\n",
      "Epoch 43: Train Loss: 0.47242722908655804, Validation Loss: 0.5858385562896729\n",
      "Epoch 44: Train Loss: 0.4438536994987064, Validation Loss: 0.5726350545883179\n",
      "Epoch 45: Train Loss: 0.4173579083548652, Validation Loss: 0.5741807222366333\n",
      "Epoch 46: Train Loss: 0.41931379503673977, Validation Loss: 0.567703127861023\n",
      "Epoch 47: Train Loss: 0.41252487897872925, Validation Loss: 0.5673854351043701\n",
      "Epoch 48: Train Loss: 0.45217911071247524, Validation Loss: 0.5640270709991455\n",
      "Epoch 49: Train Loss: 0.415977590613895, Validation Loss: 0.563279390335083\n",
      "Epoch 50: Train Loss: 0.44927094380060834, Validation Loss: 0.5676093697547913\n",
      "Epoch 51: Train Loss: 0.3744649009572135, Validation Loss: 0.5653687119483948\n",
      "Epoch 52: Train Loss: 0.4117714961369832, Validation Loss: 0.5648389458656311\n",
      "Epoch 53: Train Loss: 0.4173988302548726, Validation Loss: 0.5721523761749268\n",
      "Epoch 54: Train Loss: 0.4174359109666612, Validation Loss: 0.5723384022712708\n",
      "Epoch 55: Train Loss: 0.4008159041404724, Validation Loss: 0.5858134627342224\n",
      "Epoch 56: Train Loss: 0.4044797453615401, Validation Loss: 0.575747013092041\n",
      "Epoch 57: Train Loss: 0.39516250954733956, Validation Loss: 0.5733298063278198\n",
      "Epoch 58: Train Loss: 0.3904031117757161, Validation Loss: 0.5637966394424438\n",
      "Epoch 59: Train Loss: 0.4094484779569838, Validation Loss: 0.5601236820220947\n",
      "Epoch 60: Train Loss: 0.3921257058779399, Validation Loss: 0.5628954172134399\n",
      "Epoch 61: Train Loss: 0.37155626548661125, Validation Loss: 0.5688246488571167\n",
      "Epoch 62: Train Loss: 0.40135740571551853, Validation Loss: 0.573204517364502\n",
      "Epoch 63: Train Loss: 0.3565792540709178, Validation Loss: 0.5739274621009827\n",
      "Epoch 64: Train Loss: 0.3771831492582957, Validation Loss: 0.5718041062355042\n",
      "Epoch 65: Train Loss: 0.3640496763918135, Validation Loss: 0.5775213837623596\n",
      "Epoch 66: Train Loss: 0.35011642177899677, Validation Loss: 0.5833747982978821\n",
      "Epoch 67: Train Loss: 0.3706092768245273, Validation Loss: 0.5828171372413635\n",
      "Epoch 68: Train Loss: 0.3550783975256814, Validation Loss: 0.5742301940917969\n",
      "Epoch 69: Train Loss: 0.37979840238889057, Validation Loss: 0.5847288370132446\n",
      "Epoch 70: Train Loss: 0.35569238000445896, Validation Loss: 0.5740456581115723\n",
      "Epoch 71: Train Loss: 0.3176792876587974, Validation Loss: 0.5711255669593811\n",
      "Epoch 72: Train Loss: 0.3231416775120629, Validation Loss: 0.5804066061973572\n",
      "Epoch 73: Train Loss: 0.3318780693742964, Validation Loss: 0.5627508163452148\n",
      "Epoch 74: Train Loss: 0.35766275392638314, Validation Loss: 0.5757629871368408\n",
      "Epoch 75: Train Loss: 0.32748528983857894, Validation Loss: 0.5884209275245667\n",
      "Epoch 76: Train Loss: 0.33084721200995976, Validation Loss: 0.5799577832221985\n",
      "Epoch 77: Train Loss: 0.30635399123032886, Validation Loss: 0.585116446018219\n",
      "Epoch 78: Train Loss: 0.3219946258597904, Validation Loss: 0.5882214307785034\n",
      "Epoch 79: Train Loss: 0.3207122004694409, Validation Loss: 0.5894203186035156\n",
      "Epoch 80: Train Loss: 0.3280840598874622, Validation Loss: 0.5847371220588684\n",
      "Epoch 81: Train Loss: 0.341103454430898, Validation Loss: 0.5853784084320068\n",
      "Epoch 82: Train Loss: 0.3171706944704056, Validation Loss: 0.6011019349098206\n",
      "Epoch 83: Train Loss: 0.3669651713636186, Validation Loss: 0.5883283019065857\n",
      "Epoch 84: Train Loss: 0.31425243947241044, Validation Loss: 0.6134401559829712\n",
      "Epoch 85: Train Loss: 0.3132769862810771, Validation Loss: 0.6177285313606262\n",
      "Epoch 86: Train Loss: 0.2789120591349072, Validation Loss: 0.630608320236206\n",
      "Epoch 87: Train Loss: 0.3306585450967153, Validation Loss: 0.625260055065155\n",
      "Epoch 88: Train Loss: 0.3133516278531816, Validation Loss: 0.6372080445289612\n",
      "Epoch 89: Train Loss: 0.27700382967789966, Validation Loss: 0.6195864677429199\n",
      "Epoch 90: Train Loss: 0.33294688330756295, Validation Loss: 0.6137352585792542\n",
      "Epoch 91: Train Loss: 0.2686691747771369, Validation Loss: 0.5935684442520142\n",
      "Epoch 92: Train Loss: 0.2907160884804196, Validation Loss: 0.6030804514884949\n",
      "Epoch 93: Train Loss: 0.3102397587564256, Validation Loss: 0.6178882122039795\n",
      "Epoch 94: Train Loss: 0.26829389731089276, Validation Loss: 0.6350486874580383\n",
      "Epoch 95: Train Loss: 0.2706539001729753, Validation Loss: 0.625128984451294\n",
      "Epoch 96: Train Loss: 0.27907263735930127, Validation Loss: 0.624011218547821\n",
      "Epoch 97: Train Loss: 0.26146913402610356, Validation Loss: 0.6402216553688049\n",
      "Epoch 98: Train Loss: 0.28316957917478347, Validation Loss: 0.6230712532997131\n",
      "Epoch 99: Train Loss: 0.24667427192131677, Validation Loss: 0.6332896947860718\n",
      "Epoch 100: Train Loss: 0.2655249370468987, Validation Loss: 0.637418806552887\n",
      "Epoch 101: Train Loss: 0.28721632063388824, Validation Loss: 0.6213096380233765\n",
      "Epoch 102: Train Loss: 0.2770761516359117, Validation Loss: 0.6345024704933167\n",
      "Epoch 103: Train Loss: 0.24063792162471348, Validation Loss: 0.6345018148422241\n",
      "Epoch 104: Train Loss: 0.25581812693013084, Validation Loss: 0.6243327260017395\n",
      "Epoch 105: Train Loss: 0.2904895411597358, Validation Loss: 0.6227034330368042\n",
      "Epoch 106: Train Loss: 0.25038443009058636, Validation Loss: 0.6275023221969604\n",
      "Epoch 107: Train Loss: 0.25959037409888375, Validation Loss: 0.6283707618713379\n",
      "Epoch 108: Train Loss: 0.2669866647985246, Validation Loss: 0.5845645666122437\n",
      "Epoch 109: Train Loss: 0.24501233961847094, Validation Loss: 0.6163055300712585\n",
      "Epoch 110: Train Loss: 0.22866918477747175, Validation Loss: 0.6363012194633484\n",
      "Epoch 111: Train Loss: 0.2558952073256175, Validation Loss: 0.6424320936203003\n",
      "Epoch 112: Train Loss: 0.2590937829679913, Validation Loss: 0.6039290428161621\n",
      "Epoch 113: Train Loss: 0.2589842180411021, Validation Loss: 0.6282740831375122\n",
      "Epoch 114: Train Loss: 0.26148829691939884, Validation Loss: 0.6398077011108398\n",
      "Epoch 115: Train Loss: 0.2148525793519285, Validation Loss: 0.6684259176254272\n",
      "Epoch 116: Train Loss: 0.25968028273847366, Validation Loss: 0.6559695601463318\n",
      "Epoch 117: Train Loss: 0.22424647046460044, Validation Loss: 0.6500627994537354\n",
      "Epoch 118: Train Loss: 0.2195465945535236, Validation Loss: 0.6626859903335571\n",
      "Epoch 119: Train Loss: 0.23003767513566548, Validation Loss: 0.6438521146774292\n",
      "Epoch 120: Train Loss: 0.21833716498480904, Validation Loss: 0.6694282293319702\n",
      "Epoch 121: Train Loss: 0.2081802793674999, Validation Loss: 0.666234016418457\n",
      "Epoch 122: Train Loss: 0.2192694064643648, Validation Loss: 0.6708599925041199\n",
      "Epoch 123: Train Loss: 0.19288634922769335, Validation Loss: 0.6720921397209167\n",
      "Epoch 124: Train Loss: 0.21127817696995205, Validation Loss: 0.6481515765190125\n",
      "Epoch 125: Train Loss: 0.21588782303863102, Validation Loss: 0.6727564334869385\n",
      "Epoch 126: Train Loss: 0.25604574879010517, Validation Loss: 0.6482404470443726\n",
      "Epoch 127: Train Loss: 0.1960038741429647, Validation Loss: 0.6428804993629456\n",
      "Epoch 128: Train Loss: 0.2210844573047426, Validation Loss: 0.6411741971969604\n",
      "Epoch 129: Train Loss: 0.2411902430984709, Validation Loss: 0.6458261609077454\n",
      "Epoch 130: Train Loss: 0.20734474228488076, Validation Loss: 0.6691168546676636\n",
      "Epoch 131: Train Loss: 0.19341183453798294, Validation Loss: 0.6508613228797913\n",
      "Epoch 132: Train Loss: 0.23269924521446228, Validation Loss: 0.6658064126968384\n",
      "Epoch 133: Train Loss: 0.20723769772383901, Validation Loss: 0.6765731573104858\n",
      "Epoch 134: Train Loss: 0.2116270454393493, Validation Loss: 0.6332350373268127\n",
      "Epoch 135: Train Loss: 0.2092536621623569, Validation Loss: 0.6493301391601562\n",
      "Epoch 136: Train Loss: 0.21578272846009997, Validation Loss: 0.6638107299804688\n",
      "Epoch 137: Train Loss: 0.18193420850568348, Validation Loss: 0.6675518155097961\n",
      "Epoch 138: Train Loss: 0.21404996017615, Validation Loss: 0.6786138415336609\n",
      "Epoch 139: Train Loss: 0.21485509640640682, Validation Loss: 0.696465790271759\n",
      "Epoch 140: Train Loss: 0.23932785789171854, Validation Loss: 0.6525457501411438\n",
      "Epoch 141: Train Loss: 0.1656006963716613, Validation Loss: 0.6569766998291016\n",
      "Epoch 142: Train Loss: 0.3014618042442534, Validation Loss: 0.6022151708602905\n",
      "Epoch 143: Train Loss: 0.17361504750119316, Validation Loss: 0.5972007513046265\n",
      "Epoch 144: Train Loss: 0.167379439704948, Validation Loss: 0.6497587561607361\n",
      "Epoch 145: Train Loss: 0.1691269667612182, Validation Loss: 0.6967225670814514\n",
      "Epoch 146: Train Loss: 0.1938103288412094, Validation Loss: 0.7076279520988464\n",
      "Epoch 147: Train Loss: 0.1804189971751637, Validation Loss: 0.7263878583908081\n",
      "Epoch 148: Train Loss: 0.17980557762914234, Validation Loss: 0.7060161828994751\n",
      "Epoch 149: Train Loss: 0.2130440589454439, Validation Loss: 0.6632617712020874\n",
      "Epoch 150: Train Loss: 0.18132594972848892, Validation Loss: 0.6714875102043152\n",
      "Epoch 151: Train Loss: 0.21252159857087666, Validation Loss: 0.7024775147438049\n",
      "Epoch 152: Train Loss: 0.20438789907428953, Validation Loss: 0.6962641477584839\n",
      "Epoch 153: Train Loss: 0.19283482515149647, Validation Loss: 0.6814010739326477\n",
      "Epoch 154: Train Loss: 0.19262303908665976, Validation Loss: 0.7069578170776367\n",
      "Epoch 155: Train Loss: 0.17868196053637397, Validation Loss: 0.7081846594810486\n",
      "Epoch 156: Train Loss: 0.20725329137510723, Validation Loss: 0.7337632775306702\n",
      "Epoch 157: Train Loss: 0.16313419159915712, Validation Loss: 0.7297876477241516\n",
      "Epoch 158: Train Loss: 0.19033980949057472, Validation Loss: 0.7585481405258179\n",
      "Epoch 159: Train Loss: 0.17468402120802137, Validation Loss: 0.7283879518508911\n",
      "Epoch 160: Train Loss: 0.16040047961804602, Validation Loss: 0.718390166759491\n",
      "Epoch 161: Train Loss: 0.2145217383901278, Validation Loss: 0.7282536625862122\n",
      "Epoch 162: Train Loss: 0.15515772336059147, Validation Loss: 0.6952067017555237\n",
      "Epoch 163: Train Loss: 0.17904542717668745, Validation Loss: 0.7164945602416992\n",
      "Epoch 164: Train Loss: 0.16144076900349724, Validation Loss: 0.6988053321838379\n",
      "Epoch 165: Train Loss: 0.13749751531415516, Validation Loss: 0.6716184020042419\n",
      "Epoch 166: Train Loss: 0.16639111191034317, Validation Loss: 0.6652377843856812\n",
      "Epoch 167: Train Loss: 0.1497154724266794, Validation Loss: 0.7035183310508728\n",
      "Epoch 168: Train Loss: 0.1799608195821444, Validation Loss: 0.6893811821937561\n",
      "Epoch 169: Train Loss: 0.1644833121034834, Validation Loss: 0.692531943321228\n",
      "Epoch 170: Train Loss: 0.16436292893356746, Validation Loss: 0.7174606919288635\n",
      "Epoch 171: Train Loss: 0.1860441445476479, Validation Loss: 0.6990988850593567\n",
      "Epoch 172: Train Loss: 0.14434991859727436, Validation Loss: 0.710027277469635\n",
      "Epoch 173: Train Loss: 0.15628732326957914, Validation Loss: 0.739250123500824\n",
      "Epoch 174: Train Loss: 0.17370033098591697, Validation Loss: 0.7487088441848755\n",
      "Epoch 175: Train Loss: 0.2259485349059105, Validation Loss: 0.7789440155029297\n",
      "Epoch 176: Train Loss: 0.16915813585122427, Validation Loss: 0.723004162311554\n",
      "Epoch 177: Train Loss: 0.15913861327701145, Validation Loss: 0.739203155040741\n",
      "Epoch 178: Train Loss: 0.1714437413546774, Validation Loss: 0.7577885985374451\n",
      "Epoch 179: Train Loss: 0.16013827174901962, Validation Loss: 0.7137720584869385\n",
      "Epoch 180: Train Loss: 0.12392366843091117, Validation Loss: 0.6838130950927734\n",
      "Epoch 181: Train Loss: 0.138520004434718, Validation Loss: 0.7096939086914062\n",
      "Epoch 182: Train Loss: 0.16796412153376472, Validation Loss: 0.7564520835876465\n",
      "Epoch 183: Train Loss: 0.14843257351054084, Validation Loss: 0.750567615032196\n",
      "Epoch 184: Train Loss: 0.163656960758898, Validation Loss: 0.7281876802444458\n",
      "Epoch 185: Train Loss: 0.16305487354596457, Validation Loss: 0.7570787072181702\n",
      "Epoch 186: Train Loss: 0.16658822198708853, Validation Loss: 0.8224227428436279\n",
      "Epoch 187: Train Loss: 0.13362779633866417, Validation Loss: 0.7713042497634888\n",
      "Epoch 188: Train Loss: 0.1876970496442583, Validation Loss: 0.6832504272460938\n",
      "Epoch 189: Train Loss: 0.1426307020915879, Validation Loss: 0.7529862523078918\n",
      "Epoch 190: Train Loss: 0.11807414889335632, Validation Loss: 0.7685639262199402\n",
      "Epoch 191: Train Loss: 0.14534947027762732, Validation Loss: 0.8056087493896484\n",
      "Epoch 192: Train Loss: 0.13030154506365457, Validation Loss: 0.7715573906898499\n",
      "Epoch 193: Train Loss: 0.1392992908755938, Validation Loss: 0.788062572479248\n",
      "Epoch 194: Train Loss: 0.16109593295388752, Validation Loss: 0.7802864909172058\n",
      "Epoch 195: Train Loss: 0.28948357287380433, Validation Loss: 0.6776266098022461\n",
      "Epoch 196: Train Loss: 0.1308914605114195, Validation Loss: 0.7267553210258484\n",
      "Epoch 197: Train Loss: 0.13487947401073244, Validation Loss: 0.7375192642211914\n",
      "Epoch 198: Train Loss: 0.1395372889108128, Validation Loss: 0.7850677371025085\n",
      "Epoch 199: Train Loss: 0.15322829451825884, Validation Loss: 0.7244553565979004\n",
      "Fold 1 Metrics:\n",
      "Accuracy: 0.7272727272727273, Precision: 0.6666666666666666, Recall: 1.0, F1-score: 0.8, AUC: 0.7\n",
      "Confusion Matrix:\n",
      "[[2 3]\n",
      " [0 6]]\n",
      "Completed fold 1\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples from subject 7 to test set\n",
      "Adding 6 truth samples from subject 7 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.7342318826251559, Validation Loss: 0.6859874725341797\n",
      "Epoch 1: Train Loss: 0.6863512529267205, Validation Loss: 0.6828227043151855\n",
      "Epoch 2: Train Loss: 0.6808455718888177, Validation Loss: 0.6820492148399353\n",
      "Epoch 3: Train Loss: 0.6337383521927727, Validation Loss: 0.683061957359314\n",
      "Epoch 4: Train Loss: 0.6655262377527025, Validation Loss: 0.6850581765174866\n",
      "Epoch 5: Train Loss: 0.7002487116389804, Validation Loss: 0.6858423352241516\n",
      "Epoch 6: Train Loss: 0.6759812765651279, Validation Loss: 0.6858382821083069\n",
      "Epoch 7: Train Loss: 0.6705974539120992, Validation Loss: 0.6864018440246582\n",
      "Epoch 8: Train Loss: 0.6389037834273444, Validation Loss: 0.6862894892692566\n",
      "Epoch 9: Train Loss: 0.6652223004235162, Validation Loss: 0.6870445013046265\n",
      "Epoch 10: Train Loss: 0.6604784064822726, Validation Loss: 0.6869561672210693\n",
      "Epoch 11: Train Loss: 0.6794364849726359, Validation Loss: 0.688537061214447\n",
      "Epoch 12: Train Loss: 0.6192914578649733, Validation Loss: 0.6917478442192078\n",
      "Epoch 13: Train Loss: 0.6117279529571533, Validation Loss: 0.693350076675415\n",
      "Epoch 14: Train Loss: 0.6155001653565301, Validation Loss: 0.6943721771240234\n",
      "Epoch 15: Train Loss: 0.637823243935903, Validation Loss: 0.6957429647445679\n",
      "Epoch 16: Train Loss: 0.6251875162124634, Validation Loss: 0.6959350109100342\n",
      "Epoch 17: Train Loss: 0.6147259407573276, Validation Loss: 0.6953561305999756\n",
      "Epoch 18: Train Loss: 0.594228466351827, Validation Loss: 0.6945950984954834\n",
      "Epoch 19: Train Loss: 0.6008388466305203, Validation Loss: 0.6944919228553772\n",
      "Epoch 20: Train Loss: 0.6325677765740289, Validation Loss: 0.695931613445282\n",
      "Epoch 21: Train Loss: 0.6180485288302103, Validation Loss: 0.6965258121490479\n",
      "Epoch 22: Train Loss: 0.6400770677460564, Validation Loss: 0.6992599964141846\n",
      "Epoch 23: Train Loss: 0.6211911903487312, Validation Loss: 0.7014752626419067\n",
      "Epoch 24: Train Loss: 0.6099966433313158, Validation Loss: 0.7025277018547058\n",
      "Epoch 25: Train Loss: 0.6278149220678542, Validation Loss: 0.7040425539016724\n",
      "Epoch 26: Train Loss: 0.6017869313557943, Validation Loss: 0.7051222920417786\n",
      "Epoch 27: Train Loss: 0.63575170106358, Validation Loss: 0.7053123116493225\n",
      "Epoch 28: Train Loss: 0.5680638021892972, Validation Loss: 0.7062729597091675\n",
      "Epoch 29: Train Loss: 0.5874244703186883, Validation Loss: 0.7052654027938843\n",
      "Epoch 30: Train Loss: 0.561063418785731, Validation Loss: 0.7093148231506348\n",
      "Epoch 31: Train Loss: 0.606071756945716, Validation Loss: 0.7119605541229248\n",
      "Epoch 32: Train Loss: 0.5456627574231889, Validation Loss: 0.712489902973175\n",
      "Epoch 33: Train Loss: 0.5648004346423678, Validation Loss: 0.7171409130096436\n",
      "Epoch 34: Train Loss: 0.5549375414848328, Validation Loss: 0.7188258171081543\n",
      "Epoch 35: Train Loss: 0.5737328860494826, Validation Loss: 0.7186614871025085\n",
      "Epoch 36: Train Loss: 0.5353438489966922, Validation Loss: 0.7211495041847229\n",
      "Epoch 37: Train Loss: 0.5630281600687239, Validation Loss: 0.7202691435813904\n",
      "Epoch 38: Train Loss: 0.5651601155598959, Validation Loss: 0.7218929529190063\n",
      "Epoch 39: Train Loss: 0.5671077801121606, Validation Loss: 0.7226607203483582\n",
      "Epoch 40: Train Loss: 0.555302169587877, Validation Loss: 0.7235004305839539\n",
      "Epoch 41: Train Loss: 0.5446209212144216, Validation Loss: 0.7236665487289429\n",
      "Epoch 42: Train Loss: 0.5318956938054826, Validation Loss: 0.7255938649177551\n",
      "Epoch 43: Train Loss: 0.5540038976404402, Validation Loss: 0.7296106219291687\n",
      "Epoch 44: Train Loss: 0.5142912964026133, Validation Loss: 0.7284272909164429\n",
      "Epoch 45: Train Loss: 0.5331186354160309, Validation Loss: 0.7316218614578247\n",
      "Epoch 46: Train Loss: 0.525534987449646, Validation Loss: 0.7326884269714355\n",
      "Epoch 47: Train Loss: 0.517581108543608, Validation Loss: 0.7350698113441467\n",
      "Epoch 48: Train Loss: 0.5047312875588735, Validation Loss: 0.7347373962402344\n",
      "Epoch 49: Train Loss: 0.5164962510267893, Validation Loss: 0.7341390252113342\n",
      "Epoch 50: Train Loss: 0.5763049821058909, Validation Loss: 0.7389423251152039\n",
      "Epoch 51: Train Loss: 0.5205620163016849, Validation Loss: 0.7424924969673157\n",
      "Epoch 52: Train Loss: 0.4886365797784593, Validation Loss: 0.7466163635253906\n",
      "Epoch 53: Train Loss: 0.4970823327700297, Validation Loss: 0.7488547563552856\n",
      "Epoch 54: Train Loss: 0.4794075886408488, Validation Loss: 0.749186098575592\n",
      "Epoch 55: Train Loss: 0.5032269987795088, Validation Loss: 0.750893771648407\n",
      "Epoch 56: Train Loss: 0.4710656338267856, Validation Loss: 0.7492680549621582\n",
      "Epoch 57: Train Loss: 0.4758249753051334, Validation Loss: 0.7492259740829468\n",
      "Epoch 58: Train Loss: 0.49804986516634625, Validation Loss: 0.7529202699661255\n",
      "Epoch 59: Train Loss: 0.48712366157107884, Validation Loss: 0.7506999373435974\n",
      "Epoch 60: Train Loss: 0.47872984409332275, Validation Loss: 0.7597718834877014\n",
      "Epoch 61: Train Loss: 0.4719760815302531, Validation Loss: 0.7624244689941406\n",
      "Epoch 62: Train Loss: 0.47569231854544747, Validation Loss: 0.7674176692962646\n",
      "Epoch 63: Train Loss: 0.46790698832935756, Validation Loss: 0.7687172889709473\n",
      "Epoch 64: Train Loss: 0.45827927854326034, Validation Loss: 0.7693786025047302\n",
      "Epoch 65: Train Loss: 0.4869308802816603, Validation Loss: 0.769731879234314\n",
      "Epoch 66: Train Loss: 0.4922571082909902, Validation Loss: 0.7710112929344177\n",
      "Epoch 67: Train Loss: 0.4491078290674422, Validation Loss: 0.7725329399108887\n",
      "Epoch 68: Train Loss: 0.47752873102823895, Validation Loss: 0.773351788520813\n",
      "Epoch 69: Train Loss: 0.4636754658487108, Validation Loss: 0.7730529308319092\n",
      "Epoch 70: Train Loss: 0.4713999264770084, Validation Loss: 0.7733725905418396\n",
      "Epoch 71: Train Loss: 0.45446280638376874, Validation Loss: 0.7775018811225891\n",
      "Epoch 72: Train Loss: 0.4518653717305925, Validation Loss: 0.780712902545929\n",
      "Epoch 73: Train Loss: 0.4455849296516842, Validation Loss: 0.7839703559875488\n",
      "Epoch 74: Train Loss: 0.5136541492409177, Validation Loss: 0.78797447681427\n",
      "Epoch 75: Train Loss: 0.45273202657699585, Validation Loss: 0.7855817675590515\n",
      "Epoch 76: Train Loss: 0.42725993196169537, Validation Loss: 0.7901910543441772\n",
      "Epoch 77: Train Loss: 0.39743707246250576, Validation Loss: 0.79184490442276\n",
      "Epoch 78: Train Loss: 0.3954382340113322, Validation Loss: 0.7925813794136047\n",
      "Epoch 79: Train Loss: 0.44773690236939323, Validation Loss: 0.7930504083633423\n",
      "Epoch 80: Train Loss: 0.4285975197950999, Validation Loss: 0.7963674068450928\n",
      "Epoch 81: Train Loss: 0.3977164063188765, Validation Loss: 0.7950505614280701\n",
      "Epoch 82: Train Loss: 0.42255350947380066, Validation Loss: 0.7987037301063538\n",
      "Epoch 83: Train Loss: 0.41465138726764256, Validation Loss: 0.8032849431037903\n",
      "Epoch 84: Train Loss: 0.40248680114746094, Validation Loss: 0.8017776012420654\n",
      "Epoch 85: Train Loss: 0.402517663107978, Validation Loss: 0.8037779927253723\n",
      "Epoch 86: Train Loss: 0.41649695568614536, Validation Loss: 0.8059781789779663\n",
      "Epoch 87: Train Loss: 0.39616016546885174, Validation Loss: 0.8051595687866211\n",
      "Epoch 88: Train Loss: 0.41618577308124965, Validation Loss: 0.8023101687431335\n",
      "Epoch 89: Train Loss: 0.40571645895640057, Validation Loss: 0.8027384877204895\n",
      "Epoch 90: Train Loss: 0.3954440090391371, Validation Loss: 0.8044423460960388\n",
      "Epoch 91: Train Loss: 0.4059505694442325, Validation Loss: 0.8134711384773254\n",
      "Epoch 92: Train Loss: 0.3838620185852051, Validation Loss: 0.8194642066955566\n",
      "Epoch 93: Train Loss: 0.35555823809570736, Validation Loss: 0.8167414665222168\n",
      "Epoch 94: Train Loss: 0.3585168421268463, Validation Loss: 0.8165752291679382\n",
      "Epoch 95: Train Loss: 0.38913462724950576, Validation Loss: 0.8078997731208801\n",
      "Epoch 96: Train Loss: 0.3805573019716475, Validation Loss: 0.808310866355896\n",
      "Epoch 97: Train Loss: 0.3720540437433455, Validation Loss: 0.8067699074745178\n",
      "Epoch 98: Train Loss: 0.36855209204885697, Validation Loss: 0.8079344034194946\n",
      "Epoch 99: Train Loss: 0.3269554128249486, Validation Loss: 0.8095834851264954\n",
      "Epoch 100: Train Loss: 0.3229619943433338, Validation Loss: 0.8019025921821594\n",
      "Epoch 101: Train Loss: 0.3502385053369734, Validation Loss: 0.7933642864227295\n",
      "Epoch 102: Train Loss: 0.33395736085044014, Validation Loss: 0.7945867776870728\n",
      "Epoch 103: Train Loss: 0.31782018807199264, Validation Loss: 0.7938705682754517\n",
      "Epoch 104: Train Loss: 0.3035077022181617, Validation Loss: 0.7908727526664734\n",
      "Epoch 105: Train Loss: 0.3166549801826477, Validation Loss: 0.7837087512016296\n",
      "Epoch 106: Train Loss: 0.3093294898668925, Validation Loss: 0.784608006477356\n",
      "Epoch 107: Train Loss: 0.3256644407908122, Validation Loss: 0.7899729013442993\n",
      "Epoch 108: Train Loss: 0.34991615017255145, Validation Loss: 0.7868242263793945\n",
      "Epoch 109: Train Loss: 0.33493854602177936, Validation Loss: 0.790833592414856\n",
      "Epoch 110: Train Loss: 0.3440399418274562, Validation Loss: 0.7961267232894897\n",
      "Epoch 111: Train Loss: 0.292850808136993, Validation Loss: 0.7956937551498413\n",
      "Epoch 112: Train Loss: 0.31038323375913834, Validation Loss: 0.8001857399940491\n",
      "Epoch 113: Train Loss: 0.3031192587481605, Validation Loss: 0.80283123254776\n",
      "Epoch 114: Train Loss: 0.30780042707920074, Validation Loss: 0.7954351305961609\n",
      "Epoch 115: Train Loss: 0.3430838982264201, Validation Loss: 0.7974468469619751\n",
      "Epoch 116: Train Loss: 0.28717514872550964, Validation Loss: 0.8024022579193115\n",
      "Epoch 117: Train Loss: 0.2879948019981384, Validation Loss: 0.8097243905067444\n",
      "Epoch 118: Train Loss: 0.2866865148146947, Validation Loss: 0.8072971105575562\n",
      "Epoch 119: Train Loss: 0.2657048768467373, Validation Loss: 0.8060229420661926\n",
      "Epoch 120: Train Loss: 0.2691139313909743, Validation Loss: 0.8018572926521301\n",
      "Epoch 121: Train Loss: 0.27993477549817825, Validation Loss: 0.8108646273612976\n",
      "Epoch 122: Train Loss: 0.26281848384274376, Validation Loss: 0.8090268969535828\n",
      "Epoch 123: Train Loss: 0.24947246578004625, Validation Loss: 0.809414267539978\n",
      "Epoch 124: Train Loss: 0.28569336732228595, Validation Loss: 0.8042672872543335\n",
      "Epoch 125: Train Loss: 0.24520641482538647, Validation Loss: 0.810777485370636\n",
      "Epoch 126: Train Loss: 0.2824379884534412, Validation Loss: 0.8042627573013306\n",
      "Epoch 127: Train Loss: 0.24837863776418898, Validation Loss: 0.8069978356361389\n",
      "Epoch 128: Train Loss: 0.25777876211537254, Validation Loss: 0.8252294063568115\n",
      "Epoch 129: Train Loss: 0.27827849321895176, Validation Loss: 0.8263880014419556\n",
      "Epoch 130: Train Loss: 0.26366162962383694, Validation Loss: 0.830371081829071\n",
      "Epoch 131: Train Loss: 0.24217967689037323, Validation Loss: 0.8326255679130554\n",
      "Epoch 132: Train Loss: 0.2669874644941754, Validation Loss: 0.8381370306015015\n",
      "Epoch 133: Train Loss: 0.24951657321718004, Validation Loss: 0.8386515974998474\n",
      "Epoch 134: Train Loss: 0.24145140084955427, Validation Loss: 0.8347032070159912\n",
      "Epoch 135: Train Loss: 0.26358894341521794, Validation Loss: 0.8434616327285767\n",
      "Epoch 136: Train Loss: 0.2175123236245579, Validation Loss: 0.8406610488891602\n",
      "Epoch 137: Train Loss: 0.24705839488241407, Validation Loss: 0.8408552408218384\n",
      "Epoch 138: Train Loss: 0.3014714750978682, Validation Loss: 0.8521682620048523\n",
      "Epoch 139: Train Loss: 0.21411684238248402, Validation Loss: 0.8504279255867004\n",
      "Epoch 140: Train Loss: 0.21162771350807613, Validation Loss: 0.8781788945198059\n",
      "Epoch 141: Train Loss: 0.27683864037195843, Validation Loss: 0.8501790761947632\n",
      "Epoch 142: Train Loss: 0.21275544166564941, Validation Loss: 0.843912661075592\n",
      "Epoch 143: Train Loss: 0.24444622629218632, Validation Loss: 0.8465107083320618\n",
      "Epoch 144: Train Loss: 0.21969056501984596, Validation Loss: 0.8601045608520508\n",
      "Epoch 145: Train Loss: 0.28415415849950576, Validation Loss: 0.8570566177368164\n",
      "Epoch 146: Train Loss: 0.23191510306464302, Validation Loss: 0.853302001953125\n",
      "Epoch 147: Train Loss: 0.22601328624619377, Validation Loss: 0.8674739599227905\n",
      "Epoch 148: Train Loss: 0.21869312723477682, Validation Loss: 0.856488823890686\n",
      "Epoch 149: Train Loss: 0.24458188811937967, Validation Loss: 0.8743820190429688\n",
      "Epoch 150: Train Loss: 0.20571799410714042, Validation Loss: 0.8879963159561157\n",
      "Epoch 151: Train Loss: 0.21807337635093266, Validation Loss: 0.8726504445075989\n",
      "Epoch 152: Train Loss: 0.20208804143799675, Validation Loss: 0.8775990605354309\n",
      "Epoch 153: Train Loss: 0.21777363121509552, Validation Loss: 0.8763938546180725\n",
      "Epoch 154: Train Loss: 0.23889098895920646, Validation Loss: 0.8800444006919861\n",
      "Epoch 155: Train Loss: 0.21663512041171393, Validation Loss: 0.8780657052993774\n",
      "Epoch 156: Train Loss: 0.19899352060423958, Validation Loss: 0.881600558757782\n",
      "Epoch 157: Train Loss: 0.22034523553318447, Validation Loss: 0.8772917985916138\n",
      "Epoch 158: Train Loss: 0.20227973494264814, Validation Loss: 0.8766449093818665\n",
      "Epoch 159: Train Loss: 0.19677821381224525, Validation Loss: 0.885983407497406\n",
      "Epoch 160: Train Loss: 0.20020571682188246, Validation Loss: 0.8804059624671936\n",
      "Epoch 161: Train Loss: 0.2419801933897866, Validation Loss: 0.8909378051757812\n",
      "Epoch 162: Train Loss: 0.2183576010995441, Validation Loss: 0.8797172904014587\n",
      "Epoch 163: Train Loss: 0.2141760074430042, Validation Loss: 0.877115786075592\n",
      "Epoch 164: Train Loss: 0.19911974006228977, Validation Loss: 0.8963861465454102\n",
      "Epoch 165: Train Loss: 0.18401767396264607, Validation Loss: 0.8805236220359802\n",
      "Epoch 166: Train Loss: 0.1783866493238343, Validation Loss: 0.891247570514679\n",
      "Epoch 167: Train Loss: 0.2272412959072325, Validation Loss: 0.8906580209732056\n",
      "Epoch 168: Train Loss: 0.19901576472653282, Validation Loss: 0.887558102607727\n",
      "Epoch 169: Train Loss: 0.1834318381216791, Validation Loss: 0.8937031030654907\n",
      "Epoch 170: Train Loss: 0.19208668337927925, Validation Loss: 0.8902429342269897\n",
      "Epoch 171: Train Loss: 0.18914102845721775, Validation Loss: 0.8903323411941528\n",
      "Epoch 172: Train Loss: 0.24961515681611168, Validation Loss: 0.9121622443199158\n",
      "Epoch 173: Train Loss: 0.17258762568235397, Validation Loss: 0.8999481201171875\n",
      "Epoch 174: Train Loss: 0.20175426950057349, Validation Loss: 0.9030195474624634\n",
      "Epoch 175: Train Loss: 0.1792712261279424, Validation Loss: 0.9038110375404358\n",
      "Epoch 176: Train Loss: 0.18826762669616276, Validation Loss: 0.8989078402519226\n",
      "Epoch 177: Train Loss: 0.21653786674141884, Validation Loss: 0.9065796136856079\n",
      "Epoch 178: Train Loss: 0.20008567803435856, Validation Loss: 0.9033721089363098\n",
      "Epoch 179: Train Loss: 0.14788289699289534, Validation Loss: 0.9035254716873169\n",
      "Epoch 180: Train Loss: 0.1865218463871214, Validation Loss: 0.9171998500823975\n",
      "Epoch 181: Train Loss: 0.18981650471687317, Validation Loss: 0.9051087498664856\n",
      "Epoch 182: Train Loss: 0.1964385923412111, Validation Loss: 0.9027186632156372\n",
      "Epoch 183: Train Loss: 0.1749278555313746, Validation Loss: 0.9006486535072327\n",
      "Epoch 184: Train Loss: 0.19377432515223822, Validation Loss: 0.9094096422195435\n",
      "Epoch 185: Train Loss: 0.15717228833172056, Validation Loss: 0.9289771318435669\n",
      "Epoch 186: Train Loss: 0.16058541420433256, Validation Loss: 0.9142334461212158\n",
      "Epoch 187: Train Loss: 0.19575447423590553, Validation Loss: 0.9149102568626404\n",
      "Epoch 188: Train Loss: 0.17266673429144752, Validation Loss: 0.9160583019256592\n",
      "Epoch 189: Train Loss: 0.20244810233513513, Validation Loss: 0.9132528305053711\n",
      "Epoch 190: Train Loss: 0.2349072280857298, Validation Loss: 0.9142125844955444\n",
      "Epoch 191: Train Loss: 0.17449782664577165, Validation Loss: 0.9314889907836914\n",
      "Epoch 192: Train Loss: 0.16198079619142744, Validation Loss: 0.9377967119216919\n",
      "Epoch 193: Train Loss: 0.2297184889515241, Validation Loss: 0.998394787311554\n",
      "Epoch 194: Train Loss: 0.1363224966658486, Validation Loss: 0.946702778339386\n",
      "Epoch 195: Train Loss: 0.169349933664004, Validation Loss: 0.9416645169258118\n",
      "Epoch 196: Train Loss: 0.16113889631297854, Validation Loss: 0.9446788430213928\n",
      "Epoch 197: Train Loss: 0.1562838860683971, Validation Loss: 0.9396213293075562\n",
      "Epoch 198: Train Loss: 0.1659670855022139, Validation Loss: 0.9724971652030945\n",
      "Epoch 199: Train Loss: 0.16215168105231392, Validation Loss: 0.9589232802391052\n",
      "Fold 2 Metrics:\n",
      "Accuracy: 0.45454545454545453, Precision: 0.5, Recall: 0.16666666666666666, F1-score: 0.25, AUC: 0.4833333333333334\n",
      "Confusion Matrix:\n",
      "[[4 1]\n",
      " [5 1]]\n",
      "Completed fold 2\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples from subject 3 to test set\n",
      "Adding 6 truth samples from subject 3 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.7108192443847656, Validation Loss: 0.6891956329345703\n",
      "Epoch 1: Train Loss: 0.7095129953490363, Validation Loss: 0.6852129697799683\n",
      "Epoch 2: Train Loss: 0.6747208105193244, Validation Loss: 0.6852781176567078\n",
      "Epoch 3: Train Loss: 0.6778759492768182, Validation Loss: 0.6842724680900574\n",
      "Epoch 4: Train Loss: 0.6782289809650845, Validation Loss: 0.6852751970291138\n",
      "Epoch 5: Train Loss: 0.65158079067866, Validation Loss: 0.6877760887145996\n",
      "Epoch 6: Train Loss: 0.6779926617940267, Validation Loss: 0.6876606941223145\n",
      "Epoch 7: Train Loss: 0.65911865234375, Validation Loss: 0.6895601153373718\n",
      "Epoch 8: Train Loss: 0.6451903647846646, Validation Loss: 0.6893293857574463\n",
      "Epoch 9: Train Loss: 0.6466107765833536, Validation Loss: 0.689178466796875\n",
      "Epoch 10: Train Loss: 0.697521812385983, Validation Loss: 0.6971829533576965\n",
      "Epoch 11: Train Loss: 0.648805922932095, Validation Loss: 0.6903894543647766\n",
      "Epoch 12: Train Loss: 0.6426169077555338, Validation Loss: 0.6895217299461365\n",
      "Epoch 13: Train Loss: 0.6419212023417155, Validation Loss: 0.6908919215202332\n",
      "Epoch 14: Train Loss: 0.6228654649522569, Validation Loss: 0.6946409344673157\n",
      "Epoch 15: Train Loss: 0.6738422248098586, Validation Loss: 0.6941813826560974\n",
      "Epoch 16: Train Loss: 0.6240411467022366, Validation Loss: 0.6919285655021667\n",
      "Epoch 17: Train Loss: 0.612931662135654, Validation Loss: 0.6925188899040222\n",
      "Epoch 18: Train Loss: 0.6254030267397562, Validation Loss: 0.6923601627349854\n",
      "Epoch 19: Train Loss: 0.6222068402502272, Validation Loss: 0.6930685639381409\n",
      "Epoch 20: Train Loss: 0.6361380683051215, Validation Loss: 0.6973504424095154\n",
      "Epoch 21: Train Loss: 0.6062857309977213, Validation Loss: 0.695823073387146\n",
      "Epoch 22: Train Loss: 0.6182560722033182, Validation Loss: 0.6941587328910828\n",
      "Epoch 23: Train Loss: 0.6058939430448744, Validation Loss: 0.6955094337463379\n",
      "Epoch 24: Train Loss: 0.5848644574483236, Validation Loss: 0.6976643800735474\n",
      "Epoch 25: Train Loss: 0.5961940884590149, Validation Loss: 0.7013401389122009\n",
      "Epoch 26: Train Loss: 0.5779409839047326, Validation Loss: 0.7021344304084778\n",
      "Epoch 27: Train Loss: 0.6132670309808519, Validation Loss: 0.7025996446609497\n",
      "Epoch 28: Train Loss: 0.5853287114037408, Validation Loss: 0.7030905485153198\n",
      "Epoch 29: Train Loss: 0.5656507081455655, Validation Loss: 0.702822208404541\n",
      "Epoch 30: Train Loss: 0.6153179473347135, Validation Loss: 0.709358811378479\n",
      "Epoch 31: Train Loss: 0.600333551565806, Validation Loss: 0.7162318229675293\n",
      "Epoch 32: Train Loss: 0.5825457705391778, Validation Loss: 0.716888427734375\n",
      "Epoch 33: Train Loss: 0.5583131545119815, Validation Loss: 0.7190693020820618\n",
      "Epoch 34: Train Loss: 0.587894833750195, Validation Loss: 0.718673586845398\n",
      "Epoch 35: Train Loss: 0.5877416597472297, Validation Loss: 0.7156743407249451\n",
      "Epoch 36: Train Loss: 0.5571819576952193, Validation Loss: 0.7176022529602051\n",
      "Epoch 37: Train Loss: 0.5409783588515388, Validation Loss: 0.7145187854766846\n",
      "Epoch 38: Train Loss: 0.5521883467833201, Validation Loss: 0.7146497964859009\n",
      "Epoch 39: Train Loss: 0.5655991799301572, Validation Loss: 0.7156664133071899\n",
      "Epoch 40: Train Loss: 0.5692157844702402, Validation Loss: 0.7162095904350281\n",
      "Epoch 41: Train Loss: 0.5836910671657987, Validation Loss: 0.7117251753807068\n",
      "Epoch 42: Train Loss: 0.538206183248096, Validation Loss: 0.7147027254104614\n",
      "Epoch 43: Train Loss: 0.551500035656823, Validation Loss: 0.715732753276825\n",
      "Epoch 44: Train Loss: 0.5455155703756545, Validation Loss: 0.7136322259902954\n",
      "Epoch 45: Train Loss: 0.5399584935771095, Validation Loss: 0.716926634311676\n",
      "Epoch 46: Train Loss: 0.5375961164633433, Validation Loss: 0.7168455123901367\n",
      "Epoch 47: Train Loss: 0.5415975517696805, Validation Loss: 0.7150505781173706\n",
      "Epoch 48: Train Loss: 0.5164683328734504, Validation Loss: 0.7145804166793823\n",
      "Epoch 49: Train Loss: 0.5453473561339908, Validation Loss: 0.7154355049133301\n",
      "Epoch 50: Train Loss: 0.524121105670929, Validation Loss: 0.7072529792785645\n",
      "Epoch 51: Train Loss: 0.5062010354465909, Validation Loss: 0.7078390121459961\n",
      "Epoch 52: Train Loss: 0.4966650340292189, Validation Loss: 0.703437089920044\n",
      "Epoch 53: Train Loss: 0.5309946073426141, Validation Loss: 0.7020952701568604\n",
      "Epoch 54: Train Loss: 0.48246561156378853, Validation Loss: 0.7064633369445801\n",
      "Epoch 55: Train Loss: 0.5157382057772743, Validation Loss: 0.7046017646789551\n",
      "Epoch 56: Train Loss: 0.5122338665856255, Validation Loss: 0.7059822082519531\n",
      "Epoch 57: Train Loss: 0.48307080732451546, Validation Loss: 0.7054116129875183\n",
      "Epoch 58: Train Loss: 0.49964844849374557, Validation Loss: 0.7073315978050232\n",
      "Epoch 59: Train Loss: 0.5013493796189626, Validation Loss: 0.7072184681892395\n",
      "Epoch 60: Train Loss: 0.5112693442238702, Validation Loss: 0.7166767120361328\n",
      "Epoch 61: Train Loss: 0.5184263620111678, Validation Loss: 0.7105816602706909\n",
      "Epoch 62: Train Loss: 0.5099223885271285, Validation Loss: 0.7046812772750854\n",
      "Epoch 63: Train Loss: 0.4828140437602997, Validation Loss: 0.7089462876319885\n",
      "Epoch 64: Train Loss: 0.4830781751208835, Validation Loss: 0.7127260565757751\n",
      "Epoch 65: Train Loss: 0.492611825466156, Validation Loss: 0.7126964926719666\n",
      "Epoch 66: Train Loss: 0.4832134544849396, Validation Loss: 0.7134294509887695\n",
      "Epoch 67: Train Loss: 0.49726415011617875, Validation Loss: 0.7151188850402832\n",
      "Epoch 68: Train Loss: 0.49578791856765747, Validation Loss: 0.7105416655540466\n",
      "Epoch 69: Train Loss: 0.452545874648624, Validation Loss: 0.7135477662086487\n",
      "Epoch 70: Train Loss: 0.4750107593006558, Validation Loss: 0.7120065689086914\n",
      "Epoch 71: Train Loss: 0.48318184084362453, Validation Loss: 0.7051687240600586\n",
      "Epoch 72: Train Loss: 0.465200533469518, Validation Loss: 0.7067537307739258\n",
      "Epoch 73: Train Loss: 0.4446729322274526, Validation Loss: 0.7035103440284729\n",
      "Epoch 74: Train Loss: 0.4733402530352275, Validation Loss: 0.6984347105026245\n",
      "Epoch 75: Train Loss: 0.45702681607670254, Validation Loss: 0.6992457509040833\n",
      "Epoch 76: Train Loss: 0.4571865068541633, Validation Loss: 0.7005003690719604\n",
      "Epoch 77: Train Loss: 0.4345409903261397, Validation Loss: 0.6996466517448425\n",
      "Epoch 78: Train Loss: 0.482540014717314, Validation Loss: 0.7000651359558105\n",
      "Epoch 79: Train Loss: 0.4176499711142646, Validation Loss: 0.7019977569580078\n",
      "Epoch 80: Train Loss: 0.4824315243297153, Validation Loss: 0.7075375318527222\n",
      "Epoch 81: Train Loss: 0.44363579485151505, Validation Loss: 0.7049214839935303\n",
      "Epoch 82: Train Loss: 0.45185157656669617, Validation Loss: 0.6975495219230652\n",
      "Epoch 83: Train Loss: 0.43698501255777145, Validation Loss: 0.6933056712150574\n",
      "Epoch 84: Train Loss: 0.42872022920184666, Validation Loss: 0.6945401430130005\n",
      "Epoch 85: Train Loss: 0.42561277747154236, Validation Loss: 0.6909612417221069\n",
      "Epoch 86: Train Loss: 0.4149131112628513, Validation Loss: 0.6918541789054871\n",
      "Epoch 87: Train Loss: 0.40821411543422276, Validation Loss: 0.6921572089195251\n",
      "Epoch 88: Train Loss: 0.45376186900668675, Validation Loss: 0.6942528486251831\n",
      "Epoch 89: Train Loss: 0.4117569393581814, Validation Loss: 0.6951713562011719\n",
      "Epoch 90: Train Loss: 0.43078962630695766, Validation Loss: 0.6960780024528503\n",
      "Epoch 91: Train Loss: 0.3925485412279765, Validation Loss: 0.7022041082382202\n",
      "Epoch 92: Train Loss: 0.4332358737786611, Validation Loss: 0.7018579840660095\n",
      "Epoch 93: Train Loss: 0.38953879806730485, Validation Loss: 0.6877924799919128\n",
      "Epoch 94: Train Loss: 0.43466896149847245, Validation Loss: 0.6895029544830322\n",
      "Epoch 95: Train Loss: 0.409709417157703, Validation Loss: 0.6862196326255798\n",
      "Epoch 96: Train Loss: 0.40567174222734237, Validation Loss: 0.6870222091674805\n",
      "Epoch 97: Train Loss: 0.3846160140302446, Validation Loss: 0.6814791560173035\n",
      "Epoch 98: Train Loss: 0.4057236876752641, Validation Loss: 0.6781912446022034\n",
      "Epoch 99: Train Loss: 0.4209505683845944, Validation Loss: 0.6802106499671936\n",
      "Epoch 100: Train Loss: 0.41959986752933925, Validation Loss: 0.6766784191131592\n",
      "Epoch 101: Train Loss: 0.40170438090960187, Validation Loss: 0.6719729900360107\n",
      "Epoch 102: Train Loss: 0.39449577861362034, Validation Loss: 0.6690575480461121\n",
      "Epoch 103: Train Loss: 0.36951830983161926, Validation Loss: 0.6690514087677002\n",
      "Epoch 104: Train Loss: 0.38812799586190116, Validation Loss: 0.6617627143859863\n",
      "Epoch 105: Train Loss: 0.38532109061876935, Validation Loss: 0.6616396903991699\n",
      "Epoch 106: Train Loss: 0.3768445716963874, Validation Loss: 0.6643294095993042\n",
      "Epoch 107: Train Loss: 0.38533342215749955, Validation Loss: 0.6651497483253479\n",
      "Epoch 108: Train Loss: 0.3892703420586056, Validation Loss: 0.6616925001144409\n",
      "Epoch 109: Train Loss: 0.3514598144425286, Validation Loss: 0.6656866073608398\n",
      "Epoch 110: Train Loss: 0.37825790378782487, Validation Loss: 0.6686382293701172\n",
      "Epoch 111: Train Loss: 0.3840671910179986, Validation Loss: 0.6734117269515991\n",
      "Epoch 112: Train Loss: 0.36698055267333984, Validation Loss: 0.6803461909294128\n",
      "Epoch 113: Train Loss: 0.3624027967453003, Validation Loss: 0.679871141910553\n",
      "Epoch 114: Train Loss: 0.3569264163573583, Validation Loss: 0.6854064464569092\n",
      "Epoch 115: Train Loss: 0.38207533293300205, Validation Loss: 0.6876389384269714\n",
      "Epoch 116: Train Loss: 0.3746042317814297, Validation Loss: 0.6848199367523193\n",
      "Epoch 117: Train Loss: 0.367959126830101, Validation Loss: 0.6840271353721619\n",
      "Epoch 118: Train Loss: 0.3733677996529473, Validation Loss: 0.6827533841133118\n",
      "Epoch 119: Train Loss: 0.38757168915536666, Validation Loss: 0.6810375452041626\n",
      "Epoch 120: Train Loss: 0.35030322935846114, Validation Loss: 0.6791222095489502\n",
      "Epoch 121: Train Loss: 0.3571778999434577, Validation Loss: 0.6810371279716492\n",
      "Epoch 122: Train Loss: 0.3479245834880405, Validation Loss: 0.6893705725669861\n",
      "Epoch 123: Train Loss: 0.3548034926255544, Validation Loss: 0.6881446838378906\n",
      "Epoch 124: Train Loss: 0.36622051729096305, Validation Loss: 0.6922127604484558\n",
      "Epoch 125: Train Loss: 0.3609614935186174, Validation Loss: 0.700829803943634\n",
      "Epoch 126: Train Loss: 0.3565829296906789, Validation Loss: 0.6990324854850769\n",
      "Epoch 127: Train Loss: 0.36716211835543316, Validation Loss: 0.6865568161010742\n",
      "Epoch 128: Train Loss: 0.33091593782107037, Validation Loss: 0.6842222213745117\n",
      "Epoch 129: Train Loss: 0.33611047599050736, Validation Loss: 0.6821969747543335\n",
      "Epoch 130: Train Loss: 0.35404587123129105, Validation Loss: 0.686381995677948\n",
      "Epoch 131: Train Loss: 0.3534225705597136, Validation Loss: 0.700072169303894\n",
      "Epoch 132: Train Loss: 0.3291954927974277, Validation Loss: 0.6950297355651855\n",
      "Epoch 133: Train Loss: 0.32124308082792496, Validation Loss: 0.6937488317489624\n",
      "Epoch 134: Train Loss: 0.30158694419595933, Validation Loss: 0.6999286413192749\n",
      "Epoch 135: Train Loss: 0.3214086327287886, Validation Loss: 0.6940538287162781\n",
      "Epoch 136: Train Loss: 0.34014879663785297, Validation Loss: 0.7001619935035706\n",
      "Epoch 137: Train Loss: 0.31445882055494523, Validation Loss: 0.7009372711181641\n",
      "Epoch 138: Train Loss: 0.30297887490855324, Validation Loss: 0.7031606435775757\n",
      "Epoch 139: Train Loss: 0.3037051790290409, Validation Loss: 0.7055814266204834\n",
      "Epoch 140: Train Loss: 0.32730311983161503, Validation Loss: 0.7056273221969604\n",
      "Epoch 141: Train Loss: 0.31225620044602287, Validation Loss: 0.7066550254821777\n",
      "Epoch 142: Train Loss: 0.33078912066088784, Validation Loss: 0.7131532430648804\n",
      "Epoch 143: Train Loss: 0.3192614217599233, Validation Loss: 0.7017315626144409\n",
      "Epoch 144: Train Loss: 0.3218180007404751, Validation Loss: 0.6974122524261475\n",
      "Epoch 145: Train Loss: 0.30949316753281486, Validation Loss: 0.7028785347938538\n",
      "Epoch 146: Train Loss: 0.2760978705353207, Validation Loss: 0.7046476602554321\n",
      "Epoch 147: Train Loss: 0.3043541494343016, Validation Loss: 0.7040087580680847\n",
      "Epoch 148: Train Loss: 0.3141801059246063, Validation Loss: 0.7014815807342529\n",
      "Epoch 149: Train Loss: 0.2780418230427636, Validation Loss: 0.7047563195228577\n",
      "Epoch 150: Train Loss: 0.31535737713177997, Validation Loss: 0.7040584087371826\n",
      "Epoch 151: Train Loss: 0.3344249261750115, Validation Loss: 0.6904443502426147\n",
      "Epoch 152: Train Loss: 0.32535645365715027, Validation Loss: 0.687159538269043\n",
      "Epoch 153: Train Loss: 0.30343687368763816, Validation Loss: 0.6958562731742859\n",
      "Epoch 154: Train Loss: 0.2825718820095062, Validation Loss: 0.6950575709342957\n",
      "Epoch 155: Train Loss: 0.27631400525569916, Validation Loss: 0.6966397166252136\n",
      "Epoch 156: Train Loss: 0.27982711295286816, Validation Loss: 0.7032194137573242\n",
      "Epoch 157: Train Loss: 0.27512355479929185, Validation Loss: 0.7191088795661926\n",
      "Epoch 158: Train Loss: 0.2617782966958152, Validation Loss: 0.724033534526825\n",
      "Epoch 159: Train Loss: 0.2779911888970269, Validation Loss: 0.7117640376091003\n",
      "Epoch 160: Train Loss: 0.2554609907997979, Validation Loss: 0.714997410774231\n",
      "Epoch 161: Train Loss: 0.29825305773152244, Validation Loss: 0.7150735855102539\n",
      "Epoch 162: Train Loss: 0.26517905129326713, Validation Loss: 0.7234603762626648\n",
      "Epoch 163: Train Loss: 0.2846309145291646, Validation Loss: 0.7386420965194702\n",
      "Epoch 164: Train Loss: 0.2876199848122067, Validation Loss: 0.7454017400741577\n",
      "Epoch 165: Train Loss: 0.2813310507271025, Validation Loss: 0.736145555973053\n",
      "Epoch 166: Train Loss: 0.27377615372339886, Validation Loss: 0.7356715202331543\n",
      "Epoch 167: Train Loss: 0.25699785020616317, Validation Loss: 0.7300317287445068\n",
      "Epoch 168: Train Loss: 0.28491465085082585, Validation Loss: 0.7272265553474426\n",
      "Epoch 169: Train Loss: 0.26713163157304126, Validation Loss: 0.7273130416870117\n",
      "Epoch 170: Train Loss: 0.24881459110312992, Validation Loss: 0.7274191975593567\n",
      "Epoch 171: Train Loss: 0.2391468750105964, Validation Loss: 0.7493121027946472\n",
      "Epoch 172: Train Loss: 0.25280560221936965, Validation Loss: 0.7461032867431641\n",
      "Epoch 173: Train Loss: 0.24441847536298963, Validation Loss: 0.763123095035553\n",
      "Epoch 174: Train Loss: 0.24366657932599387, Validation Loss: 0.7405675053596497\n",
      "Epoch 175: Train Loss: 0.24327649838394588, Validation Loss: 0.7562429308891296\n",
      "Epoch 176: Train Loss: 0.24996147553126016, Validation Loss: 0.7552595138549805\n",
      "Epoch 177: Train Loss: 0.23561389413144854, Validation Loss: 0.7412467002868652\n",
      "Epoch 178: Train Loss: 0.24719257983896467, Validation Loss: 0.7590758800506592\n",
      "Epoch 179: Train Loss: 0.21540103521611956, Validation Loss: 0.7461990118026733\n",
      "Epoch 180: Train Loss: 0.22448143694135878, Validation Loss: 0.7660287022590637\n",
      "Epoch 181: Train Loss: 0.2623479664325714, Validation Loss: 0.7656432390213013\n",
      "Epoch 182: Train Loss: 0.22960453728834787, Validation Loss: 0.7745388150215149\n",
      "Epoch 183: Train Loss: 0.22394305964310965, Validation Loss: 0.7822757959365845\n",
      "Epoch 184: Train Loss: 0.22364429632822672, Validation Loss: 0.7867841720581055\n",
      "Epoch 185: Train Loss: 0.23573305209477743, Validation Loss: 0.7784015536308289\n",
      "Epoch 186: Train Loss: 0.26171432601081, Validation Loss: 0.783125102519989\n",
      "Epoch 187: Train Loss: 0.22678323421213362, Validation Loss: 0.7881484627723694\n",
      "Epoch 188: Train Loss: 0.23064559366967943, Validation Loss: 0.7850953936576843\n",
      "Epoch 189: Train Loss: 0.23988941311836243, Validation Loss: 0.7979465126991272\n",
      "Epoch 190: Train Loss: 0.23360604047775269, Validation Loss: 0.80362468957901\n",
      "Epoch 191: Train Loss: 0.204536199155781, Validation Loss: 0.8292098045349121\n",
      "Epoch 192: Train Loss: 0.19779004818863338, Validation Loss: 0.8295398950576782\n",
      "Epoch 193: Train Loss: 0.20503590007623038, Validation Loss: 0.8320079445838928\n",
      "Epoch 194: Train Loss: 0.22192941274907854, Validation Loss: 0.833012580871582\n",
      "Epoch 195: Train Loss: 0.2254762566751904, Validation Loss: 0.8367520570755005\n",
      "Epoch 196: Train Loss: 0.22146448327435386, Validation Loss: 0.8377173542976379\n",
      "Epoch 197: Train Loss: 0.21393850280178917, Validation Loss: 0.8483424782752991\n",
      "Epoch 198: Train Loss: 0.2168528644575013, Validation Loss: 0.8333196043968201\n",
      "Epoch 199: Train Loss: 0.2306214744846026, Validation Loss: 0.8441846370697021\n",
      "Fold 3 Metrics:\n",
      "Accuracy: 0.45454545454545453, Precision: 0.5, Recall: 0.6666666666666666, F1-score: 0.5714285714285714, AUC: 0.43333333333333324\n",
      "Confusion Matrix:\n",
      "[[1 4]\n",
      " [2 4]]\n",
      "Completed fold 3\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples from subject 10 to test set\n",
      "Adding 6 truth samples from subject 10 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.6893341541290283, Validation Loss: 0.6780109405517578\n",
      "Epoch 1: Train Loss: 0.6670318577024672, Validation Loss: 0.6737399101257324\n",
      "Epoch 2: Train Loss: 0.6480829980638292, Validation Loss: 0.6745322942733765\n",
      "Epoch 3: Train Loss: 0.686710086133745, Validation Loss: 0.6769334077835083\n",
      "Epoch 4: Train Loss: 0.6471440659628974, Validation Loss: 0.6786484718322754\n",
      "Epoch 5: Train Loss: 0.6505479878849454, Validation Loss: 0.6798210740089417\n",
      "Epoch 6: Train Loss: 0.6752622988488939, Validation Loss: 0.6801234483718872\n",
      "Epoch 7: Train Loss: 0.6269341641002231, Validation Loss: 0.6809603571891785\n",
      "Epoch 8: Train Loss: 0.6203737788730197, Validation Loss: 0.6811199188232422\n",
      "Epoch 9: Train Loss: 0.6544429328706529, Validation Loss: 0.6809720396995544\n",
      "Epoch 10: Train Loss: 0.6373725401030647, Validation Loss: 0.6860589385032654\n",
      "Epoch 11: Train Loss: 0.596008472972446, Validation Loss: 0.6917724013328552\n",
      "Epoch 12: Train Loss: 0.6226405832502577, Validation Loss: 0.6963986754417419\n",
      "Epoch 13: Train Loss: 0.6135767367151048, Validation Loss: 0.6990519762039185\n",
      "Epoch 14: Train Loss: 0.5901810725529989, Validation Loss: 0.7020314931869507\n",
      "Epoch 15: Train Loss: 0.5807596610652076, Validation Loss: 0.7048186659812927\n",
      "Epoch 16: Train Loss: 0.5899143550131056, Validation Loss: 0.7059062719345093\n",
      "Epoch 17: Train Loss: 0.5965951681137085, Validation Loss: 0.7067020535469055\n",
      "Epoch 18: Train Loss: 0.5789486898316277, Validation Loss: 0.7070736885070801\n",
      "Epoch 19: Train Loss: 0.5695682730939653, Validation Loss: 0.7075142860412598\n",
      "Epoch 20: Train Loss: 0.5682144463062286, Validation Loss: 0.7129764556884766\n",
      "Epoch 21: Train Loss: 0.5399174524678124, Validation Loss: 0.7173706889152527\n",
      "Epoch 22: Train Loss: 0.5435153113471137, Validation Loss: 0.7205336689949036\n",
      "Epoch 23: Train Loss: 0.5386154221163856, Validation Loss: 0.7200047969818115\n",
      "Epoch 24: Train Loss: 0.5642612675825754, Validation Loss: 0.7196884751319885\n",
      "Epoch 25: Train Loss: 0.5535761912663778, Validation Loss: 0.7206851840019226\n",
      "Epoch 26: Train Loss: 0.5484329130914476, Validation Loss: 0.7204364538192749\n",
      "Epoch 27: Train Loss: 0.5486769378185272, Validation Loss: 0.721241295337677\n",
      "Epoch 28: Train Loss: 0.5290433863798777, Validation Loss: 0.7230064272880554\n",
      "Epoch 29: Train Loss: 0.5582281715340085, Validation Loss: 0.7227191925048828\n",
      "Epoch 30: Train Loss: 0.545771923330095, Validation Loss: 0.72590172290802\n",
      "Epoch 31: Train Loss: 0.5451458195845286, Validation Loss: 0.7340950965881348\n",
      "Epoch 32: Train Loss: 0.5241657661067115, Validation Loss: 0.7381790280342102\n",
      "Epoch 33: Train Loss: 0.5234836505519019, Validation Loss: 0.7438390254974365\n",
      "Epoch 34: Train Loss: 0.4924890266524421, Validation Loss: 0.7488540410995483\n",
      "Epoch 35: Train Loss: 0.5119492212931315, Validation Loss: 0.7548300623893738\n",
      "Epoch 36: Train Loss: 0.5048032568560706, Validation Loss: 0.7568472027778625\n",
      "Epoch 37: Train Loss: 0.49586179190211827, Validation Loss: 0.7593175768852234\n",
      "Epoch 38: Train Loss: 0.47885873913764954, Validation Loss: 0.7566445469856262\n",
      "Epoch 39: Train Loss: 0.48208559883965385, Validation Loss: 0.7576417922973633\n",
      "Epoch 40: Train Loss: 0.4710899359650082, Validation Loss: 0.7653119564056396\n",
      "Epoch 41: Train Loss: 0.4793243143293593, Validation Loss: 0.7644458413124084\n",
      "Epoch 42: Train Loss: 0.45567799276775783, Validation Loss: 0.7719926834106445\n",
      "Epoch 43: Train Loss: 0.5034345620208316, Validation Loss: 0.7739099264144897\n",
      "Epoch 44: Train Loss: 0.45376726653840804, Validation Loss: 0.7752525210380554\n",
      "Epoch 45: Train Loss: 0.46674728724691605, Validation Loss: 0.7749652862548828\n",
      "Epoch 46: Train Loss: 0.42610420617792344, Validation Loss: 0.7781718969345093\n",
      "Epoch 47: Train Loss: 0.47971393002404106, Validation Loss: 0.7783709764480591\n",
      "Epoch 48: Train Loss: 0.44666221737861633, Validation Loss: 0.7767819762229919\n",
      "Epoch 49: Train Loss: 0.45332609944873387, Validation Loss: 0.7791664004325867\n",
      "Epoch 50: Train Loss: 0.4581421481238471, Validation Loss: 0.792408287525177\n",
      "Epoch 51: Train Loss: 0.43733153740564984, Validation Loss: 0.7942853569984436\n",
      "Epoch 52: Train Loss: 0.44884442951944137, Validation Loss: 0.8045497536659241\n",
      "Epoch 53: Train Loss: 0.426236629486084, Validation Loss: 0.8030616641044617\n",
      "Epoch 54: Train Loss: 0.4213021993637085, Validation Loss: 0.8041902780532837\n",
      "Epoch 55: Train Loss: 0.4306903514597151, Validation Loss: 0.8056607842445374\n",
      "Epoch 56: Train Loss: 0.42174357838100857, Validation Loss: 0.811083197593689\n",
      "Epoch 57: Train Loss: 0.4389309850003984, Validation Loss: 0.8080094456672668\n",
      "Epoch 58: Train Loss: 0.41512662172317505, Validation Loss: 0.8122465014457703\n",
      "Epoch 59: Train Loss: 0.43258175253868103, Validation Loss: 0.8064563274383545\n",
      "Epoch 60: Train Loss: 0.40768296851052177, Validation Loss: 0.8148925304412842\n",
      "Epoch 61: Train Loss: 0.42306337422794765, Validation Loss: 0.8175622820854187\n",
      "Epoch 62: Train Loss: 0.41259073548846775, Validation Loss: 0.8289077281951904\n",
      "Epoch 63: Train Loss: 0.38736763927671647, Validation Loss: 0.8279632329940796\n",
      "Epoch 64: Train Loss: 0.40816254748238456, Validation Loss: 0.8297021389007568\n",
      "Epoch 65: Train Loss: 0.403418630361557, Validation Loss: 0.8279895782470703\n",
      "Epoch 66: Train Loss: 0.40478158328268266, Validation Loss: 0.8272555470466614\n",
      "Epoch 67: Train Loss: 0.4207453727722168, Validation Loss: 0.83381587266922\n",
      "Epoch 68: Train Loss: 0.37087350752618575, Validation Loss: 0.8249602317810059\n",
      "Epoch 69: Train Loss: 0.3765590621365441, Validation Loss: 0.8276599049568176\n",
      "Epoch 70: Train Loss: 0.39480070107513004, Validation Loss: 0.8284636735916138\n",
      "Epoch 71: Train Loss: 0.3852622475888994, Validation Loss: 0.8303003311157227\n",
      "Epoch 72: Train Loss: 0.3845943080054389, Validation Loss: 0.8248425722122192\n",
      "Epoch 73: Train Loss: 0.37493906087345547, Validation Loss: 0.8356779217720032\n",
      "Epoch 74: Train Loss: 0.3863859971364339, Validation Loss: 0.8359057903289795\n",
      "Epoch 75: Train Loss: 0.3822739256752862, Validation Loss: 0.8392186760902405\n",
      "Epoch 76: Train Loss: 0.34974152512020534, Validation Loss: 0.8316317796707153\n",
      "Epoch 77: Train Loss: 0.3534950249724918, Validation Loss: 0.8248786330223083\n",
      "Epoch 78: Train Loss: 0.374067054854499, Validation Loss: 0.8245790600776672\n",
      "Epoch 79: Train Loss: 0.34098614090018803, Validation Loss: 0.816739022731781\n",
      "Epoch 80: Train Loss: 0.3527815143267314, Validation Loss: 0.8258471488952637\n",
      "Epoch 81: Train Loss: 0.37081370419926113, Validation Loss: 0.8284479379653931\n",
      "Epoch 82: Train Loss: 0.33429474300808376, Validation Loss: 0.8318835496902466\n",
      "Epoch 83: Train Loss: 0.34162235922283596, Validation Loss: 0.8352382779121399\n",
      "Epoch 84: Train Loss: 0.33555594583352405, Validation Loss: 0.8241462707519531\n",
      "Epoch 85: Train Loss: 0.32957205673058826, Validation Loss: 0.8105350732803345\n",
      "Epoch 86: Train Loss: 0.3189045786857605, Validation Loss: 0.8153455257415771\n",
      "Epoch 87: Train Loss: 0.33178771866692436, Validation Loss: 0.8150818943977356\n",
      "Epoch 88: Train Loss: 0.34712592098448014, Validation Loss: 0.811184823513031\n",
      "Epoch 89: Train Loss: 0.32021881143252057, Validation Loss: 0.81937175989151\n",
      "Epoch 90: Train Loss: 0.3445456425348918, Validation Loss: 0.8195806741714478\n",
      "Epoch 91: Train Loss: 0.30029527015156215, Validation Loss: 0.822967529296875\n",
      "Epoch 92: Train Loss: 0.30878695514467025, Validation Loss: 0.8028838634490967\n",
      "Epoch 93: Train Loss: 0.2870463646120495, Validation Loss: 0.8023571968078613\n",
      "Epoch 94: Train Loss: 0.32286807894706726, Validation Loss: 0.7908886075019836\n",
      "Epoch 95: Train Loss: 0.3086620387103822, Validation Loss: 0.7888129353523254\n",
      "Epoch 96: Train Loss: 0.2983788152535756, Validation Loss: 0.7993359565734863\n",
      "Epoch 97: Train Loss: 0.3107473717795478, Validation Loss: 0.8082931041717529\n",
      "Epoch 98: Train Loss: 0.3005071249273088, Validation Loss: 0.7984306216239929\n",
      "Epoch 99: Train Loss: 0.30851732359992134, Validation Loss: 0.7962095141410828\n",
      "Epoch 100: Train Loss: 0.3034946752919091, Validation Loss: 0.7970418930053711\n",
      "Epoch 101: Train Loss: 0.2772023810280694, Validation Loss: 0.7731205821037292\n",
      "Epoch 102: Train Loss: 0.2997487369510863, Validation Loss: 0.785506546497345\n",
      "Epoch 103: Train Loss: 0.2778441690736347, Validation Loss: 0.7855303883552551\n",
      "Epoch 104: Train Loss: 0.26389260921213364, Validation Loss: 0.7794203758239746\n",
      "Epoch 105: Train Loss: 0.27405084835158455, Validation Loss: 0.7978376150131226\n",
      "Epoch 106: Train Loss: 0.2803933670123418, Validation Loss: 0.809457540512085\n",
      "Epoch 107: Train Loss: 0.2876767615477244, Validation Loss: 0.7898116111755371\n",
      "Epoch 108: Train Loss: 0.2623664935429891, Validation Loss: 0.7985461950302124\n",
      "Epoch 109: Train Loss: 0.28093768821822274, Validation Loss: 0.8022786974906921\n",
      "Epoch 110: Train Loss: 0.278734071387185, Validation Loss: 0.8273963332176208\n",
      "Epoch 111: Train Loss: 0.24502201047208574, Validation Loss: 0.8297770619392395\n",
      "Epoch 112: Train Loss: 0.2717720303270552, Validation Loss: 0.8580520153045654\n",
      "Epoch 113: Train Loss: 0.2489025741815567, Validation Loss: 0.8691199421882629\n",
      "Epoch 114: Train Loss: 0.24502858519554138, Validation Loss: 0.8417108654975891\n",
      "Epoch 115: Train Loss: 0.25782887306478286, Validation Loss: 0.8541696667671204\n",
      "Epoch 116: Train Loss: 0.2964390367269516, Validation Loss: 0.8223099708557129\n",
      "Epoch 117: Train Loss: 0.24072897930939993, Validation Loss: 0.8546133041381836\n",
      "Epoch 118: Train Loss: 0.2589872115188175, Validation Loss: 0.8403154015541077\n",
      "Epoch 119: Train Loss: 0.26388410561614567, Validation Loss: 0.8527854084968567\n",
      "Epoch 120: Train Loss: 0.24851332439316642, Validation Loss: 0.8520148992538452\n",
      "Epoch 121: Train Loss: 0.22961877783139548, Validation Loss: 0.879102885723114\n",
      "Epoch 122: Train Loss: 0.25124435126781464, Validation Loss: 0.9111610054969788\n",
      "Epoch 123: Train Loss: 0.23412347005473244, Validation Loss: 0.9133275747299194\n",
      "Epoch 124: Train Loss: 0.22925100392765468, Validation Loss: 0.8997674584388733\n",
      "Epoch 125: Train Loss: 0.2285635769367218, Validation Loss: 0.9007306694984436\n",
      "Epoch 126: Train Loss: 0.24020417696899837, Validation Loss: 0.8940524458885193\n",
      "Epoch 127: Train Loss: 0.21224366625150046, Validation Loss: 0.9031921029090881\n",
      "Epoch 128: Train Loss: 0.23692193792925942, Validation Loss: 0.909130871295929\n",
      "Epoch 129: Train Loss: 0.23504914508925545, Validation Loss: 0.9196456670761108\n",
      "Epoch 130: Train Loss: 0.24381934271918404, Validation Loss: 0.9011409878730774\n",
      "Epoch 131: Train Loss: 0.2376822249756919, Validation Loss: 0.914275586605072\n",
      "Epoch 132: Train Loss: 0.23032408290439182, Validation Loss: 0.9353271722793579\n",
      "Epoch 133: Train Loss: 0.21026035812166002, Validation Loss: 0.9324051141738892\n",
      "Epoch 134: Train Loss: 0.21136094464196098, Validation Loss: 0.92983078956604\n",
      "Epoch 135: Train Loss: 0.2028675658835305, Validation Loss: 0.9101306796073914\n",
      "Epoch 136: Train Loss: 0.19536772618691126, Validation Loss: 0.9119408130645752\n",
      "Epoch 137: Train Loss: 0.20365872316890293, Validation Loss: 0.9109753966331482\n",
      "Epoch 138: Train Loss: 0.169640451669693, Validation Loss: 0.9023674130439758\n",
      "Epoch 139: Train Loss: 0.20813913726144367, Validation Loss: 0.9084388017654419\n",
      "Epoch 140: Train Loss: 0.20448458857006496, Validation Loss: 0.930650532245636\n",
      "Epoch 141: Train Loss: 0.19026290045844185, Validation Loss: 0.916873574256897\n",
      "Epoch 142: Train Loss: 0.20574530131287044, Validation Loss: 0.9297311305999756\n",
      "Epoch 143: Train Loss: 0.20374325414498648, Validation Loss: 0.9195414781570435\n",
      "Epoch 144: Train Loss: 0.16151410175694358, Validation Loss: 0.9236754775047302\n",
      "Epoch 145: Train Loss: 0.17207473268111548, Validation Loss: 0.9552282691001892\n",
      "Epoch 146: Train Loss: 0.17343259851137796, Validation Loss: 0.960237443447113\n",
      "Epoch 147: Train Loss: 0.2087255459692743, Validation Loss: 0.9778249263763428\n",
      "Epoch 148: Train Loss: 0.16106850488318336, Validation Loss: 0.9617660641670227\n",
      "Epoch 149: Train Loss: 0.1735142543911934, Validation Loss: 0.9514285922050476\n",
      "Epoch 150: Train Loss: 0.20026971068647173, Validation Loss: 0.961540937423706\n",
      "Epoch 151: Train Loss: 0.17231724990738762, Validation Loss: 0.9610374569892883\n",
      "Epoch 152: Train Loss: 0.1689396326740583, Validation Loss: 0.9687523245811462\n",
      "Epoch 153: Train Loss: 0.17774821983443367, Validation Loss: 0.9431110620498657\n",
      "Epoch 154: Train Loss: 0.1839013761944241, Validation Loss: 0.9545885324478149\n",
      "Epoch 155: Train Loss: 0.1647935907046, Validation Loss: 0.9932420253753662\n",
      "Epoch 156: Train Loss: 0.19380032353931004, Validation Loss: 0.9968845844268799\n",
      "Epoch 157: Train Loss: 0.16531146648857328, Validation Loss: 0.9771193861961365\n",
      "Epoch 158: Train Loss: 0.1647899266746309, Validation Loss: 1.0032148361206055\n",
      "Epoch 159: Train Loss: 0.1621418736047215, Validation Loss: 1.0017105340957642\n",
      "Epoch 160: Train Loss: 0.1728038771284951, Validation Loss: 1.0004128217697144\n",
      "Epoch 161: Train Loss: 0.15573030213514963, Validation Loss: 1.0255119800567627\n",
      "Epoch 162: Train Loss: 0.18088815278477138, Validation Loss: 1.0061490535736084\n",
      "Epoch 163: Train Loss: 0.16456589682234657, Validation Loss: 1.054274559020996\n",
      "Epoch 164: Train Loss: 0.20248914841148588, Validation Loss: 1.1384385824203491\n",
      "Epoch 165: Train Loss: 0.15180054927865663, Validation Loss: 1.0912667512893677\n",
      "Epoch 166: Train Loss: 0.13957534647650188, Validation Loss: 1.0757533311843872\n",
      "Epoch 167: Train Loss: 0.15925435390737322, Validation Loss: 1.0756045579910278\n",
      "Epoch 168: Train Loss: 0.17056464155515036, Validation Loss: 1.096653699874878\n",
      "Epoch 169: Train Loss: 0.16999486751026577, Validation Loss: 1.0651148557662964\n",
      "Epoch 170: Train Loss: 0.13401534159978232, Validation Loss: 1.1180813312530518\n",
      "Epoch 171: Train Loss: 0.13931912349330056, Validation Loss: 1.144791841506958\n",
      "Epoch 172: Train Loss: 0.14105700453122458, Validation Loss: 1.1343107223510742\n",
      "Epoch 173: Train Loss: 0.14667420254813301, Validation Loss: 1.1755626201629639\n",
      "Epoch 174: Train Loss: 0.14740972386466134, Validation Loss: 1.1800826787948608\n",
      "Epoch 175: Train Loss: 0.13982970350318485, Validation Loss: 1.1372157335281372\n",
      "Epoch 176: Train Loss: 0.14838012059529623, Validation Loss: 1.1756497621536255\n",
      "Epoch 177: Train Loss: 0.14006062348683676, Validation Loss: 1.128706693649292\n",
      "Epoch 178: Train Loss: 0.15340501070022583, Validation Loss: 1.1289629936218262\n",
      "Epoch 179: Train Loss: 0.17674390350778899, Validation Loss: 1.0997228622436523\n",
      "Epoch 180: Train Loss: 0.1194551611940066, Validation Loss: 1.1547166109085083\n",
      "Epoch 181: Train Loss: 0.15390896631611717, Validation Loss: 1.1620687246322632\n",
      "Epoch 182: Train Loss: 0.15236598915523952, Validation Loss: 1.2409828901290894\n",
      "Epoch 183: Train Loss: 0.1340352619687716, Validation Loss: 1.216888189315796\n",
      "Epoch 184: Train Loss: 0.14532656636503008, Validation Loss: 1.2640153169631958\n",
      "Epoch 185: Train Loss: 0.12407533493306902, Validation Loss: 1.216780424118042\n",
      "Epoch 186: Train Loss: 0.13552921762069067, Validation Loss: 1.198264241218567\n",
      "Epoch 187: Train Loss: 0.12355373799800873, Validation Loss: 1.1801522970199585\n",
      "Epoch 188: Train Loss: 0.12104391720559862, Validation Loss: 1.2019113302230835\n",
      "Epoch 189: Train Loss: 0.12120233848690987, Validation Loss: 1.1877076625823975\n",
      "Epoch 190: Train Loss: 0.12820747246344885, Validation Loss: 1.1647862195968628\n",
      "Epoch 191: Train Loss: 0.13011090374655193, Validation Loss: 1.2042688131332397\n",
      "Epoch 192: Train Loss: 0.1246097559730212, Validation Loss: 1.2244848012924194\n",
      "Epoch 193: Train Loss: 0.14210437652137545, Validation Loss: 1.1680257320404053\n",
      "Epoch 194: Train Loss: 0.11710866375101937, Validation Loss: 1.2429461479187012\n",
      "Epoch 195: Train Loss: 0.11557477050357395, Validation Loss: 1.2661527395248413\n",
      "Epoch 196: Train Loss: 0.11825736653473642, Validation Loss: 1.3179945945739746\n",
      "Epoch 197: Train Loss: 0.09768365820248921, Validation Loss: 1.2955290079116821\n",
      "Epoch 198: Train Loss: 0.10835674819019106, Validation Loss: 1.3409065008163452\n",
      "Epoch 199: Train Loss: 0.12170055177476671, Validation Loss: 1.3255300521850586\n",
      "Fold 4 Metrics:\n",
      "Accuracy: 0.5454545454545454, Precision: 0.5454545454545454, Recall: 1.0, F1-score: 0.7058823529411765, AUC: 0.5\n",
      "Confusion Matrix:\n",
      "[[0 5]\n",
      " [0 6]]\n",
      "Completed fold 4\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples from subject 13 to test set\n",
      "Adding 6 truth samples from subject 13 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.7282942401038276, Validation Loss: 0.6916123032569885\n",
      "Epoch 1: Train Loss: 0.6903113457891676, Validation Loss: 0.696734607219696\n",
      "Epoch 2: Train Loss: 0.6948827968703376, Validation Loss: 0.701815128326416\n",
      "Epoch 3: Train Loss: 0.6821861531999376, Validation Loss: 0.7031895518302917\n",
      "Epoch 4: Train Loss: 0.700249969959259, Validation Loss: 0.7028365135192871\n",
      "Epoch 5: Train Loss: 0.672219627433353, Validation Loss: 0.7022073268890381\n",
      "Epoch 6: Train Loss: 0.6582719617419772, Validation Loss: 0.6990699172019958\n",
      "Epoch 7: Train Loss: 0.6790633930100335, Validation Loss: 0.698445737361908\n",
      "Epoch 8: Train Loss: 0.6859749025768704, Validation Loss: 0.6960590481758118\n",
      "Epoch 9: Train Loss: 0.7021739019287957, Validation Loss: 0.6966233253479004\n",
      "Epoch 10: Train Loss: 0.6558326217863295, Validation Loss: 0.6928489208221436\n",
      "Epoch 11: Train Loss: 0.6733014980951945, Validation Loss: 0.6922641396522522\n",
      "Epoch 12: Train Loss: 0.6431264413727654, Validation Loss: 0.6870959401130676\n",
      "Epoch 13: Train Loss: 0.6211620966593424, Validation Loss: 0.6841258406639099\n",
      "Epoch 14: Train Loss: 0.6429395410749648, Validation Loss: 0.6776968240737915\n",
      "Epoch 15: Train Loss: 0.6588472061687045, Validation Loss: 0.6762546300888062\n",
      "Epoch 16: Train Loss: 0.5972925225893656, Validation Loss: 0.6732746958732605\n",
      "Epoch 17: Train Loss: 0.634148485130734, Validation Loss: 0.6736916899681091\n",
      "Epoch 18: Train Loss: 0.6338479320208231, Validation Loss: 0.6707842946052551\n",
      "Epoch 19: Train Loss: 0.6361309091250101, Validation Loss: 0.6731835007667542\n",
      "Epoch 20: Train Loss: 0.6278062727716234, Validation Loss: 0.6672106385231018\n",
      "Epoch 21: Train Loss: 0.6288614736662971, Validation Loss: 0.6644978523254395\n",
      "Epoch 22: Train Loss: 0.6362825632095337, Validation Loss: 0.6624577045440674\n",
      "Epoch 23: Train Loss: 0.6094840963681539, Validation Loss: 0.6619789004325867\n",
      "Epoch 24: Train Loss: 0.6039630836910672, Validation Loss: 0.6583236455917358\n",
      "Epoch 25: Train Loss: 0.6076137390401628, Validation Loss: 0.6592191457748413\n",
      "Epoch 26: Train Loss: 0.6188730597496033, Validation Loss: 0.6582678556442261\n",
      "Epoch 27: Train Loss: 0.5949760542975532, Validation Loss: 0.6599425673484802\n",
      "Epoch 28: Train Loss: 0.571010086271498, Validation Loss: 0.6581262350082397\n",
      "Epoch 29: Train Loss: 0.5887003971470727, Validation Loss: 0.65995854139328\n",
      "Epoch 30: Train Loss: 0.5830571783913506, Validation Loss: 0.6564770340919495\n",
      "Epoch 31: Train Loss: 0.5746945076518588, Validation Loss: 0.6533351540565491\n",
      "Epoch 32: Train Loss: 0.5770574609438578, Validation Loss: 0.6545628905296326\n",
      "Epoch 33: Train Loss: 0.5975953903463151, Validation Loss: 0.6519150137901306\n",
      "Epoch 34: Train Loss: 0.5723300708664788, Validation Loss: 0.651265025138855\n",
      "Epoch 35: Train Loss: 0.5861010683907403, Validation Loss: 0.6489266753196716\n",
      "Epoch 36: Train Loss: 0.5555323461691538, Validation Loss: 0.6457041501998901\n",
      "Epoch 37: Train Loss: 0.574841128455268, Validation Loss: 0.6441776156425476\n",
      "Epoch 38: Train Loss: 0.5477873285611471, Validation Loss: 0.6454712748527527\n",
      "Epoch 39: Train Loss: 0.5491903556717767, Validation Loss: 0.6462160348892212\n",
      "Epoch 40: Train Loss: 0.5658158825503455, Validation Loss: 0.6430381536483765\n",
      "Epoch 41: Train Loss: 0.5792079567909241, Validation Loss: 0.642210066318512\n",
      "Epoch 42: Train Loss: 0.5593508647547828, Validation Loss: 0.6467661261558533\n",
      "Epoch 43: Train Loss: 0.5590942270225949, Validation Loss: 0.6481792330741882\n",
      "Epoch 44: Train Loss: 0.5296011997593774, Validation Loss: 0.648677408695221\n",
      "Epoch 45: Train Loss: 0.5441586242781745, Validation Loss: 0.6465941071510315\n",
      "Epoch 46: Train Loss: 0.5115264024999406, Validation Loss: 0.6432032585144043\n",
      "Epoch 47: Train Loss: 0.5303632815678915, Validation Loss: 0.6461970210075378\n",
      "Epoch 48: Train Loss: 0.5550407469272614, Validation Loss: 0.6456037759780884\n",
      "Epoch 49: Train Loss: 0.5346785121493869, Validation Loss: 0.6451961398124695\n",
      "Epoch 50: Train Loss: 0.5349492761823866, Validation Loss: 0.6468379497528076\n",
      "Epoch 51: Train Loss: 0.5603623688220978, Validation Loss: 0.6457707285881042\n",
      "Epoch 52: Train Loss: 0.5010591480467055, Validation Loss: 0.6511049270629883\n",
      "Epoch 53: Train Loss: 0.5278929041491615, Validation Loss: 0.6544981598854065\n",
      "Epoch 54: Train Loss: 0.49054640531539917, Validation Loss: 0.6561154723167419\n",
      "Epoch 55: Train Loss: 0.5258159107632108, Validation Loss: 0.6531163454055786\n",
      "Epoch 56: Train Loss: 0.5151608188947042, Validation Loss: 0.650929868221283\n",
      "Epoch 57: Train Loss: 0.49333928359879387, Validation Loss: 0.6540148258209229\n",
      "Epoch 58: Train Loss: 0.491382810804579, Validation Loss: 0.6519087553024292\n",
      "Epoch 59: Train Loss: 0.4946102566189236, Validation Loss: 0.6521189212799072\n",
      "Epoch 60: Train Loss: 0.4792088468869527, Validation Loss: 0.6441677808761597\n",
      "Epoch 61: Train Loss: 0.4883089496029748, Validation Loss: 0.6391226649284363\n",
      "Epoch 62: Train Loss: 0.4737705919477675, Validation Loss: 0.6297599077224731\n",
      "Epoch 63: Train Loss: 0.47337977753745186, Validation Loss: 0.623546302318573\n",
      "Epoch 64: Train Loss: 0.4399905204772949, Validation Loss: 0.6167710423469543\n",
      "Epoch 65: Train Loss: 0.47745444046126473, Validation Loss: 0.6163138747215271\n",
      "Epoch 66: Train Loss: 0.4446377820438809, Validation Loss: 0.6123973727226257\n",
      "Epoch 67: Train Loss: 0.41331280436780715, Validation Loss: 0.6096869707107544\n",
      "Epoch 68: Train Loss: 0.42862136827574837, Validation Loss: 0.6081445813179016\n",
      "Epoch 69: Train Loss: 0.4637090497546726, Validation Loss: 0.6094207763671875\n",
      "Epoch 70: Train Loss: 0.42947398291693795, Validation Loss: 0.6041870713233948\n",
      "Epoch 71: Train Loss: 0.45386576652526855, Validation Loss: 0.5958190560340881\n",
      "Epoch 72: Train Loss: 0.4423067470391591, Validation Loss: 0.5871948599815369\n",
      "Epoch 73: Train Loss: 0.44648436705271405, Validation Loss: 0.5800142884254456\n",
      "Epoch 74: Train Loss: 0.4199361503124237, Validation Loss: 0.5756456851959229\n",
      "Epoch 75: Train Loss: 0.4416959484418233, Validation Loss: 0.572288453578949\n",
      "Epoch 76: Train Loss: 0.4083100888464186, Validation Loss: 0.5775279998779297\n",
      "Epoch 77: Train Loss: 0.4064062436421712, Validation Loss: 0.5720872282981873\n",
      "Epoch 78: Train Loss: 0.45748210284445023, Validation Loss: 0.5723535418510437\n",
      "Epoch 79: Train Loss: 0.3963891665140788, Validation Loss: 0.5669749975204468\n",
      "Epoch 80: Train Loss: 0.41035789251327515, Validation Loss: 0.5650256872177124\n",
      "Epoch 81: Train Loss: 0.42069104313850403, Validation Loss: 0.5586998462677002\n",
      "Epoch 82: Train Loss: 0.4351625872982873, Validation Loss: 0.5559694766998291\n",
      "Epoch 83: Train Loss: 0.40978173746003044, Validation Loss: 0.552290678024292\n",
      "Epoch 84: Train Loss: 0.4106857180595398, Validation Loss: 0.5592033863067627\n",
      "Epoch 85: Train Loss: 0.40662049916055465, Validation Loss: 0.5560204982757568\n",
      "Epoch 86: Train Loss: 0.3669378062089284, Validation Loss: 0.5661697387695312\n",
      "Epoch 87: Train Loss: 0.39543794923358494, Validation Loss: 0.5661628246307373\n",
      "Epoch 88: Train Loss: 0.3888834946685367, Validation Loss: 0.5678163766860962\n",
      "Epoch 89: Train Loss: 0.38213275538550484, Validation Loss: 0.5664175748825073\n",
      "Epoch 90: Train Loss: 0.38125985860824585, Validation Loss: 0.5490764379501343\n",
      "Epoch 91: Train Loss: 0.35048172540134853, Validation Loss: 0.5436132550239563\n",
      "Epoch 92: Train Loss: 0.3528011341889699, Validation Loss: 0.5466358661651611\n",
      "Epoch 93: Train Loss: 0.34454158941904706, Validation Loss: 0.5407255291938782\n",
      "Epoch 94: Train Loss: 0.35776353875796, Validation Loss: 0.5373156070709229\n",
      "Epoch 95: Train Loss: 0.33960723876953125, Validation Loss: 0.5338886976242065\n",
      "Epoch 96: Train Loss: 0.3531339599026574, Validation Loss: 0.5316832065582275\n",
      "Epoch 97: Train Loss: 0.36061766081386143, Validation Loss: 0.5337019562721252\n",
      "Epoch 98: Train Loss: 0.3451310396194458, Validation Loss: 0.5446712970733643\n",
      "Epoch 99: Train Loss: 0.37872666120529175, Validation Loss: 0.5545797944068909\n",
      "Epoch 100: Train Loss: 0.35887498325771755, Validation Loss: 0.5685855746269226\n",
      "Epoch 101: Train Loss: 0.33571663830015397, Validation Loss: 0.5624685287475586\n",
      "Epoch 102: Train Loss: 0.35505488846037125, Validation Loss: 0.5622233152389526\n",
      "Epoch 103: Train Loss: 0.3464289042684767, Validation Loss: 0.5981144309043884\n",
      "Epoch 104: Train Loss: 0.34234481387668186, Validation Loss: 0.5891019701957703\n",
      "Epoch 105: Train Loss: 0.30518077976173824, Validation Loss: 0.5941518545150757\n",
      "Epoch 106: Train Loss: 0.3325263609488805, Validation Loss: 0.6009785532951355\n",
      "Epoch 107: Train Loss: 0.32760248084863025, Validation Loss: 0.5893789529800415\n",
      "Epoch 108: Train Loss: 0.3138141830762227, Validation Loss: 0.5890949368476868\n",
      "Epoch 109: Train Loss: 0.31782061689429814, Validation Loss: 0.6040304899215698\n",
      "Epoch 110: Train Loss: 0.3220418774419361, Validation Loss: 0.570915937423706\n",
      "Epoch 111: Train Loss: 0.32834310001797146, Validation Loss: 0.5661872029304504\n",
      "Epoch 112: Train Loss: 0.3203062911828359, Validation Loss: 0.5837911367416382\n",
      "Epoch 113: Train Loss: 0.32715146740277606, Validation Loss: 0.5663713216781616\n",
      "Epoch 114: Train Loss: 0.3181905034515593, Validation Loss: 0.5836842060089111\n",
      "Epoch 115: Train Loss: 0.30447567171520656, Validation Loss: 0.5866899490356445\n",
      "Epoch 116: Train Loss: 0.2924165378014247, Validation Loss: 0.6036335825920105\n",
      "Epoch 117: Train Loss: 0.2967516630887985, Validation Loss: 0.5977500677108765\n",
      "Epoch 118: Train Loss: 0.2956468529171414, Validation Loss: 0.5929163694381714\n",
      "Epoch 119: Train Loss: 0.2741500478651788, Validation Loss: 0.5760483741760254\n",
      "Epoch 120: Train Loss: 0.31185929973920185, Validation Loss: 0.6036303639411926\n",
      "Epoch 121: Train Loss: 0.30906883709960514, Validation Loss: 0.6112456917762756\n",
      "Epoch 122: Train Loss: 0.3024246080054177, Validation Loss: 0.5933995842933655\n",
      "Epoch 123: Train Loss: 0.28752286897765267, Validation Loss: 0.6036571264266968\n",
      "Epoch 124: Train Loss: 0.24490508437156677, Validation Loss: 0.6044853925704956\n",
      "Epoch 125: Train Loss: 0.3026403271489673, Validation Loss: 0.5873277187347412\n",
      "Epoch 126: Train Loss: 0.25587207906776005, Validation Loss: 0.5921085476875305\n",
      "Epoch 127: Train Loss: 0.24978015157911512, Validation Loss: 0.60971599817276\n",
      "Epoch 128: Train Loss: 0.3017440272702111, Validation Loss: 0.6076589822769165\n",
      "Epoch 129: Train Loss: 0.308422048886617, Validation Loss: 0.6135751605033875\n",
      "Epoch 130: Train Loss: 0.27563216123316026, Validation Loss: 0.6330081820487976\n",
      "Epoch 131: Train Loss: 0.2985679705937703, Validation Loss: 0.6117089986801147\n",
      "Epoch 132: Train Loss: 0.2577999234199524, Validation Loss: 0.5990954041481018\n",
      "Epoch 133: Train Loss: 0.2983744160996543, Validation Loss: 0.5802009105682373\n",
      "Epoch 134: Train Loss: 0.2545834514829848, Validation Loss: 0.5983845591545105\n",
      "Epoch 135: Train Loss: 0.2410896983411577, Validation Loss: 0.6110813021659851\n",
      "Epoch 136: Train Loss: 0.25810419354173875, Validation Loss: 0.6094182133674622\n",
      "Epoch 137: Train Loss: 0.234775862760014, Validation Loss: 0.6022093296051025\n",
      "Epoch 138: Train Loss: 0.23897467884752485, Validation Loss: 0.6142361164093018\n",
      "Epoch 139: Train Loss: 0.25949015882280135, Validation Loss: 0.6073874831199646\n",
      "Epoch 140: Train Loss: 0.2738410300678677, Validation Loss: 0.5992128252983093\n",
      "Epoch 141: Train Loss: 0.2795843982862102, Validation Loss: 0.6343967914581299\n",
      "Epoch 142: Train Loss: 0.24965806967682308, Validation Loss: 0.6271188855171204\n",
      "Epoch 143: Train Loss: 0.242997901307212, Validation Loss: 0.6265472173690796\n",
      "Epoch 144: Train Loss: 0.2615926679637697, Validation Loss: 0.6308248043060303\n",
      "Epoch 145: Train Loss: 0.23979621132214865, Validation Loss: 0.5979576706886292\n",
      "Epoch 146: Train Loss: 0.2509134527709749, Validation Loss: 0.6187259554862976\n",
      "Epoch 147: Train Loss: 0.23632851243019104, Validation Loss: 0.6357220411300659\n",
      "Epoch 148: Train Loss: 0.22767401238282522, Validation Loss: 0.6152936220169067\n",
      "Epoch 149: Train Loss: 0.2281494504875607, Validation Loss: 0.6199450492858887\n",
      "Epoch 150: Train Loss: 0.24770621293120915, Validation Loss: 0.6340569853782654\n",
      "Epoch 151: Train Loss: 0.2426386930876308, Validation Loss: 0.6338361501693726\n",
      "Epoch 152: Train Loss: 0.23025664687156677, Validation Loss: 0.6138196587562561\n",
      "Epoch 153: Train Loss: 0.2082662342323197, Validation Loss: 0.5979875922203064\n",
      "Epoch 154: Train Loss: 0.23670271618498695, Validation Loss: 0.6003440618515015\n",
      "Epoch 155: Train Loss: 0.2336115770869785, Validation Loss: 0.5747637152671814\n",
      "Epoch 156: Train Loss: 0.23614665038055843, Validation Loss: 0.5793439745903015\n",
      "Epoch 157: Train Loss: 0.23984874453809527, Validation Loss: 0.5954094529151917\n",
      "Epoch 158: Train Loss: 0.23578657706578574, Validation Loss: 0.5853381752967834\n",
      "Epoch 159: Train Loss: 0.22305383616023594, Validation Loss: 0.5692987442016602\n",
      "Epoch 160: Train Loss: 0.23105857438511318, Validation Loss: 0.5943112373352051\n",
      "Epoch 161: Train Loss: 0.2322194261683358, Validation Loss: 0.6119627356529236\n",
      "Epoch 162: Train Loss: 0.1900119392408265, Validation Loss: 0.5937505960464478\n",
      "Epoch 163: Train Loss: 0.21556436022122702, Validation Loss: 0.5933635830879211\n",
      "Epoch 164: Train Loss: 0.2136930442518658, Validation Loss: 0.5715209245681763\n",
      "Epoch 165: Train Loss: 0.1804186072614458, Validation Loss: 0.5706639885902405\n",
      "Epoch 166: Train Loss: 0.19362469762563705, Validation Loss: 0.5728042125701904\n",
      "Epoch 167: Train Loss: 0.18229075769583383, Validation Loss: 0.5699211359024048\n",
      "Epoch 168: Train Loss: 0.21642625828584036, Validation Loss: 0.5840850472450256\n",
      "Epoch 169: Train Loss: 0.20167178246710035, Validation Loss: 0.5882773995399475\n",
      "Epoch 170: Train Loss: 0.1917820374170939, Validation Loss: 0.5754354000091553\n",
      "Epoch 171: Train Loss: 0.22475476976897982, Validation Loss: 0.5527485609054565\n",
      "Epoch 172: Train Loss: 0.20157151917616525, Validation Loss: 0.5988337993621826\n",
      "Epoch 173: Train Loss: 0.1868817839357588, Validation Loss: 0.6085221171379089\n",
      "Epoch 174: Train Loss: 0.23886356502771378, Validation Loss: 0.6203333735466003\n",
      "Epoch 175: Train Loss: 0.1995084211230278, Validation Loss: 0.6067470908164978\n",
      "Epoch 176: Train Loss: 0.20225317279497781, Validation Loss: 0.5910204648971558\n",
      "Epoch 177: Train Loss: 0.18729243179162344, Validation Loss: 0.6055594086647034\n",
      "Epoch 178: Train Loss: 0.18418019182152218, Validation Loss: 0.6008821129798889\n",
      "Epoch 179: Train Loss: 0.17783348924583858, Validation Loss: 0.618994414806366\n",
      "Epoch 180: Train Loss: 0.21811689933141074, Validation Loss: 0.6071115136146545\n",
      "Epoch 181: Train Loss: 0.15896885179811054, Validation Loss: 0.55989670753479\n",
      "Epoch 182: Train Loss: 0.18113374627298778, Validation Loss: 0.5890641808509827\n",
      "Epoch 183: Train Loss: 0.18150140013959673, Validation Loss: 0.5557803511619568\n",
      "Epoch 184: Train Loss: 0.1778844412830141, Validation Loss: 0.5057629942893982\n",
      "Epoch 185: Train Loss: 0.17176347805394065, Validation Loss: 0.4913330376148224\n",
      "Epoch 186: Train Loss: 0.17795231441656748, Validation Loss: 0.5096835494041443\n",
      "Epoch 187: Train Loss: 0.1873196123374833, Validation Loss: 0.5574522614479065\n",
      "Epoch 188: Train Loss: 0.19460688945319918, Validation Loss: 0.5571709275245667\n",
      "Epoch 189: Train Loss: 0.15636942204501894, Validation Loss: 0.5269957184791565\n",
      "Epoch 190: Train Loss: 0.17106734216213226, Validation Loss: 0.530673086643219\n",
      "Epoch 191: Train Loss: 0.165619524816672, Validation Loss: 0.5302668213844299\n",
      "Epoch 192: Train Loss: 0.20915380782551235, Validation Loss: 0.544506847858429\n",
      "Epoch 193: Train Loss: 0.1692488102449311, Validation Loss: 0.4829046130180359\n",
      "Epoch 194: Train Loss: 0.13539973729186588, Validation Loss: 0.509083092212677\n",
      "Epoch 195: Train Loss: 0.17576679421795738, Validation Loss: 0.5406762957572937\n",
      "Epoch 196: Train Loss: 0.15818758722808626, Validation Loss: 0.5407931804656982\n",
      "Epoch 197: Train Loss: 0.17197546362876892, Validation Loss: 0.5058485269546509\n",
      "Epoch 198: Train Loss: 0.18471853103902605, Validation Loss: 0.5414958596229553\n",
      "Epoch 199: Train Loss: 0.1436743400990963, Validation Loss: 0.49766674637794495\n",
      "Fold 5 Metrics:\n",
      "Accuracy: 0.8181818181818182, Precision: 0.75, Recall: 1.0, F1-score: 0.8571428571428571, AUC: 0.8\n",
      "Confusion Matrix:\n",
      "[[3 2]\n",
      " [0 6]]\n",
      "Completed fold 5\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples from subject 6 to test set\n",
      "Adding 6 truth samples from subject 6 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.666766709751553, Validation Loss: 0.7002674341201782\n",
      "Epoch 1: Train Loss: 0.6697104374567667, Validation Loss: 0.7089802622795105\n",
      "Epoch 2: Train Loss: 0.6731422146161398, Validation Loss: 0.7187948822975159\n",
      "Epoch 3: Train Loss: 0.69330718782213, Validation Loss: 0.7250807285308838\n",
      "Epoch 4: Train Loss: 0.6575677726003859, Validation Loss: 0.728071391582489\n",
      "Epoch 5: Train Loss: 0.6555191212230258, Validation Loss: 0.7287740707397461\n",
      "Epoch 6: Train Loss: 0.6572709613376193, Validation Loss: 0.7303481698036194\n",
      "Epoch 7: Train Loss: 0.6458362936973572, Validation Loss: 0.7311127781867981\n",
      "Epoch 8: Train Loss: 0.6401806738641527, Validation Loss: 0.7310360074043274\n",
      "Epoch 9: Train Loss: 0.6560111906793382, Validation Loss: 0.7298566699028015\n",
      "Epoch 10: Train Loss: 0.6216354436344571, Validation Loss: 0.7328200936317444\n",
      "Epoch 11: Train Loss: 0.6760129398769803, Validation Loss: 0.7325757741928101\n",
      "Epoch 12: Train Loss: 0.6173421674304538, Validation Loss: 0.7324820756912231\n",
      "Epoch 13: Train Loss: 0.622591687573327, Validation Loss: 0.7325394749641418\n",
      "Epoch 14: Train Loss: 0.6154134737120734, Validation Loss: 0.7309579849243164\n",
      "Epoch 15: Train Loss: 0.6018977562586466, Validation Loss: 0.7309151887893677\n",
      "Epoch 16: Train Loss: 0.5948103268941244, Validation Loss: 0.730887234210968\n",
      "Epoch 17: Train Loss: 0.6108051869604323, Validation Loss: 0.730129599571228\n",
      "Epoch 18: Train Loss: 0.6263415614763895, Validation Loss: 0.729141354560852\n",
      "Epoch 19: Train Loss: 0.6001659135023752, Validation Loss: 0.7294376492500305\n",
      "Epoch 20: Train Loss: 0.6098152796427408, Validation Loss: 0.7304864525794983\n",
      "Epoch 21: Train Loss: 0.59340414736006, Validation Loss: 0.7308477163314819\n",
      "Epoch 22: Train Loss: 0.5833192931281196, Validation Loss: 0.729841947555542\n",
      "Epoch 23: Train Loss: 0.586789071559906, Validation Loss: 0.7287650108337402\n",
      "Epoch 24: Train Loss: 0.575471838315328, Validation Loss: 0.7273386120796204\n",
      "Epoch 25: Train Loss: 0.5592286189397176, Validation Loss: 0.7264595031738281\n",
      "Epoch 26: Train Loss: 0.5726461642318301, Validation Loss: 0.7257771492004395\n",
      "Epoch 27: Train Loss: 0.5753172006871965, Validation Loss: 0.7256413102149963\n",
      "Epoch 28: Train Loss: 0.578012055820889, Validation Loss: 0.7256410121917725\n",
      "Epoch 29: Train Loss: 0.557362887594435, Validation Loss: 0.7259019017219543\n",
      "Epoch 30: Train Loss: 0.5640668008062575, Validation Loss: 0.7272332906723022\n",
      "Epoch 31: Train Loss: 0.5683254996935526, Validation Loss: 0.7285350561141968\n",
      "Epoch 32: Train Loss: 0.5796158181296455, Validation Loss: 0.7266125679016113\n",
      "Epoch 33: Train Loss: 0.5544088847107358, Validation Loss: 0.7253470420837402\n",
      "Epoch 34: Train Loss: 0.5264381965001425, Validation Loss: 0.7239740490913391\n",
      "Epoch 35: Train Loss: 0.5320005781120725, Validation Loss: 0.7224794030189514\n",
      "Epoch 36: Train Loss: 0.569478495253457, Validation Loss: 0.7221056818962097\n",
      "Epoch 37: Train Loss: 0.532686816321479, Validation Loss: 0.7227423787117004\n",
      "Epoch 38: Train Loss: 0.5321463015344408, Validation Loss: 0.7228577136993408\n",
      "Epoch 39: Train Loss: 0.537269522746404, Validation Loss: 0.7233178615570068\n",
      "Epoch 40: Train Loss: 0.535323484076394, Validation Loss: 0.7207894325256348\n",
      "Epoch 41: Train Loss: 0.5309005578358968, Validation Loss: 0.7213084697723389\n",
      "Epoch 42: Train Loss: 0.5282537639141083, Validation Loss: 0.7209014892578125\n",
      "Epoch 43: Train Loss: 0.520385848151313, Validation Loss: 0.7189797759056091\n",
      "Epoch 44: Train Loss: 0.4908876683976915, Validation Loss: 0.7192025184631348\n",
      "Epoch 45: Train Loss: 0.507710744937261, Validation Loss: 0.7194777727127075\n",
      "Epoch 46: Train Loss: 0.5085143049558004, Validation Loss: 0.7177724838256836\n",
      "Epoch 47: Train Loss: 0.5334378149774339, Validation Loss: 0.7179133296012878\n",
      "Epoch 48: Train Loss: 0.5034225384394327, Validation Loss: 0.716162383556366\n",
      "Epoch 49: Train Loss: 0.5286245909002092, Validation Loss: 0.7165130376815796\n",
      "Epoch 50: Train Loss: 0.46609139773580766, Validation Loss: 0.7153951525688171\n",
      "Epoch 51: Train Loss: 0.5279558433426751, Validation Loss: 0.7146182656288147\n",
      "Epoch 52: Train Loss: 0.5016288061936697, Validation Loss: 0.7104315757751465\n",
      "Epoch 53: Train Loss: 0.4840595523516337, Validation Loss: 0.7095443606376648\n",
      "Epoch 54: Train Loss: 0.48484426736831665, Validation Loss: 0.7078403234481812\n",
      "Epoch 55: Train Loss: 0.4719294508298238, Validation Loss: 0.7074854969978333\n",
      "Epoch 56: Train Loss: 0.4829099608792199, Validation Loss: 0.7068777084350586\n",
      "Epoch 57: Train Loss: 0.46708406342400444, Validation Loss: 0.7063634395599365\n",
      "Epoch 58: Train Loss: 0.46089833974838257, Validation Loss: 0.7068842649459839\n",
      "Epoch 59: Train Loss: 0.46014196011755204, Validation Loss: 0.7065339088439941\n",
      "Epoch 60: Train Loss: 0.4861620134777493, Validation Loss: 0.705367386341095\n",
      "Epoch 61: Train Loss: 0.4489008453157213, Validation Loss: 0.7028470635414124\n",
      "Epoch 62: Train Loss: 0.4663584795263078, Validation Loss: 0.7005062103271484\n",
      "Epoch 63: Train Loss: 0.4524533318148719, Validation Loss: 0.7004551887512207\n",
      "Epoch 64: Train Loss: 0.47609379556443954, Validation Loss: 0.7013729810714722\n",
      "Epoch 65: Train Loss: 0.46070243583785164, Validation Loss: 0.7011558413505554\n",
      "Epoch 66: Train Loss: 0.45474959744347465, Validation Loss: 0.6999166011810303\n",
      "Epoch 67: Train Loss: 0.4470693700843387, Validation Loss: 0.6992260813713074\n",
      "Epoch 68: Train Loss: 0.42603323856989544, Validation Loss: 0.6994432210922241\n",
      "Epoch 69: Train Loss: 0.4258737067381541, Validation Loss: 0.6985098719596863\n",
      "Epoch 70: Train Loss: 0.4372088611125946, Validation Loss: 0.694421112537384\n",
      "Epoch 71: Train Loss: 0.4387058946821425, Validation Loss: 0.6947700381278992\n",
      "Epoch 72: Train Loss: 0.4375367859999339, Validation Loss: 0.6933660507202148\n",
      "Epoch 73: Train Loss: 0.40538021590974593, Validation Loss: 0.6932021975517273\n",
      "Epoch 74: Train Loss: 0.39524610506163704, Validation Loss: 0.6922775506973267\n",
      "Epoch 75: Train Loss: 0.4108079738087124, Validation Loss: 0.6900709271430969\n",
      "Epoch 76: Train Loss: 0.3914857241842482, Validation Loss: 0.6898336410522461\n",
      "Epoch 77: Train Loss: 0.3804763721095191, Validation Loss: 0.6901649236679077\n",
      "Epoch 78: Train Loss: 0.40119388699531555, Validation Loss: 0.6905643343925476\n",
      "Epoch 79: Train Loss: 0.36823071042696637, Validation Loss: 0.6888110041618347\n",
      "Epoch 80: Train Loss: 0.40175944566726685, Validation Loss: 0.6886574029922485\n",
      "Epoch 81: Train Loss: 0.40041644871234894, Validation Loss: 0.6853789687156677\n",
      "Epoch 82: Train Loss: 0.38024301992522347, Validation Loss: 0.6854332685470581\n",
      "Epoch 83: Train Loss: 0.38634468449486625, Validation Loss: 0.6819849610328674\n",
      "Epoch 84: Train Loss: 0.36087514956792194, Validation Loss: 0.6813904643058777\n",
      "Epoch 85: Train Loss: 0.3636120061079661, Validation Loss: 0.6814621686935425\n",
      "Epoch 86: Train Loss: 0.422202424870597, Validation Loss: 0.6822996735572815\n",
      "Epoch 87: Train Loss: 0.38227663106388515, Validation Loss: 0.681141197681427\n",
      "Epoch 88: Train Loss: 0.3599873450067308, Validation Loss: 0.6803669929504395\n",
      "Epoch 89: Train Loss: 0.3582707676622603, Validation Loss: 0.681911051273346\n",
      "Epoch 90: Train Loss: 0.37370432747734916, Validation Loss: 0.6810720562934875\n",
      "Epoch 91: Train Loss: 0.34886981050173443, Validation Loss: 0.679366946220398\n",
      "Epoch 92: Train Loss: 0.37511999408404034, Validation Loss: 0.6789098978042603\n",
      "Epoch 93: Train Loss: 0.368025079369545, Validation Loss: 0.6792501211166382\n",
      "Epoch 94: Train Loss: 0.36071157455444336, Validation Loss: 0.6772854328155518\n",
      "Epoch 95: Train Loss: 0.36594873004489475, Validation Loss: 0.67801833152771\n",
      "Epoch 96: Train Loss: 0.3234431660837597, Validation Loss: 0.6784439086914062\n",
      "Epoch 97: Train Loss: 0.3694148229228126, Validation Loss: 0.6764718890190125\n",
      "Epoch 98: Train Loss: 0.35240962770250106, Validation Loss: 0.6748650074005127\n",
      "Epoch 99: Train Loss: 0.33772729337215424, Validation Loss: 0.6756311655044556\n",
      "Epoch 100: Train Loss: 0.3427871796819899, Validation Loss: 0.6734797358512878\n",
      "Epoch 101: Train Loss: 0.34283899267514545, Validation Loss: 0.6693342924118042\n",
      "Epoch 102: Train Loss: 0.3797400693098704, Validation Loss: 0.6665922999382019\n",
      "Epoch 103: Train Loss: 0.3258434004253811, Validation Loss: 0.662763774394989\n",
      "Epoch 104: Train Loss: 0.3075251744853126, Validation Loss: 0.6623873710632324\n",
      "Epoch 105: Train Loss: 0.32297587725851273, Validation Loss: 0.6615607142448425\n",
      "Epoch 106: Train Loss: 0.361021767059962, Validation Loss: 0.6593116521835327\n",
      "Epoch 107: Train Loss: 0.3278418646918403, Validation Loss: 0.6599646806716919\n",
      "Epoch 108: Train Loss: 0.32744701703389484, Validation Loss: 0.6594120860099792\n",
      "Epoch 109: Train Loss: 0.32972890469763017, Validation Loss: 0.6589874625205994\n",
      "Epoch 110: Train Loss: 0.3242285235060586, Validation Loss: 0.6574652791023254\n",
      "Epoch 111: Train Loss: 0.3146892140309016, Validation Loss: 0.65672767162323\n",
      "Epoch 112: Train Loss: 0.34135784208774567, Validation Loss: 0.6567270755767822\n",
      "Epoch 113: Train Loss: 0.32844358848200905, Validation Loss: 0.6537589430809021\n",
      "Epoch 114: Train Loss: 0.30912893348269993, Validation Loss: 0.6528606414794922\n",
      "Epoch 115: Train Loss: 0.31191639767752755, Validation Loss: 0.6489735841751099\n",
      "Epoch 116: Train Loss: 0.3098936395512687, Validation Loss: 0.6485718488693237\n",
      "Epoch 117: Train Loss: 0.31445420119497514, Validation Loss: 0.6470409035682678\n",
      "Epoch 118: Train Loss: 0.30950695607397294, Validation Loss: 0.6469154357910156\n",
      "Epoch 119: Train Loss: 0.29645982053544784, Validation Loss: 0.6477463841438293\n",
      "Epoch 120: Train Loss: 0.28213546011183, Validation Loss: 0.6471534371376038\n",
      "Epoch 121: Train Loss: 0.31389159129725563, Validation Loss: 0.6463843584060669\n",
      "Epoch 122: Train Loss: 0.2788289032048649, Validation Loss: 0.6453825831413269\n",
      "Epoch 123: Train Loss: 0.27228063096602756, Validation Loss: 0.6455321907997131\n",
      "Epoch 124: Train Loss: 0.3125900626182556, Validation Loss: 0.6432947516441345\n",
      "Epoch 125: Train Loss: 0.27575040691428715, Validation Loss: 0.6407356858253479\n",
      "Epoch 126: Train Loss: 0.29551515314314103, Validation Loss: 0.6378371715545654\n",
      "Epoch 127: Train Loss: 0.29899510906802285, Validation Loss: 0.6351155638694763\n",
      "Epoch 128: Train Loss: 0.2984815736611684, Validation Loss: 0.6368135213851929\n",
      "Epoch 129: Train Loss: 0.2719915260871251, Validation Loss: 0.6373666524887085\n",
      "Epoch 130: Train Loss: 0.277977842423651, Validation Loss: 0.6335905194282532\n",
      "Epoch 131: Train Loss: 0.30107623007562423, Validation Loss: 0.6293781995773315\n",
      "Epoch 132: Train Loss: 0.27295026845402187, Validation Loss: 0.6291444301605225\n",
      "Epoch 133: Train Loss: 0.27981169025103253, Validation Loss: 0.6261994242668152\n",
      "Epoch 134: Train Loss: 0.2786618007553948, Validation Loss: 0.6250883936882019\n",
      "Epoch 135: Train Loss: 0.25158217880460954, Validation Loss: 0.6248019933700562\n",
      "Epoch 136: Train Loss: 0.27327919668621486, Validation Loss: 0.6239901185035706\n",
      "Epoch 137: Train Loss: 0.2490268333090676, Validation Loss: 0.6236740946769714\n",
      "Epoch 138: Train Loss: 0.26649435195657944, Validation Loss: 0.6224868893623352\n",
      "Epoch 139: Train Loss: 0.27318469021055436, Validation Loss: 0.6225875616073608\n",
      "Epoch 140: Train Loss: 0.269906943043073, Validation Loss: 0.6243473291397095\n",
      "Epoch 141: Train Loss: 0.2935466567675273, Validation Loss: 0.6259848475456238\n",
      "Epoch 142: Train Loss: 0.3295031372043822, Validation Loss: 0.6318656802177429\n",
      "Epoch 143: Train Loss: 0.30043480628066593, Validation Loss: 0.6352676153182983\n",
      "Epoch 144: Train Loss: 0.25299761361545986, Validation Loss: 0.6336320638656616\n",
      "Epoch 145: Train Loss: 0.2749258329470952, Validation Loss: 0.6319794654846191\n",
      "Epoch 146: Train Loss: 0.24289658996793959, Validation Loss: 0.6304541230201721\n",
      "Epoch 147: Train Loss: 0.24198354283968607, Validation Loss: 0.6309873461723328\n",
      "Epoch 148: Train Loss: 0.2562635822428597, Validation Loss: 0.6288079023361206\n",
      "Epoch 149: Train Loss: 0.2706749786933263, Validation Loss: 0.6310415863990784\n",
      "Epoch 150: Train Loss: 0.25475383632712895, Validation Loss: 0.6251130104064941\n",
      "Epoch 151: Train Loss: 0.2641497784190708, Validation Loss: 0.6163132786750793\n",
      "Epoch 152: Train Loss: 0.2756662782695558, Validation Loss: 0.6147744655609131\n",
      "Epoch 153: Train Loss: 0.24740526411268446, Validation Loss: 0.6118382215499878\n",
      "Epoch 154: Train Loss: 0.2329188303814994, Validation Loss: 0.609022319316864\n",
      "Epoch 155: Train Loss: 0.2561263342698415, Validation Loss: 0.6088652610778809\n",
      "Epoch 156: Train Loss: 0.23324093222618103, Validation Loss: 0.6086341738700867\n",
      "Epoch 157: Train Loss: 0.21419554948806763, Validation Loss: 0.6064470410346985\n",
      "Epoch 158: Train Loss: 0.22931810882356432, Validation Loss: 0.6058944463729858\n",
      "Epoch 159: Train Loss: 0.20511956181791094, Validation Loss: 0.6057679057121277\n",
      "Epoch 160: Train Loss: 0.2574321660730574, Validation Loss: 0.6053586602210999\n",
      "Epoch 161: Train Loss: 0.197371115287145, Validation Loss: 0.6049829721450806\n",
      "Epoch 162: Train Loss: 0.2399324725071589, Validation Loss: 0.6074346303939819\n",
      "Epoch 163: Train Loss: 0.2727140320671929, Validation Loss: 0.6043336987495422\n",
      "Epoch 164: Train Loss: 0.21780638148387274, Validation Loss: 0.598270833492279\n",
      "Epoch 165: Train Loss: 0.22907174047496584, Validation Loss: 0.5983157753944397\n",
      "Epoch 166: Train Loss: 0.22609807385338676, Validation Loss: 0.5980604887008667\n",
      "Epoch 167: Train Loss: 0.2587895426485274, Validation Loss: 0.6003324389457703\n",
      "Epoch 168: Train Loss: 0.23905019958813986, Validation Loss: 0.5976867079734802\n",
      "Epoch 169: Train Loss: 0.22875405268536675, Validation Loss: 0.5969805717468262\n",
      "Epoch 170: Train Loss: 0.19640918324391046, Validation Loss: 0.5963401198387146\n",
      "Epoch 171: Train Loss: 0.24332545697689056, Validation Loss: 0.5942479372024536\n",
      "Epoch 172: Train Loss: 0.2366312411096361, Validation Loss: 0.5964601635932922\n",
      "Epoch 173: Train Loss: 0.2203395515680313, Validation Loss: 0.5962477326393127\n",
      "Epoch 174: Train Loss: 0.22986545165379843, Validation Loss: 0.6003361940383911\n",
      "Epoch 175: Train Loss: 0.20974370506074694, Validation Loss: 0.6030257940292358\n",
      "Epoch 176: Train Loss: 0.2214367753929562, Validation Loss: 0.6039003729820251\n",
      "Epoch 177: Train Loss: 0.26760081781281364, Validation Loss: 0.6090895533561707\n",
      "Epoch 178: Train Loss: 0.1955254003405571, Validation Loss: 0.6052859425544739\n",
      "Epoch 179: Train Loss: 0.19988779723644257, Validation Loss: 0.6057385802268982\n",
      "Epoch 180: Train Loss: 0.24767084419727325, Validation Loss: 0.6070661544799805\n",
      "Epoch 181: Train Loss: 0.25406743586063385, Validation Loss: 0.6041170954704285\n",
      "Epoch 182: Train Loss: 0.20882019069459704, Validation Loss: 0.6015214920043945\n",
      "Epoch 183: Train Loss: 0.2081771418452263, Validation Loss: 0.6001543402671814\n",
      "Epoch 184: Train Loss: 0.23482966919740042, Validation Loss: 0.6008521318435669\n",
      "Epoch 185: Train Loss: 0.23088135653071934, Validation Loss: 0.598293662071228\n",
      "Epoch 186: Train Loss: 0.20280298425091636, Validation Loss: 0.5961266756057739\n",
      "Epoch 187: Train Loss: 0.19949030876159668, Validation Loss: 0.5946166515350342\n",
      "Epoch 188: Train Loss: 0.18155637797382143, Validation Loss: 0.6004167199134827\n",
      "Epoch 189: Train Loss: 0.2690044922961129, Validation Loss: 0.5973700881004333\n",
      "Epoch 190: Train Loss: 0.2204701097475158, Validation Loss: 0.5913313031196594\n",
      "Epoch 191: Train Loss: 0.26771683825386894, Validation Loss: 0.5871297121047974\n",
      "Epoch 192: Train Loss: 0.19418641097015804, Validation Loss: 0.5854517817497253\n",
      "Epoch 193: Train Loss: 0.19812891715102726, Validation Loss: 0.5837867259979248\n",
      "Epoch 194: Train Loss: 0.17853340837690565, Validation Loss: 0.5860271453857422\n",
      "Epoch 195: Train Loss: 0.21720746490690443, Validation Loss: 0.5865378975868225\n",
      "Epoch 196: Train Loss: 0.17849600066741309, Validation Loss: 0.5862441658973694\n",
      "Epoch 197: Train Loss: 0.1911257662706905, Validation Loss: 0.5889074802398682\n",
      "Epoch 198: Train Loss: 0.21031299067868126, Validation Loss: 0.5850047469139099\n",
      "Epoch 199: Train Loss: 0.1821464573343595, Validation Loss: 0.5841979384422302\n",
      "Fold 6 Metrics:\n",
      "Accuracy: 0.8181818181818182, Precision: 1.0, Recall: 0.6666666666666666, F1-score: 0.8, AUC: 0.8333333333333333\n",
      "Confusion Matrix:\n",
      "[[5 0]\n",
      " [2 4]]\n",
      "Completed fold 6\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples from subject 12 to test set\n",
      "Adding 6 truth samples from subject 12 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.7442934248182509, Validation Loss: 0.7010872960090637\n",
      "Epoch 1: Train Loss: 0.6980431013637118, Validation Loss: 0.7035008072853088\n",
      "Epoch 2: Train Loss: 0.6941084861755371, Validation Loss: 0.7066508531570435\n",
      "Epoch 3: Train Loss: 0.7321292890442742, Validation Loss: 0.7073175311088562\n",
      "Epoch 4: Train Loss: 0.7118194500605265, Validation Loss: 0.7067135572433472\n",
      "Epoch 5: Train Loss: 0.6835799482133653, Validation Loss: 0.7066546082496643\n",
      "Epoch 6: Train Loss: 0.7119754486613803, Validation Loss: 0.7055906057357788\n",
      "Epoch 7: Train Loss: 0.6656366851594713, Validation Loss: 0.7055018544197083\n",
      "Epoch 8: Train Loss: 0.691962308353848, Validation Loss: 0.7048105597496033\n",
      "Epoch 9: Train Loss: 0.6425875756475661, Validation Loss: 0.7055225968360901\n",
      "Epoch 10: Train Loss: 0.6669897569550408, Validation Loss: 0.700957179069519\n",
      "Epoch 11: Train Loss: 0.668938954671224, Validation Loss: 0.6976413130760193\n",
      "Epoch 12: Train Loss: 0.6828130218717787, Validation Loss: 0.6960711479187012\n",
      "Epoch 13: Train Loss: 0.6904351909955343, Validation Loss: 0.694172739982605\n",
      "Epoch 14: Train Loss: 0.6776576108402677, Validation Loss: 0.6940276026725769\n",
      "Epoch 15: Train Loss: 0.651490310827891, Validation Loss: 0.6928747892379761\n",
      "Epoch 16: Train Loss: 0.639842861228519, Validation Loss: 0.6929405331611633\n",
      "Epoch 17: Train Loss: 0.6505569285816617, Validation Loss: 0.6920436024665833\n",
      "Epoch 18: Train Loss: 0.6592632532119751, Validation Loss: 0.6925559043884277\n",
      "Epoch 19: Train Loss: 0.6619443694750468, Validation Loss: 0.6923958659172058\n",
      "Epoch 20: Train Loss: 0.6429836683803134, Validation Loss: 0.6892164349555969\n",
      "Epoch 21: Train Loss: 0.6277914312150743, Validation Loss: 0.6897483468055725\n",
      "Epoch 22: Train Loss: 0.6244804329342313, Validation Loss: 0.6894258260726929\n",
      "Epoch 23: Train Loss: 0.6396323111322191, Validation Loss: 0.6877099275588989\n",
      "Epoch 24: Train Loss: 0.6390482319725884, Validation Loss: 0.6846315264701843\n",
      "Epoch 25: Train Loss: 0.6448304322030809, Validation Loss: 0.6825809478759766\n",
      "Epoch 26: Train Loss: 0.6073161098692152, Validation Loss: 0.6818873286247253\n",
      "Epoch 27: Train Loss: 0.6101684504085116, Validation Loss: 0.6815866231918335\n",
      "Epoch 28: Train Loss: 0.5980754362212287, Validation Loss: 0.6807328462600708\n",
      "Epoch 29: Train Loss: 0.6207688252131144, Validation Loss: 0.6809442043304443\n",
      "Epoch 30: Train Loss: 0.6037369701597426, Validation Loss: 0.6795956492424011\n",
      "Epoch 31: Train Loss: 0.5847970777087741, Validation Loss: 0.6758658289909363\n",
      "Epoch 32: Train Loss: 0.5965190662278069, Validation Loss: 0.6747032403945923\n",
      "Epoch 33: Train Loss: 0.6033252212736342, Validation Loss: 0.6747671961784363\n",
      "Epoch 34: Train Loss: 0.6132786803775363, Validation Loss: 0.6742597818374634\n",
      "Epoch 35: Train Loss: 0.5478321611881256, Validation Loss: 0.6727547645568848\n",
      "Epoch 36: Train Loss: 0.5859455598725213, Validation Loss: 0.6717624068260193\n",
      "Epoch 37: Train Loss: 0.5643190476629469, Validation Loss: 0.6722041964530945\n",
      "Epoch 38: Train Loss: 0.564944714307785, Validation Loss: 0.6712369322776794\n",
      "Epoch 39: Train Loss: 0.5767302413781484, Validation Loss: 0.6708201169967651\n",
      "Epoch 40: Train Loss: 0.5725259979565939, Validation Loss: 0.6707738637924194\n",
      "Epoch 41: Train Loss: 0.5925438867674934, Validation Loss: 0.6673023104667664\n",
      "Epoch 42: Train Loss: 0.5707488126224942, Validation Loss: 0.6657535433769226\n",
      "Epoch 43: Train Loss: 0.5857903162638346, Validation Loss: 0.6655211448669434\n",
      "Epoch 44: Train Loss: 0.5762092471122742, Validation Loss: 0.6636234521865845\n",
      "Epoch 45: Train Loss: 0.5593532721201578, Validation Loss: 0.6624515652656555\n",
      "Epoch 46: Train Loss: 0.5883472826745775, Validation Loss: 0.661638081073761\n",
      "Epoch 47: Train Loss: 0.5497727228535546, Validation Loss: 0.6612333059310913\n",
      "Epoch 48: Train Loss: 0.5346495078669654, Validation Loss: 0.6594537496566772\n",
      "Epoch 49: Train Loss: 0.544649236732059, Validation Loss: 0.6593969464302063\n",
      "Epoch 50: Train Loss: 0.5171130233340793, Validation Loss: 0.6611111164093018\n",
      "Epoch 51: Train Loss: 0.5555729998482598, Validation Loss: 0.6584188938140869\n",
      "Epoch 52: Train Loss: 0.540337966548072, Validation Loss: 0.6592385768890381\n",
      "Epoch 53: Train Loss: 0.5774218738079071, Validation Loss: 0.6586907505989075\n",
      "Epoch 54: Train Loss: 0.5364695191383362, Validation Loss: 0.6577634811401367\n",
      "Epoch 55: Train Loss: 0.48741966485977173, Validation Loss: 0.6550622582435608\n",
      "Epoch 56: Train Loss: 0.5212837623225318, Validation Loss: 0.6561444997787476\n",
      "Epoch 57: Train Loss: 0.499130990770128, Validation Loss: 0.6566170454025269\n",
      "Epoch 58: Train Loss: 0.5078591472572751, Validation Loss: 0.6553338766098022\n",
      "Epoch 59: Train Loss: 0.5294960637887319, Validation Loss: 0.6554943919181824\n",
      "Epoch 60: Train Loss: 0.5124968720806969, Validation Loss: 0.6514721512794495\n",
      "Epoch 61: Train Loss: 0.5442008872826894, Validation Loss: 0.6496449708938599\n",
      "Epoch 62: Train Loss: 0.4819723400804732, Validation Loss: 0.647018551826477\n",
      "Epoch 63: Train Loss: 0.5132450130250719, Validation Loss: 0.6469348073005676\n",
      "Epoch 64: Train Loss: 0.5178229543897841, Validation Loss: 0.6442878246307373\n",
      "Epoch 65: Train Loss: 0.5102029045422872, Validation Loss: 0.6449025869369507\n",
      "Epoch 66: Train Loss: 0.506822751628028, Validation Loss: 0.6454194188117981\n",
      "Epoch 67: Train Loss: 0.5024914575947655, Validation Loss: 0.6478441953659058\n",
      "Epoch 68: Train Loss: 0.49645434154404533, Validation Loss: 0.6461455821990967\n",
      "Epoch 69: Train Loss: 0.5108863446447585, Validation Loss: 0.6457585692405701\n",
      "Epoch 70: Train Loss: 0.4812185333834754, Validation Loss: 0.6477625370025635\n",
      "Epoch 71: Train Loss: 0.48315148221121895, Validation Loss: 0.64609295129776\n",
      "Epoch 72: Train Loss: 0.5177013443575965, Validation Loss: 0.6514334678649902\n",
      "Epoch 73: Train Loss: 0.4390941957632701, Validation Loss: 0.6505247354507446\n",
      "Epoch 74: Train Loss: 0.4809953272342682, Validation Loss: 0.6495766043663025\n",
      "Epoch 75: Train Loss: 0.4812984698348575, Validation Loss: 0.6525862216949463\n",
      "Epoch 76: Train Loss: 0.4905313220289018, Validation Loss: 0.653242826461792\n",
      "Epoch 77: Train Loss: 0.4469211134645674, Validation Loss: 0.6529586911201477\n",
      "Epoch 78: Train Loss: 0.4713718394438426, Validation Loss: 0.6522112488746643\n",
      "Epoch 79: Train Loss: 0.4679315189520518, Validation Loss: 0.6500643491744995\n",
      "Epoch 80: Train Loss: 0.49274816777971053, Validation Loss: 0.6452689170837402\n",
      "Epoch 81: Train Loss: 0.4817039834128486, Validation Loss: 0.6420467495918274\n",
      "Epoch 82: Train Loss: 0.46233534150653416, Validation Loss: 0.6415280103683472\n",
      "Epoch 83: Train Loss: 0.44433631168471444, Validation Loss: 0.6415254473686218\n",
      "Epoch 84: Train Loss: 0.4534655213356018, Validation Loss: 0.6422268748283386\n",
      "Epoch 85: Train Loss: 0.4618653721279568, Validation Loss: 0.6445141434669495\n",
      "Epoch 86: Train Loss: 0.4664709336227841, Validation Loss: 0.6439907550811768\n",
      "Epoch 87: Train Loss: 0.4628315501742893, Validation Loss: 0.6430519819259644\n",
      "Epoch 88: Train Loss: 0.4516037238968743, Validation Loss: 0.6452838182449341\n",
      "Epoch 89: Train Loss: 0.44370193282763165, Validation Loss: 0.6430673003196716\n",
      "Epoch 90: Train Loss: 0.45888065298398334, Validation Loss: 0.6430099010467529\n",
      "Epoch 91: Train Loss: 0.44174498319625854, Validation Loss: 0.6406586766242981\n",
      "Epoch 92: Train Loss: 0.4799814422925313, Validation Loss: 0.6352130770683289\n",
      "Epoch 93: Train Loss: 0.41706981923845077, Validation Loss: 0.6381155848503113\n",
      "Epoch 94: Train Loss: 0.4185183710522122, Validation Loss: 0.6347965598106384\n",
      "Epoch 95: Train Loss: 0.41404884060223895, Validation Loss: 0.6359871625900269\n",
      "Epoch 96: Train Loss: 0.43306196067068314, Validation Loss: 0.6360722184181213\n",
      "Epoch 97: Train Loss: 0.4202576180299123, Validation Loss: 0.6349543929100037\n",
      "Epoch 98: Train Loss: 0.4071119361453586, Validation Loss: 0.6353465914726257\n",
      "Epoch 99: Train Loss: 0.4554983874162038, Validation Loss: 0.6340208649635315\n",
      "Epoch 100: Train Loss: 0.4021731482611762, Validation Loss: 0.6390314698219299\n",
      "Epoch 101: Train Loss: 0.44345395101441276, Validation Loss: 0.6422275900840759\n",
      "Epoch 102: Train Loss: 0.43050819635391235, Validation Loss: 0.6423361897468567\n",
      "Epoch 103: Train Loss: 0.4065866586234834, Validation Loss: 0.6432310342788696\n",
      "Epoch 104: Train Loss: 0.4084220396147834, Validation Loss: 0.6459851861000061\n",
      "Epoch 105: Train Loss: 0.4022606876161363, Validation Loss: 0.6418128609657288\n",
      "Epoch 106: Train Loss: 0.42856569753752816, Validation Loss: 0.6440778374671936\n",
      "Epoch 107: Train Loss: 0.37626474764611983, Validation Loss: 0.6428542733192444\n",
      "Epoch 108: Train Loss: 0.4193712870279948, Validation Loss: 0.6436333656311035\n",
      "Epoch 109: Train Loss: 0.41774741146299577, Validation Loss: 0.6417171359062195\n",
      "Epoch 110: Train Loss: 0.38557883765962386, Validation Loss: 0.6403977274894714\n",
      "Epoch 111: Train Loss: 0.39085103074709576, Validation Loss: 0.6387327909469604\n",
      "Epoch 112: Train Loss: 0.39172031150923836, Validation Loss: 0.6419951319694519\n",
      "Epoch 113: Train Loss: 0.3854323559337192, Validation Loss: 0.6421833634376526\n",
      "Epoch 114: Train Loss: 0.3886980646186405, Validation Loss: 0.6442998647689819\n",
      "Epoch 115: Train Loss: 0.3787456154823303, Validation Loss: 0.6469963192939758\n",
      "Epoch 116: Train Loss: 0.38149695926242405, Validation Loss: 0.6453584432601929\n",
      "Epoch 117: Train Loss: 0.3757365081045363, Validation Loss: 0.650612473487854\n",
      "Epoch 118: Train Loss: 0.37184131476614213, Validation Loss: 0.6479579210281372\n",
      "Epoch 119: Train Loss: 0.36693836251894635, Validation Loss: 0.6501835584640503\n",
      "Epoch 120: Train Loss: 0.36765952077176833, Validation Loss: 0.6556481122970581\n",
      "Epoch 121: Train Loss: 0.3748178945647346, Validation Loss: 0.6551375985145569\n",
      "Epoch 122: Train Loss: 0.36143643657366437, Validation Loss: 0.6555806994438171\n",
      "Epoch 123: Train Loss: 0.38450052671962315, Validation Loss: 0.657719075679779\n",
      "Epoch 124: Train Loss: 0.37703317403793335, Validation Loss: 0.6583102345466614\n",
      "Epoch 125: Train Loss: 0.35685846043957603, Validation Loss: 0.6576865315437317\n",
      "Epoch 126: Train Loss: 0.3426063855489095, Validation Loss: 0.6627453565597534\n",
      "Epoch 127: Train Loss: 0.3581928180323707, Validation Loss: 0.6617854237556458\n",
      "Epoch 128: Train Loss: 0.3372746871577369, Validation Loss: 0.662766695022583\n",
      "Epoch 129: Train Loss: 0.3381028374036153, Validation Loss: 0.6638992428779602\n",
      "Epoch 130: Train Loss: 0.3621715870168474, Validation Loss: 0.6714299321174622\n",
      "Epoch 131: Train Loss: 0.3449563880761464, Validation Loss: 0.6863930821418762\n",
      "Epoch 132: Train Loss: 0.3462109433280097, Validation Loss: 0.687796950340271\n",
      "Epoch 133: Train Loss: 0.34194714162084794, Validation Loss: 0.6989942193031311\n",
      "Epoch 134: Train Loss: 0.36670180161794025, Validation Loss: 0.7058972716331482\n",
      "Epoch 135: Train Loss: 0.31370967957708573, Validation Loss: 0.7051219344139099\n",
      "Epoch 136: Train Loss: 0.31753172477086383, Validation Loss: 0.7047143578529358\n",
      "Epoch 137: Train Loss: 0.3273303326633241, Validation Loss: 0.7114149332046509\n",
      "Epoch 138: Train Loss: 0.33142543501324123, Validation Loss: 0.7199687957763672\n",
      "Epoch 139: Train Loss: 0.3380514217747582, Validation Loss: 0.7184339165687561\n",
      "Epoch 140: Train Loss: 0.3785641955004798, Validation Loss: 0.721371054649353\n",
      "Epoch 141: Train Loss: 0.3260142571396298, Validation Loss: 0.7262812852859497\n",
      "Epoch 142: Train Loss: 0.298347532749176, Validation Loss: 0.7363038063049316\n",
      "Epoch 143: Train Loss: 0.30358590020073783, Validation Loss: 0.7549998164176941\n",
      "Epoch 144: Train Loss: 0.30465015603436363, Validation Loss: 0.7688668966293335\n",
      "Epoch 145: Train Loss: 0.31857577297422623, Validation Loss: 0.7597012519836426\n",
      "Epoch 146: Train Loss: 0.3124583860238393, Validation Loss: 0.7639041543006897\n",
      "Epoch 147: Train Loss: 0.30972577797042, Validation Loss: 0.7726455330848694\n",
      "Epoch 148: Train Loss: 0.2980688065290451, Validation Loss: 0.7697484493255615\n",
      "Epoch 149: Train Loss: 0.31116658283604515, Validation Loss: 0.7750903964042664\n",
      "Epoch 150: Train Loss: 0.32860394650035435, Validation Loss: 0.7944812774658203\n",
      "Epoch 151: Train Loss: 0.33604083789719474, Validation Loss: 0.805733323097229\n",
      "Epoch 152: Train Loss: 0.2800467560688655, Validation Loss: 0.8379647731781006\n",
      "Epoch 153: Train Loss: 0.30695785250928664, Validation Loss: 0.8592962026596069\n",
      "Epoch 154: Train Loss: 0.3131743123133977, Validation Loss: 0.8690374493598938\n",
      "Epoch 155: Train Loss: 0.2651316656006707, Validation Loss: 0.8871536254882812\n",
      "Epoch 156: Train Loss: 0.29238894912931657, Validation Loss: 0.8984983563423157\n",
      "Epoch 157: Train Loss: 0.26434432135687935, Validation Loss: 0.9105848073959351\n",
      "Epoch 158: Train Loss: 0.26882124112712014, Validation Loss: 0.919611394405365\n",
      "Epoch 159: Train Loss: 0.2786840150753657, Validation Loss: 0.9167584776878357\n",
      "Epoch 160: Train Loss: 0.3063131719827652, Validation Loss: 0.9393174648284912\n",
      "Epoch 161: Train Loss: 0.26436493794123334, Validation Loss: 0.9848137497901917\n",
      "Epoch 162: Train Loss: 0.2638651728630066, Validation Loss: 1.001446008682251\n",
      "Epoch 163: Train Loss: 0.2659551782740487, Validation Loss: 1.0405914783477783\n",
      "Epoch 164: Train Loss: 0.2429086383846071, Validation Loss: 1.0731953382492065\n",
      "Epoch 165: Train Loss: 0.27094928754700554, Validation Loss: 1.0935832262039185\n",
      "Epoch 166: Train Loss: 0.2623681409491433, Validation Loss: 1.099363923072815\n",
      "Epoch 167: Train Loss: 0.2453842196199629, Validation Loss: 1.1185023784637451\n",
      "Epoch 168: Train Loss: 0.2633421305153105, Validation Loss: 1.1223750114440918\n",
      "Epoch 169: Train Loss: 0.24997364315721723, Validation Loss: 1.1149821281433105\n",
      "Epoch 170: Train Loss: 0.2663992659913169, Validation Loss: 1.146850347518921\n",
      "Epoch 171: Train Loss: 0.2722698864009645, Validation Loss: 1.1671226024627686\n",
      "Epoch 172: Train Loss: 0.2536892278326882, Validation Loss: 1.194900631904602\n",
      "Epoch 173: Train Loss: 0.22964533666769663, Validation Loss: 1.2176282405853271\n",
      "Epoch 174: Train Loss: 0.23132075203789604, Validation Loss: 1.2541680335998535\n",
      "Epoch 175: Train Loss: 0.24163952304257286, Validation Loss: 1.2843095064163208\n",
      "Epoch 176: Train Loss: 0.25948617855707806, Validation Loss: 1.2929661273956299\n",
      "Epoch 177: Train Loss: 0.2715410689512889, Validation Loss: 1.279867172241211\n",
      "Epoch 178: Train Loss: 0.24061708483431074, Validation Loss: 1.294036626815796\n",
      "Epoch 179: Train Loss: 0.23142083485921225, Validation Loss: 1.283397912979126\n",
      "Epoch 180: Train Loss: 0.22312701576285893, Validation Loss: 1.3229659795761108\n",
      "Epoch 181: Train Loss: 0.2515123022927178, Validation Loss: 1.3548524379730225\n",
      "Epoch 182: Train Loss: 0.263423600130611, Validation Loss: 1.3924283981323242\n",
      "Epoch 183: Train Loss: 0.2144942217402988, Validation Loss: 1.3911263942718506\n",
      "Epoch 184: Train Loss: 0.218213419119517, Validation Loss: 1.40915048122406\n",
      "Epoch 185: Train Loss: 0.24325274096594918, Validation Loss: 1.4105225801467896\n",
      "Epoch 186: Train Loss: 0.21241443438662422, Validation Loss: 1.3964756727218628\n",
      "Epoch 187: Train Loss: 0.22654975785149467, Validation Loss: 1.419193983078003\n",
      "Epoch 188: Train Loss: 0.22030177298519346, Validation Loss: 1.4241361618041992\n",
      "Epoch 189: Train Loss: 0.25959491233030957, Validation Loss: 1.4499262571334839\n",
      "Epoch 190: Train Loss: 0.2126146306594213, Validation Loss: 1.5211563110351562\n",
      "Epoch 191: Train Loss: 0.24121426377031538, Validation Loss: 1.572438359260559\n",
      "Epoch 192: Train Loss: 0.22345899707741207, Validation Loss: 1.5211524963378906\n",
      "Epoch 193: Train Loss: 0.21637666391001809, Validation Loss: 1.4970840215682983\n",
      "Epoch 194: Train Loss: 0.20806768867704603, Validation Loss: 1.5330508947372437\n",
      "Epoch 195: Train Loss: 0.2308173808786604, Validation Loss: 1.5335922241210938\n",
      "Epoch 196: Train Loss: 0.19464618464310965, Validation Loss: 1.5436122417449951\n",
      "Epoch 197: Train Loss: 0.2159493582116233, Validation Loss: 1.5169878005981445\n",
      "Epoch 198: Train Loss: 0.20828206919961506, Validation Loss: 1.526340365409851\n",
      "Epoch 199: Train Loss: 0.21877344946066538, Validation Loss: 1.558606505393982\n",
      "Fold 7 Metrics:\n",
      "Accuracy: 0.5454545454545454, Precision: 0.5454545454545454, Recall: 1.0, F1-score: 0.7058823529411765, AUC: 0.5\n",
      "Confusion Matrix:\n",
      "[[0 5]\n",
      " [0 6]]\n",
      "Completed fold 7\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples from subject 9 to test set\n",
      "Adding 6 truth samples from subject 9 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.7272976769341363, Validation Loss: 0.685539960861206\n",
      "Epoch 1: Train Loss: 0.6660444339116415, Validation Loss: 0.6805691719055176\n",
      "Epoch 2: Train Loss: 0.7168097164895799, Validation Loss: 0.6771286129951477\n",
      "Epoch 3: Train Loss: 0.6802753806114197, Validation Loss: 0.6757525205612183\n",
      "Epoch 4: Train Loss: 0.6788454188240899, Validation Loss: 0.6763206720352173\n",
      "Epoch 5: Train Loss: 0.6805652909808688, Validation Loss: 0.6766279935836792\n",
      "Epoch 6: Train Loss: 0.6753742628627353, Validation Loss: 0.6783204674720764\n",
      "Epoch 7: Train Loss: 0.6740802327791849, Validation Loss: 0.6784070134162903\n",
      "Epoch 8: Train Loss: 0.6563799182573954, Validation Loss: 0.6785981059074402\n",
      "Epoch 9: Train Loss: 0.6717848049269782, Validation Loss: 0.67816561460495\n",
      "Epoch 10: Train Loss: 0.6457711060841879, Validation Loss: 0.6811407804489136\n",
      "Epoch 11: Train Loss: 0.6731768846511841, Validation Loss: 0.6827104687690735\n",
      "Epoch 12: Train Loss: 0.6510911782582601, Validation Loss: 0.6851425170898438\n",
      "Epoch 13: Train Loss: 0.6395649313926697, Validation Loss: 0.6871345043182373\n",
      "Epoch 14: Train Loss: 0.6348406142658658, Validation Loss: 0.6917742490768433\n",
      "Epoch 15: Train Loss: 0.6324090427822537, Validation Loss: 0.6942299604415894\n",
      "Epoch 16: Train Loss: 0.6314842932754092, Validation Loss: 0.6945986747741699\n",
      "Epoch 17: Train Loss: 0.595627960231569, Validation Loss: 0.6956775784492493\n",
      "Epoch 18: Train Loss: 0.59762109319369, Validation Loss: 0.6961754560470581\n",
      "Epoch 19: Train Loss: 0.6260096894370185, Validation Loss: 0.6955722570419312\n",
      "Epoch 20: Train Loss: 0.5903457668092515, Validation Loss: 0.6999632716178894\n",
      "Epoch 21: Train Loss: 0.6174685557683309, Validation Loss: 0.7041760683059692\n",
      "Epoch 22: Train Loss: 0.6087685492303636, Validation Loss: 0.7083905339241028\n",
      "Epoch 23: Train Loss: 0.6517803933885362, Validation Loss: 0.7105298638343811\n",
      "Epoch 24: Train Loss: 0.5915332668357425, Validation Loss: 0.7139371037483215\n",
      "Epoch 25: Train Loss: 0.6266006098853217, Validation Loss: 0.7155883312225342\n",
      "Epoch 26: Train Loss: 0.6096414195166694, Validation Loss: 0.7186900973320007\n",
      "Epoch 27: Train Loss: 0.5821011265118917, Validation Loss: 0.7195695638656616\n",
      "Epoch 28: Train Loss: 0.5961992541948954, Validation Loss: 0.7208600044250488\n",
      "Epoch 29: Train Loss: 0.5803141593933105, Validation Loss: 0.7208185195922852\n",
      "Epoch 30: Train Loss: 0.5795653661092123, Validation Loss: 0.7243666052818298\n",
      "Epoch 31: Train Loss: 0.5741657382912106, Validation Loss: 0.7300374507904053\n",
      "Epoch 32: Train Loss: 0.5675048132737478, Validation Loss: 0.737471342086792\n",
      "Epoch 33: Train Loss: 0.5563908881611295, Validation Loss: 0.7443885207176208\n",
      "Epoch 34: Train Loss: 0.5804289248254564, Validation Loss: 0.7475860714912415\n",
      "Epoch 35: Train Loss: 0.5385861330562167, Validation Loss: 0.7501088976860046\n",
      "Epoch 36: Train Loss: 0.5738752484321594, Validation Loss: 0.7517712712287903\n",
      "Epoch 37: Train Loss: 0.5645868447091844, Validation Loss: 0.75253826379776\n",
      "Epoch 38: Train Loss: 0.5390993588500552, Validation Loss: 0.7533822655677795\n",
      "Epoch 39: Train Loss: 0.5300984349515703, Validation Loss: 0.7533965706825256\n",
      "Epoch 40: Train Loss: 0.5375602510240343, Validation Loss: 0.7576141357421875\n",
      "Epoch 41: Train Loss: 0.5561273396015167, Validation Loss: 0.7645455598831177\n",
      "Epoch 42: Train Loss: 0.5560159418318007, Validation Loss: 0.7689850926399231\n",
      "Epoch 43: Train Loss: 0.5300803813669417, Validation Loss: 0.7722141146659851\n",
      "Epoch 44: Train Loss: 0.5325208273198869, Validation Loss: 0.7781135439872742\n",
      "Epoch 45: Train Loss: 0.559038907289505, Validation Loss: 0.781558632850647\n",
      "Epoch 46: Train Loss: 0.5135393473837111, Validation Loss: 0.785548746585846\n",
      "Epoch 47: Train Loss: 0.4976628091600206, Validation Loss: 0.7877793908119202\n",
      "Epoch 48: Train Loss: 0.485476639535692, Validation Loss: 0.7873170375823975\n",
      "Epoch 49: Train Loss: 0.5005712807178497, Validation Loss: 0.7875901460647583\n",
      "Epoch 50: Train Loss: 0.5270171099238925, Validation Loss: 0.7952714562416077\n",
      "Epoch 51: Train Loss: 0.5093395743105147, Validation Loss: 0.8019622564315796\n",
      "Epoch 52: Train Loss: 0.5457354287306467, Validation Loss: 0.8072635531425476\n",
      "Epoch 53: Train Loss: 0.5085070365005069, Validation Loss: 0.8134422302246094\n",
      "Epoch 54: Train Loss: 0.4990428884824117, Validation Loss: 0.8157427906990051\n",
      "Epoch 55: Train Loss: 0.4975769254896376, Validation Loss: 0.8185786604881287\n",
      "Epoch 56: Train Loss: 0.49237845010227627, Validation Loss: 0.8214185237884521\n",
      "Epoch 57: Train Loss: 0.4859204391638438, Validation Loss: 0.8257908821105957\n",
      "Epoch 58: Train Loss: 0.489113708337148, Validation Loss: 0.8274844884872437\n",
      "Epoch 59: Train Loss: 0.4919893476698134, Validation Loss: 0.8285312056541443\n",
      "Epoch 60: Train Loss: 0.4621461199389564, Validation Loss: 0.8336646556854248\n",
      "Epoch 61: Train Loss: 0.4910830027527279, Validation Loss: 0.8453980684280396\n",
      "Epoch 62: Train Loss: 0.46694497267405194, Validation Loss: 0.8594009280204773\n",
      "Epoch 63: Train Loss: 0.4816398620605469, Validation Loss: 0.8694823980331421\n",
      "Epoch 64: Train Loss: 0.44302086697684395, Validation Loss: 0.8737663626670837\n",
      "Epoch 65: Train Loss: 0.4680972860919105, Validation Loss: 0.8732106685638428\n",
      "Epoch 66: Train Loss: 0.4783761468198564, Validation Loss: 0.8756726980209351\n",
      "Epoch 67: Train Loss: 0.48019499911202324, Validation Loss: 0.8780286908149719\n",
      "Epoch 68: Train Loss: 0.44193703929583233, Validation Loss: 0.8805429339408875\n",
      "Epoch 69: Train Loss: 0.4915289150344001, Validation Loss: 0.879516065120697\n",
      "Epoch 70: Train Loss: 0.4810769524839189, Validation Loss: 0.8928461074829102\n",
      "Epoch 71: Train Loss: 0.46364283230569625, Validation Loss: 0.9089264273643494\n",
      "Epoch 72: Train Loss: 0.4476732909679413, Validation Loss: 0.9128702282905579\n",
      "Epoch 73: Train Loss: 0.43967755966716343, Validation Loss: 0.9194892644882202\n",
      "Epoch 74: Train Loss: 0.44726165797975326, Validation Loss: 0.9278577566146851\n",
      "Epoch 75: Train Loss: 0.41829222109582687, Validation Loss: 0.9258337020874023\n",
      "Epoch 76: Train Loss: 0.41169004638989765, Validation Loss: 0.9302968382835388\n",
      "Epoch 77: Train Loss: 0.4357866876655155, Validation Loss: 0.9394789934158325\n",
      "Epoch 78: Train Loss: 0.4629311230447557, Validation Loss: 0.9465934634208679\n",
      "Epoch 79: Train Loss: 0.442101303074095, Validation Loss: 0.9480266571044922\n",
      "Epoch 80: Train Loss: 0.44223113854726154, Validation Loss: 0.9555995464324951\n",
      "Epoch 81: Train Loss: 0.4381738305091858, Validation Loss: 0.9594863653182983\n",
      "Epoch 82: Train Loss: 0.4081003682480918, Validation Loss: 0.9631233215332031\n",
      "Epoch 83: Train Loss: 0.4303079942862193, Validation Loss: 0.9809383749961853\n",
      "Epoch 84: Train Loss: 0.4073301785522037, Validation Loss: 0.9822903275489807\n",
      "Epoch 85: Train Loss: 0.4016396949688594, Validation Loss: 0.9934571981430054\n",
      "Epoch 86: Train Loss: 0.3966639240582784, Validation Loss: 1.0010818243026733\n",
      "Epoch 87: Train Loss: 0.4138310452302297, Validation Loss: 1.0045514106750488\n",
      "Epoch 88: Train Loss: 0.4289595815870497, Validation Loss: 1.008806824684143\n",
      "Epoch 89: Train Loss: 0.3841956953207652, Validation Loss: 1.0034723281860352\n",
      "Epoch 90: Train Loss: 0.40973061323165894, Validation Loss: 1.0141332149505615\n",
      "Epoch 91: Train Loss: 0.3801682773563597, Validation Loss: 1.0272774696350098\n",
      "Epoch 92: Train Loss: 0.37588659591144985, Validation Loss: 1.0387085676193237\n",
      "Epoch 93: Train Loss: 0.3706304281949997, Validation Loss: 1.0535709857940674\n",
      "Epoch 94: Train Loss: 0.40626540117793614, Validation Loss: 1.0608261823654175\n",
      "Epoch 95: Train Loss: 0.39369426833258736, Validation Loss: 1.0650267601013184\n",
      "Epoch 96: Train Loss: 0.3646492626931932, Validation Loss: 1.073570966720581\n",
      "Epoch 97: Train Loss: 0.39615869853231644, Validation Loss: 1.0765334367752075\n",
      "Epoch 98: Train Loss: 0.3953126139110989, Validation Loss: 1.072623610496521\n",
      "Epoch 99: Train Loss: 0.34871696101294625, Validation Loss: 1.0754094123840332\n",
      "Epoch 100: Train Loss: 0.36398904191123116, Validation Loss: 1.0941927433013916\n",
      "Epoch 101: Train Loss: 0.33415328297350144, Validation Loss: 1.111571192741394\n",
      "Epoch 102: Train Loss: 0.3572836021582286, Validation Loss: 1.1327226161956787\n",
      "Epoch 103: Train Loss: 0.3596098588572608, Validation Loss: 1.1355222463607788\n",
      "Epoch 104: Train Loss: 0.3429715285698573, Validation Loss: 1.1383343935012817\n",
      "Epoch 105: Train Loss: 0.3495146814319823, Validation Loss: 1.128989338874817\n",
      "Epoch 106: Train Loss: 0.3767920798725552, Validation Loss: 1.13002347946167\n",
      "Epoch 107: Train Loss: 0.35153887503676945, Validation Loss: 1.1299090385437012\n",
      "Epoch 108: Train Loss: 0.32820458047919804, Validation Loss: 1.1502397060394287\n",
      "Epoch 109: Train Loss: 0.38958899014525944, Validation Loss: 1.1553007364273071\n",
      "Epoch 110: Train Loss: 0.3229309784041511, Validation Loss: 1.153725028038025\n",
      "Epoch 111: Train Loss: 0.3857941859298282, Validation Loss: 1.1524227857589722\n",
      "Epoch 112: Train Loss: 0.33565573228730095, Validation Loss: 1.1614038944244385\n",
      "Epoch 113: Train Loss: 0.3137585719426473, Validation Loss: 1.1601706743240356\n",
      "Epoch 114: Train Loss: 0.34873319334454006, Validation Loss: 1.1780295372009277\n",
      "Epoch 115: Train Loss: 0.3075346019532945, Validation Loss: 1.1611175537109375\n",
      "Epoch 116: Train Loss: 0.30237624877028996, Validation Loss: 1.184664249420166\n",
      "Epoch 117: Train Loss: 0.34068349997202557, Validation Loss: 1.1817570924758911\n",
      "Epoch 118: Train Loss: 0.3410210642549727, Validation Loss: 1.1741056442260742\n",
      "Epoch 119: Train Loss: 0.327942135433356, Validation Loss: 1.1751489639282227\n",
      "Epoch 120: Train Loss: 0.32288241386413574, Validation Loss: 1.1910910606384277\n",
      "Epoch 121: Train Loss: 0.2932948453558816, Validation Loss: 1.2039906978607178\n",
      "Epoch 122: Train Loss: 0.3270697328779433, Validation Loss: 1.2204301357269287\n",
      "Epoch 123: Train Loss: 0.2975250631570816, Validation Loss: 1.2309510707855225\n",
      "Epoch 124: Train Loss: 0.3084806419081158, Validation Loss: 1.25324285030365\n",
      "Epoch 125: Train Loss: 0.29650528066688114, Validation Loss: 1.251496434211731\n",
      "Epoch 126: Train Loss: 0.2991844101084603, Validation Loss: 1.25458824634552\n",
      "Epoch 127: Train Loss: 0.3069617102543513, Validation Loss: 1.2611221075057983\n",
      "Epoch 128: Train Loss: 0.2954540815618303, Validation Loss: 1.2455438375473022\n",
      "Epoch 129: Train Loss: 0.3115459597773022, Validation Loss: 1.2433803081512451\n",
      "Epoch 130: Train Loss: 0.33669619427786934, Validation Loss: 1.2759923934936523\n",
      "Epoch 131: Train Loss: 0.33162209060457015, Validation Loss: 1.2679263353347778\n",
      "Epoch 132: Train Loss: 0.29461733003457385, Validation Loss: 1.2605503797531128\n",
      "Epoch 133: Train Loss: 0.28576571494340897, Validation Loss: 1.2584879398345947\n",
      "Epoch 134: Train Loss: 0.29622506101926166, Validation Loss: 1.260105848312378\n",
      "Epoch 135: Train Loss: 0.2959901342789332, Validation Loss: 1.2632325887680054\n",
      "Epoch 136: Train Loss: 0.28637588520844776, Validation Loss: 1.2728112936019897\n",
      "Epoch 137: Train Loss: 0.2618662218252818, Validation Loss: 1.2777339220046997\n",
      "Epoch 138: Train Loss: 0.2890063640144136, Validation Loss: 1.269062876701355\n",
      "Epoch 139: Train Loss: 0.2657257152928246, Validation Loss: 1.2668651342391968\n",
      "Epoch 140: Train Loss: 0.2646459076139662, Validation Loss: 1.2904139757156372\n",
      "Epoch 141: Train Loss: 0.26516847643587327, Validation Loss: 1.3235366344451904\n",
      "Epoch 142: Train Loss: 0.28740253216690487, Validation Loss: 1.3326890468597412\n",
      "Epoch 143: Train Loss: 0.3148955735895369, Validation Loss: 1.3240360021591187\n",
      "Epoch 144: Train Loss: 0.24810850040780175, Validation Loss: 1.3092588186264038\n",
      "Epoch 145: Train Loss: 0.27526617381307816, Validation Loss: 1.3114590644836426\n",
      "Epoch 146: Train Loss: 0.26501481648948455, Validation Loss: 1.3271392583847046\n",
      "Epoch 147: Train Loss: 0.2718111856116189, Validation Loss: 1.327755331993103\n",
      "Epoch 148: Train Loss: 0.2387742499510447, Validation Loss: 1.3363209962844849\n",
      "Epoch 149: Train Loss: 0.3655655135711034, Validation Loss: 1.3256967067718506\n",
      "Epoch 150: Train Loss: 0.2501294430759218, Validation Loss: 1.3184024095535278\n",
      "Epoch 151: Train Loss: 0.2610253228081597, Validation Loss: 1.3405565023422241\n",
      "Epoch 152: Train Loss: 0.2579667915900548, Validation Loss: 1.3526033163070679\n",
      "Epoch 153: Train Loss: 0.26507296330398983, Validation Loss: 1.3508074283599854\n",
      "Epoch 154: Train Loss: 0.2556295411454307, Validation Loss: 1.3459676504135132\n",
      "Epoch 155: Train Loss: 0.24874488181538051, Validation Loss: 1.3559986352920532\n",
      "Epoch 156: Train Loss: 0.2258758114443885, Validation Loss: 1.3696558475494385\n",
      "Epoch 157: Train Loss: 0.2520841509103775, Validation Loss: 1.385298728942871\n",
      "Epoch 158: Train Loss: 0.2466285013490253, Validation Loss: 1.362734079360962\n",
      "Epoch 159: Train Loss: 0.2634318851762348, Validation Loss: 1.3675488233566284\n",
      "Epoch 160: Train Loss: 0.23124379913012186, Validation Loss: 1.3830900192260742\n",
      "Epoch 161: Train Loss: 0.24126501960886848, Validation Loss: 1.3711141347885132\n",
      "Epoch 162: Train Loss: 0.2425285710228814, Validation Loss: 1.3942241668701172\n",
      "Epoch 163: Train Loss: 0.24448170761267343, Validation Loss: 1.3752570152282715\n",
      "Epoch 164: Train Loss: 0.2065672907564375, Validation Loss: 1.3761396408081055\n",
      "Epoch 165: Train Loss: 0.21737101011806065, Validation Loss: 1.3766076564788818\n",
      "Epoch 166: Train Loss: 0.2249128818511963, Validation Loss: 1.3931844234466553\n",
      "Epoch 167: Train Loss: 0.2076241804493798, Validation Loss: 1.3984878063201904\n",
      "Epoch 168: Train Loss: 0.31836625768078697, Validation Loss: 1.4315193891525269\n",
      "Epoch 169: Train Loss: 0.2567739370796416, Validation Loss: 1.4046196937561035\n",
      "Epoch 170: Train Loss: 0.24126226206620535, Validation Loss: 1.3951972723007202\n",
      "Epoch 171: Train Loss: 0.21479744381374782, Validation Loss: 1.379211187362671\n",
      "Epoch 172: Train Loss: 0.22340146866109636, Validation Loss: 1.414168357849121\n",
      "Epoch 173: Train Loss: 0.22597427831755745, Validation Loss: 1.4370073080062866\n",
      "Epoch 174: Train Loss: 0.22379283275869158, Validation Loss: 1.4323681592941284\n",
      "Epoch 175: Train Loss: 0.22193539142608643, Validation Loss: 1.432784080505371\n",
      "Epoch 176: Train Loss: 0.2527169750796424, Validation Loss: 1.434248685836792\n",
      "Epoch 177: Train Loss: 0.21036813325352138, Validation Loss: 1.4534050226211548\n",
      "Epoch 178: Train Loss: 0.21596917510032654, Validation Loss: 1.4593244791030884\n",
      "Epoch 179: Train Loss: 0.19690706415308845, Validation Loss: 1.4637035131454468\n",
      "Epoch 180: Train Loss: 0.2156675863597128, Validation Loss: 1.4592928886413574\n",
      "Epoch 181: Train Loss: 0.22924351029925877, Validation Loss: 1.4447112083435059\n",
      "Epoch 182: Train Loss: 0.2038896249400245, Validation Loss: 1.4728407859802246\n",
      "Epoch 183: Train Loss: 0.21941288974550036, Validation Loss: 1.5031427145004272\n",
      "Epoch 184: Train Loss: 0.2099911024173101, Validation Loss: 1.4862792491912842\n",
      "Epoch 185: Train Loss: 0.21136996067232555, Validation Loss: 1.5065398216247559\n",
      "Epoch 186: Train Loss: 0.26352742811044055, Validation Loss: 1.4934253692626953\n",
      "Epoch 187: Train Loss: 0.20438786844412485, Validation Loss: 1.521668791770935\n",
      "Epoch 188: Train Loss: 0.19266175892617968, Validation Loss: 1.508797287940979\n",
      "Epoch 189: Train Loss: 0.22200104925367567, Validation Loss: 1.5027875900268555\n",
      "Epoch 190: Train Loss: 0.20342563672198188, Validation Loss: 1.5559828281402588\n",
      "Epoch 191: Train Loss: 0.1992437019944191, Validation Loss: 1.5327774286270142\n",
      "Epoch 192: Train Loss: 0.19698713885413277, Validation Loss: 1.545737385749817\n",
      "Epoch 193: Train Loss: 0.19693495829900107, Validation Loss: 1.5515241622924805\n",
      "Epoch 194: Train Loss: 0.238721231619517, Validation Loss: 1.5733988285064697\n",
      "Epoch 195: Train Loss: 0.1541229577528106, Validation Loss: 1.5392755270004272\n",
      "Epoch 196: Train Loss: 0.18905597842401928, Validation Loss: 1.5227916240692139\n",
      "Epoch 197: Train Loss: 0.2222826596763399, Validation Loss: 1.5347800254821777\n",
      "Epoch 198: Train Loss: 0.1874279561969969, Validation Loss: 1.5419116020202637\n",
      "Epoch 199: Train Loss: 0.17622943802012336, Validation Loss: 1.5419679880142212\n",
      "Fold 8 Metrics:\n",
      "Accuracy: 0.36363636363636365, Precision: 0.4444444444444444, Recall: 0.6666666666666666, F1-score: 0.5333333333333333, AUC: 0.3333333333333333\n",
      "Confusion Matrix:\n",
      "[[0 5]\n",
      " [2 4]]\n",
      "Completed fold 8\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples from subject 4 to test set\n",
      "Adding 6 truth samples from subject 4 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.7436924245622423, Validation Loss: 0.6788754463195801\n",
      "Epoch 1: Train Loss: 0.7108125752872891, Validation Loss: 0.6679654717445374\n",
      "Epoch 2: Train Loss: 0.7012283007303873, Validation Loss: 0.6600975394248962\n",
      "Epoch 3: Train Loss: 0.7262540856997172, Validation Loss: 0.6579214930534363\n",
      "Epoch 4: Train Loss: 0.6944631934165955, Validation Loss: 0.6573999524116516\n",
      "Epoch 5: Train Loss: 0.638678510983785, Validation Loss: 0.6580834984779358\n",
      "Epoch 6: Train Loss: 0.6577559444639418, Validation Loss: 0.6583779454231262\n",
      "Epoch 7: Train Loss: 0.6829434302118089, Validation Loss: 0.6586175560951233\n",
      "Epoch 8: Train Loss: 0.6767698460155063, Validation Loss: 0.6596032381057739\n",
      "Epoch 9: Train Loss: 0.6780375242233276, Validation Loss: 0.6590179204940796\n",
      "Epoch 10: Train Loss: 0.676883684264289, Validation Loss: 0.6598805785179138\n",
      "Epoch 11: Train Loss: 0.6921494603157043, Validation Loss: 0.6608098745346069\n",
      "Epoch 12: Train Loss: 0.6585216257307265, Validation Loss: 0.664339005947113\n",
      "Epoch 13: Train Loss: 0.6558825506104363, Validation Loss: 0.6682479977607727\n",
      "Epoch 14: Train Loss: 0.6291257540384928, Validation Loss: 0.6699769496917725\n",
      "Epoch 15: Train Loss: 0.6241397460301717, Validation Loss: 0.6701536178588867\n",
      "Epoch 16: Train Loss: 0.654512968328264, Validation Loss: 0.6713299751281738\n",
      "Epoch 17: Train Loss: 0.64490321609709, Validation Loss: 0.6713685989379883\n",
      "Epoch 18: Train Loss: 0.5957355300585429, Validation Loss: 0.6717684268951416\n",
      "Epoch 19: Train Loss: 0.6026475363307529, Validation Loss: 0.6709855794906616\n",
      "Epoch 20: Train Loss: 0.6316470437579684, Validation Loss: 0.671341061592102\n",
      "Epoch 21: Train Loss: 0.5841278963618808, Validation Loss: 0.6711857914924622\n",
      "Epoch 22: Train Loss: 0.591373880704244, Validation Loss: 0.6733128428459167\n",
      "Epoch 23: Train Loss: 0.571980228026708, Validation Loss: 0.6733541488647461\n",
      "Epoch 24: Train Loss: 0.578224884139167, Validation Loss: 0.6750386953353882\n",
      "Epoch 25: Train Loss: 0.5869516531626383, Validation Loss: 0.6787278056144714\n",
      "Epoch 26: Train Loss: 0.5467223624388377, Validation Loss: 0.6798074245452881\n",
      "Epoch 27: Train Loss: 0.5827139086193509, Validation Loss: 0.6790550947189331\n",
      "Epoch 28: Train Loss: 0.5652460720804002, Validation Loss: 0.6796983480453491\n",
      "Epoch 29: Train Loss: 0.5725544095039368, Validation Loss: 0.6802186965942383\n",
      "Epoch 30: Train Loss: 0.5689909954865774, Validation Loss: 0.683005690574646\n",
      "Epoch 31: Train Loss: 0.584813396135966, Validation Loss: 0.6902574896812439\n",
      "Epoch 32: Train Loss: 0.5214050610860189, Validation Loss: 0.691937267780304\n",
      "Epoch 33: Train Loss: 0.5657272239526113, Validation Loss: 0.6971966624259949\n",
      "Epoch 34: Train Loss: 0.5299626290798187, Validation Loss: 0.6966003179550171\n",
      "Epoch 35: Train Loss: 0.5260034137301974, Validation Loss: 0.6970334053039551\n",
      "Epoch 36: Train Loss: 0.5143047273159027, Validation Loss: 0.6956741809844971\n",
      "Epoch 37: Train Loss: 0.4983210663000743, Validation Loss: 0.693720281124115\n",
      "Epoch 38: Train Loss: 0.5029732916090224, Validation Loss: 0.6949706673622131\n",
      "Epoch 39: Train Loss: 0.4919922451178233, Validation Loss: 0.6932947635650635\n",
      "Epoch 40: Train Loss: 0.5064446694321103, Validation Loss: 0.698008120059967\n",
      "Epoch 41: Train Loss: 0.5148574875460731, Validation Loss: 0.7009850740432739\n",
      "Epoch 42: Train Loss: 0.4849935968716939, Validation Loss: 0.7072833180427551\n",
      "Epoch 43: Train Loss: 0.4855627616246541, Validation Loss: 0.7092368602752686\n",
      "Epoch 44: Train Loss: 0.5072782999939389, Validation Loss: 0.7156985998153687\n",
      "Epoch 45: Train Loss: 0.46223979194959003, Validation Loss: 0.7173461318016052\n",
      "Epoch 46: Train Loss: 0.4553544686900245, Validation Loss: 0.7210858464241028\n",
      "Epoch 47: Train Loss: 0.4720700879891713, Validation Loss: 0.7249765992164612\n",
      "Epoch 48: Train Loss: 0.4889820913473765, Validation Loss: 0.7224545478820801\n",
      "Epoch 49: Train Loss: 0.46211838059955174, Validation Loss: 0.7253286242485046\n",
      "Epoch 50: Train Loss: 0.46496538321177167, Validation Loss: 0.7383373379707336\n",
      "Epoch 51: Train Loss: 0.4387023283375634, Validation Loss: 0.7422372102737427\n",
      "Epoch 52: Train Loss: 0.45154893067147994, Validation Loss: 0.7508578896522522\n",
      "Epoch 53: Train Loss: 0.470625638961792, Validation Loss: 0.7595534920692444\n",
      "Epoch 54: Train Loss: 0.4447993006971147, Validation Loss: 0.767569363117218\n",
      "Epoch 55: Train Loss: 0.4409646557437049, Validation Loss: 0.7744258046150208\n",
      "Epoch 56: Train Loss: 0.41063998805152047, Validation Loss: 0.7731764912605286\n",
      "Epoch 57: Train Loss: 0.4425525797737969, Validation Loss: 0.7773724794387817\n",
      "Epoch 58: Train Loss: 0.43239819010098773, Validation Loss: 0.7848792672157288\n",
      "Epoch 59: Train Loss: 0.44177625907791984, Validation Loss: 0.7781919240951538\n",
      "Epoch 60: Train Loss: 0.42742219898435807, Validation Loss: 0.792678952217102\n",
      "Epoch 61: Train Loss: 0.3962696310546663, Validation Loss: 0.8070105910301208\n",
      "Epoch 62: Train Loss: 0.4413788616657257, Validation Loss: 0.8181852102279663\n",
      "Epoch 63: Train Loss: 0.3918195830451118, Validation Loss: 0.8215801119804382\n",
      "Epoch 64: Train Loss: 0.4115614692370097, Validation Loss: 0.8254539370536804\n",
      "Epoch 65: Train Loss: 0.38214946786562604, Validation Loss: 0.8225992321968079\n",
      "Epoch 66: Train Loss: 0.3781868318716685, Validation Loss: 0.8242334127426147\n",
      "Epoch 67: Train Loss: 0.4174342453479767, Validation Loss: 0.8300968408584595\n",
      "Epoch 68: Train Loss: 0.3899082690477371, Validation Loss: 0.8274060487747192\n",
      "Epoch 69: Train Loss: 0.3899151186148326, Validation Loss: 0.8339338302612305\n",
      "Epoch 70: Train Loss: 0.38597157928678727, Validation Loss: 0.8439710140228271\n",
      "Epoch 71: Train Loss: 0.3620911356475618, Validation Loss: 0.8670497536659241\n",
      "Epoch 72: Train Loss: 0.3989533053504096, Validation Loss: 0.881003201007843\n",
      "Epoch 73: Train Loss: 0.3846673154168659, Validation Loss: 0.8900265097618103\n",
      "Epoch 74: Train Loss: 0.36024825606081223, Validation Loss: 0.8907449245452881\n",
      "Epoch 75: Train Loss: 0.3590191337797377, Validation Loss: 0.8949588537216187\n",
      "Epoch 76: Train Loss: 0.35324734780523515, Validation Loss: 0.8889932036399841\n",
      "Epoch 77: Train Loss: 0.3548259801334805, Validation Loss: 0.8917434811592102\n",
      "Epoch 78: Train Loss: 0.36378684143225354, Validation Loss: 0.8882784247398376\n",
      "Epoch 79: Train Loss: 0.35471029745207894, Validation Loss: 0.8935655951499939\n",
      "Epoch 80: Train Loss: 0.33573389384481644, Validation Loss: 0.9058587551116943\n",
      "Epoch 81: Train Loss: 0.3253284692764282, Validation Loss: 0.9226725101470947\n",
      "Epoch 82: Train Loss: 0.3343496753109826, Validation Loss: 0.934626579284668\n",
      "Epoch 83: Train Loss: 0.32561580505636, Validation Loss: 0.9444276094436646\n",
      "Epoch 84: Train Loss: 0.34453292522165513, Validation Loss: 0.9528844356536865\n",
      "Epoch 85: Train Loss: 0.37395688229137, Validation Loss: 0.9572202563285828\n",
      "Epoch 86: Train Loss: 0.33034622338083053, Validation Loss: 0.9563218951225281\n",
      "Epoch 87: Train Loss: 0.34973731968137955, Validation Loss: 0.9520377516746521\n",
      "Epoch 88: Train Loss: 0.340937508477105, Validation Loss: 0.9480262398719788\n",
      "Epoch 89: Train Loss: 0.3200176242325041, Validation Loss: 0.9438717365264893\n",
      "Epoch 90: Train Loss: 0.3206262985865275, Validation Loss: 0.9614810347557068\n",
      "Epoch 91: Train Loss: 0.34851712650722927, Validation Loss: 0.9934058785438538\n",
      "Epoch 92: Train Loss: 0.32771271301640403, Validation Loss: 0.9960098266601562\n",
      "Epoch 93: Train Loss: 0.3360515418979857, Validation Loss: 1.0048667192459106\n",
      "Epoch 94: Train Loss: 0.31247808370325303, Validation Loss: 1.0100431442260742\n",
      "Epoch 95: Train Loss: 0.27161580986446804, Validation Loss: 1.0228102207183838\n",
      "Epoch 96: Train Loss: 0.3072957860098945, Validation Loss: 1.0250842571258545\n",
      "Epoch 97: Train Loss: 0.3153594020340178, Validation Loss: 1.0261260271072388\n",
      "Epoch 98: Train Loss: 0.2608979576163822, Validation Loss: 1.0208452939987183\n",
      "Epoch 99: Train Loss: 0.31236345403724247, Validation Loss: 1.0230391025543213\n",
      "Epoch 100: Train Loss: 0.27032817237906986, Validation Loss: 1.0562200546264648\n",
      "Epoch 101: Train Loss: 0.2904842479361428, Validation Loss: 1.07816743850708\n",
      "Epoch 102: Train Loss: 0.28639670875337386, Validation Loss: 1.0873359441757202\n",
      "Epoch 103: Train Loss: 0.2906876719660229, Validation Loss: 1.0971977710723877\n",
      "Epoch 104: Train Loss: 0.2806684656275643, Validation Loss: 1.1016414165496826\n",
      "Epoch 105: Train Loss: 0.2919299718406465, Validation Loss: 1.1040520668029785\n",
      "Epoch 106: Train Loss: 0.36451179451412624, Validation Loss: 1.1263749599456787\n",
      "Epoch 107: Train Loss: 0.29620515803496045, Validation Loss: 1.1125833988189697\n",
      "Epoch 108: Train Loss: 0.27701878878805375, Validation Loss: 1.1146613359451294\n",
      "Epoch 109: Train Loss: 0.2673247439993752, Validation Loss: 1.099556803703308\n",
      "Epoch 110: Train Loss: 0.2803805735376146, Validation Loss: 1.1298208236694336\n",
      "Epoch 111: Train Loss: 0.2816629591915343, Validation Loss: 1.1595158576965332\n",
      "Epoch 112: Train Loss: 0.27342622975508374, Validation Loss: 1.1572041511535645\n",
      "Epoch 113: Train Loss: 0.24332774513297611, Validation Loss: 1.1691230535507202\n",
      "Epoch 114: Train Loss: 0.26817525426546734, Validation Loss: 1.1804919242858887\n",
      "Epoch 115: Train Loss: 0.27164219154251945, Validation Loss: 1.193097472190857\n",
      "Epoch 116: Train Loss: 0.27703215347396004, Validation Loss: 1.1974538564682007\n",
      "Epoch 117: Train Loss: 0.28212378587987685, Validation Loss: 1.188528299331665\n",
      "Epoch 118: Train Loss: 0.23124263021681044, Validation Loss: 1.1958624124526978\n",
      "Epoch 119: Train Loss: 0.27706728047794765, Validation Loss: 1.2037179470062256\n",
      "Epoch 120: Train Loss: 0.24856074319945443, Validation Loss: 1.2272568941116333\n",
      "Epoch 121: Train Loss: 0.24269392920864952, Validation Loss: 1.2524181604385376\n",
      "Epoch 122: Train Loss: 0.2897340870565838, Validation Loss: 1.247755765914917\n",
      "Epoch 123: Train Loss: 0.24894508885012734, Validation Loss: 1.2560334205627441\n",
      "Epoch 124: Train Loss: 0.20300564666589102, Validation Loss: 1.270546555519104\n",
      "Epoch 125: Train Loss: 0.25549986627366805, Validation Loss: 1.2717534303665161\n",
      "Epoch 126: Train Loss: 0.22442834741539425, Validation Loss: 1.2904703617095947\n",
      "Epoch 127: Train Loss: 0.2471004608604643, Validation Loss: 1.297264575958252\n",
      "Epoch 128: Train Loss: 0.21569270888964334, Validation Loss: 1.2910884618759155\n",
      "Epoch 129: Train Loss: 0.221605122089386, Validation Loss: 1.2781720161437988\n",
      "Epoch 130: Train Loss: 0.2737848245435291, Validation Loss: 1.2965497970581055\n",
      "Epoch 131: Train Loss: 0.2110888452993499, Validation Loss: 1.335556149482727\n",
      "Epoch 132: Train Loss: 0.20271986888514626, Validation Loss: 1.3531827926635742\n",
      "Epoch 133: Train Loss: 0.2277046971850925, Validation Loss: 1.3640178442001343\n",
      "Epoch 134: Train Loss: 0.2613968352476756, Validation Loss: 1.3726816177368164\n",
      "Epoch 135: Train Loss: 0.21480310459931692, Validation Loss: 1.3554292917251587\n",
      "Epoch 136: Train Loss: 0.20511406660079956, Validation Loss: 1.3628149032592773\n",
      "Epoch 137: Train Loss: 0.21015603674782646, Validation Loss: 1.3701510429382324\n",
      "Epoch 138: Train Loss: 0.20305406550566354, Validation Loss: 1.3553966283798218\n",
      "Epoch 139: Train Loss: 0.2225488026936849, Validation Loss: 1.352282166481018\n",
      "Epoch 140: Train Loss: 0.19863572716712952, Validation Loss: 1.3467925786972046\n",
      "Epoch 141: Train Loss: 0.20657541188928816, Validation Loss: 1.362459421157837\n",
      "Epoch 142: Train Loss: 0.19447927673657736, Validation Loss: 1.4058893918991089\n",
      "Epoch 143: Train Loss: 0.1936322334739897, Validation Loss: 1.4407321214675903\n",
      "Epoch 144: Train Loss: 0.2406238102250629, Validation Loss: 1.4743701219558716\n",
      "Epoch 145: Train Loss: 0.21929413825273514, Validation Loss: 1.4714728593826294\n",
      "Epoch 146: Train Loss: 0.19162243687444264, Validation Loss: 1.467136025428772\n",
      "Epoch 147: Train Loss: 0.2207797004116906, Validation Loss: 1.482547402381897\n",
      "Epoch 148: Train Loss: 0.19752592510647243, Validation Loss: 1.479311227798462\n",
      "Epoch 149: Train Loss: 0.19586563772625393, Validation Loss: 1.4805128574371338\n",
      "Epoch 150: Train Loss: 0.22918480220768186, Validation Loss: 1.5125948190689087\n",
      "Epoch 151: Train Loss: 0.20047730538580152, Validation Loss: 1.5714855194091797\n",
      "Epoch 152: Train Loss: 0.20373933182822335, Validation Loss: 1.566758155822754\n",
      "Epoch 153: Train Loss: 0.2376149503721131, Validation Loss: 1.5809776782989502\n",
      "Epoch 154: Train Loss: 0.18873704887098736, Validation Loss: 1.5819388628005981\n",
      "Epoch 155: Train Loss: 0.22391763660642836, Validation Loss: 1.568873405456543\n",
      "Epoch 156: Train Loss: 0.28281769653161365, Validation Loss: 1.5922247171401978\n",
      "Epoch 157: Train Loss: 0.17585170393188795, Validation Loss: 1.5655601024627686\n",
      "Epoch 158: Train Loss: 0.1885772736536132, Validation Loss: 1.5639461278915405\n",
      "Epoch 159: Train Loss: 0.17849671675099266, Validation Loss: 1.558075189590454\n",
      "Epoch 160: Train Loss: 0.1801301207807329, Validation Loss: 1.592888593673706\n",
      "Epoch 161: Train Loss: 0.18853849338160622, Validation Loss: 1.658658504486084\n",
      "Epoch 162: Train Loss: 0.18658029536406198, Validation Loss: 1.6748347282409668\n",
      "Epoch 163: Train Loss: 0.19817306929164463, Validation Loss: 1.6663192510604858\n",
      "Epoch 164: Train Loss: 0.1777066911260287, Validation Loss: 1.6493022441864014\n",
      "Epoch 165: Train Loss: 0.16506633948948649, Validation Loss: 1.6853954792022705\n",
      "Epoch 166: Train Loss: 0.19296758208009931, Validation Loss: 1.702767252922058\n",
      "Epoch 167: Train Loss: 0.2044400539663103, Validation Loss: 1.6953189373016357\n",
      "Epoch 168: Train Loss: 0.21178360283374786, Validation Loss: 1.704294204711914\n",
      "Epoch 169: Train Loss: 0.16941806177298227, Validation Loss: 1.696277141571045\n",
      "Epoch 170: Train Loss: 0.1743674013349745, Validation Loss: 1.6990514993667603\n",
      "Epoch 171: Train Loss: 0.23813724352253807, Validation Loss: 1.6932902336120605\n",
      "Epoch 172: Train Loss: 0.17698812981446585, Validation Loss: 1.7577846050262451\n",
      "Epoch 173: Train Loss: 0.17760023267732727, Validation Loss: 1.7601981163024902\n",
      "Epoch 174: Train Loss: 0.13454027970631918, Validation Loss: 1.8046234846115112\n",
      "Epoch 175: Train Loss: 0.14471696731117037, Validation Loss: 1.828443169593811\n",
      "Epoch 176: Train Loss: 0.1897853049967024, Validation Loss: 1.8336749076843262\n",
      "Epoch 177: Train Loss: 0.16011317777964804, Validation Loss: 1.7940661907196045\n",
      "Epoch 178: Train Loss: 0.14413091788689295, Validation Loss: 1.8176143169403076\n",
      "Epoch 179: Train Loss: 0.17787697911262512, Validation Loss: 1.852283000946045\n",
      "Epoch 180: Train Loss: 0.16056357986397213, Validation Loss: 1.8280974626541138\n",
      "Epoch 181: Train Loss: 0.16026365591420066, Validation Loss: 1.8216769695281982\n",
      "Epoch 182: Train Loss: 0.16351982288890415, Validation Loss: 1.844105839729309\n",
      "Epoch 183: Train Loss: 0.1550364444653193, Validation Loss: 1.8717710971832275\n",
      "Epoch 184: Train Loss: 0.16377776116132736, Validation Loss: 1.876315712928772\n",
      "Epoch 185: Train Loss: 0.15884926211502817, Validation Loss: 1.9054073095321655\n",
      "Epoch 186: Train Loss: 0.20313304248783323, Validation Loss: 1.9666080474853516\n",
      "Epoch 187: Train Loss: 0.15090365459521612, Validation Loss: 1.9177504777908325\n",
      "Epoch 188: Train Loss: 0.13879772813783753, Validation Loss: 1.9207278490066528\n",
      "Epoch 189: Train Loss: 0.16867617517709732, Validation Loss: 1.9075920581817627\n",
      "Epoch 190: Train Loss: 0.1366561320092943, Validation Loss: 1.9459434747695923\n",
      "Epoch 191: Train Loss: 0.14939162507653236, Validation Loss: 1.9402588605880737\n",
      "Epoch 192: Train Loss: 0.16464202933841282, Validation Loss: 1.9367384910583496\n",
      "Epoch 193: Train Loss: 0.13464166389571297, Validation Loss: 1.9911869764328003\n",
      "Epoch 194: Train Loss: 0.1540696020755503, Validation Loss: 2.0349490642547607\n",
      "Epoch 195: Train Loss: 0.13049675110313627, Validation Loss: 2.0660817623138428\n",
      "Epoch 196: Train Loss: 0.12473989029725392, Validation Loss: 2.04577374458313\n",
      "Epoch 197: Train Loss: 0.1536317475967937, Validation Loss: 2.0518157482147217\n",
      "Epoch 198: Train Loss: 0.15399069339036942, Validation Loss: 2.0677502155303955\n",
      "Epoch 199: Train Loss: 0.14119058971603712, Validation Loss: 2.0609560012817383\n",
      "Fold 9 Metrics:\n",
      "Accuracy: 0.18181818181818182, Precision: 0.0, Recall: 0.0, F1-score: 0.0, AUC: 0.2\n",
      "Confusion Matrix:\n",
      "[[2 3]\n",
      " [6 0]]\n",
      "Completed fold 9\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples from subject 5 to test set\n",
      "Adding 6 truth samples from subject 5 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.7327120237880282, Validation Loss: 0.6830573678016663\n",
      "Epoch 1: Train Loss: 0.671942777103848, Validation Loss: 0.6804412007331848\n",
      "Epoch 2: Train Loss: 0.7150981293784248, Validation Loss: 0.6823062300682068\n",
      "Epoch 3: Train Loss: 0.6819336944156222, Validation Loss: 0.6866733431816101\n",
      "Epoch 4: Train Loss: 0.675076961517334, Validation Loss: 0.6921892762184143\n",
      "Epoch 5: Train Loss: 0.6653831799825033, Validation Loss: 0.6955282092094421\n",
      "Epoch 6: Train Loss: 0.6491655045085483, Validation Loss: 0.6973370313644409\n",
      "Epoch 7: Train Loss: 0.6683513654602898, Validation Loss: 0.6971926689147949\n",
      "Epoch 8: Train Loss: 0.6330047249794006, Validation Loss: 0.6973559856414795\n",
      "Epoch 9: Train Loss: 0.654129876030816, Validation Loss: 0.6978920102119446\n",
      "Epoch 10: Train Loss: 0.6386152803897858, Validation Loss: 0.7036687135696411\n",
      "Epoch 11: Train Loss: 0.6349287430445353, Validation Loss: 0.7068648338317871\n",
      "Epoch 12: Train Loss: 0.6245669656329684, Validation Loss: 0.7103408575057983\n",
      "Epoch 13: Train Loss: 0.6733526852395799, Validation Loss: 0.7122523784637451\n",
      "Epoch 14: Train Loss: 0.6217520634333292, Validation Loss: 0.714225172996521\n",
      "Epoch 15: Train Loss: 0.6030831668112013, Validation Loss: 0.7154029607772827\n",
      "Epoch 16: Train Loss: 0.6196093459924062, Validation Loss: 0.7162801027297974\n",
      "Epoch 17: Train Loss: 0.5908048748970032, Validation Loss: 0.7155964970588684\n",
      "Epoch 18: Train Loss: 0.593995624118381, Validation Loss: 0.715969979763031\n",
      "Epoch 19: Train Loss: 0.587488075097402, Validation Loss: 0.7165001630783081\n",
      "Epoch 20: Train Loss: 0.6091367072529263, Validation Loss: 0.7175592184066772\n",
      "Epoch 21: Train Loss: 0.5932354198561774, Validation Loss: 0.7238383889198303\n",
      "Epoch 22: Train Loss: 0.5731783443027072, Validation Loss: 0.7292990684509277\n",
      "Epoch 23: Train Loss: 0.603882180319892, Validation Loss: 0.7324125170707703\n",
      "Epoch 24: Train Loss: 0.5692740049627092, Validation Loss: 0.734929621219635\n",
      "Epoch 25: Train Loss: 0.5908895995881822, Validation Loss: 0.735710859298706\n",
      "Epoch 26: Train Loss: 0.5509549246893989, Validation Loss: 0.7365865111351013\n",
      "Epoch 27: Train Loss: 0.5462830166021982, Validation Loss: 0.7335910201072693\n",
      "Epoch 28: Train Loss: 0.5409001244439019, Validation Loss: 0.7359398007392883\n",
      "Epoch 29: Train Loss: 0.577301088306639, Validation Loss: 0.7356488108634949\n",
      "Epoch 30: Train Loss: 0.5621086425251431, Validation Loss: 0.7427650094032288\n",
      "Epoch 31: Train Loss: 0.5584591991371579, Validation Loss: 0.7444674372673035\n",
      "Epoch 32: Train Loss: 0.5985679825146993, Validation Loss: 0.7475600838661194\n",
      "Epoch 33: Train Loss: 0.5450622075133853, Validation Loss: 0.7512378096580505\n",
      "Epoch 34: Train Loss: 0.5609570642312368, Validation Loss: 0.754016637802124\n",
      "Epoch 35: Train Loss: 0.5460508200857375, Validation Loss: 0.7542026042938232\n",
      "Epoch 36: Train Loss: 0.5362225274244944, Validation Loss: 0.7576441764831543\n",
      "Epoch 37: Train Loss: 0.5255638029840257, Validation Loss: 0.7592360377311707\n",
      "Epoch 38: Train Loss: 0.5177446703116099, Validation Loss: 0.7602888941764832\n",
      "Epoch 39: Train Loss: 0.5124901996718513, Validation Loss: 0.7605992555618286\n",
      "Epoch 40: Train Loss: 0.5583135088284811, Validation Loss: 0.7625317573547363\n",
      "Epoch 41: Train Loss: 0.5445760124259524, Validation Loss: 0.7674646377563477\n",
      "Epoch 42: Train Loss: 0.5367465648386214, Validation Loss: 0.7685882449150085\n",
      "Epoch 43: Train Loss: 0.5107980767885844, Validation Loss: 0.7704314589500427\n",
      "Epoch 44: Train Loss: 0.5091897514131334, Validation Loss: 0.7775177955627441\n",
      "Epoch 45: Train Loss: 0.5044881833924187, Validation Loss: 0.7774251103401184\n",
      "Epoch 46: Train Loss: 0.502835277054045, Validation Loss: 0.7803649306297302\n",
      "Epoch 47: Train Loss: 0.5075323714150323, Validation Loss: 0.7815554738044739\n",
      "Epoch 48: Train Loss: 0.5123251809014214, Validation Loss: 0.7831506133079529\n",
      "Epoch 49: Train Loss: 0.4886007010936737, Validation Loss: 0.7819450497627258\n",
      "Epoch 50: Train Loss: 0.512070013417138, Validation Loss: 0.7881608009338379\n",
      "Epoch 51: Train Loss: 0.48736316627926296, Validation Loss: 0.7972672581672668\n",
      "Epoch 52: Train Loss: 0.4529476828045315, Validation Loss: 0.7967599630355835\n",
      "Epoch 53: Train Loss: 0.47023238738377887, Validation Loss: 0.8008562326431274\n",
      "Epoch 54: Train Loss: 0.48846308721436393, Validation Loss: 0.8046851754188538\n",
      "Epoch 55: Train Loss: 0.47132635447714066, Validation Loss: 0.8070996999740601\n",
      "Epoch 56: Train Loss: 0.43942151467005414, Validation Loss: 0.8096511960029602\n",
      "Epoch 57: Train Loss: 0.46029045846727157, Validation Loss: 0.8128159642219543\n",
      "Epoch 58: Train Loss: 0.46668140755759346, Validation Loss: 0.8105185627937317\n",
      "Epoch 59: Train Loss: 0.4644063115119934, Validation Loss: 0.8112996816635132\n",
      "Epoch 60: Train Loss: 0.4752649830447303, Validation Loss: 0.8145067095756531\n",
      "Epoch 61: Train Loss: 0.4497714473141564, Validation Loss: 0.8185054063796997\n",
      "Epoch 62: Train Loss: 0.43525005711449516, Validation Loss: 0.8220846056938171\n",
      "Epoch 63: Train Loss: 0.43855178025033736, Validation Loss: 0.825925350189209\n",
      "Epoch 64: Train Loss: 0.4370175401369731, Validation Loss: 0.8289179801940918\n",
      "Epoch 65: Train Loss: 0.4587658941745758, Validation Loss: 0.8336127996444702\n",
      "Epoch 66: Train Loss: 0.45607422788937885, Validation Loss: 0.8366390466690063\n",
      "Epoch 67: Train Loss: 0.38278113140000236, Validation Loss: 0.8351544737815857\n",
      "Epoch 68: Train Loss: 0.43205591705110336, Validation Loss: 0.8387486934661865\n",
      "Epoch 69: Train Loss: 0.4550636410713196, Validation Loss: 0.8370747566223145\n",
      "Epoch 70: Train Loss: 0.47769492864608765, Validation Loss: 0.8416194915771484\n",
      "Epoch 71: Train Loss: 0.4370426767402225, Validation Loss: 0.8477479219436646\n",
      "Epoch 72: Train Loss: 0.4681969980398814, Validation Loss: 0.8528887033462524\n",
      "Epoch 73: Train Loss: 0.41433502237002057, Validation Loss: 0.8549529314041138\n",
      "Epoch 74: Train Loss: 0.41645259658495587, Validation Loss: 0.8609563112258911\n",
      "Epoch 75: Train Loss: 0.40262021289931404, Validation Loss: 0.860355794429779\n",
      "Epoch 76: Train Loss: 0.428670197725296, Validation Loss: 0.8591606020927429\n",
      "Epoch 77: Train Loss: 0.39908135930697125, Validation Loss: 0.8559297323226929\n",
      "Epoch 78: Train Loss: 0.379557599623998, Validation Loss: 0.8607020974159241\n",
      "Epoch 79: Train Loss: 0.4281051788065169, Validation Loss: 0.860831081867218\n",
      "Epoch 80: Train Loss: 0.35887348651885986, Validation Loss: 0.8717765212059021\n",
      "Epoch 81: Train Loss: 0.39059637321366203, Validation Loss: 0.8745248913764954\n",
      "Epoch 82: Train Loss: 0.395828730530209, Validation Loss: 0.8808085918426514\n",
      "Epoch 83: Train Loss: 0.3806360363960266, Validation Loss: 0.884991466999054\n",
      "Epoch 84: Train Loss: 0.3846001856856876, Validation Loss: 0.8876528143882751\n",
      "Epoch 85: Train Loss: 0.4001731541421678, Validation Loss: 0.88685542345047\n",
      "Epoch 86: Train Loss: 0.38503315548102063, Validation Loss: 0.8916900753974915\n",
      "Epoch 87: Train Loss: 0.36520515713426804, Validation Loss: 0.8903120160102844\n",
      "Epoch 88: Train Loss: 0.3793155782752567, Validation Loss: 0.89284747838974\n",
      "Epoch 89: Train Loss: 0.38403069641855025, Validation Loss: 0.8915533423423767\n",
      "Epoch 90: Train Loss: 0.3631770743264092, Validation Loss: 0.8964414000511169\n",
      "Epoch 91: Train Loss: 0.4084674252404107, Validation Loss: 0.9019114971160889\n",
      "Epoch 92: Train Loss: 0.3882893655035231, Validation Loss: 0.9039695262908936\n",
      "Epoch 93: Train Loss: 0.37409236696031356, Validation Loss: 0.9090994000434875\n",
      "Epoch 94: Train Loss: 0.3606371482213338, Validation Loss: 0.9177051186561584\n",
      "Epoch 95: Train Loss: 0.33856244881947833, Validation Loss: 0.9285553097724915\n",
      "Epoch 96: Train Loss: 0.37708496385150486, Validation Loss: 0.9273821115493774\n",
      "Epoch 97: Train Loss: 0.33254574404822457, Validation Loss: 0.9272775650024414\n",
      "Epoch 98: Train Loss: 0.3400641944673326, Validation Loss: 0.9262210130691528\n",
      "Epoch 99: Train Loss: 0.3313777546087901, Validation Loss: 0.9227613806724548\n",
      "Epoch 100: Train Loss: 0.34560736020406085, Validation Loss: 0.9311038255691528\n",
      "Epoch 101: Train Loss: 0.3794363952345318, Validation Loss: 0.9402444958686829\n",
      "Epoch 102: Train Loss: 0.3520600034130944, Validation Loss: 0.9463793039321899\n",
      "Epoch 103: Train Loss: 0.34907007879681057, Validation Loss: 0.9541128277778625\n",
      "Epoch 104: Train Loss: 0.3159487230910195, Validation Loss: 0.9582720994949341\n",
      "Epoch 105: Train Loss: 0.32289915945794845, Validation Loss: 0.9660465717315674\n",
      "Epoch 106: Train Loss: 0.3333845833937327, Validation Loss: 0.964418351650238\n",
      "Epoch 107: Train Loss: 0.3568613396750556, Validation Loss: 0.9636573195457458\n",
      "Epoch 108: Train Loss: 0.35829730166329277, Validation Loss: 0.9673788547515869\n",
      "Epoch 109: Train Loss: 0.32583176261848873, Validation Loss: 0.9655687808990479\n",
      "Epoch 110: Train Loss: 0.3311597754557927, Validation Loss: 0.9618458151817322\n",
      "Epoch 111: Train Loss: 0.3430048922697703, Validation Loss: 0.9715320467948914\n",
      "Epoch 112: Train Loss: 0.30804454618030125, Validation Loss: 0.9832635521888733\n",
      "Epoch 113: Train Loss: 0.33147684401936, Validation Loss: 0.9830634593963623\n",
      "Epoch 114: Train Loss: 0.33987729085816276, Validation Loss: 0.992380678653717\n",
      "Epoch 115: Train Loss: 0.32137929730945164, Validation Loss: 1.0024129152297974\n",
      "Epoch 116: Train Loss: 0.3062659154335658, Validation Loss: 0.9929053783416748\n",
      "Epoch 117: Train Loss: 0.30306461453437805, Validation Loss: 0.9917188286781311\n",
      "Epoch 118: Train Loss: 0.3026890324221717, Validation Loss: 0.9963435530662537\n",
      "Epoch 119: Train Loss: 0.3128226399421692, Validation Loss: 0.9990631341934204\n",
      "Epoch 120: Train Loss: 0.36613013015853035, Validation Loss: 1.008255124092102\n",
      "Epoch 121: Train Loss: 0.31777145134078133, Validation Loss: 1.0040128231048584\n",
      "Epoch 122: Train Loss: 0.31523005498780143, Validation Loss: 1.0096664428710938\n",
      "Epoch 123: Train Loss: 0.2946442796124352, Validation Loss: 1.0146926641464233\n",
      "Epoch 124: Train Loss: 0.296406058801545, Validation Loss: 1.0191030502319336\n",
      "Epoch 125: Train Loss: 0.29245563348134357, Validation Loss: 1.0254698991775513\n",
      "Epoch 126: Train Loss: 0.2631757805744807, Validation Loss: 1.018030047416687\n",
      "Epoch 127: Train Loss: 0.2982832607295778, Validation Loss: 1.015791416168213\n",
      "Epoch 128: Train Loss: 0.28629940417077804, Validation Loss: 1.0265235900878906\n",
      "Epoch 129: Train Loss: 0.3085823605457942, Validation Loss: 1.0210729837417603\n",
      "Epoch 130: Train Loss: 0.2776968893077638, Validation Loss: 1.0285639762878418\n",
      "Epoch 131: Train Loss: 0.27668629421128166, Validation Loss: 1.0371772050857544\n",
      "Epoch 132: Train Loss: 0.2876133057806227, Validation Loss: 1.0526118278503418\n",
      "Epoch 133: Train Loss: 0.27501433591047925, Validation Loss: 1.055361270904541\n",
      "Epoch 134: Train Loss: 0.27920184863938224, Validation Loss: 1.0627946853637695\n",
      "Epoch 135: Train Loss: 0.2695275909370846, Validation Loss: 1.057993769645691\n",
      "Epoch 136: Train Loss: 0.2614505605565177, Validation Loss: 1.064741849899292\n",
      "Epoch 137: Train Loss: 0.27295665939648944, Validation Loss: 1.0638066530227661\n",
      "Epoch 138: Train Loss: 0.26050198905997807, Validation Loss: 1.0636283159255981\n",
      "Epoch 139: Train Loss: 0.26536045637395644, Validation Loss: 1.0707833766937256\n",
      "Epoch 140: Train Loss: 0.26503268546528286, Validation Loss: 1.0744117498397827\n",
      "Epoch 141: Train Loss: 0.28360380397902596, Validation Loss: 1.0723974704742432\n",
      "Epoch 142: Train Loss: 0.27640234927336377, Validation Loss: 1.0767890214920044\n",
      "Epoch 143: Train Loss: 0.27121254636181724, Validation Loss: 1.0835283994674683\n",
      "Epoch 144: Train Loss: 0.2477702879243427, Validation Loss: 1.0919899940490723\n",
      "Epoch 145: Train Loss: 0.280343911714024, Validation Loss: 1.098154902458191\n",
      "Epoch 146: Train Loss: 0.24789086977640787, Validation Loss: 1.1032193899154663\n",
      "Epoch 147: Train Loss: 0.2650894936588075, Validation Loss: 1.1037471294403076\n",
      "Epoch 148: Train Loss: 0.2873411195145713, Validation Loss: 1.0930882692337036\n",
      "Epoch 149: Train Loss: 0.2487899644507302, Validation Loss: 1.1059595346450806\n",
      "Epoch 150: Train Loss: 0.2517540223068661, Validation Loss: 1.1158642768859863\n",
      "Epoch 151: Train Loss: 0.25656727618641323, Validation Loss: 1.115014910697937\n",
      "Epoch 152: Train Loss: 0.24003619369533327, Validation Loss: 1.1322287321090698\n",
      "Epoch 153: Train Loss: 0.21911307921012244, Validation Loss: 1.1435353755950928\n",
      "Epoch 154: Train Loss: 0.2370711730586158, Validation Loss: 1.152047038078308\n",
      "Epoch 155: Train Loss: 0.2579754243294398, Validation Loss: 1.1528894901275635\n",
      "Epoch 156: Train Loss: 0.22594253967205682, Validation Loss: 1.154728651046753\n",
      "Epoch 157: Train Loss: 0.246736036406623, Validation Loss: 1.1610989570617676\n",
      "Epoch 158: Train Loss: 0.2484338598118888, Validation Loss: 1.160707950592041\n",
      "Epoch 159: Train Loss: 0.24914929105175865, Validation Loss: 1.1503580808639526\n",
      "Epoch 160: Train Loss: 0.25585945281717515, Validation Loss: 1.1524611711502075\n",
      "Epoch 161: Train Loss: 0.24849797950850594, Validation Loss: 1.1638480424880981\n",
      "Epoch 162: Train Loss: 0.2311728729142083, Validation Loss: 1.1798425912857056\n",
      "Epoch 163: Train Loss: 0.24420677291022408, Validation Loss: 1.1817430257797241\n",
      "Epoch 164: Train Loss: 0.22869788689745796, Validation Loss: 1.1817123889923096\n",
      "Epoch 165: Train Loss: 0.23048465119467842, Validation Loss: 1.1778517961502075\n",
      "Epoch 166: Train Loss: 0.23533592207564247, Validation Loss: 1.1863579750061035\n",
      "Epoch 167: Train Loss: 0.24181966318024528, Validation Loss: 1.1905081272125244\n",
      "Epoch 168: Train Loss: 0.2176005078686608, Validation Loss: 1.1765971183776855\n",
      "Epoch 169: Train Loss: 0.2365879151556227, Validation Loss: 1.1810343265533447\n",
      "Epoch 170: Train Loss: 0.24467861983511183, Validation Loss: 1.1820653676986694\n",
      "Epoch 171: Train Loss: 0.22986849480205113, Validation Loss: 1.200378179550171\n",
      "Epoch 172: Train Loss: 0.2402202073070738, Validation Loss: 1.2075400352478027\n",
      "Epoch 173: Train Loss: 0.2179915416571829, Validation Loss: 1.2173330783843994\n",
      "Epoch 174: Train Loss: 0.22342979949381617, Validation Loss: 1.2184449434280396\n",
      "Epoch 175: Train Loss: 0.20071077346801758, Validation Loss: 1.2198221683502197\n",
      "Epoch 176: Train Loss: 0.19499153892199197, Validation Loss: 1.2274645566940308\n",
      "Epoch 177: Train Loss: 0.21931220756636727, Validation Loss: 1.2243337631225586\n",
      "Epoch 178: Train Loss: 0.2341295133034388, Validation Loss: 1.2267979383468628\n",
      "Epoch 179: Train Loss: 0.1957166807519065, Validation Loss: 1.2272040843963623\n",
      "Epoch 180: Train Loss: 0.19965439041455588, Validation Loss: 1.23390531539917\n",
      "Epoch 181: Train Loss: 0.21060700135098565, Validation Loss: 1.2378963232040405\n",
      "Epoch 182: Train Loss: 0.22558875547515023, Validation Loss: 1.239296793937683\n",
      "Epoch 183: Train Loss: 0.19505408157904944, Validation Loss: 1.2483938932418823\n",
      "Epoch 184: Train Loss: 0.20998884075217777, Validation Loss: 1.2559508085250854\n",
      "Epoch 185: Train Loss: 0.209514276848899, Validation Loss: 1.2647877931594849\n",
      "Epoch 186: Train Loss: 0.1982007854514652, Validation Loss: 1.2651443481445312\n",
      "Epoch 187: Train Loss: 0.19314161812265715, Validation Loss: 1.2670592069625854\n",
      "Epoch 188: Train Loss: 0.17185778667529425, Validation Loss: 1.2633119821548462\n",
      "Epoch 189: Train Loss: 0.2132193288869328, Validation Loss: 1.2637100219726562\n",
      "Epoch 190: Train Loss: 0.18215436985095343, Validation Loss: 1.270316481590271\n",
      "Epoch 191: Train Loss: 0.18777434445089763, Validation Loss: 1.2855509519577026\n",
      "Epoch 192: Train Loss: 0.21366523868507808, Validation Loss: 1.296761155128479\n",
      "Epoch 193: Train Loss: 0.18711523214975992, Validation Loss: 1.2931500673294067\n",
      "Epoch 194: Train Loss: 0.18714218503899044, Validation Loss: 1.300194501876831\n",
      "Epoch 195: Train Loss: 0.19250078664885628, Validation Loss: 1.2835917472839355\n",
      "Epoch 196: Train Loss: 0.2415179287393888, Validation Loss: 1.2919977903366089\n",
      "Epoch 197: Train Loss: 0.19956228137016296, Validation Loss: 1.2904566526412964\n",
      "Epoch 198: Train Loss: 0.21582084397474924, Validation Loss: 1.2990542650222778\n",
      "Epoch 199: Train Loss: 0.213975437813335, Validation Loss: 1.2976760864257812\n",
      "Fold 10 Metrics:\n",
      "Accuracy: 0.18181818181818182, Precision: 0.2857142857142857, Recall: 0.3333333333333333, F1-score: 0.3076923076923077, AUC: 0.16666666666666666\n",
      "Confusion Matrix:\n",
      "[[0 5]\n",
      " [4 2]]\n",
      "Completed fold 10\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples from subject 1 to test set\n",
      "Adding 6 truth samples from subject 1 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.7061368425687155, Validation Loss: 0.7083689570426941\n",
      "Epoch 1: Train Loss: 0.7146313389142355, Validation Loss: 0.7295035719871521\n",
      "Epoch 2: Train Loss: 0.6935778458913168, Validation Loss: 0.7489741444587708\n",
      "Epoch 3: Train Loss: 0.6551002462704977, Validation Loss: 0.759781002998352\n",
      "Epoch 4: Train Loss: 0.6497795714272393, Validation Loss: 0.7746652364730835\n",
      "Epoch 5: Train Loss: 0.672661993238661, Validation Loss: 0.7787116765975952\n",
      "Epoch 6: Train Loss: 0.6390362580617269, Validation Loss: 0.7856116890907288\n",
      "Epoch 7: Train Loss: 0.6708338525560167, Validation Loss: 0.7867332100868225\n",
      "Epoch 8: Train Loss: 0.6456671953201294, Validation Loss: 0.7876689434051514\n",
      "Epoch 9: Train Loss: 0.6478239960140653, Validation Loss: 0.7887506484985352\n",
      "Epoch 10: Train Loss: 0.6678567661179436, Validation Loss: 0.7884262204170227\n",
      "Epoch 11: Train Loss: 0.6104632086224027, Validation Loss: 0.793240487575531\n",
      "Epoch 12: Train Loss: 0.6288583411110772, Validation Loss: 0.8018290996551514\n",
      "Epoch 13: Train Loss: 0.598213255405426, Validation Loss: 0.8058845400810242\n",
      "Epoch 14: Train Loss: 0.6229466199874878, Validation Loss: 0.8089594841003418\n",
      "Epoch 15: Train Loss: 0.6190175215403239, Validation Loss: 0.8112618923187256\n",
      "Epoch 16: Train Loss: 0.6225896146562364, Validation Loss: 0.8157868981361389\n",
      "Epoch 17: Train Loss: 0.6310938265588548, Validation Loss: 0.8176770806312561\n",
      "Epoch 18: Train Loss: 0.5973200135760837, Validation Loss: 0.8189685940742493\n",
      "Epoch 19: Train Loss: 0.6166451242234972, Validation Loss: 0.8167126774787903\n",
      "Epoch 20: Train Loss: 0.6018995046615601, Validation Loss: 0.8262513875961304\n",
      "Epoch 21: Train Loss: 0.6285633709695604, Validation Loss: 0.8404361009597778\n",
      "Epoch 22: Train Loss: 0.5589133534166548, Validation Loss: 0.8448577523231506\n",
      "Epoch 23: Train Loss: 0.5675947533713447, Validation Loss: 0.8553701639175415\n",
      "Epoch 24: Train Loss: 0.568845636314816, Validation Loss: 0.8666570782661438\n",
      "Epoch 25: Train Loss: 0.5644167065620422, Validation Loss: 0.8779228329658508\n",
      "Epoch 26: Train Loss: 0.5751835836304559, Validation Loss: 0.8790479302406311\n",
      "Epoch 27: Train Loss: 0.5694205297364129, Validation Loss: 0.8832746148109436\n",
      "Epoch 28: Train Loss: 0.5275462667147318, Validation Loss: 0.8854129910469055\n",
      "Epoch 29: Train Loss: 0.5537188715404935, Validation Loss: 0.8866921663284302\n",
      "Epoch 30: Train Loss: 0.5664593544271257, Validation Loss: 0.8919660449028015\n",
      "Epoch 31: Train Loss: 0.5492416057321761, Validation Loss: 0.8997126817703247\n",
      "Epoch 32: Train Loss: 0.5578272773159875, Validation Loss: 0.916016161441803\n",
      "Epoch 33: Train Loss: 0.5455647773212857, Validation Loss: 0.9173576235771179\n",
      "Epoch 34: Train Loss: 0.5287623736593459, Validation Loss: 0.924791693687439\n",
      "Epoch 35: Train Loss: 0.5380790928999583, Validation Loss: 0.932823121547699\n",
      "Epoch 36: Train Loss: 0.5155285100142161, Validation Loss: 0.9298037886619568\n",
      "Epoch 37: Train Loss: 0.5349169009261661, Validation Loss: 0.9318660497665405\n",
      "Epoch 38: Train Loss: 0.5141259200043149, Validation Loss: 0.9322566390037537\n",
      "Epoch 39: Train Loss: 0.5256971253289117, Validation Loss: 0.9338150024414062\n",
      "Epoch 40: Train Loss: 0.5062744352552626, Validation Loss: 0.9420158267021179\n",
      "Epoch 41: Train Loss: 0.49927753541204667, Validation Loss: 0.9515062570571899\n",
      "Epoch 42: Train Loss: 0.5206105179256864, Validation Loss: 0.9644473791122437\n",
      "Epoch 43: Train Loss: 0.5104043483734131, Validation Loss: 0.9711803197860718\n",
      "Epoch 44: Train Loss: 0.4971538616551293, Validation Loss: 0.9784330129623413\n",
      "Epoch 45: Train Loss: 0.48386172784699333, Validation Loss: 0.9910953640937805\n",
      "Epoch 46: Train Loss: 0.5013013515207503, Validation Loss: 0.9872392416000366\n",
      "Epoch 47: Train Loss: 0.46962946984503007, Validation Loss: 0.9820291996002197\n",
      "Epoch 48: Train Loss: 0.5020252466201782, Validation Loss: 0.9851917624473572\n",
      "Epoch 49: Train Loss: 0.47798966699176365, Validation Loss: 0.983501672744751\n",
      "Epoch 50: Train Loss: 0.5100170175234476, Validation Loss: 0.9935559630393982\n",
      "Epoch 51: Train Loss: 0.48145418034659493, Validation Loss: 1.0019339323043823\n",
      "Epoch 52: Train Loss: 0.4562370280424754, Validation Loss: 1.013235330581665\n",
      "Epoch 53: Train Loss: 0.4400912953747643, Validation Loss: 1.0303798913955688\n",
      "Epoch 54: Train Loss: 0.44391058219803703, Validation Loss: 1.0357096195220947\n",
      "Epoch 55: Train Loss: 0.46676360898547703, Validation Loss: 1.0450387001037598\n",
      "Epoch 56: Train Loss: 0.446143819226159, Validation Loss: 1.0475739240646362\n",
      "Epoch 57: Train Loss: 0.4425620602236854, Validation Loss: 1.0587736368179321\n",
      "Epoch 58: Train Loss: 0.4508006340927548, Validation Loss: 1.0645149946212769\n",
      "Epoch 59: Train Loss: 0.4694916903972626, Validation Loss: 1.0599253177642822\n",
      "Epoch 60: Train Loss: 0.4596892231040531, Validation Loss: 1.0810197591781616\n",
      "Epoch 61: Train Loss: 0.4501306348376804, Validation Loss: 1.0987721681594849\n",
      "Epoch 62: Train Loss: 0.3999631338649326, Validation Loss: 1.1143763065338135\n",
      "Epoch 63: Train Loss: 0.42009654972288346, Validation Loss: 1.1123924255371094\n",
      "Epoch 64: Train Loss: 0.4202902449501885, Validation Loss: 1.1258476972579956\n",
      "Epoch 65: Train Loss: 0.43773311376571655, Validation Loss: 1.1406108140945435\n",
      "Epoch 66: Train Loss: 0.4387233820226457, Validation Loss: 1.1600648164749146\n",
      "Epoch 67: Train Loss: 0.4150555464956496, Validation Loss: 1.1505708694458008\n",
      "Epoch 68: Train Loss: 0.4457380175590515, Validation Loss: 1.1515207290649414\n",
      "Epoch 69: Train Loss: 0.4392775893211365, Validation Loss: 1.156589388847351\n",
      "Epoch 70: Train Loss: 0.41927168766657513, Validation Loss: 1.1664090156555176\n",
      "Epoch 71: Train Loss: 0.3782547977235582, Validation Loss: 1.1672900915145874\n",
      "Epoch 72: Train Loss: 0.42801135778427124, Validation Loss: 1.1775912046432495\n",
      "Epoch 73: Train Loss: 0.39978257152769303, Validation Loss: 1.1934860944747925\n",
      "Epoch 74: Train Loss: 0.3992672363917033, Validation Loss: 1.1946336030960083\n",
      "Epoch 75: Train Loss: 0.3930017550786336, Validation Loss: 1.2084301710128784\n",
      "Epoch 76: Train Loss: 0.41870976156658596, Validation Loss: 1.2177742719650269\n",
      "Epoch 77: Train Loss: 0.3802705473370022, Validation Loss: 1.2157206535339355\n",
      "Epoch 78: Train Loss: 0.3869561619228787, Validation Loss: 1.2176330089569092\n",
      "Epoch 79: Train Loss: 0.408094412750668, Validation Loss: 1.2109181880950928\n",
      "Epoch 80: Train Loss: 0.39213037490844727, Validation Loss: 1.2277460098266602\n",
      "Epoch 81: Train Loss: 0.39133986830711365, Validation Loss: 1.2468781471252441\n",
      "Epoch 82: Train Loss: 0.4041999512248569, Validation Loss: 1.2693904638290405\n",
      "Epoch 83: Train Loss: 0.37065864933861625, Validation Loss: 1.2840756177902222\n",
      "Epoch 84: Train Loss: 0.3679880632294549, Validation Loss: 1.2894967794418335\n",
      "Epoch 85: Train Loss: 0.361506129304568, Validation Loss: 1.295447587966919\n",
      "Epoch 86: Train Loss: 0.36900218162271714, Validation Loss: 1.3065030574798584\n",
      "Epoch 87: Train Loss: 0.36873262458377415, Validation Loss: 1.3221255540847778\n",
      "Epoch 88: Train Loss: 0.3723149961895413, Validation Loss: 1.3263599872589111\n",
      "Epoch 89: Train Loss: 0.40825029876497054, Validation Loss: 1.3250234127044678\n",
      "Epoch 90: Train Loss: 0.3650979846715927, Validation Loss: 1.3348462581634521\n",
      "Epoch 91: Train Loss: 0.33247533440589905, Validation Loss: 1.3467133045196533\n",
      "Epoch 92: Train Loss: 0.34622234768337673, Validation Loss: 1.353873610496521\n",
      "Epoch 93: Train Loss: 0.34083641237682766, Validation Loss: 1.3575637340545654\n",
      "Epoch 94: Train Loss: 0.3722952836089664, Validation Loss: 1.3577203750610352\n",
      "Epoch 95: Train Loss: 0.3778281874126858, Validation Loss: 1.3795878887176514\n",
      "Epoch 96: Train Loss: 0.34085731705029804, Validation Loss: 1.3884057998657227\n",
      "Epoch 97: Train Loss: 0.36902892258432174, Validation Loss: 1.394091010093689\n",
      "Epoch 98: Train Loss: 0.37706541683938766, Validation Loss: 1.3950366973876953\n",
      "Epoch 99: Train Loss: 0.3619759711954329, Validation Loss: 1.3836222887039185\n",
      "Epoch 100: Train Loss: 0.356575436062283, Validation Loss: 1.405411958694458\n",
      "Epoch 101: Train Loss: 0.3822081254588233, Validation Loss: 1.4296895265579224\n",
      "Epoch 102: Train Loss: 0.35274850991037154, Validation Loss: 1.427659034729004\n",
      "Epoch 103: Train Loss: 0.3187878305713336, Validation Loss: 1.4334304332733154\n",
      "Epoch 104: Train Loss: 0.3197469843758477, Validation Loss: 1.444838523864746\n",
      "Epoch 105: Train Loss: 0.34893067015541923, Validation Loss: 1.4620848894119263\n",
      "Epoch 106: Train Loss: 0.335654326611095, Validation Loss: 1.4571043252944946\n",
      "Epoch 107: Train Loss: 0.36087676882743835, Validation Loss: 1.4678149223327637\n",
      "Epoch 108: Train Loss: 0.36342863904105294, Validation Loss: 1.4754878282546997\n",
      "Epoch 109: Train Loss: 0.3019856396648619, Validation Loss: 1.482089877128601\n",
      "Epoch 110: Train Loss: 0.33076361152860856, Validation Loss: 1.4836466312408447\n",
      "Epoch 111: Train Loss: 0.3142486247751448, Validation Loss: 1.4713289737701416\n",
      "Epoch 112: Train Loss: 0.340685678852929, Validation Loss: 1.4879531860351562\n",
      "Epoch 113: Train Loss: 0.3155888484583961, Validation Loss: 1.5308489799499512\n",
      "Epoch 114: Train Loss: 0.30776125689347583, Validation Loss: 1.5344899892807007\n",
      "Epoch 115: Train Loss: 0.31410180860095555, Validation Loss: 1.5370867252349854\n",
      "Epoch 116: Train Loss: 0.3043805956840515, Validation Loss: 1.5399383306503296\n",
      "Epoch 117: Train Loss: 0.3234502457910114, Validation Loss: 1.5527163743972778\n",
      "Epoch 118: Train Loss: 0.30132994055747986, Validation Loss: 1.5344157218933105\n",
      "Epoch 119: Train Loss: 0.30120207203759086, Validation Loss: 1.5395641326904297\n",
      "Epoch 120: Train Loss: 0.30577179458406234, Validation Loss: 1.5288774967193604\n",
      "Epoch 121: Train Loss: 0.2920088999801212, Validation Loss: 1.5519508123397827\n",
      "Epoch 122: Train Loss: 0.31166057619783616, Validation Loss: 1.5600345134735107\n",
      "Epoch 123: Train Loss: 0.2861943311161465, Validation Loss: 1.5717058181762695\n",
      "Epoch 124: Train Loss: 0.32565528485510087, Validation Loss: 1.5965052843093872\n",
      "Epoch 125: Train Loss: 0.30388879776000977, Validation Loss: 1.6016433238983154\n",
      "Epoch 126: Train Loss: 0.275731611582968, Validation Loss: 1.5978831052780151\n",
      "Epoch 127: Train Loss: 0.2823839220735762, Validation Loss: 1.5909713506698608\n",
      "Epoch 128: Train Loss: 0.3023555941051907, Validation Loss: 1.5716087818145752\n",
      "Epoch 129: Train Loss: 0.272765071855651, Validation Loss: 1.5868631601333618\n",
      "Epoch 130: Train Loss: 0.27443066073788536, Validation Loss: 1.6018122434616089\n",
      "Epoch 131: Train Loss: 0.26159936520788407, Validation Loss: 1.622606635093689\n",
      "Epoch 132: Train Loss: 0.294782065682941, Validation Loss: 1.6239900588989258\n",
      "Epoch 133: Train Loss: 0.2554881423711777, Validation Loss: 1.6349331140518188\n",
      "Epoch 134: Train Loss: 0.28503340151574874, Validation Loss: 1.6243631839752197\n",
      "Epoch 135: Train Loss: 0.27910715341567993, Validation Loss: 1.6447932720184326\n",
      "Epoch 136: Train Loss: 0.2506798737578922, Validation Loss: 1.6495932340621948\n",
      "Epoch 137: Train Loss: 0.26498810450236004, Validation Loss: 1.649564266204834\n",
      "Epoch 138: Train Loss: 0.2839852008554671, Validation Loss: 1.6554081439971924\n",
      "Epoch 139: Train Loss: 0.25641920665899914, Validation Loss: 1.6649188995361328\n",
      "Epoch 140: Train Loss: 0.2838994877205955, Validation Loss: 1.6735399961471558\n",
      "Epoch 141: Train Loss: 0.2930712087286843, Validation Loss: 1.7010760307312012\n",
      "Epoch 142: Train Loss: 0.25062575605180526, Validation Loss: 1.692955732345581\n",
      "Epoch 143: Train Loss: 0.2733313524060779, Validation Loss: 1.7114511728286743\n",
      "Epoch 144: Train Loss: 0.2654626601272159, Validation Loss: 1.7151340246200562\n",
      "Epoch 145: Train Loss: 0.27596966591146255, Validation Loss: 1.7327135801315308\n",
      "Epoch 146: Train Loss: 0.2659011483192444, Validation Loss: 1.7366175651550293\n",
      "Epoch 147: Train Loss: 0.24787007768948874, Validation Loss: 1.751341462135315\n",
      "Epoch 148: Train Loss: 0.2417310525973638, Validation Loss: 1.7473350763320923\n",
      "Epoch 149: Train Loss: 0.26043011744817096, Validation Loss: 1.7411071062088013\n",
      "Epoch 150: Train Loss: 0.24013346433639526, Validation Loss: 1.7571806907653809\n",
      "Epoch 151: Train Loss: 0.2536574966377682, Validation Loss: 1.7921653985977173\n",
      "Epoch 152: Train Loss: 0.2606080257230335, Validation Loss: 1.819196343421936\n",
      "Epoch 153: Train Loss: 0.27044425739182365, Validation Loss: 1.8103598356246948\n",
      "Epoch 154: Train Loss: 0.23659948963258, Validation Loss: 1.8006949424743652\n",
      "Epoch 155: Train Loss: 0.25901081495814854, Validation Loss: 1.8247627019882202\n",
      "Epoch 156: Train Loss: 0.24356256425380707, Validation Loss: 1.8241173028945923\n",
      "Epoch 157: Train Loss: 0.2254036565621694, Validation Loss: 1.8177745342254639\n",
      "Epoch 158: Train Loss: 0.2345228973362181, Validation Loss: 1.7946314811706543\n",
      "Epoch 159: Train Loss: 0.23646828863355848, Validation Loss: 1.7994608879089355\n",
      "Epoch 160: Train Loss: 0.21933012869622973, Validation Loss: 1.8182578086853027\n",
      "Epoch 161: Train Loss: 0.2497077054447598, Validation Loss: 1.863217830657959\n",
      "Epoch 162: Train Loss: 0.21957845075262916, Validation Loss: 1.8593509197235107\n",
      "Epoch 163: Train Loss: 0.2449397345383962, Validation Loss: 1.9022433757781982\n",
      "Epoch 164: Train Loss: 0.2327092770073149, Validation Loss: 1.898147463798523\n",
      "Epoch 165: Train Loss: 0.22492305603292254, Validation Loss: 1.8715449571609497\n",
      "Epoch 166: Train Loss: 0.2379052076074812, Validation Loss: 1.9021682739257812\n",
      "Epoch 167: Train Loss: 0.23627805213133493, Validation Loss: 1.9108688831329346\n",
      "Epoch 168: Train Loss: 0.22465870281060538, Validation Loss: 1.9317939281463623\n",
      "Epoch 169: Train Loss: 0.1995047570930587, Validation Loss: 1.9264891147613525\n",
      "Epoch 170: Train Loss: 0.21966672357585695, Validation Loss: 1.9271621704101562\n",
      "Epoch 171: Train Loss: 0.19022441572613186, Validation Loss: 1.9270685911178589\n",
      "Epoch 172: Train Loss: 0.1884169831044144, Validation Loss: 1.9283407926559448\n",
      "Epoch 173: Train Loss: 0.1942978302637736, Validation Loss: 1.9600627422332764\n",
      "Epoch 174: Train Loss: 0.2181226760149002, Validation Loss: 1.9994685649871826\n",
      "Epoch 175: Train Loss: 0.2062775625122918, Validation Loss: 2.008962392807007\n",
      "Epoch 176: Train Loss: 0.2203621305525303, Validation Loss: 2.0294716358184814\n",
      "Epoch 177: Train Loss: 0.21000457720624077, Validation Loss: 2.0187761783599854\n",
      "Epoch 178: Train Loss: 0.18830284807417128, Validation Loss: 2.007391929626465\n",
      "Epoch 179: Train Loss: 0.1825883628593551, Validation Loss: 2.0358352661132812\n",
      "Epoch 180: Train Loss: 0.20162837704022726, Validation Loss: 1.9916754961013794\n",
      "Epoch 181: Train Loss: 0.19839517937766182, Validation Loss: 1.9885600805282593\n",
      "Epoch 182: Train Loss: 0.2100616908735699, Validation Loss: 1.9745216369628906\n",
      "Epoch 183: Train Loss: 0.1835597844587432, Validation Loss: 2.020920753479004\n",
      "Epoch 184: Train Loss: 0.19251907037364113, Validation Loss: 2.0569539070129395\n",
      "Epoch 185: Train Loss: 0.1846154746082094, Validation Loss: 2.0649945735931396\n",
      "Epoch 186: Train Loss: 0.18509830948379305, Validation Loss: 2.066767930984497\n",
      "Epoch 187: Train Loss: 0.1839853674173355, Validation Loss: 2.065645456314087\n",
      "Epoch 188: Train Loss: 0.2106606430477566, Validation Loss: 2.062094211578369\n",
      "Epoch 189: Train Loss: 0.17273912123507923, Validation Loss: 2.088207483291626\n",
      "Epoch 190: Train Loss: 0.20029255085521275, Validation Loss: 2.067544937133789\n",
      "Epoch 191: Train Loss: 0.19689825922250748, Validation Loss: 2.0832841396331787\n",
      "Epoch 192: Train Loss: 0.19729249427715936, Validation Loss: 2.071298599243164\n",
      "Epoch 193: Train Loss: 0.15664397676785788, Validation Loss: 2.0702223777770996\n",
      "Epoch 194: Train Loss: 0.1693987680806054, Validation Loss: 2.09516978263855\n",
      "Epoch 195: Train Loss: 0.15300849245654213, Validation Loss: 2.106828212738037\n",
      "Epoch 196: Train Loss: 0.20037644935978782, Validation Loss: 2.1264681816101074\n",
      "Epoch 197: Train Loss: 0.18262756533092922, Validation Loss: 2.096036911010742\n",
      "Epoch 198: Train Loss: 0.14691684602035415, Validation Loss: 2.1000936031341553\n",
      "Epoch 199: Train Loss: 0.16167395396365059, Validation Loss: 2.106074810028076\n",
      "Fold 11 Metrics:\n",
      "Accuracy: 0.2727272727272727, Precision: 0.25, Recall: 0.16666666666666666, F1-score: 0.2, AUC: 0.2833333333333334\n",
      "Confusion Matrix:\n",
      "[[2 3]\n",
      " [5 1]]\n",
      "Completed fold 11\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples from subject 2 to test set\n",
      "Adding 6 truth samples from subject 2 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.7152683337529501, Validation Loss: 0.7331206202507019\n",
      "Epoch 1: Train Loss: 0.6974136365784539, Validation Loss: 0.7663964629173279\n",
      "Epoch 2: Train Loss: 0.679916520913442, Validation Loss: 0.7988269329071045\n",
      "Epoch 3: Train Loss: 0.7227110862731934, Validation Loss: 0.8293948173522949\n",
      "Epoch 4: Train Loss: 0.6915386451615227, Validation Loss: 0.8571551442146301\n",
      "Epoch 5: Train Loss: 0.7047217951880561, Validation Loss: 0.8671572804450989\n",
      "Epoch 6: Train Loss: 0.6747422681914436, Validation Loss: 0.868142306804657\n",
      "Epoch 7: Train Loss: 0.684373312526279, Validation Loss: 0.870628297328949\n",
      "Epoch 8: Train Loss: 0.6533551149898105, Validation Loss: 0.8749538064002991\n",
      "Epoch 9: Train Loss: 0.6706413494216071, Validation Loss: 0.8808998465538025\n",
      "Epoch 10: Train Loss: 0.6717965602874756, Validation Loss: 0.8889792561531067\n",
      "Epoch 11: Train Loss: 0.6540183027585348, Validation Loss: 0.8879476189613342\n",
      "Epoch 12: Train Loss: 0.6569735010464987, Validation Loss: 0.8791875243186951\n",
      "Epoch 13: Train Loss: 0.6114805605676439, Validation Loss: 0.8860062956809998\n",
      "Epoch 14: Train Loss: 0.6430743071768019, Validation Loss: 0.8948688507080078\n",
      "Epoch 15: Train Loss: 0.6540301177236769, Validation Loss: 0.9037342071533203\n",
      "Epoch 16: Train Loss: 0.6332726213667128, Validation Loss: 0.9057410359382629\n",
      "Epoch 17: Train Loss: 0.6324847605493333, Validation Loss: 0.9128848314285278\n",
      "Epoch 18: Train Loss: 0.6397691832648383, Validation Loss: 0.9128448367118835\n",
      "Epoch 19: Train Loss: 0.6264458563592699, Validation Loss: 0.9088224768638611\n",
      "Epoch 20: Train Loss: 0.6498755746417575, Validation Loss: 0.8982275128364563\n",
      "Epoch 21: Train Loss: 0.6268560356563992, Validation Loss: 0.9152203798294067\n",
      "Epoch 22: Train Loss: 0.612048347791036, Validation Loss: 0.9125375747680664\n",
      "Epoch 23: Train Loss: 0.6140020456578996, Validation Loss: 0.9211408495903015\n",
      "Epoch 24: Train Loss: 0.614557683467865, Validation Loss: 0.9205482602119446\n",
      "Epoch 25: Train Loss: 0.570372588104672, Validation Loss: 0.9326145648956299\n",
      "Epoch 26: Train Loss: 0.5942321485943265, Validation Loss: 0.9237788915634155\n",
      "Epoch 27: Train Loss: 0.5649437871244218, Validation Loss: 0.9267345666885376\n",
      "Epoch 28: Train Loss: 0.5858906077014076, Validation Loss: 0.9213271737098694\n",
      "Epoch 29: Train Loss: 0.5888408621152242, Validation Loss: 0.9200872182846069\n",
      "Epoch 30: Train Loss: 0.5851967963907454, Validation Loss: 0.9346042275428772\n",
      "Epoch 31: Train Loss: 0.6056028074688382, Validation Loss: 0.9399377703666687\n",
      "Epoch 32: Train Loss: 0.5706581936942207, Validation Loss: 0.9327993392944336\n",
      "Epoch 33: Train Loss: 0.5934820373853048, Validation Loss: 0.9446483254432678\n",
      "Epoch 34: Train Loss: 0.5798138214482201, Validation Loss: 0.9492371082305908\n",
      "Epoch 35: Train Loss: 0.574051191409429, Validation Loss: 0.9590684771537781\n",
      "Epoch 36: Train Loss: 0.5600573652320437, Validation Loss: 0.947388231754303\n",
      "Epoch 37: Train Loss: 0.532314529021581, Validation Loss: 0.9525620937347412\n",
      "Epoch 38: Train Loss: 0.5832378334469266, Validation Loss: 0.9331727623939514\n",
      "Epoch 39: Train Loss: 0.5766416192054749, Validation Loss: 0.9498201608657837\n",
      "Epoch 40: Train Loss: 0.5364807910389371, Validation Loss: 0.9294689297676086\n",
      "Epoch 41: Train Loss: 0.5747930771774716, Validation Loss: 0.9454072713851929\n",
      "Epoch 42: Train Loss: 0.5333569347858429, Validation Loss: 0.9493127465248108\n",
      "Epoch 43: Train Loss: 0.5374927255842421, Validation Loss: 0.9350465536117554\n",
      "Epoch 44: Train Loss: 0.5306227339638604, Validation Loss: 0.9509496092796326\n",
      "Epoch 45: Train Loss: 0.533791744046741, Validation Loss: 0.950526773929596\n",
      "Epoch 46: Train Loss: 0.5249707202116648, Validation Loss: 0.9727105498313904\n",
      "Epoch 47: Train Loss: 0.5182906919055514, Validation Loss: 0.952964723110199\n",
      "Epoch 48: Train Loss: 0.5354584720399644, Validation Loss: 0.970582902431488\n",
      "Epoch 49: Train Loss: 0.5278477436966367, Validation Loss: 0.9508878588676453\n",
      "Epoch 50: Train Loss: 0.5039715038405524, Validation Loss: 0.9694881439208984\n",
      "Epoch 51: Train Loss: 0.5130118462774489, Validation Loss: 0.9724722504615784\n",
      "Epoch 52: Train Loss: 0.5082168413533105, Validation Loss: 0.9714176058769226\n",
      "Epoch 53: Train Loss: 0.4812975592083401, Validation Loss: 0.967790424823761\n",
      "Epoch 54: Train Loss: 0.5173480113347372, Validation Loss: 0.9666957259178162\n",
      "Epoch 55: Train Loss: 0.5149342748853896, Validation Loss: 0.9641265273094177\n",
      "Epoch 56: Train Loss: 0.49442898564868504, Validation Loss: 0.9325939416885376\n",
      "Epoch 57: Train Loss: 0.47451672289106583, Validation Loss: 0.9602528810501099\n",
      "Epoch 58: Train Loss: 0.49782142374250626, Validation Loss: 0.9723844528198242\n",
      "Epoch 59: Train Loss: 0.49368592765596175, Validation Loss: 0.9655826091766357\n",
      "Epoch 60: Train Loss: 0.4715041650666131, Validation Loss: 0.9522709250450134\n",
      "Epoch 61: Train Loss: 0.4732755687501695, Validation Loss: 0.9503639340400696\n",
      "Epoch 62: Train Loss: 0.465388235118654, Validation Loss: 0.9528017044067383\n",
      "Epoch 63: Train Loss: 0.4811311893992954, Validation Loss: 0.9686349630355835\n",
      "Epoch 64: Train Loss: 0.4765995343526204, Validation Loss: 0.9727014899253845\n",
      "Epoch 65: Train Loss: 0.45420390036371017, Validation Loss: 0.9663890600204468\n",
      "Epoch 66: Train Loss: 0.45164861612849766, Validation Loss: 0.9592952728271484\n",
      "Epoch 67: Train Loss: 0.44017259279886883, Validation Loss: 0.9569871425628662\n",
      "Epoch 68: Train Loss: 0.41461238596174455, Validation Loss: 0.9601756930351257\n",
      "Epoch 69: Train Loss: 0.4454004466533661, Validation Loss: 0.9492636919021606\n",
      "Epoch 70: Train Loss: 0.44721414645512897, Validation Loss: 0.9502145051956177\n",
      "Epoch 71: Train Loss: 0.444216701719496, Validation Loss: 0.9587566256523132\n",
      "Epoch 72: Train Loss: 0.4603949189186096, Validation Loss: 0.9575086236000061\n",
      "Epoch 73: Train Loss: 0.4609663751390245, Validation Loss: 0.9524813890457153\n",
      "Epoch 74: Train Loss: 0.44956974188486737, Validation Loss: 0.9553529024124146\n",
      "Epoch 75: Train Loss: 0.4187762671046787, Validation Loss: 0.9774370789527893\n",
      "Epoch 76: Train Loss: 0.4106044007672204, Validation Loss: 0.961007297039032\n",
      "Epoch 77: Train Loss: 0.39962993727789986, Validation Loss: 0.9727780818939209\n",
      "Epoch 78: Train Loss: 0.42288880546887714, Validation Loss: 0.9774752259254456\n",
      "Epoch 79: Train Loss: 0.42846061123741996, Validation Loss: 0.9792544841766357\n",
      "Epoch 80: Train Loss: 0.4406486683421665, Validation Loss: 0.9430463910102844\n",
      "Epoch 81: Train Loss: 0.4419606228669484, Validation Loss: 0.9301161170005798\n",
      "Epoch 82: Train Loss: 0.4068795790274938, Validation Loss: 0.9307817220687866\n",
      "Epoch 83: Train Loss: 0.3849985996882121, Validation Loss: 0.93631911277771\n",
      "Epoch 84: Train Loss: 0.40152450733714634, Validation Loss: 0.923660397529602\n",
      "Epoch 85: Train Loss: 0.39817960063616437, Validation Loss: 0.9330501556396484\n",
      "Epoch 86: Train Loss: 0.40098629064030117, Validation Loss: 0.9233465194702148\n",
      "Epoch 87: Train Loss: 0.3909031119611528, Validation Loss: 0.9365789294242859\n",
      "Epoch 88: Train Loss: 0.36741378241115147, Validation Loss: 0.9493093490600586\n",
      "Epoch 89: Train Loss: 0.3698125746515062, Validation Loss: 0.9179622530937195\n",
      "Epoch 90: Train Loss: 0.37905894054306877, Validation Loss: 0.9351892471313477\n",
      "Epoch 91: Train Loss: 0.3761186765299903, Validation Loss: 0.9335288405418396\n",
      "Epoch 92: Train Loss: 0.371064907974667, Validation Loss: 0.9506908059120178\n",
      "Epoch 93: Train Loss: 0.3857325812180837, Validation Loss: 0.9492725729942322\n",
      "Epoch 94: Train Loss: 0.3908786144521501, Validation Loss: 0.9708431363105774\n",
      "Epoch 95: Train Loss: 0.39445244934823775, Validation Loss: 0.982368528842926\n",
      "Epoch 96: Train Loss: 0.4003298249509599, Validation Loss: 0.9943021535873413\n",
      "Epoch 97: Train Loss: 0.3531630337238312, Validation Loss: 0.9719686508178711\n",
      "Epoch 98: Train Loss: 0.37092314494980705, Validation Loss: 0.9515834450721741\n",
      "Epoch 99: Train Loss: 0.3622136314709981, Validation Loss: 0.9705490469932556\n",
      "Epoch 100: Train Loss: 0.341129869222641, Validation Loss: 0.9670255184173584\n",
      "Epoch 101: Train Loss: 0.34088371528519523, Validation Loss: 0.9716060161590576\n",
      "Epoch 102: Train Loss: 0.33168164061175454, Validation Loss: 0.9568700194358826\n",
      "Epoch 103: Train Loss: 0.360513793097602, Validation Loss: 0.9690189957618713\n",
      "Epoch 104: Train Loss: 0.33433303402529824, Validation Loss: 0.9487611651420593\n",
      "Epoch 105: Train Loss: 0.35123539633221096, Validation Loss: 0.9744882583618164\n",
      "Epoch 106: Train Loss: 0.336292843023936, Validation Loss: 0.9749855399131775\n",
      "Epoch 107: Train Loss: 0.30274198452631634, Validation Loss: 0.9688610434532166\n",
      "Epoch 108: Train Loss: 0.32689181963602704, Validation Loss: 0.9573718309402466\n",
      "Epoch 109: Train Loss: 0.3106451547808117, Validation Loss: 0.9598018527030945\n",
      "Epoch 110: Train Loss: 0.30591340776946807, Validation Loss: 0.9709416031837463\n",
      "Epoch 111: Train Loss: 0.338645425107744, Validation Loss: 0.9827684164047241\n",
      "Epoch 112: Train Loss: 0.3243764142195384, Validation Loss: 0.9721210598945618\n",
      "Epoch 113: Train Loss: 0.32445239358478123, Validation Loss: 1.0051904916763306\n",
      "Epoch 114: Train Loss: 0.31150879793696934, Validation Loss: 1.0150587558746338\n",
      "Epoch 115: Train Loss: 0.29385074310832554, Validation Loss: 1.0232930183410645\n",
      "Epoch 116: Train Loss: 0.3220046079821057, Validation Loss: 0.9953837990760803\n",
      "Epoch 117: Train Loss: 0.3074977828396691, Validation Loss: 1.017208456993103\n",
      "Epoch 118: Train Loss: 0.3186379075050354, Validation Loss: 1.007624626159668\n",
      "Epoch 119: Train Loss: 0.30514687134159935, Validation Loss: 1.023705005645752\n",
      "Epoch 120: Train Loss: 0.28116512795289356, Validation Loss: 1.0419093370437622\n",
      "Epoch 121: Train Loss: 0.3182566397719913, Validation Loss: 1.0485118627548218\n",
      "Epoch 122: Train Loss: 0.3185422718524933, Validation Loss: 1.055152416229248\n",
      "Epoch 123: Train Loss: 0.2896108031272888, Validation Loss: 1.05903160572052\n",
      "Epoch 124: Train Loss: 0.27656250529819065, Validation Loss: 1.0777040719985962\n",
      "Epoch 125: Train Loss: 0.31447746521896786, Validation Loss: 1.0625836849212646\n",
      "Epoch 126: Train Loss: 0.2921149581670761, Validation Loss: 1.0610594749450684\n",
      "Epoch 127: Train Loss: 0.2713451368941201, Validation Loss: 1.086243987083435\n",
      "Epoch 128: Train Loss: 0.32831736074553597, Validation Loss: 1.0883039236068726\n",
      "Epoch 129: Train Loss: 0.2380975302722719, Validation Loss: 1.0942126512527466\n",
      "Epoch 130: Train Loss: 0.290595692065027, Validation Loss: 1.0616161823272705\n",
      "Epoch 131: Train Loss: 0.28914200597339207, Validation Loss: 1.0668113231658936\n",
      "Epoch 132: Train Loss: 0.26238880554835003, Validation Loss: 1.080798625946045\n",
      "Epoch 133: Train Loss: 0.27473707993825275, Validation Loss: 1.082846760749817\n",
      "Epoch 134: Train Loss: 0.2606337236033546, Validation Loss: 1.0760866403579712\n",
      "Epoch 135: Train Loss: 0.2681456787718667, Validation Loss: 1.0758434534072876\n",
      "Epoch 136: Train Loss: 0.27539413670698804, Validation Loss: 1.086949110031128\n",
      "Epoch 137: Train Loss: 0.24565635124842325, Validation Loss: 1.0860562324523926\n",
      "Epoch 138: Train Loss: 0.3191254768106673, Validation Loss: 1.112371563911438\n",
      "Epoch 139: Train Loss: 0.2609442373116811, Validation Loss: 1.086857795715332\n",
      "Epoch 140: Train Loss: 0.2907577289475335, Validation Loss: 1.1092791557312012\n",
      "Epoch 141: Train Loss: 0.25505925549401176, Validation Loss: 1.0868260860443115\n",
      "Epoch 142: Train Loss: 0.2515061100323995, Validation Loss: 1.1338673830032349\n",
      "Epoch 143: Train Loss: 0.2891477694114049, Validation Loss: 1.1432214975357056\n",
      "Epoch 144: Train Loss: 0.2527179701460732, Validation Loss: 1.1530901193618774\n",
      "Epoch 145: Train Loss: 0.2356764922539393, Validation Loss: 1.0999773740768433\n",
      "Epoch 146: Train Loss: 0.2401427941189872, Validation Loss: 1.1280598640441895\n",
      "Epoch 147: Train Loss: 0.21959136757585737, Validation Loss: 1.147430658340454\n",
      "Epoch 148: Train Loss: 0.2509642657306459, Validation Loss: 1.1151025295257568\n",
      "Epoch 149: Train Loss: 0.21281727817323473, Validation Loss: 1.1315722465515137\n",
      "Epoch 150: Train Loss: 0.23861586882008445, Validation Loss: 1.1225621700286865\n",
      "Epoch 151: Train Loss: 0.22018065220779842, Validation Loss: 1.1425808668136597\n",
      "Epoch 152: Train Loss: 0.23010019461313883, Validation Loss: 1.1479518413543701\n",
      "Epoch 153: Train Loss: 0.2142980545759201, Validation Loss: 1.1584458351135254\n",
      "Epoch 154: Train Loss: 0.19871258321735594, Validation Loss: 1.175196886062622\n",
      "Epoch 155: Train Loss: 0.26402827186716926, Validation Loss: 1.1762278079986572\n",
      "Epoch 156: Train Loss: 0.20335998924242127, Validation Loss: 1.1876314878463745\n",
      "Epoch 157: Train Loss: 0.22060252063804203, Validation Loss: 1.1974061727523804\n",
      "Epoch 158: Train Loss: 0.23160195681783888, Validation Loss: 1.1947568655014038\n",
      "Epoch 159: Train Loss: 0.2711815767818027, Validation Loss: 1.1902462244033813\n",
      "Epoch 160: Train Loss: 0.17963768872949812, Validation Loss: 1.1946088075637817\n",
      "Epoch 161: Train Loss: 0.24308007127708858, Validation Loss: 1.1893515586853027\n",
      "Epoch 162: Train Loss: 0.22607814603381687, Validation Loss: 1.222369909286499\n",
      "Epoch 163: Train Loss: 0.20708301415046057, Validation Loss: 1.2433233261108398\n",
      "Epoch 164: Train Loss: 0.21491226885053846, Validation Loss: 1.2300416231155396\n",
      "Epoch 165: Train Loss: 0.20897448890739018, Validation Loss: 1.267322301864624\n",
      "Epoch 166: Train Loss: 0.23262158615721595, Validation Loss: 1.2649997472763062\n",
      "Epoch 167: Train Loss: 0.22740863925880855, Validation Loss: 1.241703748703003\n",
      "Epoch 168: Train Loss: 0.25718231085273957, Validation Loss: 1.261374831199646\n",
      "Epoch 169: Train Loss: 0.19920235375563303, Validation Loss: 1.2357407808303833\n",
      "Epoch 170: Train Loss: 0.23285076849990421, Validation Loss: 1.205094337463379\n",
      "Epoch 171: Train Loss: 0.20055649346775478, Validation Loss: 1.2107805013656616\n",
      "Epoch 172: Train Loss: 0.19758879144986471, Validation Loss: 1.2096425294876099\n",
      "Epoch 173: Train Loss: 0.21128878908024895, Validation Loss: 1.2282994985580444\n",
      "Epoch 174: Train Loss: 0.2712910655472014, Validation Loss: 1.2866390943527222\n",
      "Epoch 175: Train Loss: 0.19754077825281355, Validation Loss: 1.2743961811065674\n",
      "Epoch 176: Train Loss: 0.17668005575736365, Validation Loss: 1.2704375982284546\n",
      "Epoch 177: Train Loss: 0.2286788390742408, Validation Loss: 1.254599690437317\n",
      "Epoch 178: Train Loss: 0.23061989578935835, Validation Loss: 1.2362779378890991\n",
      "Epoch 179: Train Loss: 0.2021324551767773, Validation Loss: 1.2759004831314087\n",
      "Epoch 180: Train Loss: 0.1849791548318333, Validation Loss: 1.2817742824554443\n",
      "Epoch 181: Train Loss: 0.19074469721979564, Validation Loss: 1.2825690507888794\n",
      "Epoch 182: Train Loss: 0.21398327416843838, Validation Loss: 1.2835310697555542\n",
      "Epoch 183: Train Loss: 0.2085750906003846, Validation Loss: 1.252169132232666\n",
      "Epoch 184: Train Loss: 0.2019042745232582, Validation Loss: 1.2625467777252197\n",
      "Epoch 185: Train Loss: 0.20263170285357368, Validation Loss: 1.2740517854690552\n",
      "Epoch 186: Train Loss: 0.181131265229649, Validation Loss: 1.2943671941757202\n",
      "Epoch 187: Train Loss: 0.19048193262683022, Validation Loss: 1.311891794204712\n",
      "Epoch 188: Train Loss: 0.1554581117298868, Validation Loss: 1.3023326396942139\n",
      "Epoch 189: Train Loss: 0.18084112306435904, Validation Loss: 1.2678899765014648\n",
      "Epoch 190: Train Loss: 0.19311840915017658, Validation Loss: 1.2855581045150757\n",
      "Epoch 191: Train Loss: 0.18742907171448073, Validation Loss: 1.2746440172195435\n",
      "Epoch 192: Train Loss: 0.20068053156137466, Validation Loss: 1.312011957168579\n",
      "Epoch 193: Train Loss: 0.194113965663645, Validation Loss: 1.3224319219589233\n",
      "Epoch 194: Train Loss: 0.1810208467973603, Validation Loss: 1.3398829698562622\n",
      "Epoch 195: Train Loss: 0.16866047266456816, Validation Loss: 1.3409535884857178\n",
      "Epoch 196: Train Loss: 0.1650613190399276, Validation Loss: 1.369491457939148\n",
      "Epoch 197: Train Loss: 0.2079652895530065, Validation Loss: 1.3776719570159912\n",
      "Epoch 198: Train Loss: 0.19126617080635494, Validation Loss: 1.3721504211425781\n",
      "Epoch 199: Train Loss: 0.1910181012418535, Validation Loss: 1.3361237049102783\n",
      "Fold 12 Metrics:\n",
      "Accuracy: 0.36363636363636365, Precision: 0.4, Recall: 0.3333333333333333, F1-score: 0.36363636363636365, AUC: 0.36666666666666664\n",
      "Confusion Matrix:\n",
      "[[2 3]\n",
      " [4 2]]\n",
      "Completed fold 12\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples from subject 11 to test set\n",
      "Adding 6 truth samples from subject 11 to test set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.7294569942686293, Validation Loss: 0.6793316602706909\n",
      "Epoch 1: Train Loss: 0.7051962614059448, Validation Loss: 0.6719551086425781\n",
      "Epoch 2: Train Loss: 0.6653288139237298, Validation Loss: 0.6665723919868469\n",
      "Epoch 3: Train Loss: 0.6985842718018426, Validation Loss: 0.6636520028114319\n",
      "Epoch 4: Train Loss: 0.7020649313926697, Validation Loss: 0.6632663607597351\n",
      "Epoch 5: Train Loss: 0.672233349747128, Validation Loss: 0.6625586152076721\n",
      "Epoch 6: Train Loss: 0.6857445306248136, Validation Loss: 0.6615686416625977\n",
      "Epoch 7: Train Loss: 0.6850064661767747, Validation Loss: 0.6604682803153992\n",
      "Epoch 8: Train Loss: 0.661012225680881, Validation Loss: 0.660643994808197\n",
      "Epoch 9: Train Loss: 0.6622153189447191, Validation Loss: 0.6607522964477539\n",
      "Epoch 10: Train Loss: 0.6828636858198378, Validation Loss: 0.6553499102592468\n",
      "Epoch 11: Train Loss: 0.6517336832152473, Validation Loss: 0.6505479216575623\n",
      "Epoch 12: Train Loss: 0.6636314656999376, Validation Loss: 0.6462423205375671\n",
      "Epoch 13: Train Loss: 0.6411726176738739, Validation Loss: 0.6431617140769958\n",
      "Epoch 14: Train Loss: 0.6220479806264242, Validation Loss: 0.63896244764328\n",
      "Epoch 15: Train Loss: 0.6119704378975762, Validation Loss: 0.6371752619743347\n",
      "Epoch 16: Train Loss: 0.6307609412405226, Validation Loss: 0.636698305606842\n",
      "Epoch 17: Train Loss: 0.6431868275006613, Validation Loss: 0.6354743242263794\n",
      "Epoch 18: Train Loss: 0.642105221748352, Validation Loss: 0.6362007856369019\n",
      "Epoch 19: Train Loss: 0.6101304623815749, Validation Loss: 0.6377624869346619\n",
      "Epoch 20: Train Loss: 0.6245391832457649, Validation Loss: 0.6345450282096863\n",
      "Epoch 21: Train Loss: 0.6305471460024515, Validation Loss: 0.6344873309135437\n",
      "Epoch 22: Train Loss: 0.6156701511806912, Validation Loss: 0.6298123002052307\n",
      "Epoch 23: Train Loss: 0.5948777000109354, Validation Loss: 0.624789834022522\n",
      "Epoch 24: Train Loss: 0.6226549281014336, Validation Loss: 0.6233829855918884\n",
      "Epoch 25: Train Loss: 0.5836296346452501, Validation Loss: 0.6224285364151001\n",
      "Epoch 26: Train Loss: 0.5916541483667161, Validation Loss: 0.6210037469863892\n",
      "Epoch 27: Train Loss: 0.6001532276471456, Validation Loss: 0.6206204891204834\n",
      "Epoch 28: Train Loss: 0.5814986162715488, Validation Loss: 0.6205130219459534\n",
      "Epoch 29: Train Loss: 0.5822205874654982, Validation Loss: 0.6193476915359497\n",
      "Epoch 30: Train Loss: 0.5957246886359321, Validation Loss: 0.6172192096710205\n",
      "Epoch 31: Train Loss: 0.5932112336158752, Validation Loss: 0.6172037720680237\n",
      "Epoch 32: Train Loss: 0.5652144153912863, Validation Loss: 0.6103776693344116\n",
      "Epoch 33: Train Loss: 0.5714664922820197, Validation Loss: 0.6056663393974304\n",
      "Epoch 34: Train Loss: 0.586079455084271, Validation Loss: 0.6052182912826538\n",
      "Epoch 35: Train Loss: 0.542361299196879, Validation Loss: 0.6037151217460632\n",
      "Epoch 36: Train Loss: 0.5597385201189253, Validation Loss: 0.6044741272926331\n",
      "Epoch 37: Train Loss: 0.5643199616008334, Validation Loss: 0.6035724878311157\n",
      "Epoch 38: Train Loss: 0.552059272925059, Validation Loss: 0.6040880084037781\n",
      "Epoch 39: Train Loss: 0.5589306354522705, Validation Loss: 0.6018415689468384\n",
      "Epoch 40: Train Loss: 0.5661919679906633, Validation Loss: 0.6002681851387024\n",
      "Epoch 41: Train Loss: 0.535242971446779, Validation Loss: 0.5984335541725159\n",
      "Epoch 42: Train Loss: 0.5358064075311025, Validation Loss: 0.596937358379364\n",
      "Epoch 43: Train Loss: 0.5252733528614044, Validation Loss: 0.5908137559890747\n",
      "Epoch 44: Train Loss: 0.5336012740929922, Validation Loss: 0.5911371111869812\n",
      "Epoch 45: Train Loss: 0.5506913993093703, Validation Loss: 0.59214186668396\n",
      "Epoch 46: Train Loss: 0.5029103027449714, Validation Loss: 0.5912061333656311\n",
      "Epoch 47: Train Loss: 0.5165273182921939, Validation Loss: 0.5929694771766663\n",
      "Epoch 48: Train Loss: 0.5401622752348582, Validation Loss: 0.5932241678237915\n",
      "Epoch 49: Train Loss: 0.532894147766961, Validation Loss: 0.5912643671035767\n",
      "Epoch 50: Train Loss: 0.5309605565336015, Validation Loss: 0.586378812789917\n",
      "Epoch 51: Train Loss: 0.5287598934438493, Validation Loss: 0.5842607617378235\n",
      "Epoch 52: Train Loss: 0.492826435301039, Validation Loss: 0.5817722082138062\n",
      "Epoch 53: Train Loss: 0.5213479830159081, Validation Loss: 0.5804824829101562\n",
      "Epoch 54: Train Loss: 0.501212865114212, Validation Loss: 0.5780633091926575\n",
      "Epoch 55: Train Loss: 0.4728938274913364, Validation Loss: 0.5720471739768982\n",
      "Epoch 56: Train Loss: 0.48938477370474076, Validation Loss: 0.5710423588752747\n",
      "Epoch 57: Train Loss: 0.484806583987342, Validation Loss: 0.5731114149093628\n",
      "Epoch 58: Train Loss: 0.4988224109013875, Validation Loss: 0.5698760151863098\n",
      "Epoch 59: Train Loss: 0.4965044922298855, Validation Loss: 0.5713496208190918\n",
      "Epoch 60: Train Loss: 0.48979508876800537, Validation Loss: 0.5691238641738892\n",
      "Epoch 61: Train Loss: 0.4913717971907722, Validation Loss: 0.5688743591308594\n",
      "Epoch 62: Train Loss: 0.5084796614117093, Validation Loss: 0.5684828758239746\n",
      "Epoch 63: Train Loss: 0.49605004323853386, Validation Loss: 0.5693820118904114\n",
      "Epoch 64: Train Loss: 0.46342044075330097, Validation Loss: 0.5655705332756042\n",
      "Epoch 65: Train Loss: 0.4776851501729753, Validation Loss: 0.5653740763664246\n",
      "Epoch 66: Train Loss: 0.48011358910136753, Validation Loss: 0.5618463754653931\n",
      "Epoch 67: Train Loss: 0.47360582153002423, Validation Loss: 0.5632986426353455\n",
      "Epoch 68: Train Loss: 0.45938124921586776, Validation Loss: 0.5608111023902893\n",
      "Epoch 69: Train Loss: 0.4662033650610182, Validation Loss: 0.5638633966445923\n",
      "Epoch 70: Train Loss: 0.4692222972710927, Validation Loss: 0.5561007261276245\n",
      "Epoch 71: Train Loss: 0.45962173740069073, Validation Loss: 0.5582937598228455\n",
      "Epoch 72: Train Loss: 0.444887508948644, Validation Loss: 0.5600854158401489\n",
      "Epoch 73: Train Loss: 0.4638427495956421, Validation Loss: 0.5566611886024475\n",
      "Epoch 74: Train Loss: 0.45662516686651444, Validation Loss: 0.5583900213241577\n",
      "Epoch 75: Train Loss: 0.41267790065871346, Validation Loss: 0.5587047338485718\n",
      "Epoch 76: Train Loss: 0.45033559534284806, Validation Loss: 0.5527083873748779\n",
      "Epoch 77: Train Loss: 0.4311745862166087, Validation Loss: 0.5551187992095947\n",
      "Epoch 78: Train Loss: 0.4172659052742852, Validation Loss: 0.5550445318222046\n",
      "Epoch 79: Train Loss: 0.4447537395689223, Validation Loss: 0.5526189804077148\n",
      "Epoch 80: Train Loss: 0.4341194960806105, Validation Loss: 0.5519494414329529\n",
      "Epoch 81: Train Loss: 0.4317249655723572, Validation Loss: 0.5551530718803406\n",
      "Epoch 82: Train Loss: 0.42786668406592476, Validation Loss: 0.5528830289840698\n",
      "Epoch 83: Train Loss: 0.4291884203751882, Validation Loss: 0.5466979742050171\n",
      "Epoch 84: Train Loss: 0.41983382900555927, Validation Loss: 0.5488252639770508\n",
      "Epoch 85: Train Loss: 0.4224054283565945, Validation Loss: 0.5410206317901611\n",
      "Epoch 86: Train Loss: 0.43895451227823895, Validation Loss: 0.5400323271751404\n",
      "Epoch 87: Train Loss: 0.42198502355151707, Validation Loss: 0.5412763953208923\n",
      "Epoch 88: Train Loss: 0.40565647681554157, Validation Loss: 0.5375691056251526\n",
      "Epoch 89: Train Loss: 0.4074501362111833, Validation Loss: 0.540768563747406\n",
      "Epoch 90: Train Loss: 0.38464391231536865, Validation Loss: 0.5381336808204651\n",
      "Epoch 91: Train Loss: 0.4005839063061608, Validation Loss: 0.5368107557296753\n",
      "Epoch 92: Train Loss: 0.3988335099485185, Validation Loss: 0.5334151983261108\n",
      "Epoch 93: Train Loss: 0.3987111515469021, Validation Loss: 0.5250023603439331\n",
      "Epoch 94: Train Loss: 0.3849160538779365, Validation Loss: 0.5229718089103699\n",
      "Epoch 95: Train Loss: 0.39587630497084725, Validation Loss: 0.5139402151107788\n",
      "Epoch 96: Train Loss: 0.37774131695429486, Validation Loss: 0.5131453275680542\n",
      "Epoch 97: Train Loss: 0.37912123070822823, Validation Loss: 0.5125265717506409\n",
      "Epoch 98: Train Loss: 0.39117039574517143, Validation Loss: 0.5124080181121826\n",
      "Epoch 99: Train Loss: 0.3849557273917728, Validation Loss: 0.5092324018478394\n",
      "Epoch 100: Train Loss: 0.3678209152486589, Validation Loss: 0.5009588599205017\n",
      "Epoch 101: Train Loss: 0.37180903885099625, Validation Loss: 0.4925277531147003\n",
      "Epoch 102: Train Loss: 0.3594791971974903, Validation Loss: 0.4913264513015747\n",
      "Epoch 103: Train Loss: 0.3484095268779331, Validation Loss: 0.48476627469062805\n",
      "Epoch 104: Train Loss: 0.34089697235160404, Validation Loss: 0.4765034317970276\n",
      "Epoch 105: Train Loss: 0.35234622491730583, Validation Loss: 0.4696022868156433\n",
      "Epoch 106: Train Loss: 0.3734236690733168, Validation Loss: 0.4610435664653778\n",
      "Epoch 107: Train Loss: 0.36145373847749496, Validation Loss: 0.46277666091918945\n",
      "Epoch 108: Train Loss: 0.37206243144141304, Validation Loss: 0.46036049723625183\n",
      "Epoch 109: Train Loss: 0.32000549634297687, Validation Loss: 0.45920902490615845\n",
      "Epoch 110: Train Loss: 0.3897682378689448, Validation Loss: 0.4477807581424713\n",
      "Epoch 111: Train Loss: 0.36128003398577374, Validation Loss: 0.44407567381858826\n",
      "Epoch 112: Train Loss: 0.33264197243584526, Validation Loss: 0.43424931168556213\n",
      "Epoch 113: Train Loss: 0.33960123360157013, Validation Loss: 0.4296171963214874\n",
      "Epoch 114: Train Loss: 0.3743247588475545, Validation Loss: 0.42409321665763855\n",
      "Epoch 115: Train Loss: 0.3305552336904738, Validation Loss: 0.4236968159675598\n",
      "Epoch 116: Train Loss: 0.34599905874994064, Validation Loss: 0.420910120010376\n",
      "Epoch 117: Train Loss: 0.35597148537635803, Validation Loss: 0.418470561504364\n",
      "Epoch 118: Train Loss: 0.3447156912750668, Validation Loss: 0.42088544368743896\n",
      "Epoch 119: Train Loss: 0.3401751716931661, Validation Loss: 0.42094188928604126\n",
      "Epoch 120: Train Loss: 0.33282500836584306, Validation Loss: 0.4086333215236664\n",
      "Epoch 121: Train Loss: 0.3482404235336516, Validation Loss: 0.39676588773727417\n",
      "Epoch 122: Train Loss: 0.33172348307238686, Validation Loss: 0.37775278091430664\n",
      "Epoch 123: Train Loss: 0.30733262333605027, Validation Loss: 0.3676000237464905\n",
      "Epoch 124: Train Loss: 0.3204635547267066, Validation Loss: 0.3673320710659027\n",
      "Epoch 125: Train Loss: 0.31429266267352635, Validation Loss: 0.37223613262176514\n",
      "Epoch 126: Train Loss: 0.3059428168667687, Validation Loss: 0.3722522258758545\n",
      "Epoch 127: Train Loss: 0.3191187034050624, Validation Loss: 0.3725317418575287\n",
      "Epoch 128: Train Loss: 0.34298864669269985, Validation Loss: 0.37164950370788574\n",
      "Epoch 129: Train Loss: 0.32070033417807686, Validation Loss: 0.3716909885406494\n",
      "Epoch 130: Train Loss: 0.3010657611820433, Validation Loss: 0.3728060722351074\n",
      "Epoch 131: Train Loss: 0.2863616231415007, Validation Loss: 0.36365270614624023\n",
      "Epoch 132: Train Loss: 0.3004220889674293, Validation Loss: 0.3609957695007324\n",
      "Epoch 133: Train Loss: 0.3079403208361732, Validation Loss: 0.3564959466457367\n",
      "Epoch 134: Train Loss: 0.3073430425590939, Validation Loss: 0.350845605134964\n",
      "Epoch 135: Train Loss: 0.3004443214999305, Validation Loss: 0.34749525785446167\n",
      "Epoch 136: Train Loss: 0.2733459340201484, Validation Loss: 0.35460007190704346\n",
      "Epoch 137: Train Loss: 0.283317727347215, Validation Loss: 0.3487294614315033\n",
      "Epoch 138: Train Loss: 0.3233686519993676, Validation Loss: 0.34769582748413086\n",
      "Epoch 139: Train Loss: 0.27339115076594883, Validation Loss: 0.34664472937583923\n",
      "Epoch 140: Train Loss: 0.2933702568213145, Validation Loss: 0.3464433550834656\n",
      "Epoch 141: Train Loss: 0.26640843517250484, Validation Loss: 0.3440127372741699\n",
      "Epoch 142: Train Loss: 0.26520933045281303, Validation Loss: 0.35074150562286377\n",
      "Epoch 143: Train Loss: 0.2608228176832199, Validation Loss: 0.34843823313713074\n",
      "Epoch 144: Train Loss: 0.29654981361495125, Validation Loss: 0.3490135669708252\n",
      "Epoch 145: Train Loss: 0.27920281555917525, Validation Loss: 0.34311532974243164\n",
      "Epoch 146: Train Loss: 0.2801256676514943, Validation Loss: 0.3421066701412201\n",
      "Epoch 147: Train Loss: 0.2673758483595318, Validation Loss: 0.340816855430603\n",
      "Epoch 148: Train Loss: 0.2652094347609414, Validation Loss: 0.33850687742233276\n",
      "Epoch 149: Train Loss: 0.2710401664177577, Validation Loss: 0.33922234177589417\n",
      "Epoch 150: Train Loss: 0.2809987540046374, Validation Loss: 0.3358202874660492\n",
      "Epoch 151: Train Loss: 0.3080955131186379, Validation Loss: 0.3243151605129242\n",
      "Epoch 152: Train Loss: 0.27245934969849056, Validation Loss: 0.3228370249271393\n",
      "Epoch 153: Train Loss: 0.2551064093907674, Validation Loss: 0.32220175862312317\n",
      "Epoch 154: Train Loss: 0.2606551812754737, Validation Loss: 0.3141712248325348\n",
      "Epoch 155: Train Loss: 0.2639060053560469, Validation Loss: 0.3173674941062927\n",
      "Epoch 156: Train Loss: 0.24847539431518978, Validation Loss: 0.31628063321113586\n",
      "Epoch 157: Train Loss: 0.2492778996626536, Validation Loss: 0.31451356410980225\n",
      "Epoch 158: Train Loss: 0.24556535482406616, Validation Loss: 0.31287673115730286\n",
      "Epoch 159: Train Loss: 0.24399562345610726, Validation Loss: 0.31512248516082764\n",
      "Epoch 160: Train Loss: 0.25818854239251876, Validation Loss: 0.31143906712532043\n",
      "Epoch 161: Train Loss: 0.28454751272996265, Validation Loss: 0.31167924404144287\n",
      "Epoch 162: Train Loss: 0.25353747771845925, Validation Loss: 0.32151365280151367\n",
      "Epoch 163: Train Loss: 0.24088430570231545, Validation Loss: 0.3334972858428955\n",
      "Epoch 164: Train Loss: 0.23272383709748587, Validation Loss: 0.32989925146102905\n",
      "Epoch 165: Train Loss: 0.25783272253142464, Validation Loss: 0.3211771249771118\n",
      "Epoch 166: Train Loss: 0.29082142147752976, Validation Loss: 0.3168196380138397\n",
      "Epoch 167: Train Loss: 0.28008435004287296, Validation Loss: 0.3152312636375427\n",
      "Epoch 168: Train Loss: 0.24759547246827018, Validation Loss: 0.3132229745388031\n",
      "Epoch 169: Train Loss: 0.2616419080230925, Validation Loss: 0.3118695616722107\n",
      "Epoch 170: Train Loss: 0.2423491444852617, Validation Loss: 0.3151446580886841\n",
      "Epoch 171: Train Loss: 0.23755534655518, Validation Loss: 0.31770244240760803\n",
      "Epoch 172: Train Loss: 0.24682772987418705, Validation Loss: 0.30895766615867615\n",
      "Epoch 173: Train Loss: 0.2827654878298442, Validation Loss: 0.30408817529678345\n",
      "Epoch 174: Train Loss: 0.23492088582780626, Validation Loss: 0.30553334951400757\n",
      "Epoch 175: Train Loss: 0.22531098789638943, Validation Loss: 0.2997642159461975\n",
      "Epoch 176: Train Loss: 0.24093940026230282, Validation Loss: 0.29924923181533813\n",
      "Epoch 177: Train Loss: 0.239045027229521, Validation Loss: 0.2914743721485138\n",
      "Epoch 178: Train Loss: 0.23276149895456102, Validation Loss: 0.29254812002182007\n",
      "Epoch 179: Train Loss: 0.26408091021908653, Validation Loss: 0.29129350185394287\n",
      "Epoch 180: Train Loss: 0.24603113366497886, Validation Loss: 0.29262563586235046\n",
      "Epoch 181: Train Loss: 0.21594767769177756, Validation Loss: 0.279206246137619\n",
      "Epoch 182: Train Loss: 0.25341249836815727, Validation Loss: 0.27375566959381104\n",
      "Epoch 183: Train Loss: 0.2352354046371248, Validation Loss: 0.265058308839798\n",
      "Epoch 184: Train Loss: 0.21330191526148054, Validation Loss: 0.2637068033218384\n",
      "Epoch 185: Train Loss: 0.24567792895767424, Validation Loss: 0.26598894596099854\n",
      "Epoch 186: Train Loss: 0.23756222592459786, Validation Loss: 0.2665501832962036\n",
      "Epoch 187: Train Loss: 0.2020529160896937, Validation Loss: 0.2684049606323242\n",
      "Epoch 188: Train Loss: 0.22086160712771946, Validation Loss: 0.2674678564071655\n",
      "Epoch 189: Train Loss: 0.2066091067261166, Validation Loss: 0.26516321301460266\n",
      "Epoch 190: Train Loss: 0.24682869017124176, Validation Loss: 0.26631325483322144\n",
      "Epoch 191: Train Loss: 0.2544047733147939, Validation Loss: 0.2824781835079193\n",
      "Epoch 192: Train Loss: 0.19145667470163769, Validation Loss: 0.288174033164978\n",
      "Epoch 193: Train Loss: 0.22945256531238556, Validation Loss: 0.2861747741699219\n",
      "Epoch 194: Train Loss: 0.2344418250852161, Validation Loss: 0.2837981581687927\n",
      "Epoch 195: Train Loss: 0.21200060016579098, Validation Loss: 0.27779990434646606\n",
      "Epoch 196: Train Loss: 0.2175273886985249, Validation Loss: 0.27674397826194763\n",
      "Epoch 197: Train Loss: 0.27714922693040633, Validation Loss: 0.2759774923324585\n",
      "Epoch 198: Train Loss: 0.20006436026758617, Validation Loss: 0.2786048650741577\n",
      "Epoch 199: Train Loss: 0.19547824892732832, Validation Loss: 0.277662456035614\n",
      "Fold 13 Metrics:\n",
      "Accuracy: 0.9090909090909091, Precision: 1.0, Recall: 0.8333333333333334, F1-score: 0.9090909090909091, AUC: 0.9166666666666667\n",
      "Confusion Matrix:\n",
      "[[5 0]\n",
      " [1 5]]\n",
      "Completed fold 13\n",
      "--------------------------------------------------\n",
      "Overall Metrics:\n",
      "Accuracy: 0.5104895104895105, Precision: 0.5465116279069767, Recall: 0.6025641025641025, F1-score: 0.573170731707317, AUC: 0.5012820512820513\n",
      "Overall Confusion Matrix:\n",
      "[[26 39]\n",
      " [31 47]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "model_save_dir = r'C:\\Users\\User\\Documents\\Lie detect data\\Model'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion Matrix'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='viridis', xticklabels=['lie', 'truth'], yticklabels=['lie', 'truth'])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.show()\n",
    "\n",
    "# Define a function to load and pad data\n",
    "def load_data(data_dir, max_length):\n",
    "    subject_data = {'lie': {}, 'truth': {}}\n",
    "    \n",
    "    file_list = os.listdir(data_dir)\n",
    "    \n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Determine if the file is 'truth' or 'lie'\n",
    "        label_type = 'truth' if 'truth' in file else 'lie'\n",
    "        subj_id = int(file.split('_')[-1].split('.')[0])\n",
    "        \n",
    "        # Grouping logic\n",
    "        if label_type == 'lie':\n",
    "            # Mapping each 5 lie samples to one subject\n",
    "            subject_key = (subj_id - 1) // 5 + 1\n",
    "        else:  # 'truth'\n",
    "            # Mapping each 6 truth samples to one subject\n",
    "            subject_key = (subj_id - 1) // 6 + 1\n",
    "            \n",
    "        # Initialize the subject's list if it doesn't exist\n",
    "        if subject_key not in subject_data[label_type]:\n",
    "            subject_data[label_type][subject_key] = []\n",
    "            \n",
    "        # Extract only the portion of the data from time length 3000 to 3750\n",
    "        data_subset = data[:, 0:3750]\n",
    "\n",
    "        # Pad or truncate the data to match max_length\n",
    "        if data_subset.shape[1] > max_length:\n",
    "            processed_data = data_subset[:, :max_length]  # Truncate if it exceeds max_length\n",
    "        else:\n",
    "            processed_data = np.zeros((data_subset.shape[0], max_length))\n",
    "            processed_data[:, :data_subset.shape[1]] = data_subset  # Pad if it is shorter than max_length\n",
    "        \n",
    "        # Add the processed data to the appropriate list\n",
    "        subject_data[label_type][subject_key].append(processed_data)\n",
    "    \n",
    "    return subject_data\n",
    "\n",
    "# Load dataset and pad/truncate the data\n",
    "data_dir = \"C:\\\\Users\\\\User\\\\Documents\\\\Lie detect data\\\\7M_EEGData\"\n",
    "max_length = 3750 # Define maximum length for padding\n",
    "subject_data = load_data(data_dir, max_length)\n",
    "\n",
    "# Count the total number of samples\n",
    "num_lie_samples = sum(len(subject_data['lie'][subject_key]) for subject_key in subject_data['lie'])\n",
    "num_truth_samples = sum(len(subject_data['truth'][subject_key]) for subject_key in subject_data['truth'])\n",
    "\n",
    "print(f\"Number of 'lie' samples: {num_lie_samples}\")\n",
    "print(f\"Number of 'truth' samples: {num_truth_samples}\")\n",
    "print(f\"Total number of samples: {num_lie_samples + num_truth_samples}\")\n",
    "\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure the data is reshaped to [1, Chans, Samples] for Conv2d input\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32).unsqueeze(0), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# New EEGNet Model Definition in PyTorch\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, nb_classes, Chans=65, Samples=3750,\n",
    "                 dropoutRate=0.5, kernLength=125, F1=8, \n",
    "                 D=2, F2=16, norm_rate=0.25, dropoutType='Dropout'):\n",
    "        super(EEGNet, self).__init__()\n",
    "\n",
    "        if dropoutType == 'SpatialDropout2D':\n",
    "            dropoutLayer = nn.Dropout2d\n",
    "        elif dropoutType == 'Dropout':\n",
    "            dropoutLayer = nn.Dropout\n",
    "        else:\n",
    "            raise ValueError('dropoutType must be one of \"SpatialDropout2D\" or \"Dropout\".')\n",
    "\n",
    "        # Block 1\n",
    "        self.conv1 = nn.Conv2d(1, F1, (1, kernLength), padding='same', bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "        \n",
    "        self.depthwiseConv = nn.Conv2d(F1, F1 * D, (Chans, 1), \n",
    "                                       groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(F1 * D)\n",
    "        self.elu = nn.ELU()\n",
    "        self.avgpool1 = nn.AvgPool2d((1, 4))\n",
    "        self.dropout1 = dropoutLayer(dropoutRate)\n",
    "\n",
    "        # Block 2\n",
    "        self.separableConv = nn.Sequential(\n",
    "            nn.Conv2d(F1 * D, F1 * D, (1, 16), padding='same', groups=F1 * D, bias=False),\n",
    "            nn.Conv2d(F1 * D, F2, (1, 1), bias=False)\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(F2)\n",
    "        self.avgpool2 = nn.AvgPool2d((1, 8))\n",
    "        self.dropout2 = dropoutLayer(dropoutRate)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(F2 * (Samples // (4 * 8)), nb_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input block\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.depthwiseConv(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.avgpool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Block 2\n",
    "        x = self.separableConv(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.avgpool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Flatten before the fully connected layer\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        # Fully connected output\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_and_evaluate(train_loader, val_loader, y_train):\n",
    "    # Initialize the updated EEGNet model\n",
    "    model = EEGNet(nb_classes=2, Chans=65, Samples=3750,\n",
    "                   dropoutRate=0.5, kernLength=125, F1=8, D=2, F2=16, norm_rate=0.25, dropoutType='Dropout').to(device)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "    num_epochs = 200\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 200\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "            fold_model_path = os.path.join(model_save_dir, f'revise_model_fold_{fold_idx}.pth')\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': best_val_loss,\n",
    "            }, fold_model_path)\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize the cross-validation\n",
    "subject_ids = list(range(1, 14))  # Since there are 13 subjects for lie and truth\n",
    "random.shuffle(subject_ids)  # Shuffle to ensure random selection\n",
    "\n",
    "# Initialize arrays to store all labels, predictions, and metrics\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "fold_accuracies = []\n",
    "fold_precisions = []\n",
    "fold_recalls = []\n",
    "fold_f1_scores = []\n",
    "fold_aucs = []\n",
    "fold_conf_matrices = []\n",
    "\n",
    "for fold_idx in range(13):\n",
    "    # Split subjects into test and training sets\n",
    "    test_subject = subject_ids[fold_idx]\n",
    "\n",
    "    # Prepare training and test data\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for subject_id in subject_ids:\n",
    "        lie_samples = subject_data['lie'].get(subject_id, [])\n",
    "        truth_samples = subject_data['truth'].get(subject_id, [])\n",
    "\n",
    "        if subject_id == test_subject:\n",
    "            X_test.extend(lie_samples)\n",
    "            y_test.extend([0] * len(lie_samples))\n",
    "            print(f\"Adding {len(lie_samples)} lie samples from subject {subject_id} to test set\")\n",
    "            X_test.extend(truth_samples)\n",
    "            y_test.extend([1] * len(truth_samples))\n",
    "            print(f\"Adding {len(truth_samples)} truth samples from subject {subject_id} to test set\")\n",
    "        else:\n",
    "            X_train.extend(lie_samples)\n",
    "            y_train.extend([0] * len(lie_samples))\n",
    "            X_train.extend(truth_samples)\n",
    "            y_train.extend([1] * len(truth_samples))\n",
    "            print(f\"Adding {len(lie_samples)} lie samples and {len(truth_samples)} truth samples from subject {subject_id} to train set\")\n",
    "\n",
    "    print(f\"Total training samples: {len(X_train)}, Total test samples: {len(X_test)}\")\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    # Normalize data using scaler fitted on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_test = scaler.transform(X_test.reshape(X_test.shape[0], -1))\n",
    "    X_train = X_train.reshape(-1, 65, max_length)\n",
    "    X_test = X_test.reshape(-1, 65, max_length)\n",
    "\n",
    "    print(X_test.shape)\n",
    "    print(X_train.shape)\n",
    "\n",
    "    # Save the scaler to a file\n",
    "    with open(r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\RevisedEEGNet_scaler_{}.pkl'.format(fold_idx), 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = EEGDataset(X_train, y_train)\n",
    "    test_dataset = EEGDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    model = train_and_evaluate(train_loader, test_loader, y_train)\n",
    "\n",
    "    # Evaluate on the test set and calculate metrics\n",
    "    model.eval()\n",
    "    fold_labels = []\n",
    "    fold_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            fold_labels.extend(y_batch.cpu().numpy())\n",
    "            fold_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics for the current fold\n",
    "    accuracy = accuracy_score(fold_labels, fold_predictions)\n",
    "    precision = precision_score(fold_labels, fold_predictions)\n",
    "    recall = recall_score(fold_labels, fold_predictions)\n",
    "    f1 = f1_score(fold_labels, fold_predictions)\n",
    "    auc = roc_auc_score(fold_labels, fold_predictions)\n",
    "    conf_matrix = confusion_matrix(fold_labels, fold_predictions)\n",
    "\n",
    "    # Store fold metrics\n",
    "    fold_accuracies.append(accuracy)\n",
    "    fold_precisions.append(precision)\n",
    "    fold_recalls.append(recall)\n",
    "    fold_f1_scores.append(f1)\n",
    "    fold_aucs.append(auc)\n",
    "    fold_conf_matrices.append(conf_matrix)\n",
    "\n",
    "    # Print metrics for the current fold\n",
    "    print(f'Fold {fold_idx + 1} Metrics:')\n",
    "    print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Aggregate all labels and predictions for final evaluation across all folds\n",
    "    all_labels.extend(fold_labels)\n",
    "    all_predictions.extend(fold_predictions)\n",
    "\n",
    "    print(f'Completed fold {fold_idx + 1}')\n",
    "    print('-' * 50)\n",
    "\n",
    "# Calculate overall metrics across all folds\n",
    "overall_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "overall_precision = precision_score(all_labels, all_predictions)\n",
    "overall_recall = recall_score(all_labels, all_predictions)\n",
    "overall_f1 = f1_score(all_labels, all_predictions)\n",
    "overall_auc = roc_auc_score(all_labels, all_predictions)\n",
    "overall_conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(f'Overall Metrics:')\n",
    "print(f'Accuracy: {overall_accuracy}, Precision: {overall_precision}, Recall: {overall_recall}, F1-score: {overall_f1}, AUC: {overall_auc}')\n",
    "print('Overall Confusion Matrix:')\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "plot_confusion_matrix(overall_conf_matrix, title='Overall Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae639325-1800-4763-ad37-b5931e4391c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
