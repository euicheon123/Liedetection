{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d5ee57-cf0b-4330-985e-ada385863b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'lie' samples: 65\n",
      "Number of 'truth' samples: 78\n",
      "Total number of samples: 143\n",
      "Adding 5 lie samples from subject 8 to test set\n",
      "Adding 6 truth samples from subject 8 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 250)\n",
      "(132, 65, 250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\detectlie\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Convolution.cpp:1032.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.6923330545425415, Validation Loss: 0.6739305257797241\n",
      "Epoch 1: Train Loss: 0.686963415145874, Validation Loss: 0.6614761352539062\n",
      "Epoch 2: Train Loss: 0.7412827014923096, Validation Loss: 0.6469988226890564\n",
      "Epoch 3: Train Loss: 0.6600551009178162, Validation Loss: 0.6298553347587585\n",
      "Epoch 4: Train Loss: 0.6549185931682586, Validation Loss: 0.6220439076423645\n",
      "Epoch 5: Train Loss: 0.6586509704589844, Validation Loss: 0.6122717261314392\n",
      "Epoch 6: Train Loss: 0.6600031793117523, Validation Loss: 0.6051663160324097\n",
      "Epoch 7: Train Loss: 0.692110288143158, Validation Loss: 0.601341962814331\n",
      "Epoch 8: Train Loss: 0.7343206167221069, Validation Loss: 0.5922523140907288\n",
      "Epoch 9: Train Loss: 0.6661735415458679, Validation Loss: 0.5929101705551147\n",
      "Epoch 10: Train Loss: 0.7078230142593384, Validation Loss: 0.5932563543319702\n",
      "Epoch 11: Train Loss: 0.6380539238452911, Validation Loss: 0.592065691947937\n",
      "Epoch 12: Train Loss: 0.6829540967941284, Validation Loss: 0.5985586643218994\n",
      "Epoch 13: Train Loss: 0.653372061252594, Validation Loss: 0.6030022501945496\n",
      "Epoch 14: Train Loss: 0.681771171092987, Validation Loss: 0.5988978743553162\n",
      "Epoch 15: Train Loss: 0.6990732908248901, Validation Loss: 0.594070315361023\n",
      "Epoch 16: Train Loss: 0.630087411403656, Validation Loss: 0.5987173914909363\n",
      "Epoch 17: Train Loss: 0.6430053591728211, Validation Loss: 0.599709689617157\n",
      "Epoch 18: Train Loss: 0.6685460686683655, Validation Loss: 0.5991581678390503\n",
      "Epoch 19: Train Loss: 0.6399714112281799, Validation Loss: 0.5969874858856201\n",
      "Epoch 20: Train Loss: 0.683656883239746, Validation Loss: 0.5946422219276428\n",
      "Epoch 21: Train Loss: 0.7102259039878845, Validation Loss: 0.5898135900497437\n",
      "Epoch 22: Train Loss: 0.6587262749671936, Validation Loss: 0.5902440547943115\n",
      "Epoch 23: Train Loss: 0.6509195804595947, Validation Loss: 0.5908352136611938\n",
      "Epoch 24: Train Loss: 0.6334319472312927, Validation Loss: 0.5859706401824951\n",
      "Epoch 25: Train Loss: 0.6343700408935546, Validation Loss: 0.579173743724823\n",
      "Epoch 26: Train Loss: 0.652642571926117, Validation Loss: 0.578037679195404\n",
      "Epoch 27: Train Loss: 0.6955844283103942, Validation Loss: 0.5755410194396973\n",
      "Epoch 28: Train Loss: 0.6711722016334534, Validation Loss: 0.5777286291122437\n",
      "Epoch 29: Train Loss: 0.6076003968715668, Validation Loss: 0.5835304856300354\n",
      "Epoch 30: Train Loss: 0.6541220903396606, Validation Loss: 0.5777488350868225\n",
      "Epoch 31: Train Loss: 0.6194165170192718, Validation Loss: 0.573252260684967\n",
      "Epoch 32: Train Loss: 0.6341155767440796, Validation Loss: 0.5711921453475952\n",
      "Epoch 33: Train Loss: 0.6671257376670837, Validation Loss: 0.5658028721809387\n",
      "Epoch 34: Train Loss: 0.6015473484992981, Validation Loss: 0.5634534358978271\n",
      "Epoch 35: Train Loss: 0.6837595462799072, Validation Loss: 0.5665709972381592\n",
      "Epoch 36: Train Loss: 0.674052095413208, Validation Loss: 0.5623574256896973\n",
      "Epoch 37: Train Loss: 0.6918338418006897, Validation Loss: 0.5625758767127991\n",
      "Epoch 38: Train Loss: 0.6586551666259766, Validation Loss: 0.5659539699554443\n",
      "Epoch 39: Train Loss: 0.6028611421585083, Validation Loss: 0.5638428330421448\n",
      "Epoch 40: Train Loss: 0.6309978008270264, Validation Loss: 0.5584582686424255\n",
      "Epoch 41: Train Loss: 0.6302666306495667, Validation Loss: 0.557995080947876\n",
      "Epoch 42: Train Loss: 0.6949802279472351, Validation Loss: 0.5559937953948975\n",
      "Epoch 43: Train Loss: 0.7011923432350159, Validation Loss: 0.5488143563270569\n",
      "Epoch 44: Train Loss: 0.6551333069801331, Validation Loss: 0.5509978532791138\n",
      "Epoch 45: Train Loss: 0.6297984838485717, Validation Loss: 0.5500873327255249\n",
      "Epoch 46: Train Loss: 0.6939709663391114, Validation Loss: 0.5497952103614807\n",
      "Epoch 47: Train Loss: 0.6106582522392273, Validation Loss: 0.5383481383323669\n",
      "Epoch 48: Train Loss: 0.6441868662834167, Validation Loss: 0.5402544736862183\n",
      "Epoch 49: Train Loss: 0.6377379059791565, Validation Loss: 0.5461676120758057\n",
      "Epoch 50: Train Loss: 0.6121565937995911, Validation Loss: 0.5429490208625793\n",
      "Epoch 51: Train Loss: 0.6636436104774475, Validation Loss: 0.5380308628082275\n",
      "Epoch 52: Train Loss: 0.6392922401428223, Validation Loss: 0.5268639922142029\n",
      "Epoch 53: Train Loss: 0.6351072192192078, Validation Loss: 0.5367853045463562\n",
      "Epoch 54: Train Loss: 0.6255126476287842, Validation Loss: 0.5377760529518127\n",
      "Epoch 55: Train Loss: 0.61273695230484, Validation Loss: 0.5383287072181702\n",
      "Epoch 56: Train Loss: 0.6161233067512513, Validation Loss: 0.5278719067573547\n",
      "Epoch 57: Train Loss: 0.7060736298561097, Validation Loss: 0.5286184549331665\n",
      "Epoch 58: Train Loss: 0.6028693318367004, Validation Loss: 0.5266502499580383\n",
      "Epoch 59: Train Loss: 0.6102505564689636, Validation Loss: 0.5284823179244995\n",
      "Epoch 60: Train Loss: 0.6517227053642273, Validation Loss: 0.5240401029586792\n",
      "Epoch 61: Train Loss: 0.5759158849716186, Validation Loss: 0.515631914138794\n",
      "Epoch 62: Train Loss: 0.5965188503265381, Validation Loss: 0.514945924282074\n",
      "Epoch 63: Train Loss: 0.5688796579837799, Validation Loss: 0.5122498869895935\n",
      "Epoch 64: Train Loss: 0.6029915809631348, Validation Loss: 0.5170301795005798\n",
      "Epoch 65: Train Loss: 0.6328057646751404, Validation Loss: 0.5139497518539429\n",
      "Epoch 66: Train Loss: 0.5983907461166382, Validation Loss: 0.5100515484809875\n",
      "Epoch 67: Train Loss: 0.67872394323349, Validation Loss: 0.5183268189430237\n",
      "Epoch 68: Train Loss: 0.6015087187290191, Validation Loss: 0.508562445640564\n",
      "Epoch 69: Train Loss: 0.6375637114048004, Validation Loss: 0.5019987225532532\n",
      "Epoch 70: Train Loss: 0.5624251067638397, Validation Loss: 0.49920395016670227\n",
      "Epoch 71: Train Loss: 0.590064287185669, Validation Loss: 0.49508658051490784\n",
      "Epoch 72: Train Loss: 0.6150567412376404, Validation Loss: 0.48658639192581177\n",
      "Epoch 73: Train Loss: 0.60402410030365, Validation Loss: 0.48255109786987305\n",
      "Epoch 74: Train Loss: 0.6500539779663086, Validation Loss: 0.48321568965911865\n",
      "Epoch 75: Train Loss: 0.5895871639251709, Validation Loss: 0.48441240191459656\n",
      "Epoch 76: Train Loss: 0.6399479031562805, Validation Loss: 0.483378529548645\n",
      "Epoch 77: Train Loss: 0.5572246193885804, Validation Loss: 0.480957567691803\n",
      "Epoch 78: Train Loss: 0.6257604598999024, Validation Loss: 0.479365736246109\n",
      "Epoch 79: Train Loss: 0.5639709651470184, Validation Loss: 0.4850058853626251\n",
      "Epoch 80: Train Loss: 0.5860066771507263, Validation Loss: 0.4855150878429413\n",
      "Epoch 81: Train Loss: 0.5739384770393372, Validation Loss: 0.4858972728252411\n",
      "Epoch 82: Train Loss: 0.5672390222549438, Validation Loss: 0.48001909255981445\n",
      "Epoch 83: Train Loss: 0.5616340637207031, Validation Loss: 0.47789841890335083\n",
      "Epoch 84: Train Loss: 0.6082221984863281, Validation Loss: 0.470868855714798\n",
      "Epoch 85: Train Loss: 0.610103189945221, Validation Loss: 0.47288912534713745\n",
      "Epoch 86: Train Loss: 0.5872885227203369, Validation Loss: 0.46309614181518555\n",
      "Epoch 87: Train Loss: 0.5162815690040589, Validation Loss: 0.46781212091445923\n",
      "Epoch 88: Train Loss: 0.5809400677680969, Validation Loss: 0.46060749888420105\n",
      "Epoch 89: Train Loss: 0.5713106513023376, Validation Loss: 0.4624806046485901\n",
      "Epoch 90: Train Loss: 0.6172698915004731, Validation Loss: 0.46324989199638367\n",
      "Epoch 91: Train Loss: 0.5911520183086395, Validation Loss: 0.4654681086540222\n",
      "Epoch 92: Train Loss: 0.5842398822307586, Validation Loss: 0.4667778015136719\n",
      "Epoch 93: Train Loss: 0.5843061327934265, Validation Loss: 0.4574134647846222\n",
      "Epoch 94: Train Loss: 0.641540813446045, Validation Loss: 0.4565485119819641\n",
      "Epoch 95: Train Loss: 0.5717750191688538, Validation Loss: 0.45648688077926636\n",
      "Epoch 96: Train Loss: 0.5112233698368073, Validation Loss: 0.4588913023471832\n",
      "Epoch 97: Train Loss: 0.5670293211936951, Validation Loss: 0.4463809132575989\n",
      "Epoch 98: Train Loss: 0.5660716891288757, Validation Loss: 0.44114482402801514\n",
      "Epoch 99: Train Loss: 0.6197118043899537, Validation Loss: 0.4418922960758209\n",
      "Epoch 100: Train Loss: 0.6222117245197296, Validation Loss: 0.44972285628318787\n",
      "Epoch 101: Train Loss: 0.5468561947345734, Validation Loss: 0.4513802230358124\n",
      "Epoch 102: Train Loss: 0.6503812551498414, Validation Loss: 0.4494306445121765\n",
      "Epoch 103: Train Loss: 0.5287479519844055, Validation Loss: 0.44284602999687195\n",
      "Epoch 104: Train Loss: 0.5715959429740906, Validation Loss: 0.4493180215358734\n",
      "Epoch 105: Train Loss: 0.5468373537063599, Validation Loss: 0.4484937787055969\n",
      "Epoch 106: Train Loss: 0.5558885514736176, Validation Loss: 0.4499482810497284\n",
      "Epoch 107: Train Loss: 0.5863293766975403, Validation Loss: 0.4589642584323883\n",
      "Epoch 108: Train Loss: 0.519366717338562, Validation Loss: 0.456686794757843\n",
      "Epoch 109: Train Loss: 0.5409920036792755, Validation Loss: 0.4582204520702362\n",
      "Epoch 110: Train Loss: 0.546299010515213, Validation Loss: 0.4447405934333801\n",
      "Epoch 111: Train Loss: 0.552877914905548, Validation Loss: 0.4406673312187195\n",
      "Epoch 112: Train Loss: 0.5831186413764954, Validation Loss: 0.43684738874435425\n",
      "Epoch 113: Train Loss: 0.4982846736907959, Validation Loss: 0.44582679867744446\n",
      "Epoch 114: Train Loss: 0.5540238559246063, Validation Loss: 0.4406023621559143\n",
      "Epoch 115: Train Loss: 0.517325246334076, Validation Loss: 0.4491148591041565\n",
      "Epoch 116: Train Loss: 0.5345985412597656, Validation Loss: 0.442348450422287\n",
      "Epoch 117: Train Loss: 0.5947990953922272, Validation Loss: 0.4427529275417328\n",
      "Epoch 118: Train Loss: 0.5237879693508148, Validation Loss: 0.4434325098991394\n",
      "Epoch 119: Train Loss: 0.5817150831222534, Validation Loss: 0.44924622774124146\n",
      "Epoch 120: Train Loss: 0.5876651048660279, Validation Loss: 0.43712958693504333\n",
      "Epoch 121: Train Loss: 0.4923563539981842, Validation Loss: 0.43460527062416077\n",
      "Epoch 122: Train Loss: 0.5143119335174561, Validation Loss: 0.4301424026489258\n",
      "Epoch 123: Train Loss: 0.5134361684322357, Validation Loss: 0.42181405425071716\n",
      "Epoch 124: Train Loss: 0.5464678049087525, Validation Loss: 0.42434683442115784\n",
      "Epoch 125: Train Loss: 0.5380925953388214, Validation Loss: 0.42733755707740784\n",
      "Epoch 126: Train Loss: 0.5203789532184601, Validation Loss: 0.42866840958595276\n",
      "Epoch 127: Train Loss: 0.48776720762252807, Validation Loss: 0.42523181438446045\n",
      "Epoch 128: Train Loss: 0.4839931011199951, Validation Loss: 0.41694051027297974\n",
      "Epoch 129: Train Loss: 0.49324976205825805, Validation Loss: 0.4144822061061859\n",
      "Epoch 130: Train Loss: 0.5227907598018646, Validation Loss: 0.4101814925670624\n",
      "Epoch 131: Train Loss: 0.5017105937004089, Validation Loss: 0.40375518798828125\n",
      "Epoch 132: Train Loss: 0.4887368083000183, Validation Loss: 0.40328243374824524\n",
      "Epoch 133: Train Loss: 0.5295342922210693, Validation Loss: 0.40974900126457214\n",
      "Epoch 134: Train Loss: 0.5802260994911194, Validation Loss: 0.4022526443004608\n",
      "Epoch 135: Train Loss: 0.44894166588783263, Validation Loss: 0.4027351140975952\n",
      "Epoch 136: Train Loss: 0.5074317216873169, Validation Loss: 0.4016118049621582\n",
      "Epoch 137: Train Loss: 0.6980117619037628, Validation Loss: 0.4060075879096985\n",
      "Epoch 138: Train Loss: 0.49656415581703184, Validation Loss: 0.40803271532058716\n",
      "Epoch 139: Train Loss: 0.4579522371292114, Validation Loss: 0.39821872115135193\n",
      "Epoch 140: Train Loss: 0.505529808998108, Validation Loss: 0.4000324010848999\n",
      "Epoch 141: Train Loss: 0.5230221390724182, Validation Loss: 0.39293423295021057\n",
      "Epoch 142: Train Loss: 0.48032263219356536, Validation Loss: 0.39307790994644165\n",
      "Epoch 143: Train Loss: 0.49597419500350953, Validation Loss: 0.39093446731567383\n",
      "Epoch 144: Train Loss: 0.48806981444358827, Validation Loss: 0.3953370749950409\n",
      "Epoch 145: Train Loss: 0.4550288915634155, Validation Loss: 0.3943432569503784\n",
      "Epoch 146: Train Loss: 0.5493785381317139, Validation Loss: 0.39865946769714355\n",
      "Epoch 147: Train Loss: 0.5361486375331879, Validation Loss: 0.39984118938446045\n",
      "Epoch 148: Train Loss: 0.46197680830955506, Validation Loss: 0.39583060145378113\n",
      "Epoch 149: Train Loss: 0.4630067586898804, Validation Loss: 0.3902154564857483\n",
      "Epoch 150: Train Loss: 0.5236216604709625, Validation Loss: 0.38692501187324524\n",
      "Epoch 151: Train Loss: 0.4806608080863953, Validation Loss: 0.3909263610839844\n",
      "Epoch 152: Train Loss: 0.4782303750514984, Validation Loss: 0.3883226811885834\n",
      "Epoch 153: Train Loss: 0.4543188035488129, Validation Loss: 0.3803733289241791\n",
      "Epoch 154: Train Loss: 0.4682694971561432, Validation Loss: 0.3772207200527191\n",
      "Epoch 155: Train Loss: 0.4679361045360565, Validation Loss: 0.3809964656829834\n",
      "Epoch 156: Train Loss: 0.439182835817337, Validation Loss: 0.3852420747280121\n",
      "Epoch 157: Train Loss: 0.47460160255432127, Validation Loss: 0.3802136182785034\n",
      "Epoch 158: Train Loss: 0.48595005869865415, Validation Loss: 0.38039007782936096\n",
      "Epoch 159: Train Loss: 0.46571531891822815, Validation Loss: 0.38964828848838806\n",
      "Epoch 160: Train Loss: 0.5189551293849946, Validation Loss: 0.3853725492954254\n",
      "Epoch 161: Train Loss: 0.4677997291088104, Validation Loss: 0.3890323340892792\n",
      "Epoch 162: Train Loss: 0.4442596435546875, Validation Loss: 0.3898640275001526\n",
      "Epoch 163: Train Loss: 0.47264657616615297, Validation Loss: 0.3813914954662323\n",
      "Epoch 164: Train Loss: 0.5019073843955993, Validation Loss: 0.36810770630836487\n",
      "Epoch 165: Train Loss: 0.45241236686706543, Validation Loss: 0.37666594982147217\n",
      "Epoch 166: Train Loss: 0.517451387643814, Validation Loss: 0.3775997757911682\n",
      "Epoch 167: Train Loss: 0.4516558885574341, Validation Loss: 0.38157573342323303\n",
      "Epoch 168: Train Loss: 0.45928977727890014, Validation Loss: 0.3873499929904938\n",
      "Epoch 169: Train Loss: 0.4882650375366211, Validation Loss: 0.3771926760673523\n",
      "Epoch 170: Train Loss: 0.4965724229812622, Validation Loss: 0.3781876266002655\n",
      "Epoch 171: Train Loss: 0.49680707454681394, Validation Loss: 0.38479551672935486\n",
      "Epoch 172: Train Loss: 0.4765752494335175, Validation Loss: 0.37783488631248474\n",
      "Epoch 173: Train Loss: 0.4904393255710602, Validation Loss: 0.378279447555542\n",
      "Epoch 174: Train Loss: 0.45215694308280946, Validation Loss: 0.36681583523750305\n",
      "Epoch 175: Train Loss: 0.4979910969734192, Validation Loss: 0.3603741228580475\n",
      "Epoch 176: Train Loss: 0.48360885977745055, Validation Loss: 0.36006203293800354\n",
      "Epoch 177: Train Loss: 0.456949508190155, Validation Loss: 0.3558591306209564\n",
      "Epoch 178: Train Loss: 0.44539005756378175, Validation Loss: 0.36414992809295654\n",
      "Epoch 179: Train Loss: 0.4356627523899078, Validation Loss: 0.3687497675418854\n",
      "Epoch 180: Train Loss: 0.45636060237884524, Validation Loss: 0.3821324408054352\n",
      "Epoch 181: Train Loss: 0.43624343276023864, Validation Loss: 0.3757386803627014\n",
      "Epoch 182: Train Loss: 0.4064788639545441, Validation Loss: 0.3721572756767273\n",
      "Epoch 183: Train Loss: 0.4452840328216553, Validation Loss: 0.3805476725101471\n",
      "Epoch 184: Train Loss: 0.3991452604532242, Validation Loss: 0.3797757625579834\n",
      "Epoch 185: Train Loss: 0.39814581274986266, Validation Loss: 0.373384952545166\n",
      "Epoch 186: Train Loss: 0.43238418102264403, Validation Loss: 0.38094431161880493\n",
      "Epoch 187: Train Loss: 0.46093353629112244, Validation Loss: 0.3651437759399414\n",
      "Epoch 188: Train Loss: 0.4718122899532318, Validation Loss: 0.35846173763275146\n",
      "Epoch 189: Train Loss: 0.4088056802749634, Validation Loss: 0.36014869809150696\n",
      "Epoch 190: Train Loss: 0.4532484710216522, Validation Loss: 0.3602454960346222\n",
      "Epoch 191: Train Loss: 0.44852137565612793, Validation Loss: 0.37807074189186096\n",
      "Epoch 192: Train Loss: 0.47413840889930725, Validation Loss: 0.3870449662208557\n",
      "Epoch 193: Train Loss: 0.5979840517044067, Validation Loss: 0.387332022190094\n",
      "Epoch 194: Train Loss: 0.4652774751186371, Validation Loss: 0.3918689489364624\n",
      "Epoch 195: Train Loss: 0.4345350503921509, Validation Loss: 0.3928741216659546\n",
      "Epoch 196: Train Loss: 0.44643638134002683, Validation Loss: 0.3988609313964844\n",
      "Epoch 197: Train Loss: 0.3660683870315552, Validation Loss: 0.395357221364975\n",
      "Epoch 198: Train Loss: 0.5255833268165588, Validation Loss: 0.4162213206291199\n",
      "Epoch 199: Train Loss: 0.4039652407169342, Validation Loss: 0.4117061495780945\n",
      "Epoch 200: Train Loss: 0.41183829605579375, Validation Loss: 0.3917103409767151\n",
      "Epoch 201: Train Loss: 0.5047745048999787, Validation Loss: 0.39479169249534607\n",
      "Epoch 202: Train Loss: 0.3783312439918518, Validation Loss: 0.38084593415260315\n",
      "Epoch 203: Train Loss: 0.4125571846961975, Validation Loss: 0.3811882734298706\n",
      "Epoch 204: Train Loss: 0.5283526003360748, Validation Loss: 0.3946473002433777\n",
      "Epoch 205: Train Loss: 0.47095948457717896, Validation Loss: 0.3818995952606201\n",
      "Epoch 206: Train Loss: 0.44370636343955994, Validation Loss: 0.38772550225257874\n",
      "Epoch 207: Train Loss: 0.37343156039714814, Validation Loss: 0.36719292402267456\n",
      "Epoch 208: Train Loss: 0.39488649368286133, Validation Loss: 0.37314262986183167\n",
      "Epoch 209: Train Loss: 0.4744804561138153, Validation Loss: 0.3754430413246155\n",
      "Epoch 210: Train Loss: 0.4483843147754669, Validation Loss: 0.37011098861694336\n",
      "Epoch 211: Train Loss: 0.4421717762947083, Validation Loss: 0.35882872343063354\n",
      "Epoch 212: Train Loss: 0.3986684501171112, Validation Loss: 0.3605540096759796\n",
      "Epoch 213: Train Loss: 0.39298256635665896, Validation Loss: 0.3571643829345703\n",
      "Epoch 214: Train Loss: 0.36152844727039335, Validation Loss: 0.36674079298973083\n",
      "Epoch 215: Train Loss: 0.45014758706092833, Validation Loss: 0.3525320887565613\n",
      "Epoch 216: Train Loss: 0.4775823712348938, Validation Loss: 0.3643578588962555\n",
      "Epoch 217: Train Loss: 0.4046506345272064, Validation Loss: 0.3799072206020355\n",
      "Epoch 218: Train Loss: 0.43388299345970155, Validation Loss: 0.37391507625579834\n",
      "Epoch 219: Train Loss: 0.36701151728630066, Validation Loss: 0.3703759014606476\n",
      "Epoch 220: Train Loss: 0.3795777499675751, Validation Loss: 0.36840471625328064\n",
      "Epoch 221: Train Loss: 0.40693917870521545, Validation Loss: 0.3635883331298828\n",
      "Epoch 222: Train Loss: 0.4816844522953033, Validation Loss: 0.36403006315231323\n",
      "Epoch 223: Train Loss: 0.4148257315158844, Validation Loss: 0.3644196391105652\n",
      "Epoch 224: Train Loss: 0.4108545005321503, Validation Loss: 0.3534885346889496\n",
      "Epoch 225: Train Loss: 0.36839340925216674, Validation Loss: 0.35969963669776917\n",
      "Epoch 226: Train Loss: 0.35999871492385865, Validation Loss: 0.36117908358573914\n",
      "Epoch 227: Train Loss: 0.40004569888114927, Validation Loss: 0.35685741901397705\n",
      "Epoch 228: Train Loss: 0.38552882671356203, Validation Loss: 0.35951411724090576\n",
      "Epoch 229: Train Loss: 0.4491260588169098, Validation Loss: 0.3573208153247833\n",
      "Epoch 230: Train Loss: 0.4041017949581146, Validation Loss: 0.35667484998703003\n",
      "Epoch 231: Train Loss: 0.395930677652359, Validation Loss: 0.3365485668182373\n",
      "Epoch 232: Train Loss: 0.38718738555908205, Validation Loss: 0.34668147563934326\n",
      "Epoch 233: Train Loss: 0.3799311101436615, Validation Loss: 0.3621370196342468\n",
      "Epoch 234: Train Loss: 0.3713733047246933, Validation Loss: 0.36013737320899963\n",
      "Epoch 235: Train Loss: 0.36057186126708984, Validation Loss: 0.3558956980705261\n",
      "Epoch 236: Train Loss: 0.335955473780632, Validation Loss: 0.3564627766609192\n",
      "Epoch 237: Train Loss: 0.34902681708335875, Validation Loss: 0.34231626987457275\n",
      "Epoch 238: Train Loss: 0.44735565185546877, Validation Loss: 0.36238154768943787\n",
      "Epoch 239: Train Loss: 0.401273649930954, Validation Loss: 0.3639415204524994\n",
      "Epoch 240: Train Loss: 0.39006576538085935, Validation Loss: 0.36193716526031494\n",
      "Epoch 241: Train Loss: 0.3875435948371887, Validation Loss: 0.36126527190208435\n",
      "Epoch 242: Train Loss: 0.4427720487117767, Validation Loss: 0.3371769189834595\n",
      "Epoch 243: Train Loss: 0.40022173523902893, Validation Loss: 0.3534056544303894\n",
      "Epoch 244: Train Loss: 0.3702882885932922, Validation Loss: 0.35168302059173584\n",
      "Epoch 245: Train Loss: 0.41649417877197265, Validation Loss: 0.3652268350124359\n",
      "Epoch 246: Train Loss: 0.354983389377594, Validation Loss: 0.3614875376224518\n",
      "Epoch 247: Train Loss: 0.3511882185935974, Validation Loss: 0.35760298371315\n",
      "Epoch 248: Train Loss: 0.36515370607376096, Validation Loss: 0.3505028188228607\n",
      "Epoch 249: Train Loss: 0.3987355470657349, Validation Loss: 0.3489997386932373\n",
      "Epoch 250: Train Loss: 0.38917853236198424, Validation Loss: 0.3837530016899109\n",
      "Epoch 251: Train Loss: 0.3523215353488922, Validation Loss: 0.36948296427726746\n",
      "Epoch 252: Train Loss: 0.4029118299484253, Validation Loss: 0.3826886713504791\n",
      "Epoch 253: Train Loss: 0.3272798523306847, Validation Loss: 0.3501150608062744\n",
      "Epoch 254: Train Loss: 0.3565767765045166, Validation Loss: 0.3630247116088867\n",
      "Epoch 255: Train Loss: 0.38709538578987124, Validation Loss: 0.3825702369213104\n",
      "Epoch 256: Train Loss: 0.35477606356143954, Validation Loss: 0.3950383961200714\n",
      "Epoch 257: Train Loss: 0.390011191368103, Validation Loss: 0.36868634819984436\n",
      "Epoch 258: Train Loss: 0.29696036875247955, Validation Loss: 0.3640095293521881\n",
      "Epoch 259: Train Loss: 0.3812381625175476, Validation Loss: 0.3598175346851349\n",
      "Epoch 260: Train Loss: 0.3556965410709381, Validation Loss: 0.3534664511680603\n",
      "Epoch 261: Train Loss: 0.40930283069610596, Validation Loss: 0.3771991431713104\n",
      "Epoch 262: Train Loss: 0.3434860289096832, Validation Loss: 0.3942504823207855\n",
      "Epoch 263: Train Loss: 0.36078683733940126, Validation Loss: 0.3445236086845398\n",
      "Epoch 264: Train Loss: 0.3270422235131264, Validation Loss: 0.3596992790699005\n",
      "Epoch 265: Train Loss: 0.32122854590415956, Validation Loss: 0.3902445435523987\n",
      "Epoch 266: Train Loss: 0.3329418361186981, Validation Loss: 0.3636365532875061\n",
      "Epoch 267: Train Loss: 0.3648090064525604, Validation Loss: 0.37786775827407837\n",
      "Epoch 268: Train Loss: 0.34010930061340333, Validation Loss: 0.3792096674442291\n",
      "Epoch 269: Train Loss: 0.37346296608448026, Validation Loss: 0.40922823548316956\n",
      "Epoch 270: Train Loss: 0.32570756077766416, Validation Loss: 0.3869408369064331\n",
      "Epoch 271: Train Loss: 0.36918269097805023, Validation Loss: 0.3943144381046295\n",
      "Epoch 272: Train Loss: 0.3356964349746704, Validation Loss: 0.40354248881340027\n",
      "Epoch 273: Train Loss: 0.4312974750995636, Validation Loss: 0.3697371482849121\n",
      "Epoch 274: Train Loss: 0.3638081669807434, Validation Loss: 0.37761345505714417\n",
      "Epoch 275: Train Loss: 0.3215979337692261, Validation Loss: 0.3857797384262085\n",
      "Epoch 276: Train Loss: 0.36884577870368956, Validation Loss: 0.3902381956577301\n",
      "Epoch 277: Train Loss: 0.36219345331192015, Validation Loss: 0.4085552990436554\n",
      "Epoch 278: Train Loss: 0.39434211850166323, Validation Loss: 0.4241945445537567\n",
      "Epoch 279: Train Loss: 0.45594369769096377, Validation Loss: 0.4281284511089325\n",
      "Epoch 280: Train Loss: 0.38419768810272215, Validation Loss: 0.4069521725177765\n",
      "Epoch 281: Train Loss: 0.30692621469497683, Validation Loss: 0.373348206281662\n",
      "Epoch 282: Train Loss: 0.32423393428325653, Validation Loss: 0.35333043336868286\n",
      "Epoch 283: Train Loss: 0.3524277687072754, Validation Loss: 0.3610170781612396\n",
      "Epoch 284: Train Loss: 0.30071067810058594, Validation Loss: 0.37570157647132874\n",
      "Epoch 285: Train Loss: 0.33739238381385805, Validation Loss: 0.39108142256736755\n",
      "Epoch 286: Train Loss: 0.32266779243946075, Validation Loss: 0.3594077527523041\n",
      "Epoch 287: Train Loss: 0.29148094058036805, Validation Loss: 0.3736046552658081\n",
      "Epoch 288: Train Loss: 0.3200447142124176, Validation Loss: 0.3810179531574249\n",
      "Epoch 289: Train Loss: 0.4130926012992859, Validation Loss: 0.4046887159347534\n",
      "Epoch 290: Train Loss: 0.2992169111967087, Validation Loss: 0.381909042596817\n",
      "Epoch 291: Train Loss: 0.38590338826179504, Validation Loss: 0.3771304190158844\n",
      "Epoch 292: Train Loss: 0.4507362127304077, Validation Loss: 0.3884221613407135\n",
      "Epoch 293: Train Loss: 0.32712758183479307, Validation Loss: 0.3888404667377472\n",
      "Epoch 294: Train Loss: 0.3411232352256775, Validation Loss: 0.4108346104621887\n",
      "Epoch 295: Train Loss: 0.3112966179847717, Validation Loss: 0.4086163640022278\n",
      "Epoch 296: Train Loss: 0.33152279257774353, Validation Loss: 0.41501206159591675\n",
      "Epoch 297: Train Loss: 0.27595786154270174, Validation Loss: 0.4211687743663788\n",
      "Epoch 298: Train Loss: 0.3066472470760345, Validation Loss: 0.4348312318325043\n",
      "Epoch 299: Train Loss: 0.40020222663879396, Validation Loss: 0.4273260235786438\n",
      "Epoch 300: Train Loss: 0.34564111232757566, Validation Loss: 0.43867939710617065\n",
      "Epoch 301: Train Loss: 0.37025952339172363, Validation Loss: 0.4365534782409668\n",
      "Epoch 302: Train Loss: 0.3238785803318024, Validation Loss: 0.44220227003097534\n",
      "Epoch 303: Train Loss: 0.322629451751709, Validation Loss: 0.4504614472389221\n",
      "Epoch 304: Train Loss: 0.31913251876831056, Validation Loss: 0.4202789068222046\n",
      "Epoch 305: Train Loss: 0.2718606323003769, Validation Loss: 0.4390046298503876\n",
      "Epoch 306: Train Loss: 0.3556901514530182, Validation Loss: 0.41612669825553894\n",
      "Epoch 307: Train Loss: 0.321366560459137, Validation Loss: 0.38604870438575745\n",
      "Epoch 308: Train Loss: 0.4770680546760559, Validation Loss: 0.423191636800766\n",
      "Epoch 309: Train Loss: 0.35656842291355134, Validation Loss: 0.44268110394477844\n",
      "Epoch 310: Train Loss: 0.33014064431190493, Validation Loss: 0.4252801537513733\n",
      "Epoch 311: Train Loss: 0.3243549019098282, Validation Loss: 0.4384222626686096\n",
      "Epoch 312: Train Loss: 0.3292433381080627, Validation Loss: 0.4016949534416199\n",
      "Epoch 313: Train Loss: 0.3800313472747803, Validation Loss: 0.4050832688808441\n",
      "Epoch 314: Train Loss: 0.28175937533378603, Validation Loss: 0.41374269127845764\n",
      "Epoch 315: Train Loss: 0.3297847270965576, Validation Loss: 0.3778853416442871\n",
      "Epoch 316: Train Loss: 0.30646364390850067, Validation Loss: 0.38764894008636475\n",
      "Epoch 317: Train Loss: 0.28658199310302734, Validation Loss: 0.4111754894256592\n",
      "Epoch 318: Train Loss: 0.31165252029895785, Validation Loss: 0.38426530361175537\n",
      "Epoch 319: Train Loss: 0.3435509741306305, Validation Loss: 0.39187297224998474\n",
      "Epoch 320: Train Loss: 0.35695855021476747, Validation Loss: 0.378471702337265\n",
      "Epoch 321: Train Loss: 0.305915829539299, Validation Loss: 0.3749378025531769\n",
      "Epoch 322: Train Loss: 0.3336507320404053, Validation Loss: 0.40414315462112427\n",
      "Epoch 323: Train Loss: 0.3974845945835114, Validation Loss: 0.4221704602241516\n",
      "Epoch 324: Train Loss: 0.30326387882232664, Validation Loss: 0.4263140857219696\n",
      "Epoch 325: Train Loss: 0.4218345105648041, Validation Loss: 0.40181243419647217\n",
      "Epoch 326: Train Loss: 0.3658698320388794, Validation Loss: 0.3600773215293884\n",
      "Epoch 327: Train Loss: 0.4094335973262787, Validation Loss: 0.3504585325717926\n",
      "Epoch 328: Train Loss: 0.3475987881422043, Validation Loss: 0.38384416699409485\n",
      "Epoch 329: Train Loss: 0.30045144259929657, Validation Loss: 0.4011461138725281\n",
      "Epoch 330: Train Loss: 0.2784728020429611, Validation Loss: 0.38925644755363464\n",
      "Epoch 331: Train Loss: 0.39167800545692444, Validation Loss: 0.4257464110851288\n",
      "Epoch 332: Train Loss: 0.28841426223516464, Validation Loss: 0.4209199845790863\n",
      "Epoch 333: Train Loss: 0.2841082543134689, Validation Loss: 0.422042578458786\n",
      "Epoch 334: Train Loss: 0.2888251006603241, Validation Loss: 0.4478110074996948\n",
      "Epoch 335: Train Loss: 0.27948500514030455, Validation Loss: 0.4522923231124878\n",
      "Epoch 336: Train Loss: 0.3176046073436737, Validation Loss: 0.4700968563556671\n",
      "Epoch 337: Train Loss: 0.31539213061332705, Validation Loss: 0.4632166624069214\n",
      "Epoch 338: Train Loss: 0.39099845588207244, Validation Loss: 0.44799965620040894\n",
      "Epoch 339: Train Loss: 0.24373694360256196, Validation Loss: 0.41525915265083313\n",
      "Epoch 340: Train Loss: 0.3322118639945984, Validation Loss: 0.41532638669013977\n",
      "Epoch 341: Train Loss: 0.30593495070934296, Validation Loss: 0.395679235458374\n",
      "Epoch 342: Train Loss: 0.4284477889537811, Validation Loss: 0.3678140640258789\n",
      "Epoch 343: Train Loss: 0.27895344197750094, Validation Loss: 0.4040462374687195\n",
      "Epoch 344: Train Loss: 0.27010623216629026, Validation Loss: 0.42170873284339905\n",
      "Epoch 345: Train Loss: 0.3828453004360199, Validation Loss: 0.40484654903411865\n",
      "Epoch 346: Train Loss: 0.3536604106426239, Validation Loss: 0.3760795593261719\n",
      "Epoch 347: Train Loss: 0.25888105034828185, Validation Loss: 0.35859155654907227\n",
      "Epoch 348: Train Loss: 0.2774649322032928, Validation Loss: 0.39741650223731995\n",
      "Epoch 349: Train Loss: 0.2986596882343292, Validation Loss: 0.40891534090042114\n",
      "Epoch 350: Train Loss: 0.27600419521331787, Validation Loss: 0.3893662989139557\n",
      "Epoch 351: Train Loss: 0.2664244741201401, Validation Loss: 0.3877256214618683\n",
      "Epoch 352: Train Loss: 0.34837839007377625, Validation Loss: 0.3957248330116272\n",
      "Epoch 353: Train Loss: 0.28775779008865354, Validation Loss: 0.4213615357875824\n",
      "Epoch 354: Train Loss: 0.29151699542999265, Validation Loss: 0.42967596650123596\n",
      "Epoch 355: Train Loss: 0.39436469674110414, Validation Loss: 0.41817089915275574\n",
      "Epoch 356: Train Loss: 0.3468442171812057, Validation Loss: 0.3975566625595093\n",
      "Epoch 357: Train Loss: 0.29681668281555174, Validation Loss: 0.41003918647766113\n",
      "Epoch 358: Train Loss: 0.2945845365524292, Validation Loss: 0.3986611068248749\n",
      "Epoch 359: Train Loss: 0.31220383644104005, Validation Loss: 0.43137556314468384\n",
      "Epoch 360: Train Loss: 0.3127642035484314, Validation Loss: 0.4221116602420807\n",
      "Epoch 361: Train Loss: 0.452362060546875, Validation Loss: 0.4238196313381195\n",
      "Epoch 362: Train Loss: 0.25882603228092194, Validation Loss: 0.39673668146133423\n",
      "Epoch 363: Train Loss: 0.2626515835523605, Validation Loss: 0.3532419800758362\n",
      "Epoch 364: Train Loss: 0.3742230921983719, Validation Loss: 0.3939373791217804\n",
      "Epoch 365: Train Loss: 0.25844540297985075, Validation Loss: 0.3655611574649811\n",
      "Epoch 366: Train Loss: 0.3478770345449448, Validation Loss: 0.36263307929039\n",
      "Epoch 367: Train Loss: 0.264856618642807, Validation Loss: 0.3504584729671478\n",
      "Epoch 368: Train Loss: 0.30434958040714266, Validation Loss: 0.3397068381309509\n",
      "Epoch 369: Train Loss: 0.27656574845314025, Validation Loss: 0.3409286439418793\n",
      "Epoch 370: Train Loss: 0.2874866783618927, Validation Loss: 0.3555692136287689\n",
      "Epoch 371: Train Loss: 0.3660291492938995, Validation Loss: 0.32606878876686096\n",
      "Epoch 372: Train Loss: 0.2979038506746292, Validation Loss: 0.3378973603248596\n",
      "Epoch 373: Train Loss: 0.23196616172790527, Validation Loss: 0.3364737331867218\n",
      "Epoch 374: Train Loss: 0.4002482146024704, Validation Loss: 0.39319902658462524\n",
      "Epoch 375: Train Loss: 0.3061744511127472, Validation Loss: 0.42640018463134766\n",
      "Epoch 376: Train Loss: 0.4135215997695923, Validation Loss: 0.38248538970947266\n",
      "Epoch 377: Train Loss: 0.3451570332050323, Validation Loss: 0.34146565198898315\n",
      "Epoch 378: Train Loss: 0.27988964319229126, Validation Loss: 0.34822890162467957\n",
      "Epoch 379: Train Loss: 0.3824559926986694, Validation Loss: 0.32565057277679443\n",
      "Epoch 380: Train Loss: 0.2638989955186844, Validation Loss: 0.3933789134025574\n",
      "Epoch 381: Train Loss: 0.2920078247785568, Validation Loss: 0.3803081214427948\n",
      "Epoch 382: Train Loss: 0.24247588366270065, Validation Loss: 0.3907925486564636\n",
      "Epoch 383: Train Loss: 0.23657501339912415, Validation Loss: 0.4053797721862793\n",
      "Epoch 384: Train Loss: 0.2956238776445389, Validation Loss: 0.4589678645133972\n",
      "Epoch 385: Train Loss: 0.2533522665500641, Validation Loss: 0.44052380323410034\n",
      "Epoch 386: Train Loss: 0.28600889146327974, Validation Loss: 0.46305614709854126\n",
      "Epoch 387: Train Loss: 0.27247312366962434, Validation Loss: 0.41097354888916016\n",
      "Epoch 388: Train Loss: 0.29744534492492675, Validation Loss: 0.3957458436489105\n",
      "Epoch 389: Train Loss: 0.32673415541648865, Validation Loss: 0.42744508385658264\n",
      "Epoch 390: Train Loss: 0.27198120653629304, Validation Loss: 0.4544711112976074\n",
      "Epoch 391: Train Loss: 0.26258823871612547, Validation Loss: 0.45700937509536743\n",
      "Epoch 392: Train Loss: 0.24263138175010682, Validation Loss: 0.3900834619998932\n",
      "Epoch 393: Train Loss: 0.23878566920757294, Validation Loss: 0.44892436265945435\n",
      "Epoch 394: Train Loss: 0.32101320922374726, Validation Loss: 0.42794695496559143\n",
      "Epoch 395: Train Loss: 0.2723935961723328, Validation Loss: 0.41273146867752075\n",
      "Epoch 396: Train Loss: 0.2677533894777298, Validation Loss: 0.39921367168426514\n",
      "Epoch 397: Train Loss: 0.22561145201325417, Validation Loss: 0.4110880494117737\n",
      "Epoch 398: Train Loss: 0.42173326909542086, Validation Loss: 0.4546731114387512\n",
      "Epoch 399: Train Loss: 0.3350889593362808, Validation Loss: 0.4635792374610901\n",
      "Epoch 400: Train Loss: 0.2643732100725174, Validation Loss: 0.4476328492164612\n",
      "Epoch 401: Train Loss: 0.24063948094844817, Validation Loss: 0.4786297082901001\n",
      "Epoch 402: Train Loss: 0.4334817737340927, Validation Loss: 0.4523710012435913\n",
      "Epoch 403: Train Loss: 0.33612981140613557, Validation Loss: 0.49483728408813477\n",
      "Epoch 404: Train Loss: 0.24731765687465668, Validation Loss: 0.501419723033905\n",
      "Epoch 405: Train Loss: 0.25617454946041107, Validation Loss: 0.4792283773422241\n",
      "Epoch 406: Train Loss: 0.30375300645828246, Validation Loss: 0.46137553453445435\n",
      "Epoch 407: Train Loss: 0.23924817740917206, Validation Loss: 0.4594258964061737\n",
      "Epoch 408: Train Loss: 0.34001452624797823, Validation Loss: 0.4442545771598816\n",
      "Epoch 409: Train Loss: 0.22700625658035278, Validation Loss: 0.4375738799571991\n",
      "Epoch 410: Train Loss: 0.23095933049917222, Validation Loss: 0.4245702624320984\n",
      "Epoch 411: Train Loss: 0.21877403035759926, Validation Loss: 0.4221244752407074\n",
      "Epoch 412: Train Loss: 0.25130399465560915, Validation Loss: 0.4206891655921936\n",
      "Epoch 413: Train Loss: 0.22119473218917846, Validation Loss: 0.40420615673065186\n",
      "Epoch 414: Train Loss: 0.2800854057073593, Validation Loss: 0.4604710638523102\n",
      "Epoch 415: Train Loss: 0.29290912449359896, Validation Loss: 0.46704351902008057\n",
      "Epoch 416: Train Loss: 0.23293766975402833, Validation Loss: 0.4118771255016327\n",
      "Epoch 417: Train Loss: 0.3960226207971573, Validation Loss: 0.3937443792819977\n",
      "Epoch 418: Train Loss: 0.3157679378986359, Validation Loss: 0.4013194739818573\n",
      "Epoch 419: Train Loss: 0.22080851942300797, Validation Loss: 0.38127660751342773\n",
      "Epoch 420: Train Loss: 0.2500962346792221, Validation Loss: 0.35704225301742554\n",
      "Epoch 421: Train Loss: 0.22855812907218934, Validation Loss: 0.4054158926010132\n",
      "Epoch 422: Train Loss: 0.3399552255868912, Validation Loss: 0.43629828095436096\n",
      "Epoch 423: Train Loss: 0.35576037168502805, Validation Loss: 0.4495115876197815\n",
      "Epoch 424: Train Loss: 0.23165554404258729, Validation Loss: 0.4466387927532196\n",
      "Epoch 425: Train Loss: 0.2914760023355484, Validation Loss: 0.4394209086894989\n",
      "Epoch 426: Train Loss: 0.22628898918628693, Validation Loss: 0.474338173866272\n",
      "Epoch 427: Train Loss: 0.246816948056221, Validation Loss: 0.4331769049167633\n",
      "Epoch 428: Train Loss: 0.2731676399707794, Validation Loss: 0.4519854187965393\n",
      "Epoch 429: Train Loss: 0.2625340700149536, Validation Loss: 0.411938339471817\n",
      "Epoch 430: Train Loss: 0.24155252277851105, Validation Loss: 0.4404907822608948\n",
      "Epoch 431: Train Loss: 0.2383712574839592, Validation Loss: 0.42777204513549805\n",
      "Epoch 432: Train Loss: 0.5112426400184631, Validation Loss: 0.4702075719833374\n",
      "Epoch 433: Train Loss: 0.3988273859024048, Validation Loss: 0.45053738355636597\n",
      "Epoch 434: Train Loss: 0.23732168674468995, Validation Loss: 0.44595345854759216\n",
      "Epoch 435: Train Loss: 0.2823002338409424, Validation Loss: 0.45441824197769165\n",
      "Epoch 436: Train Loss: 0.29677592813968656, Validation Loss: 0.45544061064720154\n",
      "Epoch 437: Train Loss: 0.25483580231666564, Validation Loss: 0.4825437664985657\n",
      "Epoch 438: Train Loss: 0.21651972830295563, Validation Loss: 0.44971439242362976\n",
      "Epoch 439: Train Loss: 0.24503376185894013, Validation Loss: 0.41115376353263855\n",
      "Epoch 440: Train Loss: 0.3660465180873871, Validation Loss: 0.40251433849334717\n",
      "Epoch 441: Train Loss: 0.27412369251251223, Validation Loss: 0.41916924715042114\n",
      "Epoch 442: Train Loss: 0.24243913292884828, Validation Loss: 0.43106189370155334\n",
      "Epoch 443: Train Loss: 0.2208559423685074, Validation Loss: 0.40154922008514404\n",
      "Epoch 444: Train Loss: 0.27527874112129214, Validation Loss: 0.4045049250125885\n",
      "Epoch 445: Train Loss: 0.28600287437438965, Validation Loss: 0.4340868890285492\n",
      "Epoch 446: Train Loss: 0.2595421999692917, Validation Loss: 0.3926697373390198\n",
      "Epoch 447: Train Loss: 0.23033247888088226, Validation Loss: 0.38489246368408203\n",
      "Epoch 448: Train Loss: 0.23089614808559417, Validation Loss: 0.3895651400089264\n",
      "Epoch 449: Train Loss: 0.21204627752304078, Validation Loss: 0.4442566931247711\n",
      "Epoch 450: Train Loss: 0.2570032000541687, Validation Loss: 0.43002212047576904\n",
      "Epoch 451: Train Loss: 0.2266617514193058, Validation Loss: 0.42440882325172424\n",
      "Epoch 452: Train Loss: 0.20731674283742904, Validation Loss: 0.45237332582473755\n",
      "Epoch 453: Train Loss: 0.26113690435886383, Validation Loss: 0.4605955183506012\n",
      "Epoch 454: Train Loss: 0.33090759813785553, Validation Loss: 0.4757058918476105\n",
      "Epoch 455: Train Loss: 0.2013525016605854, Validation Loss: 0.4840124845504761\n",
      "Epoch 456: Train Loss: 0.22238922938704492, Validation Loss: 0.47020065784454346\n",
      "Epoch 457: Train Loss: 0.3288770496845245, Validation Loss: 0.42976129055023193\n",
      "Epoch 458: Train Loss: 0.22069062292575836, Validation Loss: 0.4612678289413452\n",
      "Epoch 459: Train Loss: 0.35969971418380736, Validation Loss: 0.4147408604621887\n",
      "Epoch 460: Train Loss: 0.2489621639251709, Validation Loss: 0.430370956659317\n",
      "Epoch 461: Train Loss: 0.25034520626068113, Validation Loss: 0.44588348269462585\n",
      "Epoch 462: Train Loss: 0.27731206715106965, Validation Loss: 0.4311995208263397\n",
      "Epoch 463: Train Loss: 0.28878186345100404, Validation Loss: 0.517366349697113\n",
      "Epoch 464: Train Loss: 0.26272356808185576, Validation Loss: 0.5330939888954163\n",
      "Epoch 465: Train Loss: 0.22649883031845092, Validation Loss: 0.4876360595226288\n",
      "Epoch 466: Train Loss: 0.243601393699646, Validation Loss: 0.47366437315940857\n",
      "Epoch 467: Train Loss: 0.28386592864990234, Validation Loss: 0.49203869700431824\n",
      "Epoch 468: Train Loss: 0.2168698489665985, Validation Loss: 0.47835901379585266\n",
      "Epoch 469: Train Loss: 0.24507706761360168, Validation Loss: 0.4965699315071106\n",
      "Epoch 470: Train Loss: 0.27558040916919707, Validation Loss: 0.42890310287475586\n",
      "Epoch 471: Train Loss: 0.28275044560432433, Validation Loss: 0.431549608707428\n",
      "Epoch 472: Train Loss: 0.29286329746246337, Validation Loss: 0.4582822024822235\n",
      "Epoch 473: Train Loss: 0.21717190146446227, Validation Loss: 0.509108304977417\n",
      "Epoch 474: Train Loss: 0.26114771962165834, Validation Loss: 0.5138234496116638\n",
      "Epoch 475: Train Loss: 0.25408299416303637, Validation Loss: 0.4662051200866699\n",
      "Epoch 476: Train Loss: 0.20009361989796162, Validation Loss: 0.42854002118110657\n",
      "Epoch 477: Train Loss: 0.5561803698539733, Validation Loss: 0.5022537708282471\n",
      "Epoch 478: Train Loss: 0.33978705406188964, Validation Loss: 0.44397813081741333\n",
      "Epoch 479: Train Loss: 0.2364276021718979, Validation Loss: 0.42564019560813904\n",
      "Epoch 480: Train Loss: 0.20375334918498994, Validation Loss: 0.4335147738456726\n",
      "Epoch 481: Train Loss: 0.20808591842651367, Validation Loss: 0.4068174958229065\n",
      "Epoch 482: Train Loss: 0.2750627279281616, Validation Loss: 0.39947229623794556\n",
      "Epoch 483: Train Loss: 0.39203176498413084, Validation Loss: 0.4639711380004883\n",
      "Epoch 484: Train Loss: 0.20461720526218413, Validation Loss: 0.45806387066841125\n",
      "Epoch 485: Train Loss: 0.2840249717235565, Validation Loss: 0.375393271446228\n",
      "Epoch 486: Train Loss: 0.21947430074214935, Validation Loss: 0.4122014045715332\n",
      "Epoch 487: Train Loss: 0.1955135002732277, Validation Loss: 0.43877142667770386\n",
      "Epoch 488: Train Loss: 0.270680645108223, Validation Loss: 0.4767792820930481\n",
      "Epoch 489: Train Loss: 0.3266077160835266, Validation Loss: 0.5206985473632812\n",
      "Epoch 490: Train Loss: 0.21425432860851287, Validation Loss: 0.4988662600517273\n",
      "Epoch 491: Train Loss: 0.18535363674163818, Validation Loss: 0.42789432406425476\n",
      "Epoch 492: Train Loss: 0.21606255024671556, Validation Loss: 0.43728771805763245\n",
      "Epoch 493: Train Loss: 0.20272239446640014, Validation Loss: 0.4777248203754425\n",
      "Epoch 494: Train Loss: 0.3784373700618744, Validation Loss: 0.5794093012809753\n",
      "Epoch 495: Train Loss: 0.2156320407986641, Validation Loss: 0.5119583606719971\n",
      "Epoch 496: Train Loss: 0.23096936643123628, Validation Loss: 0.5508375763893127\n",
      "Epoch 497: Train Loss: 0.20657791942358017, Validation Loss: 0.5322270393371582\n",
      "Epoch 498: Train Loss: 0.23770989775657653, Validation Loss: 0.5120928287506104\n",
      "Epoch 499: Train Loss: 0.31322227120399476, Validation Loss: 0.46898773312568665\n",
      "Fold 1 Metrics:\n",
      "Accuracy: 0.8181818181818182, Precision: 0.75, Recall: 1.0, F1-score: 0.8571428571428571, AUC: 0.8\n",
      "Confusion Matrix:\n",
      "[[3 2]\n",
      " [0 6]]\n",
      "Completed fold 1\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples from subject 7 to test set\n",
      "Adding 6 truth samples from subject 7 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 250)\n",
      "(132, 65, 250)\n",
      "Epoch 0: Train Loss: 0.7772483944892883, Validation Loss: 0.7150642275810242\n",
      "Epoch 1: Train Loss: 0.7006062746047974, Validation Loss: 0.7243710160255432\n",
      "Epoch 2: Train Loss: 0.762361752986908, Validation Loss: 0.7351011037826538\n",
      "Epoch 3: Train Loss: 0.7874740481376648, Validation Loss: 0.7470061182975769\n",
      "Epoch 4: Train Loss: 0.7290622472763062, Validation Loss: 0.7451543807983398\n",
      "Epoch 5: Train Loss: 0.7698410630226136, Validation Loss: 0.7517488598823547\n",
      "Epoch 6: Train Loss: 0.6882414937019348, Validation Loss: 0.7540561556816101\n",
      "Epoch 7: Train Loss: 0.7530827283859253, Validation Loss: 0.755272388458252\n",
      "Epoch 8: Train Loss: 0.8066715598106384, Validation Loss: 0.7483870983123779\n",
      "Epoch 9: Train Loss: 0.7163145184516907, Validation Loss: 0.7524350881576538\n",
      "Epoch 10: Train Loss: 0.7501991748809814, Validation Loss: 0.7506014704704285\n",
      "Epoch 11: Train Loss: 0.7277295827865601, Validation Loss: 0.7491206526756287\n",
      "Epoch 12: Train Loss: 0.7436045050621033, Validation Loss: 0.7480939030647278\n",
      "Epoch 13: Train Loss: 0.7242931604385376, Validation Loss: 0.7467069625854492\n",
      "Epoch 14: Train Loss: 0.7219693660736084, Validation Loss: 0.744030773639679\n",
      "Epoch 15: Train Loss: 0.7333613276481629, Validation Loss: 0.7405325770378113\n",
      "Epoch 16: Train Loss: 0.6515742242336273, Validation Loss: 0.7358582615852356\n",
      "Epoch 17: Train Loss: 0.7411341786384582, Validation Loss: 0.7397893071174622\n",
      "Epoch 18: Train Loss: 0.697433352470398, Validation Loss: 0.7335686683654785\n",
      "Epoch 19: Train Loss: 0.7435170412063599, Validation Loss: 0.734569251537323\n",
      "Epoch 20: Train Loss: 0.6598381161689758, Validation Loss: 0.7319498062133789\n",
      "Epoch 21: Train Loss: 0.7404191136360169, Validation Loss: 0.7295518517494202\n",
      "Epoch 22: Train Loss: 0.6691998481750489, Validation Loss: 0.7283351421356201\n",
      "Epoch 23: Train Loss: 0.7497719168663025, Validation Loss: 0.7283307313919067\n",
      "Epoch 24: Train Loss: 0.6393476963043213, Validation Loss: 0.726407527923584\n",
      "Epoch 25: Train Loss: 0.6908448934555054, Validation Loss: 0.7295739650726318\n",
      "Epoch 26: Train Loss: 0.6448192715644836, Validation Loss: 0.728569507598877\n",
      "Epoch 27: Train Loss: 0.753341281414032, Validation Loss: 0.7282698750495911\n",
      "Epoch 28: Train Loss: 0.7284745693206787, Validation Loss: 0.7279632687568665\n",
      "Epoch 29: Train Loss: 0.6705793380737305, Validation Loss: 0.7281401753425598\n",
      "Epoch 30: Train Loss: 0.7346911787986755, Validation Loss: 0.728206992149353\n",
      "Epoch 31: Train Loss: 0.6507511377334595, Validation Loss: 0.724306046962738\n",
      "Epoch 32: Train Loss: 0.7090932846069335, Validation Loss: 0.7195967435836792\n",
      "Epoch 33: Train Loss: 0.6948374509811401, Validation Loss: 0.7221074104309082\n",
      "Epoch 34: Train Loss: 0.648693835735321, Validation Loss: 0.7212677001953125\n",
      "Epoch 35: Train Loss: 0.6434941768646241, Validation Loss: 0.719271719455719\n",
      "Epoch 36: Train Loss: 0.6876043915748596, Validation Loss: 0.7196223735809326\n",
      "Epoch 37: Train Loss: 0.7022979021072387, Validation Loss: 0.7176672220230103\n",
      "Epoch 38: Train Loss: 0.6595974326133728, Validation Loss: 0.7163269519805908\n",
      "Epoch 39: Train Loss: 0.670236611366272, Validation Loss: 0.7159223556518555\n",
      "Epoch 40: Train Loss: 0.7213434457778931, Validation Loss: 0.7144104242324829\n",
      "Epoch 41: Train Loss: 0.6288040995597839, Validation Loss: 0.7132819890975952\n",
      "Epoch 42: Train Loss: 0.7151747584342957, Validation Loss: 0.706794261932373\n",
      "Epoch 43: Train Loss: 0.6842371225357056, Validation Loss: 0.7093012928962708\n",
      "Epoch 44: Train Loss: 0.6484093546867371, Validation Loss: 0.711979329586029\n",
      "Epoch 45: Train Loss: 0.6601973295211792, Validation Loss: 0.7136235237121582\n",
      "Epoch 46: Train Loss: 0.7126104235649109, Validation Loss: 0.7131195068359375\n",
      "Epoch 47: Train Loss: 0.6749758243560791, Validation Loss: 0.7153737545013428\n",
      "Epoch 48: Train Loss: 0.6173720479011535, Validation Loss: 0.7147578597068787\n",
      "Epoch 49: Train Loss: 0.6551650524139404, Validation Loss: 0.7164056897163391\n",
      "Epoch 50: Train Loss: 0.6299127817153931, Validation Loss: 0.7146101593971252\n",
      "Epoch 51: Train Loss: 0.6728366017341614, Validation Loss: 0.716279923915863\n",
      "Epoch 52: Train Loss: 0.6331725060939789, Validation Loss: 0.7191166281700134\n",
      "Epoch 53: Train Loss: 0.6111979007720947, Validation Loss: 0.7207078337669373\n",
      "Epoch 54: Train Loss: 0.684732186794281, Validation Loss: 0.7186788320541382\n",
      "Epoch 55: Train Loss: 0.6668002963066101, Validation Loss: 0.7147567272186279\n",
      "Epoch 56: Train Loss: 0.6877452254295349, Validation Loss: 0.7148900628089905\n",
      "Epoch 57: Train Loss: 0.6531051397323608, Validation Loss: 0.7160381078720093\n",
      "Epoch 58: Train Loss: 0.6347864985466003, Validation Loss: 0.7156473994255066\n",
      "Epoch 59: Train Loss: 0.6479680299758911, Validation Loss: 0.7159192562103271\n",
      "Epoch 60: Train Loss: 0.5909724771976471, Validation Loss: 0.7204376459121704\n",
      "Epoch 61: Train Loss: 0.6826143741607666, Validation Loss: 0.7234609723091125\n",
      "Epoch 62: Train Loss: 0.6474297404289245, Validation Loss: 0.7268591523170471\n",
      "Epoch 63: Train Loss: 0.6326248049736023, Validation Loss: 0.7281013131141663\n",
      "Epoch 64: Train Loss: 0.6431738495826721, Validation Loss: 0.7259767055511475\n",
      "Epoch 65: Train Loss: 0.6296435356140136, Validation Loss: 0.7247403860092163\n",
      "Epoch 66: Train Loss: 0.6026678323745728, Validation Loss: 0.7240359783172607\n",
      "Epoch 67: Train Loss: 0.696803092956543, Validation Loss: 0.7249312400817871\n",
      "Epoch 68: Train Loss: 0.6741729617118836, Validation Loss: 0.7282376289367676\n",
      "Epoch 69: Train Loss: 0.6389864206314086, Validation Loss: 0.7274928092956543\n",
      "Epoch 70: Train Loss: 0.6264611959457398, Validation Loss: 0.731306791305542\n",
      "Epoch 71: Train Loss: 0.6436357021331787, Validation Loss: 0.7313477396965027\n",
      "Epoch 72: Train Loss: 0.668273639678955, Validation Loss: 0.7320005297660828\n",
      "Epoch 73: Train Loss: 0.6509620070457458, Validation Loss: 0.7276660203933716\n",
      "Epoch 74: Train Loss: 0.5914036870002747, Validation Loss: 0.7310897707939148\n",
      "Epoch 75: Train Loss: 0.5992968082427979, Validation Loss: 0.7255675792694092\n",
      "Epoch 76: Train Loss: 0.6405526518821716, Validation Loss: 0.7287114858627319\n",
      "Epoch 77: Train Loss: 0.639897084236145, Validation Loss: 0.7236318588256836\n",
      "Epoch 78: Train Loss: 0.6932883381843566, Validation Loss: 0.7248356938362122\n",
      "Epoch 79: Train Loss: 0.5977903604507446, Validation Loss: 0.7267066240310669\n",
      "Epoch 80: Train Loss: 0.6091868281364441, Validation Loss: 0.7257655262947083\n",
      "Epoch 81: Train Loss: 0.6625670671463013, Validation Loss: 0.7220253944396973\n",
      "Epoch 82: Train Loss: 0.6338207364082337, Validation Loss: 0.7200707197189331\n",
      "Epoch 83: Train Loss: 0.5742493748664856, Validation Loss: 0.7236204147338867\n",
      "Epoch 84: Train Loss: 0.6325375318527222, Validation Loss: 0.7240297794342041\n",
      "Epoch 85: Train Loss: 0.6080808401107788, Validation Loss: 0.7204659581184387\n",
      "Epoch 86: Train Loss: 0.6269190549850464, Validation Loss: 0.7184777855873108\n",
      "Epoch 87: Train Loss: 0.584653890132904, Validation Loss: 0.7209694385528564\n",
      "Epoch 88: Train Loss: 0.6088323473930359, Validation Loss: 0.7211995124816895\n",
      "Epoch 89: Train Loss: 0.6025219857692719, Validation Loss: 0.7218952178955078\n",
      "Epoch 90: Train Loss: 0.6155141532421112, Validation Loss: 0.7257847189903259\n",
      "Epoch 91: Train Loss: 0.5884868741035462, Validation Loss: 0.7248722314834595\n",
      "Epoch 92: Train Loss: 0.5732748568058014, Validation Loss: 0.7269365191459656\n",
      "Epoch 93: Train Loss: 0.6228921413421631, Validation Loss: 0.7264432907104492\n",
      "Epoch 94: Train Loss: 0.6362300038337707, Validation Loss: 0.7266404628753662\n",
      "Epoch 95: Train Loss: 0.6273825407028198, Validation Loss: 0.7276161313056946\n",
      "Epoch 96: Train Loss: 0.6198461174964904, Validation Loss: 0.7300096154212952\n",
      "Epoch 97: Train Loss: 0.6688727140426636, Validation Loss: 0.7277172207832336\n",
      "Epoch 98: Train Loss: 0.5947455286979675, Validation Loss: 0.7299260497093201\n",
      "Epoch 99: Train Loss: 0.6267578363418579, Validation Loss: 0.7285020351409912\n",
      "Epoch 100: Train Loss: 0.580163300037384, Validation Loss: 0.730897843837738\n",
      "Epoch 101: Train Loss: 0.5948626756668091, Validation Loss: 0.73261958360672\n",
      "Epoch 102: Train Loss: 0.6136425971984864, Validation Loss: 0.7382664680480957\n",
      "Epoch 103: Train Loss: 0.6010700583457946, Validation Loss: 0.7413606643676758\n",
      "Epoch 104: Train Loss: 0.5859878301620484, Validation Loss: 0.7445659637451172\n",
      "Epoch 105: Train Loss: 0.5752885341644287, Validation Loss: 0.7441841959953308\n",
      "Epoch 106: Train Loss: 0.5579132497310638, Validation Loss: 0.738444447517395\n",
      "Epoch 107: Train Loss: 0.5856475114822388, Validation Loss: 0.7347061038017273\n",
      "Epoch 108: Train Loss: 0.6239349961280822, Validation Loss: 0.7333229184150696\n",
      "Epoch 109: Train Loss: 0.6120026111602783, Validation Loss: 0.7309583425521851\n",
      "Epoch 110: Train Loss: 0.5691869378089904, Validation Loss: 0.7406827211380005\n",
      "Epoch 111: Train Loss: 0.6230064868927002, Validation Loss: 0.7423043251037598\n",
      "Epoch 112: Train Loss: 0.6147112488746643, Validation Loss: 0.7509514689445496\n",
      "Epoch 113: Train Loss: 0.5933802843093872, Validation Loss: 0.7552007436752319\n",
      "Epoch 114: Train Loss: 0.6144389510154724, Validation Loss: 0.7561995387077332\n",
      "Epoch 115: Train Loss: 0.5559646964073182, Validation Loss: 0.7605645060539246\n",
      "Epoch 116: Train Loss: 0.5826309800148011, Validation Loss: 0.7586562037467957\n",
      "Epoch 117: Train Loss: 0.5847793936729431, Validation Loss: 0.7605080604553223\n",
      "Epoch 118: Train Loss: 0.591631555557251, Validation Loss: 0.7475183010101318\n",
      "Epoch 119: Train Loss: 0.5644548892974853, Validation Loss: 0.7521664500236511\n",
      "Epoch 120: Train Loss: 0.5715028941631317, Validation Loss: 0.7462838888168335\n",
      "Epoch 121: Train Loss: 0.5483382284641266, Validation Loss: 0.753429114818573\n",
      "Epoch 122: Train Loss: 0.6144802570343018, Validation Loss: 0.7543506026268005\n",
      "Epoch 123: Train Loss: 0.5413910746574402, Validation Loss: 0.7578919529914856\n",
      "Epoch 124: Train Loss: 0.6388396918773651, Validation Loss: 0.751217246055603\n",
      "Epoch 125: Train Loss: 0.5628470242023468, Validation Loss: 0.7589667439460754\n",
      "Epoch 126: Train Loss: 0.5954878926277161, Validation Loss: 0.7617834210395813\n",
      "Epoch 127: Train Loss: 0.6461106300354004, Validation Loss: 0.766157329082489\n",
      "Epoch 128: Train Loss: 0.561419153213501, Validation Loss: 0.7623544335365295\n",
      "Epoch 129: Train Loss: 0.5431784152984619, Validation Loss: 0.7630284428596497\n",
      "Epoch 130: Train Loss: 0.6131479859352111, Validation Loss: 0.7672070860862732\n",
      "Epoch 131: Train Loss: 0.5631418704986573, Validation Loss: 0.7712457180023193\n",
      "Epoch 132: Train Loss: 0.5816438674926758, Validation Loss: 0.7666316032409668\n",
      "Epoch 133: Train Loss: 0.6130669593811036, Validation Loss: 0.7696218490600586\n",
      "Epoch 134: Train Loss: 0.547764140367508, Validation Loss: 0.7651597857475281\n",
      "Epoch 135: Train Loss: 0.6151942729949951, Validation Loss: 0.7654940485954285\n",
      "Epoch 136: Train Loss: 0.6120903730392456, Validation Loss: 0.7666671276092529\n",
      "Epoch 137: Train Loss: 0.5095366358757019, Validation Loss: 0.7655118703842163\n",
      "Epoch 138: Train Loss: 0.5744427859783172, Validation Loss: 0.7660865187644958\n",
      "Epoch 139: Train Loss: 0.5551400542259216, Validation Loss: 0.7685571312904358\n",
      "Epoch 140: Train Loss: 0.5491406142711639, Validation Loss: 0.7673899531364441\n",
      "Epoch 141: Train Loss: 0.6092230081558228, Validation Loss: 0.7683713436126709\n",
      "Epoch 142: Train Loss: 0.5381513059139251, Validation Loss: 0.7663530111312866\n",
      "Epoch 143: Train Loss: 0.6321420848369599, Validation Loss: 0.7636775374412537\n",
      "Epoch 144: Train Loss: 0.5601209282875061, Validation Loss: 0.7633501887321472\n",
      "Epoch 145: Train Loss: 0.5580475032329559, Validation Loss: 0.7605662941932678\n",
      "Epoch 146: Train Loss: 0.535671865940094, Validation Loss: 0.7628531455993652\n",
      "Epoch 147: Train Loss: 0.5250557124614715, Validation Loss: 0.766266405582428\n",
      "Epoch 148: Train Loss: 0.5407218873500824, Validation Loss: 0.7657386064529419\n",
      "Epoch 149: Train Loss: 0.6116247773170471, Validation Loss: 0.7663559913635254\n",
      "Epoch 150: Train Loss: 0.5379888117313385, Validation Loss: 0.7667767405509949\n",
      "Epoch 151: Train Loss: 0.5285004496574401, Validation Loss: 0.7720632553100586\n",
      "Epoch 152: Train Loss: 0.5398489654064178, Validation Loss: 0.7662059664726257\n",
      "Epoch 153: Train Loss: 0.5015517830848694, Validation Loss: 0.7760814428329468\n",
      "Epoch 154: Train Loss: 0.5296422362327575, Validation Loss: 0.7777194380760193\n",
      "Epoch 155: Train Loss: 0.5380630791187286, Validation Loss: 0.7683369517326355\n",
      "Epoch 156: Train Loss: 0.5181369602680206, Validation Loss: 0.7806050181388855\n",
      "Epoch 157: Train Loss: 0.6533521771430969, Validation Loss: 0.7856443524360657\n",
      "Epoch 158: Train Loss: 0.5327346801757813, Validation Loss: 0.77723628282547\n",
      "Epoch 159: Train Loss: 0.5748267292976379, Validation Loss: 0.7710384130477905\n",
      "Epoch 160: Train Loss: 0.48798894286155703, Validation Loss: 0.7837015390396118\n",
      "Epoch 161: Train Loss: 0.6126814186573029, Validation Loss: 0.7960651516914368\n",
      "Epoch 162: Train Loss: 0.5418155550956726, Validation Loss: 0.8006828427314758\n",
      "Epoch 163: Train Loss: 0.5493569672107697, Validation Loss: 0.8105720281600952\n",
      "Epoch 164: Train Loss: 0.5294979751110077, Validation Loss: 0.8125724196434021\n",
      "Epoch 165: Train Loss: 0.5745995759963989, Validation Loss: 0.8123157620429993\n",
      "Epoch 166: Train Loss: 0.5307261407375335, Validation Loss: 0.8127486705780029\n",
      "Epoch 167: Train Loss: 0.5750735640525818, Validation Loss: 0.8063872456550598\n",
      "Epoch 168: Train Loss: 0.6471630811691285, Validation Loss: 0.8198583722114563\n",
      "Epoch 169: Train Loss: 0.5898854017257691, Validation Loss: 0.7930524945259094\n",
      "Epoch 170: Train Loss: 0.5558181405067444, Validation Loss: 0.8014602065086365\n",
      "Epoch 171: Train Loss: 0.5193688631057739, Validation Loss: 0.7991088032722473\n",
      "Epoch 172: Train Loss: 0.5135659158229828, Validation Loss: 0.8012124300003052\n",
      "Epoch 173: Train Loss: 0.6045060753822327, Validation Loss: 0.7953632473945618\n",
      "Epoch 174: Train Loss: 0.5791694462299347, Validation Loss: 0.7957793474197388\n",
      "Epoch 175: Train Loss: 0.5407994747161865, Validation Loss: 0.7928469777107239\n",
      "Epoch 176: Train Loss: 0.5348663210868836, Validation Loss: 0.8002433776855469\n",
      "Epoch 177: Train Loss: 0.5259059369564056, Validation Loss: 0.7998458743095398\n",
      "Epoch 178: Train Loss: 0.567172747850418, Validation Loss: 0.8026810884475708\n",
      "Epoch 179: Train Loss: 0.5430454730987548, Validation Loss: 0.8092385530471802\n",
      "Epoch 180: Train Loss: 0.5805994033813476, Validation Loss: 0.786806583404541\n",
      "Epoch 181: Train Loss: 0.5462651014328003, Validation Loss: 0.7912893295288086\n",
      "Epoch 182: Train Loss: 0.556706303358078, Validation Loss: 0.7957075238227844\n",
      "Epoch 183: Train Loss: 0.510363906621933, Validation Loss: 0.7819150686264038\n",
      "Epoch 184: Train Loss: 0.5374477565288543, Validation Loss: 0.7803028225898743\n",
      "Epoch 185: Train Loss: 0.5266083180904388, Validation Loss: 0.7884211540222168\n",
      "Epoch 186: Train Loss: 0.6007551550865173, Validation Loss: 0.7968568205833435\n",
      "Epoch 187: Train Loss: 0.5350861430168152, Validation Loss: 0.7960319519042969\n",
      "Epoch 188: Train Loss: 0.5717666983604431, Validation Loss: 0.781413197517395\n",
      "Epoch 189: Train Loss: 0.5841989696025849, Validation Loss: 0.7837896347045898\n",
      "Epoch 190: Train Loss: 0.4794332146644592, Validation Loss: 0.7913486361503601\n",
      "Epoch 191: Train Loss: 0.48533388376235964, Validation Loss: 0.7981921434402466\n",
      "Epoch 192: Train Loss: 0.6468565285205841, Validation Loss: 0.8142200112342834\n",
      "Epoch 193: Train Loss: 0.4711893260478973, Validation Loss: 0.8180264830589294\n",
      "Epoch 194: Train Loss: 0.5047356486320496, Validation Loss: 0.8249672055244446\n",
      "Epoch 195: Train Loss: 0.48942403197288514, Validation Loss: 0.8217172026634216\n",
      "Epoch 196: Train Loss: 0.465325528383255, Validation Loss: 0.8163467645645142\n",
      "Epoch 197: Train Loss: 0.5146459519863129, Validation Loss: 0.8180306553840637\n",
      "Epoch 198: Train Loss: 0.45203564167022703, Validation Loss: 0.824493408203125\n",
      "Epoch 199: Train Loss: 0.5210538744926453, Validation Loss: 0.8205212950706482\n",
      "Epoch 200: Train Loss: 0.5812683880329133, Validation Loss: 0.825855553150177\n",
      "Epoch 201: Train Loss: 0.5250753819942474, Validation Loss: 0.8220074772834778\n",
      "Epoch 202: Train Loss: 0.4791633665561676, Validation Loss: 0.8137254118919373\n",
      "Epoch 203: Train Loss: 0.5128348410129547, Validation Loss: 0.8184152841567993\n",
      "Epoch 204: Train Loss: 0.5474505722522736, Validation Loss: 0.8190751075744629\n",
      "Epoch 205: Train Loss: 0.4848813652992249, Validation Loss: 0.8110425472259521\n",
      "Epoch 206: Train Loss: 0.5086609065532685, Validation Loss: 0.8091104626655579\n",
      "Epoch 207: Train Loss: 0.4588932394981384, Validation Loss: 0.8146500587463379\n",
      "Epoch 208: Train Loss: 0.47777449488639834, Validation Loss: 0.811424195766449\n",
      "Epoch 209: Train Loss: 0.47127469778060915, Validation Loss: 0.8201499581336975\n",
      "Epoch 210: Train Loss: 0.5609431326389313, Validation Loss: 0.8366689682006836\n",
      "Epoch 211: Train Loss: 0.48452374935150144, Validation Loss: 0.8477891683578491\n",
      "Epoch 212: Train Loss: 0.5072368860244751, Validation Loss: 0.8358949422836304\n",
      "Epoch 213: Train Loss: 0.4869942545890808, Validation Loss: 0.8499923944473267\n",
      "Epoch 214: Train Loss: 0.5054896652698517, Validation Loss: 0.8624557852745056\n",
      "Epoch 215: Train Loss: 0.48611990213394163, Validation Loss: 0.863254964351654\n",
      "Epoch 216: Train Loss: 0.5960618317127228, Validation Loss: 0.8542011380195618\n",
      "Epoch 217: Train Loss: 0.5811773002147674, Validation Loss: 0.8596185445785522\n",
      "Epoch 218: Train Loss: 0.5158435583114624, Validation Loss: 0.86089688539505\n",
      "Epoch 219: Train Loss: 0.47003731727600095, Validation Loss: 0.8537611365318298\n",
      "Epoch 220: Train Loss: 0.5472704708576203, Validation Loss: 0.8565292358398438\n",
      "Epoch 221: Train Loss: 0.49893609881401063, Validation Loss: 0.8547661900520325\n",
      "Epoch 222: Train Loss: 0.500780314207077, Validation Loss: 0.8491228222846985\n",
      "Epoch 223: Train Loss: 0.4380061268806458, Validation Loss: 0.8495269417762756\n",
      "Epoch 224: Train Loss: 0.5941007316112519, Validation Loss: 0.8678234219551086\n",
      "Epoch 225: Train Loss: 0.5344574153423309, Validation Loss: 0.8718971014022827\n",
      "Epoch 226: Train Loss: 0.4325656592845917, Validation Loss: 0.8701352477073669\n",
      "Epoch 227: Train Loss: 0.5449535131454468, Validation Loss: 0.8751471638679504\n",
      "Epoch 228: Train Loss: 0.5069870829582215, Validation Loss: 0.8720235228538513\n",
      "Epoch 229: Train Loss: 0.5534083485603333, Validation Loss: 0.8770931363105774\n",
      "Epoch 230: Train Loss: 0.46888949275016784, Validation Loss: 0.8713981509208679\n",
      "Epoch 231: Train Loss: 0.45591944456100464, Validation Loss: 0.8625560402870178\n",
      "Epoch 232: Train Loss: 0.6696463465690613, Validation Loss: 0.8686656355857849\n",
      "Epoch 233: Train Loss: 0.43837830424308777, Validation Loss: 0.8745986223220825\n",
      "Epoch 234: Train Loss: 0.4136871099472046, Validation Loss: 0.8545306921005249\n",
      "Epoch 235: Train Loss: 0.4944001793861389, Validation Loss: 0.8589111566543579\n",
      "Epoch 236: Train Loss: 0.4277483642101288, Validation Loss: 0.8673622012138367\n",
      "Epoch 237: Train Loss: 0.4772548735141754, Validation Loss: 0.8731900453567505\n",
      "Epoch 238: Train Loss: 0.43301382660865784, Validation Loss: 0.8762667775154114\n",
      "Epoch 239: Train Loss: 0.4894456684589386, Validation Loss: 0.8873058557510376\n",
      "Epoch 240: Train Loss: 0.6074201047420502, Validation Loss: 0.9017598628997803\n",
      "Epoch 241: Train Loss: 0.5111092984676361, Validation Loss: 0.903889000415802\n",
      "Epoch 242: Train Loss: 0.5139527261257172, Validation Loss: 0.8687247633934021\n",
      "Epoch 243: Train Loss: 0.4353587806224823, Validation Loss: 0.8881055116653442\n",
      "Epoch 244: Train Loss: 0.4308643817901611, Validation Loss: 0.8911595940589905\n",
      "Epoch 245: Train Loss: 0.44786061644554137, Validation Loss: 0.8755804300308228\n",
      "Epoch 246: Train Loss: 0.481536066532135, Validation Loss: 0.8923229575157166\n",
      "Epoch 247: Train Loss: 0.48770514130592346, Validation Loss: 0.899206280708313\n",
      "Epoch 248: Train Loss: 0.4649035811424255, Validation Loss: 0.9135488271713257\n",
      "Epoch 249: Train Loss: 0.4640841484069824, Validation Loss: 0.9059891104698181\n",
      "Epoch 250: Train Loss: 0.4811608374118805, Validation Loss: 0.9106754660606384\n",
      "Epoch 251: Train Loss: 0.4558965086936951, Validation Loss: 0.9204587340354919\n",
      "Epoch 252: Train Loss: 0.4790946960449219, Validation Loss: 0.9306433200836182\n",
      "Epoch 253: Train Loss: 0.45954155921936035, Validation Loss: 0.910178005695343\n",
      "Epoch 254: Train Loss: 0.42383822798728943, Validation Loss: 0.9157170653343201\n",
      "Epoch 255: Train Loss: 0.5533400177955627, Validation Loss: 0.9207078814506531\n",
      "Epoch 256: Train Loss: 0.44429960250854494, Validation Loss: 0.9131492972373962\n",
      "Epoch 257: Train Loss: 0.47541418075561526, Validation Loss: 0.9123033285140991\n",
      "Epoch 258: Train Loss: 0.43418065309524534, Validation Loss: 0.9017273187637329\n",
      "Epoch 259: Train Loss: 0.44745099544525146, Validation Loss: 0.9100724458694458\n",
      "Epoch 260: Train Loss: 0.44878610372543337, Validation Loss: 0.9065141081809998\n",
      "Epoch 261: Train Loss: 0.4160410284996033, Validation Loss: 0.8983283638954163\n",
      "Epoch 262: Train Loss: 0.40673003196716306, Validation Loss: 0.903488278388977\n",
      "Epoch 263: Train Loss: 0.4293909966945648, Validation Loss: 0.8992460370063782\n",
      "Epoch 264: Train Loss: 0.433939266204834, Validation Loss: 0.8977727293968201\n",
      "Epoch 265: Train Loss: 0.48293753862380984, Validation Loss: 0.8781970143318176\n",
      "Epoch 266: Train Loss: 0.4070006966590881, Validation Loss: 0.8815730810165405\n",
      "Epoch 267: Train Loss: 0.4425885617733002, Validation Loss: 0.8951581120491028\n",
      "Epoch 268: Train Loss: 0.49945843815803526, Validation Loss: 0.8999547362327576\n",
      "Epoch 269: Train Loss: 0.4356240212917328, Validation Loss: 0.881566047668457\n",
      "Epoch 270: Train Loss: 0.4187044322490692, Validation Loss: 0.8985349535942078\n",
      "Epoch 271: Train Loss: 0.4211569845676422, Validation Loss: 0.9239315390586853\n",
      "Epoch 272: Train Loss: 0.530467277765274, Validation Loss: 0.9468883872032166\n",
      "Epoch 273: Train Loss: 0.399968421459198, Validation Loss: 0.956474244594574\n",
      "Epoch 274: Train Loss: 0.49538223147392274, Validation Loss: 0.9581203460693359\n",
      "Epoch 275: Train Loss: 0.4526416778564453, Validation Loss: 0.9662932753562927\n",
      "Epoch 276: Train Loss: 0.40229734778404236, Validation Loss: 0.9700136184692383\n",
      "Epoch 277: Train Loss: 0.36553347706794737, Validation Loss: 0.9654214978218079\n",
      "Epoch 278: Train Loss: 0.43681182265281676, Validation Loss: 0.9521000385284424\n",
      "Epoch 279: Train Loss: 0.39271135330200196, Validation Loss: 0.9501680135726929\n",
      "Epoch 280: Train Loss: 0.37639473378658295, Validation Loss: 0.9632279872894287\n",
      "Epoch 281: Train Loss: 0.5051678836345672, Validation Loss: 0.977940022945404\n",
      "Epoch 282: Train Loss: 0.4098217964172363, Validation Loss: 0.9636344909667969\n",
      "Epoch 283: Train Loss: 0.4311541259288788, Validation Loss: 0.9545369744300842\n",
      "Epoch 284: Train Loss: 0.4371239244937897, Validation Loss: 0.9729067087173462\n",
      "Epoch 285: Train Loss: 0.45329124927520753, Validation Loss: 0.9964914321899414\n",
      "Epoch 286: Train Loss: 0.3640155017375946, Validation Loss: 0.9838216304779053\n",
      "Epoch 287: Train Loss: 0.41570305824279785, Validation Loss: 0.9709662795066833\n",
      "Epoch 288: Train Loss: 0.39317103624343874, Validation Loss: 0.9789852499961853\n",
      "Epoch 289: Train Loss: 0.48098008036613465, Validation Loss: 0.9720509052276611\n",
      "Epoch 290: Train Loss: 0.3613020420074463, Validation Loss: 0.9670956134796143\n",
      "Epoch 291: Train Loss: 0.4485505998134613, Validation Loss: 0.9962621927261353\n",
      "Epoch 292: Train Loss: 0.3798652082681656, Validation Loss: 1.0077824592590332\n",
      "Epoch 293: Train Loss: 0.45408973693847654, Validation Loss: 1.0084868669509888\n",
      "Epoch 294: Train Loss: 0.4399352312088013, Validation Loss: 1.0133320093154907\n",
      "Epoch 295: Train Loss: 0.39421319365501406, Validation Loss: 1.013881802558899\n",
      "Epoch 296: Train Loss: 0.4610986888408661, Validation Loss: 1.0115015506744385\n",
      "Epoch 297: Train Loss: 0.4018375396728516, Validation Loss: 1.0170111656188965\n",
      "Epoch 298: Train Loss: 0.3430949687957764, Validation Loss: 0.9976492524147034\n",
      "Epoch 299: Train Loss: 0.4034990966320038, Validation Loss: 1.0116783380508423\n",
      "Epoch 300: Train Loss: 0.5380679666996002, Validation Loss: 1.0204946994781494\n",
      "Epoch 301: Train Loss: 0.505486685037613, Validation Loss: 1.029002070426941\n",
      "Epoch 302: Train Loss: 0.4101900577545166, Validation Loss: 0.9654260277748108\n",
      "Epoch 303: Train Loss: 0.40692515969276427, Validation Loss: 0.9578340649604797\n",
      "Epoch 304: Train Loss: 0.3560838341712952, Validation Loss: 0.985124409198761\n",
      "Epoch 305: Train Loss: 0.4034087836742401, Validation Loss: 0.9929489493370056\n",
      "Epoch 306: Train Loss: 0.3768609523773193, Validation Loss: 1.0037997961044312\n",
      "Epoch 307: Train Loss: 0.3556819438934326, Validation Loss: 1.0070029497146606\n",
      "Epoch 308: Train Loss: 0.42173769474029543, Validation Loss: 1.0064963102340698\n",
      "Epoch 309: Train Loss: 0.3464896231889725, Validation Loss: 1.0111981630325317\n",
      "Epoch 310: Train Loss: 0.4333792388439178, Validation Loss: 1.0290052890777588\n",
      "Epoch 311: Train Loss: 0.4523025333881378, Validation Loss: 1.0531220436096191\n",
      "Epoch 312: Train Loss: 0.3933881431818008, Validation Loss: 1.0727778673171997\n",
      "Epoch 313: Train Loss: 0.3577381014823914, Validation Loss: 1.0609537363052368\n",
      "Epoch 314: Train Loss: 0.41583460569381714, Validation Loss: 1.0469701290130615\n",
      "Epoch 315: Train Loss: 0.37025466561317444, Validation Loss: 1.058706283569336\n",
      "Epoch 316: Train Loss: 0.43707175850868224, Validation Loss: 0.9936997890472412\n",
      "Epoch 317: Train Loss: 0.34187816381454467, Validation Loss: 1.014986515045166\n",
      "Epoch 318: Train Loss: 0.3670234739780426, Validation Loss: 1.035858154296875\n",
      "Epoch 319: Train Loss: 0.40862605571746824, Validation Loss: 1.0577764511108398\n",
      "Epoch 320: Train Loss: 0.3831117212772369, Validation Loss: 1.0567492246627808\n",
      "Epoch 321: Train Loss: 0.3569764792919159, Validation Loss: 1.0423998832702637\n",
      "Epoch 322: Train Loss: 0.3288816064596176, Validation Loss: 1.0599699020385742\n",
      "Epoch 323: Train Loss: 0.4790138304233551, Validation Loss: 1.0825244188308716\n",
      "Epoch 324: Train Loss: 0.3396529912948608, Validation Loss: 1.0967987775802612\n",
      "Epoch 325: Train Loss: 0.36796711683273314, Validation Loss: 1.120239019393921\n",
      "Epoch 326: Train Loss: 0.34308294057846067, Validation Loss: 1.1108405590057373\n",
      "Epoch 327: Train Loss: 0.36954988837242125, Validation Loss: 1.0775855779647827\n",
      "Epoch 328: Train Loss: 0.4155286133289337, Validation Loss: 1.0988972187042236\n",
      "Epoch 329: Train Loss: 0.33119137287139894, Validation Loss: 1.1037020683288574\n",
      "Epoch 330: Train Loss: 0.4125826895236969, Validation Loss: 1.0932624340057373\n",
      "Epoch 331: Train Loss: 0.3911851763725281, Validation Loss: 1.1002963781356812\n",
      "Epoch 332: Train Loss: 0.35536291003227233, Validation Loss: 1.1034355163574219\n",
      "Epoch 333: Train Loss: 0.33251992166042327, Validation Loss: 1.1324338912963867\n",
      "Epoch 334: Train Loss: 0.4067712604999542, Validation Loss: 1.1088638305664062\n",
      "Epoch 335: Train Loss: 0.31231785118579863, Validation Loss: 1.1304852962493896\n",
      "Epoch 336: Train Loss: 0.3519828677177429, Validation Loss: 1.1415027379989624\n",
      "Epoch 337: Train Loss: 0.337822812795639, Validation Loss: 1.149673581123352\n",
      "Epoch 338: Train Loss: 0.4002255529165268, Validation Loss: 1.1511139869689941\n",
      "Epoch 339: Train Loss: 0.3955163836479187, Validation Loss: 1.150197148323059\n",
      "Epoch 340: Train Loss: 0.43975397348403933, Validation Loss: 1.1161534786224365\n",
      "Epoch 341: Train Loss: 0.3583228290081024, Validation Loss: 1.149222731590271\n",
      "Epoch 342: Train Loss: 0.3507642149925232, Validation Loss: 1.1635442972183228\n",
      "Epoch 343: Train Loss: 0.3307523250579834, Validation Loss: 1.1508041620254517\n",
      "Epoch 344: Train Loss: 0.42580047249794006, Validation Loss: 1.1489043235778809\n",
      "Epoch 345: Train Loss: 0.3572943687438965, Validation Loss: 1.1407917737960815\n",
      "Epoch 346: Train Loss: 0.4400626838207245, Validation Loss: 1.1588799953460693\n",
      "Epoch 347: Train Loss: 0.36162340044975283, Validation Loss: 1.1592189073562622\n",
      "Epoch 348: Train Loss: 0.36522518992424013, Validation Loss: 1.1695606708526611\n",
      "Epoch 349: Train Loss: 0.3898669719696045, Validation Loss: 1.1687464714050293\n",
      "Epoch 350: Train Loss: 0.5432575047016144, Validation Loss: 1.221186637878418\n",
      "Epoch 351: Train Loss: 0.39669378399848937, Validation Loss: 1.1902211904525757\n",
      "Epoch 352: Train Loss: 0.39143487215042116, Validation Loss: 1.1873496770858765\n",
      "Epoch 353: Train Loss: 0.4554587066173553, Validation Loss: 1.1743333339691162\n",
      "Epoch 354: Train Loss: 0.305089408159256, Validation Loss: 1.128223180770874\n",
      "Epoch 355: Train Loss: 0.31096743047237396, Validation Loss: 1.1293951272964478\n",
      "Epoch 356: Train Loss: 0.32357071042060853, Validation Loss: 1.133575677871704\n",
      "Epoch 357: Train Loss: 0.3033654659986496, Validation Loss: 1.1528199911117554\n",
      "Epoch 358: Train Loss: 0.3203819334506989, Validation Loss: 1.1561734676361084\n",
      "Epoch 359: Train Loss: 0.31823248565196993, Validation Loss: 1.1767287254333496\n",
      "Epoch 360: Train Loss: 0.4301069974899292, Validation Loss: 1.1943297386169434\n",
      "Epoch 361: Train Loss: 0.31671950966119766, Validation Loss: 1.1564120054244995\n",
      "Epoch 362: Train Loss: 0.2926734179258347, Validation Loss: 1.1589689254760742\n",
      "Epoch 363: Train Loss: 0.34744721055030825, Validation Loss: 1.1676232814788818\n",
      "Epoch 364: Train Loss: 0.35413126945495604, Validation Loss: 1.1721240282058716\n",
      "Epoch 365: Train Loss: 0.368698513507843, Validation Loss: 1.1834838390350342\n",
      "Epoch 366: Train Loss: 0.3507711708545685, Validation Loss: 1.1862410306930542\n",
      "Epoch 367: Train Loss: 0.3290852129459381, Validation Loss: 1.13883638381958\n",
      "Epoch 368: Train Loss: 0.3499982237815857, Validation Loss: 1.1549067497253418\n",
      "Epoch 369: Train Loss: 0.3485622465610504, Validation Loss: 1.1507773399353027\n",
      "Epoch 370: Train Loss: 0.3353444874286652, Validation Loss: 1.1621448993682861\n",
      "Epoch 371: Train Loss: 0.2930710047483444, Validation Loss: 1.1783918142318726\n",
      "Epoch 372: Train Loss: 0.3595589518547058, Validation Loss: 1.1879884004592896\n",
      "Epoch 373: Train Loss: 0.2947500169277191, Validation Loss: 1.20724356174469\n",
      "Epoch 374: Train Loss: 0.34576749205589297, Validation Loss: 1.2274757623672485\n",
      "Epoch 375: Train Loss: 0.3099551260471344, Validation Loss: 1.2294468879699707\n",
      "Epoch 376: Train Loss: 0.33953296542167666, Validation Loss: 1.2005250453948975\n",
      "Epoch 377: Train Loss: 0.31043280065059664, Validation Loss: 1.197205662727356\n",
      "Epoch 378: Train Loss: 0.30543923676013945, Validation Loss: 1.226494312286377\n",
      "Epoch 379: Train Loss: 0.34412996768951415, Validation Loss: 1.2401491403579712\n",
      "Epoch 380: Train Loss: 0.3172826111316681, Validation Loss: 1.2412946224212646\n",
      "Epoch 381: Train Loss: 0.3565063774585724, Validation Loss: 1.2060422897338867\n",
      "Epoch 382: Train Loss: 0.43001545667648317, Validation Loss: 1.2548060417175293\n",
      "Epoch 383: Train Loss: 0.29952042996883393, Validation Loss: 1.2671278715133667\n",
      "Epoch 384: Train Loss: 0.297361233830452, Validation Loss: 1.2658002376556396\n",
      "Epoch 385: Train Loss: 0.28578605949878694, Validation Loss: 1.26860773563385\n",
      "Epoch 386: Train Loss: 0.4262117803096771, Validation Loss: 1.26681387424469\n",
      "Epoch 387: Train Loss: 0.397191533446312, Validation Loss: 1.2733583450317383\n",
      "Epoch 388: Train Loss: 0.26821271181106565, Validation Loss: 1.2703769207000732\n",
      "Epoch 389: Train Loss: 0.2711261481046677, Validation Loss: 1.2636785507202148\n",
      "Epoch 390: Train Loss: 0.40129448771476744, Validation Loss: 1.2596932649612427\n",
      "Epoch 391: Train Loss: 0.3139533817768097, Validation Loss: 1.274063229560852\n",
      "Epoch 392: Train Loss: 0.2716153681278229, Validation Loss: 1.2625882625579834\n",
      "Epoch 393: Train Loss: 0.5872217804193497, Validation Loss: 1.2877277135849\n",
      "Epoch 394: Train Loss: 0.3084162652492523, Validation Loss: 1.233561635017395\n",
      "Epoch 395: Train Loss: 0.3485400199890137, Validation Loss: 1.2756937742233276\n",
      "Epoch 396: Train Loss: 0.39068478941917417, Validation Loss: 1.278336524963379\n",
      "Epoch 397: Train Loss: 0.30595832467079165, Validation Loss: 1.242098331451416\n",
      "Epoch 398: Train Loss: 0.3152799248695374, Validation Loss: 1.2577229738235474\n",
      "Epoch 399: Train Loss: 0.30168373584747316, Validation Loss: 1.2610039710998535\n",
      "Epoch 400: Train Loss: 0.34088607132434845, Validation Loss: 1.2479459047317505\n",
      "Epoch 401: Train Loss: 0.33221601247787474, Validation Loss: 1.2741338014602661\n",
      "Epoch 402: Train Loss: 0.27944332361221313, Validation Loss: 1.233475685119629\n",
      "Epoch 403: Train Loss: 0.27338134348392484, Validation Loss: 1.2565053701400757\n",
      "Epoch 404: Train Loss: 0.28720963597297666, Validation Loss: 1.2310264110565186\n",
      "Epoch 405: Train Loss: 0.34412446320056916, Validation Loss: 1.287484884262085\n",
      "Epoch 406: Train Loss: 0.33056946396827697, Validation Loss: 1.3223730325698853\n",
      "Epoch 407: Train Loss: 0.3030675858259201, Validation Loss: 1.2695293426513672\n",
      "Epoch 408: Train Loss: 0.26509451270103457, Validation Loss: 1.2779630422592163\n",
      "Epoch 409: Train Loss: 0.33577853739261626, Validation Loss: 1.2842992544174194\n",
      "Epoch 410: Train Loss: 0.32872732281684874, Validation Loss: 1.314278244972229\n",
      "Epoch 411: Train Loss: 0.33614336848258974, Validation Loss: 1.302904486656189\n",
      "Epoch 412: Train Loss: 0.2745559513568878, Validation Loss: 1.2798573970794678\n",
      "Epoch 413: Train Loss: 0.31771265268325805, Validation Loss: 1.2866460084915161\n",
      "Epoch 414: Train Loss: 0.43737486004829407, Validation Loss: 1.3376954793930054\n",
      "Epoch 415: Train Loss: 0.2996661365032196, Validation Loss: 1.3269470930099487\n",
      "Epoch 416: Train Loss: 0.30773247480392457, Validation Loss: 1.245281457901001\n",
      "Epoch 417: Train Loss: 0.29835466742515565, Validation Loss: 1.2797013521194458\n",
      "Epoch 418: Train Loss: 0.41075366735458374, Validation Loss: 1.3126152753829956\n",
      "Epoch 419: Train Loss: 0.34573315978050234, Validation Loss: 1.3400386571884155\n",
      "Epoch 420: Train Loss: 0.293086963891983, Validation Loss: 1.327886700630188\n",
      "Epoch 421: Train Loss: 0.30055713653564453, Validation Loss: 1.3605310916900635\n",
      "Epoch 422: Train Loss: 0.33859267830848694, Validation Loss: 1.3780019283294678\n",
      "Epoch 423: Train Loss: 0.26873300671577455, Validation Loss: 1.360482096672058\n",
      "Epoch 424: Train Loss: 0.2786117047071457, Validation Loss: 1.3634639978408813\n",
      "Epoch 425: Train Loss: 0.36199959814548494, Validation Loss: 1.3652372360229492\n",
      "Epoch 426: Train Loss: 0.27058705389499665, Validation Loss: 1.3557403087615967\n",
      "Epoch 427: Train Loss: 0.28292659521102903, Validation Loss: 1.3475341796875\n",
      "Epoch 428: Train Loss: 0.27535703778266907, Validation Loss: 1.3609633445739746\n",
      "Epoch 429: Train Loss: 0.3139499008655548, Validation Loss: 1.3899970054626465\n",
      "Epoch 430: Train Loss: 0.304399174451828, Validation Loss: 1.4022791385650635\n",
      "Epoch 431: Train Loss: 0.2576811045408249, Validation Loss: 1.3781973123550415\n",
      "Epoch 432: Train Loss: 0.28593069314956665, Validation Loss: 1.363655924797058\n",
      "Epoch 433: Train Loss: 0.25687948763370516, Validation Loss: 1.335614800453186\n",
      "Epoch 434: Train Loss: 0.2455260217189789, Validation Loss: 1.3193215131759644\n",
      "Epoch 435: Train Loss: 0.335620579123497, Validation Loss: 1.3465245962142944\n",
      "Epoch 436: Train Loss: 0.26505513191223146, Validation Loss: 1.358983039855957\n",
      "Epoch 437: Train Loss: 0.29064205586910247, Validation Loss: 1.3195159435272217\n",
      "Epoch 438: Train Loss: 0.2981769233942032, Validation Loss: 1.3510279655456543\n",
      "Epoch 439: Train Loss: 0.31568228304386137, Validation Loss: 1.3730494976043701\n",
      "Epoch 440: Train Loss: 0.32594484090805054, Validation Loss: 1.306079626083374\n",
      "Epoch 441: Train Loss: 0.3566372811794281, Validation Loss: 1.3709449768066406\n",
      "Epoch 442: Train Loss: 0.29051843881607053, Validation Loss: 1.382508397102356\n",
      "Epoch 443: Train Loss: 0.26805390119552613, Validation Loss: 1.445354700088501\n",
      "Epoch 444: Train Loss: 0.3198574334383011, Validation Loss: 1.41982102394104\n",
      "Epoch 445: Train Loss: 0.38337588906288145, Validation Loss: 1.4008442163467407\n",
      "Epoch 446: Train Loss: 0.23908143043518065, Validation Loss: 1.3958057165145874\n",
      "Epoch 447: Train Loss: 0.2884958475828171, Validation Loss: 1.3785276412963867\n",
      "Epoch 448: Train Loss: 0.29406011402606963, Validation Loss: 1.3818937540054321\n",
      "Epoch 449: Train Loss: 0.25592483282089235, Validation Loss: 1.397690773010254\n",
      "Epoch 450: Train Loss: 0.3744239568710327, Validation Loss: 1.3876553773880005\n",
      "Epoch 451: Train Loss: 0.2662511169910431, Validation Loss: 1.3886611461639404\n",
      "Epoch 452: Train Loss: 0.27159281075000763, Validation Loss: 1.369873046875\n",
      "Epoch 453: Train Loss: 0.2634775280952454, Validation Loss: 1.3495272397994995\n",
      "Epoch 454: Train Loss: 0.2725651770830154, Validation Loss: 1.3652700185775757\n",
      "Epoch 455: Train Loss: 0.29123633801937104, Validation Loss: 1.3805330991744995\n",
      "Epoch 456: Train Loss: 0.4495042085647583, Validation Loss: 1.4190783500671387\n",
      "Epoch 457: Train Loss: 0.2596115469932556, Validation Loss: 1.4033774137496948\n",
      "Epoch 458: Train Loss: 0.4012877821922302, Validation Loss: 1.4143480062484741\n",
      "Epoch 459: Train Loss: 0.29735436141490934, Validation Loss: 1.3954046964645386\n",
      "Epoch 460: Train Loss: 0.32220864593982695, Validation Loss: 1.3938716650009155\n",
      "Epoch 461: Train Loss: 0.2853112667798996, Validation Loss: 1.2978415489196777\n",
      "Epoch 462: Train Loss: 0.2910952001810074, Validation Loss: 1.27732253074646\n",
      "Epoch 463: Train Loss: 0.2523921340703964, Validation Loss: 1.2914760112762451\n",
      "Epoch 464: Train Loss: 0.29201326072216033, Validation Loss: 1.3721669912338257\n",
      "Epoch 465: Train Loss: 0.286451181769371, Validation Loss: 1.342127799987793\n",
      "Epoch 466: Train Loss: 0.2364385336637497, Validation Loss: 1.3697837591171265\n",
      "Epoch 467: Train Loss: 0.25642678141593933, Validation Loss: 1.411642074584961\n",
      "Epoch 468: Train Loss: 0.3186751753091812, Validation Loss: 1.3948253393173218\n",
      "Epoch 469: Train Loss: 0.34194886684417725, Validation Loss: 1.3552930355072021\n",
      "Epoch 470: Train Loss: 0.2976514995098114, Validation Loss: 1.4199621677398682\n",
      "Epoch 471: Train Loss: 0.27961900234222414, Validation Loss: 1.4432834386825562\n",
      "Epoch 472: Train Loss: 0.24928903877735137, Validation Loss: 1.4673900604248047\n",
      "Epoch 473: Train Loss: 0.24021396040916443, Validation Loss: 1.4942259788513184\n",
      "Epoch 474: Train Loss: 0.23651741147041322, Validation Loss: 1.4300665855407715\n",
      "Epoch 475: Train Loss: 0.27059710025787354, Validation Loss: 1.4417709112167358\n",
      "Epoch 476: Train Loss: 0.3944987148046494, Validation Loss: 1.5054699182510376\n",
      "Epoch 477: Train Loss: 0.3099158763885498, Validation Loss: 1.463358998298645\n",
      "Epoch 478: Train Loss: 0.34684808254241944, Validation Loss: 1.5291250944137573\n",
      "Epoch 479: Train Loss: 0.2428973361849785, Validation Loss: 1.4754583835601807\n",
      "Epoch 480: Train Loss: 0.2685236319899559, Validation Loss: 1.4686247110366821\n",
      "Epoch 481: Train Loss: 0.32922926247119905, Validation Loss: 1.5069046020507812\n",
      "Epoch 482: Train Loss: 0.24037948846817017, Validation Loss: 1.4883276224136353\n",
      "Epoch 483: Train Loss: 0.29308720827102663, Validation Loss: 1.4967435598373413\n",
      "Epoch 484: Train Loss: 0.2541817665100098, Validation Loss: 1.5216959714889526\n",
      "Epoch 485: Train Loss: 0.2975068807601929, Validation Loss: 1.5057673454284668\n",
      "Epoch 486: Train Loss: 0.30211872756481173, Validation Loss: 1.519046664237976\n",
      "Epoch 487: Train Loss: 0.29906568229198455, Validation Loss: 1.4568278789520264\n",
      "Epoch 488: Train Loss: 0.2825679212808609, Validation Loss: 1.4273439645767212\n",
      "Epoch 489: Train Loss: 0.26072686314582827, Validation Loss: 1.4387733936309814\n",
      "Epoch 490: Train Loss: 0.2545015186071396, Validation Loss: 1.4920283555984497\n",
      "Epoch 491: Train Loss: 0.26835869550704955, Validation Loss: 1.457442283630371\n",
      "Epoch 492: Train Loss: 0.30215132236480713, Validation Loss: 1.4586840867996216\n",
      "Epoch 493: Train Loss: 0.3490697413682938, Validation Loss: 1.4298986196517944\n",
      "Epoch 494: Train Loss: 0.32981160581111907, Validation Loss: 1.4382545948028564\n",
      "Epoch 495: Train Loss: 0.2290613532066345, Validation Loss: 1.3898730278015137\n",
      "Epoch 496: Train Loss: 0.28927392065525054, Validation Loss: 1.431433081626892\n",
      "Epoch 497: Train Loss: 0.3973472684621811, Validation Loss: 1.4726412296295166\n",
      "Epoch 498: Train Loss: 0.24188182950019838, Validation Loss: 1.4578508138656616\n",
      "Epoch 499: Train Loss: 0.20781985223293303, Validation Loss: 1.451385259628296\n",
      "Fold 2 Metrics:\n",
      "Accuracy: 0.2727272727272727, Precision: 0.3333333333333333, Recall: 0.3333333333333333, F1-score: 0.3333333333333333, AUC: 0.2666666666666666\n",
      "Confusion Matrix:\n",
      "[[1 4]\n",
      " [4 2]]\n",
      "Completed fold 2\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples from subject 3 to test set\n",
      "Adding 6 truth samples from subject 3 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 250)\n",
      "(132, 65, 250)\n",
      "Epoch 0: Train Loss: 0.6648155927658081, Validation Loss: 0.6412967443466187\n",
      "Epoch 1: Train Loss: 0.7002358317375184, Validation Loss: 0.6311526894569397\n",
      "Epoch 2: Train Loss: 0.6936002254486084, Validation Loss: 0.624323308467865\n",
      "Epoch 3: Train Loss: 0.774685287475586, Validation Loss: 0.6218220591545105\n",
      "Epoch 4: Train Loss: 0.6982611536979675, Validation Loss: 0.6153980493545532\n",
      "Epoch 5: Train Loss: 0.7409222960472107, Validation Loss: 0.6262592077255249\n",
      "Epoch 6: Train Loss: 0.6615826547145843, Validation Loss: 0.6160876154899597\n",
      "Epoch 7: Train Loss: 0.6538422703742981, Validation Loss: 0.6124759316444397\n",
      "Epoch 8: Train Loss: 0.6924461483955383, Validation Loss: 0.6080629825592041\n",
      "Epoch 9: Train Loss: 0.7246666193008423, Validation Loss: 0.6231246590614319\n",
      "Epoch 10: Train Loss: 0.6896427035331726, Validation Loss: 0.626778781414032\n",
      "Epoch 11: Train Loss: 0.6652406096458435, Validation Loss: 0.6267797350883484\n",
      "Epoch 12: Train Loss: 0.7267785549163819, Validation Loss: 0.6271926760673523\n",
      "Epoch 13: Train Loss: 0.6843846440315247, Validation Loss: 0.6389180421829224\n",
      "Epoch 14: Train Loss: 0.7413596868515014, Validation Loss: 0.6577252149581909\n",
      "Epoch 15: Train Loss: 0.7169867634773255, Validation Loss: 0.6586609482765198\n",
      "Epoch 16: Train Loss: 0.6877829432487488, Validation Loss: 0.6628592014312744\n",
      "Epoch 17: Train Loss: 0.6668472409248352, Validation Loss: 0.6615809202194214\n",
      "Epoch 18: Train Loss: 0.6718718886375428, Validation Loss: 0.6651455760002136\n",
      "Epoch 19: Train Loss: 0.6553158521652221, Validation Loss: 0.6665799021720886\n",
      "Epoch 20: Train Loss: 0.7343221664428711, Validation Loss: 0.6743170619010925\n",
      "Epoch 21: Train Loss: 0.7137033820152283, Validation Loss: 0.6945685148239136\n",
      "Epoch 22: Train Loss: 0.5995896995067597, Validation Loss: 0.7056964635848999\n",
      "Epoch 23: Train Loss: 0.6366783261299134, Validation Loss: 0.7143474817276001\n",
      "Epoch 24: Train Loss: 0.717391288280487, Validation Loss: 0.7403378486633301\n",
      "Epoch 25: Train Loss: 0.6712362051010132, Validation Loss: 0.7415282130241394\n",
      "Epoch 26: Train Loss: 0.6436967253684998, Validation Loss: 0.746187150478363\n",
      "Epoch 27: Train Loss: 0.7188542485237122, Validation Loss: 0.752882182598114\n",
      "Epoch 28: Train Loss: 0.6632493495941162, Validation Loss: 0.7516406178474426\n",
      "Epoch 29: Train Loss: 0.684153938293457, Validation Loss: 0.7597052454948425\n",
      "Epoch 30: Train Loss: 0.6476705312728882, Validation Loss: 0.7810655236244202\n",
      "Epoch 31: Train Loss: 0.6895236849784852, Validation Loss: 0.7868293523788452\n",
      "Epoch 32: Train Loss: 0.6742709755897522, Validation Loss: 0.8073197603225708\n",
      "Epoch 33: Train Loss: 0.6114696145057679, Validation Loss: 0.8126358985900879\n",
      "Epoch 34: Train Loss: 0.5986634969711304, Validation Loss: 0.8261851668357849\n",
      "Epoch 35: Train Loss: 0.6798102617263794, Validation Loss: 0.8443170189857483\n",
      "Epoch 36: Train Loss: 0.7025557279586792, Validation Loss: 0.853776752948761\n",
      "Epoch 37: Train Loss: 0.6565124988555908, Validation Loss: 0.8559311032295227\n",
      "Epoch 38: Train Loss: 0.6682246923446655, Validation Loss: 0.8573774099349976\n",
      "Epoch 39: Train Loss: 0.644658875465393, Validation Loss: 0.8585622310638428\n",
      "Epoch 40: Train Loss: 0.6183428525924682, Validation Loss: 0.875614583492279\n",
      "Epoch 41: Train Loss: 0.6227336883544922, Validation Loss: 0.8821016550064087\n",
      "Epoch 42: Train Loss: 0.6089135050773621, Validation Loss: 0.9000297784805298\n",
      "Epoch 43: Train Loss: 0.6194871664047241, Validation Loss: 0.9170526266098022\n",
      "Epoch 44: Train Loss: 0.6515242338180542, Validation Loss: 0.9262917041778564\n",
      "Epoch 45: Train Loss: 0.7279152274131775, Validation Loss: 0.929728090763092\n",
      "Epoch 46: Train Loss: 0.6342145085334778, Validation Loss: 0.92330002784729\n",
      "Epoch 47: Train Loss: 0.5919144988059998, Validation Loss: 0.918736457824707\n",
      "Epoch 48: Train Loss: 0.6280940890312194, Validation Loss: 0.9103319644927979\n",
      "Epoch 49: Train Loss: 0.5879754364490509, Validation Loss: 0.9073848128318787\n",
      "Epoch 50: Train Loss: 0.6073672711849213, Validation Loss: 0.9256357550621033\n",
      "Epoch 51: Train Loss: 0.6452699303627014, Validation Loss: 0.9589599967002869\n",
      "Epoch 52: Train Loss: 0.6156558752059936, Validation Loss: 0.9615676999092102\n",
      "Epoch 53: Train Loss: 0.6220657825469971, Validation Loss: 0.975974977016449\n",
      "Epoch 54: Train Loss: 0.6114310264587403, Validation Loss: 0.9802852272987366\n",
      "Epoch 55: Train Loss: 0.5940222382545471, Validation Loss: 0.9804959893226624\n",
      "Epoch 56: Train Loss: 0.640168058872223, Validation Loss: 0.9910979866981506\n",
      "Epoch 57: Train Loss: 0.6250242829322815, Validation Loss: 1.0033812522888184\n",
      "Epoch 58: Train Loss: 0.5875486373901367, Validation Loss: 0.9969183206558228\n",
      "Epoch 59: Train Loss: 0.6004422068595886, Validation Loss: 1.0027577877044678\n",
      "Epoch 60: Train Loss: 0.5970850050449371, Validation Loss: 1.0014307498931885\n",
      "Epoch 61: Train Loss: 0.618601405620575, Validation Loss: 0.997658908367157\n",
      "Epoch 62: Train Loss: 0.6700140357017517, Validation Loss: 1.0111192464828491\n",
      "Epoch 63: Train Loss: 0.632020103931427, Validation Loss: 1.0171271562576294\n",
      "Epoch 64: Train Loss: 0.6560903429985047, Validation Loss: 1.0258872509002686\n",
      "Epoch 65: Train Loss: 0.5579766511917115, Validation Loss: 1.0365363359451294\n",
      "Epoch 66: Train Loss: 0.6484983682632446, Validation Loss: 1.0319527387619019\n",
      "Epoch 67: Train Loss: 0.6202383160591125, Validation Loss: 1.027451992034912\n",
      "Epoch 68: Train Loss: 0.643165934085846, Validation Loss: 1.0216107368469238\n",
      "Epoch 69: Train Loss: 0.5968985795974732, Validation Loss: 1.0239120721817017\n",
      "Epoch 70: Train Loss: 0.6123316168785096, Validation Loss: 1.0179095268249512\n",
      "Epoch 71: Train Loss: 0.5951522588729858, Validation Loss: 1.0205309391021729\n",
      "Epoch 72: Train Loss: 0.5686920583248138, Validation Loss: 1.0219159126281738\n",
      "Epoch 73: Train Loss: 0.6526052951812744, Validation Loss: 1.0425423383712769\n",
      "Epoch 74: Train Loss: 0.6349812865257263, Validation Loss: 1.0773828029632568\n",
      "Epoch 75: Train Loss: 0.5389192283153534, Validation Loss: 1.0664410591125488\n",
      "Epoch 76: Train Loss: 0.6274661898612977, Validation Loss: 1.0648562908172607\n",
      "Epoch 77: Train Loss: 0.5821415662765503, Validation Loss: 1.0695825815200806\n",
      "Epoch 78: Train Loss: 0.5663346886634827, Validation Loss: 1.0733479261398315\n",
      "Epoch 79: Train Loss: 0.5937835335731506, Validation Loss: 1.0761817693710327\n",
      "Epoch 80: Train Loss: 0.6130509972572327, Validation Loss: 1.0856165885925293\n",
      "Epoch 81: Train Loss: 0.6198714137077331, Validation Loss: 1.0836119651794434\n",
      "Epoch 82: Train Loss: 0.5596742630004883, Validation Loss: 1.1001667976379395\n",
      "Epoch 83: Train Loss: 0.5438123643398285, Validation Loss: 1.098798394203186\n",
      "Epoch 84: Train Loss: 0.6135007023811341, Validation Loss: 1.1130849123001099\n",
      "Epoch 85: Train Loss: 0.6359839320182801, Validation Loss: 1.1232163906097412\n",
      "Epoch 86: Train Loss: 0.545678299665451, Validation Loss: 1.1134365797042847\n",
      "Epoch 87: Train Loss: 0.5757385194301605, Validation Loss: 1.1097317934036255\n",
      "Epoch 88: Train Loss: 0.6100483298301697, Validation Loss: 1.1010463237762451\n",
      "Epoch 89: Train Loss: 0.5591159343719483, Validation Loss: 1.0882909297943115\n",
      "Epoch 90: Train Loss: 0.6137418746948242, Validation Loss: 1.1379168033599854\n",
      "Epoch 91: Train Loss: 0.5583767890930176, Validation Loss: 1.1629263162612915\n",
      "Epoch 92: Train Loss: 0.5764046549797058, Validation Loss: 1.1659337282180786\n",
      "Epoch 93: Train Loss: 0.5906881093978882, Validation Loss: 1.1783194541931152\n",
      "Epoch 94: Train Loss: 0.514374566078186, Validation Loss: 1.172143816947937\n",
      "Epoch 95: Train Loss: 0.5807970404624939, Validation Loss: 1.203814148902893\n",
      "Epoch 96: Train Loss: 0.5209692776203155, Validation Loss: 1.2010271549224854\n",
      "Epoch 97: Train Loss: 0.5533191978931427, Validation Loss: 1.1978611946105957\n",
      "Epoch 98: Train Loss: 0.6059367656707764, Validation Loss: 1.1974929571151733\n",
      "Epoch 99: Train Loss: 0.5929579138755798, Validation Loss: 1.1937698125839233\n",
      "Epoch 100: Train Loss: 0.5447274923324585, Validation Loss: 1.215786337852478\n",
      "Epoch 101: Train Loss: 0.5560162782669067, Validation Loss: 1.2354477643966675\n",
      "Epoch 102: Train Loss: 0.5626473963260651, Validation Loss: 1.2443511486053467\n",
      "Epoch 103: Train Loss: 0.535606187582016, Validation Loss: 1.246958613395691\n",
      "Epoch 104: Train Loss: 0.5590233564376831, Validation Loss: 1.241256594657898\n",
      "Epoch 105: Train Loss: 0.6166227579116821, Validation Loss: 1.2469197511672974\n",
      "Epoch 106: Train Loss: 0.5088427007198334, Validation Loss: 1.2570123672485352\n",
      "Epoch 107: Train Loss: 0.5948750615119934, Validation Loss: 1.267673373222351\n",
      "Epoch 108: Train Loss: 0.5737088799476624, Validation Loss: 1.2801194190979004\n",
      "Epoch 109: Train Loss: 0.6166942596435547, Validation Loss: 1.2815991640090942\n",
      "Epoch 110: Train Loss: 0.5320033073425293, Validation Loss: 1.2864352464675903\n",
      "Epoch 111: Train Loss: 0.5896518468856812, Validation Loss: 1.3004034757614136\n",
      "Epoch 112: Train Loss: 0.5708629012107849, Validation Loss: 1.2991069555282593\n",
      "Epoch 113: Train Loss: 0.6182932734489441, Validation Loss: 1.2778661251068115\n",
      "Epoch 114: Train Loss: 0.5257665634155273, Validation Loss: 1.2934794425964355\n",
      "Epoch 115: Train Loss: 0.5164974987506866, Validation Loss: 1.2879818677902222\n",
      "Epoch 116: Train Loss: 0.580299460887909, Validation Loss: 1.3106549978256226\n",
      "Epoch 117: Train Loss: 0.5622995615005493, Validation Loss: 1.319813847541809\n",
      "Epoch 118: Train Loss: 0.5548373103141785, Validation Loss: 1.3107036352157593\n",
      "Epoch 119: Train Loss: 0.5125311195850373, Validation Loss: 1.3134561777114868\n",
      "Epoch 120: Train Loss: 0.5167315244674683, Validation Loss: 1.316105842590332\n",
      "Epoch 121: Train Loss: 0.5642417788505554, Validation Loss: 1.347153902053833\n",
      "Epoch 122: Train Loss: 0.5449884295463562, Validation Loss: 1.3686771392822266\n",
      "Epoch 123: Train Loss: 0.5983454823493958, Validation Loss: 1.3996537923812866\n",
      "Epoch 124: Train Loss: 0.5451813101768493, Validation Loss: 1.3936465978622437\n",
      "Epoch 125: Train Loss: 0.5432843208312989, Validation Loss: 1.3929979801177979\n",
      "Epoch 126: Train Loss: 0.5973086714744568, Validation Loss: 1.4046262502670288\n",
      "Epoch 127: Train Loss: 0.5606307983398438, Validation Loss: 1.4028146266937256\n",
      "Epoch 128: Train Loss: 0.5429244220256806, Validation Loss: 1.4042295217514038\n",
      "Epoch 129: Train Loss: 0.5229762434959412, Validation Loss: 1.373485803604126\n",
      "Epoch 130: Train Loss: 0.6066753923892975, Validation Loss: 1.3822457790374756\n",
      "Epoch 131: Train Loss: 0.49879777431488037, Validation Loss: 1.375659704208374\n",
      "Epoch 132: Train Loss: 0.5340911030769349, Validation Loss: 1.3803411722183228\n",
      "Epoch 133: Train Loss: 0.5364795386791229, Validation Loss: 1.3894704580307007\n",
      "Epoch 134: Train Loss: 0.5332012951374054, Validation Loss: 1.3834120035171509\n",
      "Epoch 135: Train Loss: 0.5294133126735687, Validation Loss: 1.3812094926834106\n",
      "Epoch 136: Train Loss: 0.5518703579902648, Validation Loss: 1.3709535598754883\n",
      "Epoch 137: Train Loss: 0.5540527939796448, Validation Loss: 1.3856804370880127\n",
      "Epoch 138: Train Loss: 0.56400545835495, Validation Loss: 1.3956353664398193\n",
      "Epoch 139: Train Loss: 0.5801756620407105, Validation Loss: 1.3901904821395874\n",
      "Epoch 140: Train Loss: 0.4902977705001831, Validation Loss: 1.3887379169464111\n",
      "Epoch 141: Train Loss: 0.536568409204483, Validation Loss: 1.4007062911987305\n",
      "Epoch 142: Train Loss: 0.49125300645828246, Validation Loss: 1.414243459701538\n",
      "Epoch 143: Train Loss: 0.5268269777297974, Validation Loss: 1.4046869277954102\n",
      "Epoch 144: Train Loss: 0.4822823226451874, Validation Loss: 1.435295820236206\n",
      "Epoch 145: Train Loss: 0.5086928009986877, Validation Loss: 1.4274951219558716\n",
      "Epoch 146: Train Loss: 0.47093337774276733, Validation Loss: 1.4256290197372437\n",
      "Epoch 147: Train Loss: 0.49544706344604494, Validation Loss: 1.4514585733413696\n",
      "Epoch 148: Train Loss: 0.5429122567176818, Validation Loss: 1.4485965967178345\n",
      "Epoch 149: Train Loss: 0.5079855799674988, Validation Loss: 1.4083868265151978\n",
      "Epoch 150: Train Loss: 0.5275988042354584, Validation Loss: 1.4680899381637573\n",
      "Epoch 151: Train Loss: 0.5099217116832733, Validation Loss: 1.5130786895751953\n",
      "Epoch 152: Train Loss: 0.5121851742267609, Validation Loss: 1.572097659111023\n",
      "Epoch 153: Train Loss: 0.4699579954147339, Validation Loss: 1.5808910131454468\n",
      "Epoch 154: Train Loss: 0.48390212655067444, Validation Loss: 1.5722217559814453\n",
      "Epoch 155: Train Loss: 0.45667709708213805, Validation Loss: 1.5761821269989014\n",
      "Epoch 156: Train Loss: 0.5061409592628479, Validation Loss: 1.5443614721298218\n",
      "Epoch 157: Train Loss: 0.5785176932811738, Validation Loss: 1.5885248184204102\n",
      "Epoch 158: Train Loss: 0.46531081199645996, Validation Loss: 1.5978116989135742\n",
      "Epoch 159: Train Loss: 0.49166646003723147, Validation Loss: 1.5891166925430298\n",
      "Epoch 160: Train Loss: 0.46132226586341857, Validation Loss: 1.5955027341842651\n",
      "Epoch 161: Train Loss: 0.4863161504268646, Validation Loss: 1.6016525030136108\n",
      "Epoch 162: Train Loss: 0.5243567526340485, Validation Loss: 1.6322513818740845\n",
      "Epoch 163: Train Loss: 0.5575734972953796, Validation Loss: 1.6105178594589233\n",
      "Epoch 164: Train Loss: 0.4803577959537506, Validation Loss: 1.626774549484253\n",
      "Epoch 165: Train Loss: 0.42805943489074705, Validation Loss: 1.611654281616211\n",
      "Epoch 166: Train Loss: 0.47734531164169314, Validation Loss: 1.6415618658065796\n",
      "Epoch 167: Train Loss: 0.44369007349014283, Validation Loss: 1.6038154363632202\n",
      "Epoch 168: Train Loss: 0.4520476281642914, Validation Loss: 1.564737319946289\n",
      "Epoch 169: Train Loss: 0.45271570682525636, Validation Loss: 1.5916635990142822\n",
      "Epoch 170: Train Loss: 0.583570021390915, Validation Loss: 1.61513352394104\n",
      "Epoch 171: Train Loss: 0.5440551578998566, Validation Loss: 1.6042068004608154\n",
      "Epoch 172: Train Loss: 0.442031466960907, Validation Loss: 1.602484107017517\n",
      "Epoch 173: Train Loss: 0.47632811665534974, Validation Loss: 1.590941071510315\n",
      "Epoch 174: Train Loss: 0.4467780292034149, Validation Loss: 1.604326844215393\n",
      "Epoch 175: Train Loss: 0.5208129942417145, Validation Loss: 1.5988037586212158\n",
      "Epoch 176: Train Loss: 0.5054546296596527, Validation Loss: 1.610192060470581\n",
      "Epoch 177: Train Loss: 0.44546971917152406, Validation Loss: 1.5654010772705078\n",
      "Epoch 178: Train Loss: 0.49404768347740174, Validation Loss: 1.6066757440567017\n",
      "Epoch 179: Train Loss: 0.4906811058521271, Validation Loss: 1.5989809036254883\n",
      "Epoch 180: Train Loss: 0.41784539818763733, Validation Loss: 1.6266498565673828\n",
      "Epoch 181: Train Loss: 0.45945257544517515, Validation Loss: 1.6427017450332642\n",
      "Epoch 182: Train Loss: 0.5079237699508667, Validation Loss: 1.6493338346481323\n",
      "Epoch 183: Train Loss: 0.4674578845500946, Validation Loss: 1.6561697721481323\n",
      "Epoch 184: Train Loss: 0.4778944253921509, Validation Loss: 1.6429060697555542\n",
      "Epoch 185: Train Loss: 0.5157185971736908, Validation Loss: 1.669739842414856\n",
      "Epoch 186: Train Loss: 0.4799813091754913, Validation Loss: 1.6585570573806763\n",
      "Epoch 187: Train Loss: 0.4509862720966339, Validation Loss: 1.6514732837677002\n",
      "Epoch 188: Train Loss: 0.4711055815219879, Validation Loss: 1.6451046466827393\n",
      "Epoch 189: Train Loss: 0.4900850236415863, Validation Loss: 1.6485121250152588\n",
      "Epoch 190: Train Loss: 0.46025956273078916, Validation Loss: 1.6436465978622437\n",
      "Epoch 191: Train Loss: 0.48656222224235535, Validation Loss: 1.6522753238677979\n",
      "Epoch 192: Train Loss: 0.4674447238445282, Validation Loss: 1.65142023563385\n",
      "Epoch 193: Train Loss: 0.45853034853935243, Validation Loss: 1.6824219226837158\n",
      "Epoch 194: Train Loss: 0.502002215385437, Validation Loss: 1.7194430828094482\n",
      "Epoch 195: Train Loss: 0.4600186586380005, Validation Loss: 1.7167478799819946\n",
      "Epoch 196: Train Loss: 0.48464515209198, Validation Loss: 1.730774164199829\n",
      "Epoch 197: Train Loss: 0.40968974828720095, Validation Loss: 1.7176111936569214\n",
      "Epoch 198: Train Loss: 0.4158224821090698, Validation Loss: 1.694697618484497\n",
      "Epoch 199: Train Loss: 0.41278802156448363, Validation Loss: 1.6981228590011597\n",
      "Epoch 200: Train Loss: 0.4283891558647156, Validation Loss: 1.696884036064148\n",
      "Epoch 201: Train Loss: 0.46283248662948606, Validation Loss: 1.713498592376709\n",
      "Epoch 202: Train Loss: 0.48080654740333556, Validation Loss: 1.7309166193008423\n",
      "Epoch 203: Train Loss: 0.41691382229328156, Validation Loss: 1.749178171157837\n",
      "Epoch 204: Train Loss: 0.4170245468616486, Validation Loss: 1.8029513359069824\n",
      "Epoch 205: Train Loss: 0.43581408858299253, Validation Loss: 1.779462456703186\n",
      "Epoch 206: Train Loss: 0.4306474506855011, Validation Loss: 1.791228175163269\n",
      "Epoch 207: Train Loss: 0.4251205563545227, Validation Loss: 1.7814644575119019\n",
      "Epoch 208: Train Loss: 0.5241536438465119, Validation Loss: 1.7529356479644775\n",
      "Epoch 209: Train Loss: 0.5507605671882629, Validation Loss: 1.763001561164856\n",
      "Epoch 210: Train Loss: 0.3891288310289383, Validation Loss: 1.7146223783493042\n",
      "Epoch 211: Train Loss: 0.46726983189582827, Validation Loss: 1.7188953161239624\n",
      "Epoch 212: Train Loss: 0.40684226155281067, Validation Loss: 1.7433695793151855\n",
      "Epoch 213: Train Loss: 0.5148588597774506, Validation Loss: 1.7349482774734497\n",
      "Epoch 214: Train Loss: 0.4927302360534668, Validation Loss: 1.7371569871902466\n",
      "Epoch 215: Train Loss: 0.39877400994300843, Validation Loss: 1.7459248304367065\n",
      "Epoch 216: Train Loss: 0.4946242332458496, Validation Loss: 1.7283846139907837\n",
      "Epoch 217: Train Loss: 0.44247687458992, Validation Loss: 1.743981122970581\n",
      "Epoch 218: Train Loss: 0.3870408356189728, Validation Loss: 1.7292660474777222\n",
      "Epoch 219: Train Loss: 0.4256981909275055, Validation Loss: 1.7162061929702759\n",
      "Epoch 220: Train Loss: 0.47475778460502627, Validation Loss: 1.7421455383300781\n",
      "Epoch 221: Train Loss: 0.4625766515731812, Validation Loss: 1.8142337799072266\n",
      "Epoch 222: Train Loss: 0.4096225619316101, Validation Loss: 1.8015936613082886\n",
      "Epoch 223: Train Loss: 0.447434401512146, Validation Loss: 1.8642849922180176\n",
      "Epoch 224: Train Loss: 0.4332126796245575, Validation Loss: 1.8688148260116577\n",
      "Epoch 225: Train Loss: 0.4241174399852753, Validation Loss: 1.869530200958252\n",
      "Epoch 226: Train Loss: 0.4229314625263214, Validation Loss: 1.863338828086853\n",
      "Epoch 227: Train Loss: 0.44299007058143614, Validation Loss: 1.8556654453277588\n",
      "Epoch 228: Train Loss: 0.4355377733707428, Validation Loss: 1.875347375869751\n",
      "Epoch 229: Train Loss: 0.47841225266456605, Validation Loss: 1.8905208110809326\n",
      "Epoch 230: Train Loss: 0.3754781663417816, Validation Loss: 1.826605200767517\n",
      "Epoch 231: Train Loss: 0.39461895227432253, Validation Loss: 1.8276846408843994\n",
      "Epoch 232: Train Loss: 0.4563290476799011, Validation Loss: 1.8062224388122559\n",
      "Epoch 233: Train Loss: 0.40291942954063414, Validation Loss: 1.8124445676803589\n",
      "Epoch 234: Train Loss: 0.40539772510528566, Validation Loss: 1.8326427936553955\n",
      "Epoch 235: Train Loss: 0.3982961118221283, Validation Loss: 1.8151968717575073\n",
      "Epoch 236: Train Loss: 0.4179697573184967, Validation Loss: 1.8170382976531982\n",
      "Epoch 237: Train Loss: 0.39370876252651216, Validation Loss: 1.8266361951828003\n",
      "Epoch 238: Train Loss: 0.4456067860126495, Validation Loss: 1.8146882057189941\n",
      "Epoch 239: Train Loss: 0.384895795583725, Validation Loss: 1.8263342380523682\n",
      "Epoch 240: Train Loss: 0.3867273688316345, Validation Loss: 1.8270503282546997\n",
      "Epoch 241: Train Loss: 0.416156268119812, Validation Loss: 1.8334429264068604\n",
      "Epoch 242: Train Loss: 0.45725034475326537, Validation Loss: 1.873872995376587\n",
      "Epoch 243: Train Loss: 0.4136993408203125, Validation Loss: 1.9266258478164673\n",
      "Epoch 244: Train Loss: 0.43869216442108155, Validation Loss: 1.9438705444335938\n",
      "Epoch 245: Train Loss: 0.5274619460105896, Validation Loss: 1.9273232221603394\n",
      "Epoch 246: Train Loss: 0.4186545729637146, Validation Loss: 1.9321274757385254\n",
      "Epoch 247: Train Loss: 0.42646018266677854, Validation Loss: 1.9178125858306885\n",
      "Epoch 248: Train Loss: 0.36524677872657774, Validation Loss: 1.87892484664917\n",
      "Epoch 249: Train Loss: 0.47406767010688783, Validation Loss: 1.8892360925674438\n",
      "Epoch 250: Train Loss: 0.467857301235199, Validation Loss: 1.811206340789795\n",
      "Epoch 251: Train Loss: 0.36915841698646545, Validation Loss: 1.7392405271530151\n",
      "Epoch 252: Train Loss: 0.3950671970844269, Validation Loss: 1.7493255138397217\n",
      "Epoch 253: Train Loss: 0.38269132375717163, Validation Loss: 1.7660763263702393\n",
      "Epoch 254: Train Loss: 0.3832639068365097, Validation Loss: 1.8027607202529907\n",
      "Epoch 255: Train Loss: 0.4722219526767731, Validation Loss: 1.8419314622879028\n",
      "Epoch 256: Train Loss: 0.3805067777633667, Validation Loss: 1.8259577751159668\n",
      "Epoch 257: Train Loss: 0.4174535393714905, Validation Loss: 1.8037829399108887\n",
      "Epoch 258: Train Loss: 0.38460286855697634, Validation Loss: 1.8121531009674072\n",
      "Epoch 259: Train Loss: 0.4280263125896454, Validation Loss: 1.8241461515426636\n",
      "Epoch 260: Train Loss: 0.3998052775859833, Validation Loss: 1.8870158195495605\n",
      "Epoch 261: Train Loss: 0.45181865692138673, Validation Loss: 1.9703978300094604\n",
      "Epoch 262: Train Loss: 0.3393966794013977, Validation Loss: 1.997308611869812\n",
      "Epoch 263: Train Loss: 0.46019227504730226, Validation Loss: 2.006566047668457\n",
      "Epoch 264: Train Loss: 0.38864246010780334, Validation Loss: 1.9831773042678833\n",
      "Epoch 265: Train Loss: 0.36288937032222746, Validation Loss: 1.9961296319961548\n",
      "Epoch 266: Train Loss: 0.4617704153060913, Validation Loss: 1.9663158655166626\n",
      "Epoch 267: Train Loss: 0.3791515827178955, Validation Loss: 1.9665699005126953\n",
      "Epoch 268: Train Loss: 0.4128005146980286, Validation Loss: 1.9827122688293457\n",
      "Epoch 269: Train Loss: 0.3686216831207275, Validation Loss: 1.9876868724822998\n",
      "Epoch 270: Train Loss: 0.45678540468215945, Validation Loss: 1.9225239753723145\n",
      "Epoch 271: Train Loss: 0.3291707545518875, Validation Loss: 1.9256491661071777\n",
      "Epoch 272: Train Loss: 0.33665500283241273, Validation Loss: 1.9227945804595947\n",
      "Epoch 273: Train Loss: 0.36194418668746947, Validation Loss: 1.9290995597839355\n",
      "Epoch 274: Train Loss: 0.34422319531440737, Validation Loss: 1.9331637620925903\n",
      "Epoch 275: Train Loss: 0.4010910212993622, Validation Loss: 1.9570088386535645\n",
      "Epoch 276: Train Loss: 0.3990058422088623, Validation Loss: 1.9566967487335205\n",
      "Epoch 277: Train Loss: 0.35398144125938413, Validation Loss: 1.9609241485595703\n",
      "Epoch 278: Train Loss: 0.35757348835468294, Validation Loss: 1.9216361045837402\n",
      "Epoch 279: Train Loss: 0.36345929503440855, Validation Loss: 1.947180151939392\n",
      "Epoch 280: Train Loss: 0.3944563329219818, Validation Loss: 1.9526883363723755\n",
      "Epoch 281: Train Loss: 0.3471781015396118, Validation Loss: 1.9319156408309937\n",
      "Epoch 282: Train Loss: 0.3969997584819794, Validation Loss: 1.955191731452942\n",
      "Epoch 283: Train Loss: 0.3250872492790222, Validation Loss: 1.9602357149124146\n",
      "Epoch 284: Train Loss: 0.37914523482322693, Validation Loss: 1.9845656156539917\n",
      "Epoch 285: Train Loss: 0.47962294816970824, Validation Loss: 1.985744595527649\n",
      "Epoch 286: Train Loss: 0.31278434693813323, Validation Loss: 1.976083755493164\n",
      "Epoch 287: Train Loss: 0.37750659584999086, Validation Loss: 1.992477297782898\n",
      "Epoch 288: Train Loss: 0.4056043863296509, Validation Loss: 1.9851800203323364\n",
      "Epoch 289: Train Loss: 0.3821903169155121, Validation Loss: 1.978798270225525\n",
      "Epoch 290: Train Loss: 0.44130460619926454, Validation Loss: 2.015597343444824\n",
      "Epoch 291: Train Loss: 0.36698034405708313, Validation Loss: 2.05007004737854\n",
      "Epoch 292: Train Loss: 0.3354448348283768, Validation Loss: 2.060347080230713\n",
      "Epoch 293: Train Loss: 0.44882946014404296, Validation Loss: 2.1013834476470947\n",
      "Epoch 294: Train Loss: 0.3615260422229767, Validation Loss: 2.0883092880249023\n",
      "Epoch 295: Train Loss: 0.3815822184085846, Validation Loss: 2.0857889652252197\n",
      "Epoch 296: Train Loss: 0.319587779045105, Validation Loss: 2.0805413722991943\n",
      "Epoch 297: Train Loss: 0.32865412831306456, Validation Loss: 2.0704832077026367\n",
      "Epoch 298: Train Loss: 0.37137895822525024, Validation Loss: 2.0841729640960693\n",
      "Epoch 299: Train Loss: 0.3194568991661072, Validation Loss: 2.083693504333496\n",
      "Epoch 300: Train Loss: 0.31617422550916674, Validation Loss: 2.0784552097320557\n",
      "Epoch 301: Train Loss: 0.41304652094841005, Validation Loss: 2.05605411529541\n",
      "Epoch 302: Train Loss: 0.3398969203233719, Validation Loss: 2.04366135597229\n",
      "Epoch 303: Train Loss: 0.36059738993644713, Validation Loss: 2.020265579223633\n",
      "Epoch 304: Train Loss: 0.4091638922691345, Validation Loss: 2.0277552604675293\n",
      "Epoch 305: Train Loss: 0.36087886691093446, Validation Loss: 2.0148062705993652\n",
      "Epoch 306: Train Loss: 0.3857930064201355, Validation Loss: 2.01347279548645\n",
      "Epoch 307: Train Loss: 0.4556546568870544, Validation Loss: 1.9796780347824097\n",
      "Epoch 308: Train Loss: 0.35064666867256167, Validation Loss: 1.9740777015686035\n",
      "Epoch 309: Train Loss: 0.31390565633773804, Validation Loss: 1.9636210203170776\n",
      "Epoch 310: Train Loss: 0.3531990647315979, Validation Loss: 1.9926825761795044\n",
      "Epoch 311: Train Loss: 0.3675600945949554, Validation Loss: 2.006101369857788\n",
      "Epoch 312: Train Loss: 0.42023983001708987, Validation Loss: 1.9878883361816406\n",
      "Epoch 313: Train Loss: 0.3187254726886749, Validation Loss: 2.0291523933410645\n",
      "Epoch 314: Train Loss: 0.3370387673377991, Validation Loss: 2.0152149200439453\n",
      "Epoch 315: Train Loss: 0.28544813543558123, Validation Loss: 2.0413408279418945\n",
      "Epoch 316: Train Loss: 0.405754154920578, Validation Loss: 2.0199944972991943\n",
      "Epoch 317: Train Loss: 0.3220509886741638, Validation Loss: 2.0417466163635254\n",
      "Epoch 318: Train Loss: 0.3663419485092163, Validation Loss: 2.033043146133423\n",
      "Epoch 319: Train Loss: 0.29490927755832674, Validation Loss: 2.0419068336486816\n",
      "Epoch 320: Train Loss: 0.38958940505981443, Validation Loss: 2.0687923431396484\n",
      "Epoch 321: Train Loss: 0.3187085956335068, Validation Loss: 2.006141424179077\n",
      "Epoch 322: Train Loss: 0.31203565895557406, Validation Loss: 1.9760724306106567\n",
      "Epoch 323: Train Loss: 0.35425023436546327, Validation Loss: 2.0038719177246094\n",
      "Epoch 324: Train Loss: 0.3213895082473755, Validation Loss: 1.943853497505188\n",
      "Epoch 325: Train Loss: 0.300432425737381, Validation Loss: 1.9717755317687988\n",
      "Epoch 326: Train Loss: 0.34360226392745974, Validation Loss: 1.96549654006958\n",
      "Epoch 327: Train Loss: 0.3127784371376038, Validation Loss: 1.9880070686340332\n",
      "Epoch 328: Train Loss: 0.33646023869514463, Validation Loss: 2.0036942958831787\n",
      "Epoch 329: Train Loss: 0.38752552270889284, Validation Loss: 2.005824089050293\n",
      "Epoch 330: Train Loss: 0.36263567209243774, Validation Loss: 1.9665013551712036\n",
      "Epoch 331: Train Loss: 0.35278300642967225, Validation Loss: 1.9702545404434204\n",
      "Epoch 332: Train Loss: 0.41668344140052793, Validation Loss: 1.9942706823349\n",
      "Epoch 333: Train Loss: 0.29628291428089143, Validation Loss: 2.074246406555176\n",
      "Epoch 334: Train Loss: 0.46007831692695617, Validation Loss: 2.0487003326416016\n",
      "Epoch 335: Train Loss: 0.38298729062080383, Validation Loss: 2.0465238094329834\n",
      "Epoch 336: Train Loss: 0.3174065828323364, Validation Loss: 1.9980124235153198\n",
      "Epoch 337: Train Loss: 0.3136489689350128, Validation Loss: 2.013115882873535\n",
      "Epoch 338: Train Loss: 0.2919018715620041, Validation Loss: 2.0435261726379395\n",
      "Epoch 339: Train Loss: 0.30228546261787415, Validation Loss: 2.0622193813323975\n",
      "Epoch 340: Train Loss: 0.32736782133579256, Validation Loss: 2.06353759765625\n",
      "Epoch 341: Train Loss: 0.3262773811817169, Validation Loss: 2.0501370429992676\n",
      "Epoch 342: Train Loss: 0.417292183637619, Validation Loss: 2.0156145095825195\n",
      "Epoch 343: Train Loss: 0.3395185053348541, Validation Loss: 2.016063690185547\n",
      "Epoch 344: Train Loss: 0.3886199653148651, Validation Loss: 2.0226447582244873\n",
      "Epoch 345: Train Loss: 0.2559424459934235, Validation Loss: 2.0517404079437256\n",
      "Epoch 346: Train Loss: 0.34673082232475283, Validation Loss: 2.027602195739746\n",
      "Epoch 347: Train Loss: 0.34627053439617156, Validation Loss: 2.0084800720214844\n",
      "Epoch 348: Train Loss: 0.3249839246273041, Validation Loss: 2.0118608474731445\n",
      "Epoch 349: Train Loss: 0.36417828500270844, Validation Loss: 2.0580575466156006\n",
      "Epoch 350: Train Loss: 0.31049821376800535, Validation Loss: 2.0535085201263428\n",
      "Epoch 351: Train Loss: 0.3187789171934128, Validation Loss: 2.02720046043396\n",
      "Epoch 352: Train Loss: 0.27200847864151, Validation Loss: 2.0074710845947266\n",
      "Epoch 353: Train Loss: 0.3098562389612198, Validation Loss: 2.048067331314087\n",
      "Epoch 354: Train Loss: 0.3124045550823212, Validation Loss: 2.0279505252838135\n",
      "Epoch 355: Train Loss: 0.3244584619998932, Validation Loss: 2.024014949798584\n",
      "Epoch 356: Train Loss: 0.34589555859565735, Validation Loss: 2.0400636196136475\n",
      "Epoch 357: Train Loss: 0.2765859216451645, Validation Loss: 2.043827772140503\n",
      "Epoch 358: Train Loss: 0.2759825050830841, Validation Loss: 2.011082649230957\n",
      "Epoch 359: Train Loss: 0.2951150953769684, Validation Loss: 2.026247024536133\n",
      "Epoch 360: Train Loss: 0.2817405879497528, Validation Loss: 2.0712878704071045\n",
      "Epoch 361: Train Loss: 0.30187113881111144, Validation Loss: 2.0522985458374023\n",
      "Epoch 362: Train Loss: 0.27928442060947417, Validation Loss: 2.109818935394287\n",
      "Epoch 363: Train Loss: 0.38651207089424133, Validation Loss: 2.1753783226013184\n",
      "Epoch 364: Train Loss: 0.31608523428440094, Validation Loss: 2.120857000350952\n",
      "Epoch 365: Train Loss: 0.36179696917533877, Validation Loss: 2.0817766189575195\n",
      "Epoch 366: Train Loss: 0.3326830804347992, Validation Loss: 1.9799818992614746\n",
      "Epoch 367: Train Loss: 0.35151167809963224, Validation Loss: 1.9900072813034058\n",
      "Epoch 368: Train Loss: 0.2761723041534424, Validation Loss: 2.010915994644165\n",
      "Epoch 369: Train Loss: 0.30507528483867646, Validation Loss: 2.038341760635376\n",
      "Epoch 370: Train Loss: 0.3598450869321823, Validation Loss: 2.0523054599761963\n",
      "Epoch 371: Train Loss: 0.2737904280424118, Validation Loss: 2.0136773586273193\n",
      "Epoch 372: Train Loss: 0.3927389860153198, Validation Loss: 1.98625910282135\n",
      "Epoch 373: Train Loss: 0.29511538445949553, Validation Loss: 2.0016629695892334\n",
      "Epoch 374: Train Loss: 0.24946504831314087, Validation Loss: 2.0331432819366455\n",
      "Epoch 375: Train Loss: 0.3033826619386673, Validation Loss: 2.095184564590454\n",
      "Epoch 376: Train Loss: 0.30863371193408967, Validation Loss: 2.0955259799957275\n",
      "Epoch 377: Train Loss: 0.2626907765865326, Validation Loss: 2.0828421115875244\n",
      "Epoch 378: Train Loss: 0.2601991146802902, Validation Loss: 2.063091993331909\n",
      "Epoch 379: Train Loss: 0.28277455270290375, Validation Loss: 2.0467967987060547\n",
      "Epoch 380: Train Loss: 0.3212870657444, Validation Loss: 2.004751682281494\n",
      "Epoch 381: Train Loss: 0.30251445770263674, Validation Loss: 2.0337629318237305\n",
      "Epoch 382: Train Loss: 0.342626291513443, Validation Loss: 2.0215978622436523\n",
      "Epoch 383: Train Loss: 0.26174395978450776, Validation Loss: 2.0925283432006836\n",
      "Epoch 384: Train Loss: 0.3365736067295074, Validation Loss: 2.1107823848724365\n",
      "Epoch 385: Train Loss: 0.25263062715530393, Validation Loss: 2.0779926776885986\n",
      "Epoch 386: Train Loss: 0.28908590972423553, Validation Loss: 2.0564825534820557\n",
      "Epoch 387: Train Loss: 0.3643226385116577, Validation Loss: 2.083707571029663\n",
      "Epoch 388: Train Loss: 0.2884411603212357, Validation Loss: 2.084547281265259\n",
      "Epoch 389: Train Loss: 0.2755246043205261, Validation Loss: 2.093735456466675\n",
      "Epoch 390: Train Loss: 0.2880283534526825, Validation Loss: 2.191594123840332\n",
      "Epoch 391: Train Loss: 0.25626962780952456, Validation Loss: 2.1957406997680664\n",
      "Epoch 392: Train Loss: 0.24042617231607438, Validation Loss: 2.211923360824585\n",
      "Epoch 393: Train Loss: 0.4047097146511078, Validation Loss: 2.2611570358276367\n",
      "Epoch 394: Train Loss: 0.2942262202501297, Validation Loss: 2.2510271072387695\n",
      "Epoch 395: Train Loss: 0.22488275170326233, Validation Loss: 2.265981674194336\n",
      "Epoch 396: Train Loss: 0.23056167513132095, Validation Loss: 2.2677664756774902\n",
      "Epoch 397: Train Loss: 0.297921296954155, Validation Loss: 2.2835803031921387\n",
      "Epoch 398: Train Loss: 0.3176421046257019, Validation Loss: 2.2863588333129883\n",
      "Epoch 399: Train Loss: 0.248293137550354, Validation Loss: 2.2987921237945557\n",
      "Epoch 400: Train Loss: 0.3121446371078491, Validation Loss: 2.204220771789551\n",
      "Epoch 401: Train Loss: 0.38256516456604006, Validation Loss: 2.133507490158081\n",
      "Epoch 402: Train Loss: 0.27169100344181063, Validation Loss: 2.0543270111083984\n",
      "Epoch 403: Train Loss: 0.2863138645887375, Validation Loss: 2.0537238121032715\n",
      "Epoch 404: Train Loss: 0.2821675181388855, Validation Loss: 2.0820133686065674\n",
      "Epoch 405: Train Loss: 0.24531356990337372, Validation Loss: 2.09481143951416\n",
      "Epoch 406: Train Loss: 0.32972691357135775, Validation Loss: 2.1006886959075928\n",
      "Epoch 407: Train Loss: 0.3201889157295227, Validation Loss: 2.132162570953369\n",
      "Epoch 408: Train Loss: 0.36563800573349, Validation Loss: 2.1070117950439453\n",
      "Epoch 409: Train Loss: 0.2691031277179718, Validation Loss: 2.0934906005859375\n",
      "Epoch 410: Train Loss: 0.36070323884487154, Validation Loss: 2.112428903579712\n",
      "Epoch 411: Train Loss: 0.2691245824098587, Validation Loss: 2.0901594161987305\n",
      "Epoch 412: Train Loss: 0.2999770104885101, Validation Loss: 2.0433199405670166\n",
      "Epoch 413: Train Loss: 0.2599526643753052, Validation Loss: 2.150515556335449\n",
      "Epoch 414: Train Loss: 0.25635111033916474, Validation Loss: 2.168998956680298\n",
      "Epoch 415: Train Loss: 0.2225763037800789, Validation Loss: 2.1922860145568848\n",
      "Epoch 416: Train Loss: 0.2374925822019577, Validation Loss: 2.2164156436920166\n",
      "Epoch 417: Train Loss: 0.2635147824883461, Validation Loss: 2.2503066062927246\n",
      "Epoch 418: Train Loss: 0.4206887573003769, Validation Loss: 2.252033233642578\n",
      "Epoch 419: Train Loss: 0.22078610956668854, Validation Loss: 2.28348445892334\n",
      "Epoch 420: Train Loss: 0.23308236598968507, Validation Loss: 2.3151302337646484\n",
      "Epoch 421: Train Loss: 0.24281845688819886, Validation Loss: 2.3109586238861084\n",
      "Epoch 422: Train Loss: 0.2263876259326935, Validation Loss: 2.2389676570892334\n",
      "Epoch 423: Train Loss: 0.3104488253593445, Validation Loss: 2.2423243522644043\n",
      "Epoch 424: Train Loss: 0.2871199905872345, Validation Loss: 2.253697395324707\n",
      "Epoch 425: Train Loss: 0.340972775220871, Validation Loss: 2.240239143371582\n",
      "Epoch 426: Train Loss: 0.27415119409561156, Validation Loss: 2.2544126510620117\n",
      "Epoch 427: Train Loss: 0.2742756813764572, Validation Loss: 2.2457735538482666\n",
      "Epoch 428: Train Loss: 0.23962152004241943, Validation Loss: 2.264923095703125\n",
      "Epoch 429: Train Loss: 0.2728907808661461, Validation Loss: 2.2249698638916016\n",
      "Epoch 430: Train Loss: 0.2698323130607605, Validation Loss: 2.2176122665405273\n",
      "Epoch 431: Train Loss: 0.20835149735212327, Validation Loss: 2.2545487880706787\n",
      "Epoch 432: Train Loss: 0.30044000744819643, Validation Loss: 2.2877063751220703\n",
      "Epoch 433: Train Loss: 0.2718754768371582, Validation Loss: 2.2940526008605957\n",
      "Epoch 434: Train Loss: 0.22593859434127808, Validation Loss: 2.2284340858459473\n",
      "Epoch 435: Train Loss: 0.3048804312944412, Validation Loss: 2.232236385345459\n",
      "Epoch 436: Train Loss: 0.22880345582962036, Validation Loss: 2.278235673904419\n",
      "Epoch 437: Train Loss: 0.2669090867042542, Validation Loss: 2.343355894088745\n",
      "Epoch 438: Train Loss: 0.2073492556810379, Validation Loss: 2.343798875808716\n",
      "Epoch 439: Train Loss: 0.2717967927455902, Validation Loss: 2.319145441055298\n",
      "Epoch 440: Train Loss: 0.27344253063201907, Validation Loss: 2.385139226913452\n",
      "Epoch 441: Train Loss: 0.21658125519752502, Validation Loss: 2.290971279144287\n",
      "Epoch 442: Train Loss: 0.3193333774805069, Validation Loss: 2.28109073638916\n",
      "Epoch 443: Train Loss: 0.27002564668655393, Validation Loss: 2.1908156871795654\n",
      "Epoch 444: Train Loss: 0.20977887660264968, Validation Loss: 2.183741331100464\n",
      "Epoch 445: Train Loss: 0.3557836651802063, Validation Loss: 2.2096199989318848\n",
      "Epoch 446: Train Loss: 0.3208592295646667, Validation Loss: 2.14906907081604\n",
      "Epoch 447: Train Loss: 0.2843592971563339, Validation Loss: 2.1380298137664795\n",
      "Epoch 448: Train Loss: 0.28975478410720823, Validation Loss: 2.1079020500183105\n",
      "Epoch 449: Train Loss: 0.3635656386613846, Validation Loss: 2.1192336082458496\n",
      "Epoch 450: Train Loss: 0.3754356622695923, Validation Loss: 2.042281150817871\n",
      "Epoch 451: Train Loss: 0.23470622003078462, Validation Loss: 1.958316445350647\n",
      "Epoch 452: Train Loss: 0.2754156768321991, Validation Loss: 1.9660427570343018\n",
      "Epoch 453: Train Loss: 0.23738549649715424, Validation Loss: 1.9646812677383423\n",
      "Epoch 454: Train Loss: 0.2605596750974655, Validation Loss: 1.9472308158874512\n",
      "Epoch 455: Train Loss: 0.24819391071796418, Validation Loss: 1.9953678846359253\n",
      "Epoch 456: Train Loss: 0.1991749733686447, Validation Loss: 2.025359630584717\n",
      "Epoch 457: Train Loss: 0.23996578454971312, Validation Loss: 2.011897563934326\n",
      "Epoch 458: Train Loss: 0.24761991649866105, Validation Loss: 1.9595611095428467\n",
      "Epoch 459: Train Loss: 0.25668208599090575, Validation Loss: 1.9901221990585327\n",
      "Epoch 460: Train Loss: 0.201126366853714, Validation Loss: 2.058032751083374\n",
      "Epoch 461: Train Loss: 0.2200598120689392, Validation Loss: 2.077946662902832\n",
      "Epoch 462: Train Loss: 0.2586084455251694, Validation Loss: 2.071173667907715\n",
      "Epoch 463: Train Loss: 0.2341179221868515, Validation Loss: 2.1905195713043213\n",
      "Epoch 464: Train Loss: 0.2201644718647003, Validation Loss: 2.2867319583892822\n",
      "Epoch 465: Train Loss: 0.25534988939762115, Validation Loss: 2.287705898284912\n",
      "Epoch 466: Train Loss: 0.3126838177442551, Validation Loss: 2.2648603916168213\n",
      "Epoch 467: Train Loss: 0.3907723695039749, Validation Loss: 2.274003028869629\n",
      "Epoch 468: Train Loss: 0.3173953652381897, Validation Loss: 2.2291388511657715\n",
      "Epoch 469: Train Loss: 0.29026630222797395, Validation Loss: 2.231738805770874\n",
      "Epoch 470: Train Loss: 0.2795501321554184, Validation Loss: 2.233919858932495\n",
      "Epoch 471: Train Loss: 0.19653192460536956, Validation Loss: 2.197354793548584\n",
      "Epoch 472: Train Loss: 0.22191699594259262, Validation Loss: 2.1656601428985596\n",
      "Epoch 473: Train Loss: 0.2782020062208176, Validation Loss: 2.14593768119812\n",
      "Epoch 474: Train Loss: 0.21079550683498383, Validation Loss: 2.176194190979004\n",
      "Epoch 475: Train Loss: 0.19525841027498245, Validation Loss: 2.1392438411712646\n",
      "Epoch 476: Train Loss: 0.30522432625293733, Validation Loss: 2.14349365234375\n",
      "Epoch 477: Train Loss: 0.2000740572810173, Validation Loss: 2.1713247299194336\n",
      "Epoch 478: Train Loss: 0.2874450355768204, Validation Loss: 2.1673314571380615\n",
      "Epoch 479: Train Loss: 0.19829095602035524, Validation Loss: 2.0896618366241455\n",
      "Epoch 480: Train Loss: 0.23461097478866577, Validation Loss: 2.0720622539520264\n",
      "Epoch 481: Train Loss: 0.2652828574180603, Validation Loss: 2.0590317249298096\n",
      "Epoch 482: Train Loss: 0.29665581583976747, Validation Loss: 2.103442668914795\n",
      "Epoch 483: Train Loss: 0.23819250166416167, Validation Loss: 2.1054468154907227\n",
      "Epoch 484: Train Loss: 0.19444980323314667, Validation Loss: 2.1449995040893555\n",
      "Epoch 485: Train Loss: 0.2149704724550247, Validation Loss: 2.129697322845459\n",
      "Epoch 486: Train Loss: 0.274323995411396, Validation Loss: 2.1893692016601562\n",
      "Epoch 487: Train Loss: 0.26691093742847444, Validation Loss: 2.228165626525879\n",
      "Epoch 488: Train Loss: 0.24461520910263063, Validation Loss: 2.2370057106018066\n",
      "Epoch 489: Train Loss: 0.2698354959487915, Validation Loss: 2.238124370574951\n",
      "Epoch 490: Train Loss: 0.2000092551112175, Validation Loss: 2.272277355194092\n",
      "Epoch 491: Train Loss: 0.36513409912586214, Validation Loss: 2.3118271827697754\n",
      "Epoch 492: Train Loss: 0.24093241691589357, Validation Loss: 2.3050851821899414\n",
      "Epoch 493: Train Loss: 0.21286000311374664, Validation Loss: 2.3215157985687256\n",
      "Epoch 494: Train Loss: 0.2234150379896164, Validation Loss: 2.2902729511260986\n",
      "Epoch 495: Train Loss: 0.22887213826179503, Validation Loss: 2.263536214828491\n",
      "Epoch 496: Train Loss: 0.27294759452342987, Validation Loss: 2.2954211235046387\n",
      "Epoch 497: Train Loss: 0.22991229295730592, Validation Loss: 2.297349452972412\n",
      "Epoch 498: Train Loss: 0.3869021266698837, Validation Loss: 2.2798914909362793\n",
      "Epoch 499: Train Loss: 0.23009933531284332, Validation Loss: 2.301217555999756\n",
      "Fold 3 Metrics:\n",
      "Accuracy: 0.2727272727272727, Precision: 0.3333333333333333, Recall: 0.3333333333333333, F1-score: 0.3333333333333333, AUC: 0.2666666666666666\n",
      "Confusion Matrix:\n",
      "[[1 4]\n",
      " [4 2]]\n",
      "Completed fold 3\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples from subject 10 to test set\n",
      "Adding 6 truth samples from subject 10 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 250)\n",
      "(132, 65, 250)\n",
      "Epoch 0: Train Loss: 0.7047542333602905, Validation Loss: 0.6771331429481506\n",
      "Epoch 1: Train Loss: 0.6949662089347839, Validation Loss: 0.6678330302238464\n",
      "Epoch 2: Train Loss: 0.7322140336036682, Validation Loss: 0.6553215384483337\n",
      "Epoch 3: Train Loss: 0.6847844839096069, Validation Loss: 0.636691153049469\n",
      "Epoch 4: Train Loss: 0.6815415382385254, Validation Loss: 0.6088351011276245\n",
      "Epoch 5: Train Loss: 0.7302584171295166, Validation Loss: 0.5987374186515808\n",
      "Epoch 6: Train Loss: 0.63426194190979, Validation Loss: 0.5749629735946655\n",
      "Epoch 7: Train Loss: 0.7115150570869446, Validation Loss: 0.568831741809845\n",
      "Epoch 8: Train Loss: 0.6677213668823242, Validation Loss: 0.5712718367576599\n",
      "Epoch 9: Train Loss: 0.7259894371032715, Validation Loss: 0.5762621164321899\n",
      "Epoch 10: Train Loss: 0.6965641379356384, Validation Loss: 0.5707769989967346\n",
      "Epoch 11: Train Loss: 0.6845313429832458, Validation Loss: 0.5765241384506226\n",
      "Epoch 12: Train Loss: 0.7199437975883484, Validation Loss: 0.5944438576698303\n",
      "Epoch 13: Train Loss: 0.6671039819717407, Validation Loss: 0.596662163734436\n",
      "Epoch 14: Train Loss: 0.6531615138053894, Validation Loss: 0.6069369912147522\n",
      "Epoch 15: Train Loss: 0.6736043810844421, Validation Loss: 0.6211827397346497\n",
      "Epoch 16: Train Loss: 0.7121268153190613, Validation Loss: 0.6167941093444824\n",
      "Epoch 17: Train Loss: 0.6212216258049011, Validation Loss: 0.6128225326538086\n",
      "Epoch 18: Train Loss: 0.6900863647460938, Validation Loss: 0.6083563566207886\n",
      "Epoch 19: Train Loss: 0.6774838328361511, Validation Loss: 0.6245785355567932\n",
      "Epoch 20: Train Loss: 0.6684307336807251, Validation Loss: 0.6376063227653503\n",
      "Epoch 21: Train Loss: 0.6450695395469666, Validation Loss: 0.6337377429008484\n",
      "Epoch 22: Train Loss: 0.6829688072204589, Validation Loss: 0.6436815857887268\n",
      "Epoch 23: Train Loss: 0.648136293888092, Validation Loss: 0.6619834899902344\n",
      "Epoch 24: Train Loss: 0.6671391367912293, Validation Loss: 0.6582619547843933\n",
      "Epoch 25: Train Loss: 0.6797220587730408, Validation Loss: 0.6663049459457397\n",
      "Epoch 26: Train Loss: 0.6657469749450684, Validation Loss: 0.666252613067627\n",
      "Epoch 27: Train Loss: 0.7157670378684997, Validation Loss: 0.6723059415817261\n",
      "Epoch 28: Train Loss: 0.621001136302948, Validation Loss: 0.6703090667724609\n",
      "Epoch 29: Train Loss: 0.5990659236907959, Validation Loss: 0.6733857989311218\n",
      "Epoch 30: Train Loss: 0.6371818423271179, Validation Loss: 0.6854370832443237\n",
      "Epoch 31: Train Loss: 0.5866061508655548, Validation Loss: 0.7068358659744263\n",
      "Epoch 32: Train Loss: 0.6330901980400085, Validation Loss: 0.7092975378036499\n",
      "Epoch 33: Train Loss: 0.634307610988617, Validation Loss: 0.7186056971549988\n",
      "Epoch 34: Train Loss: 0.6444786667823792, Validation Loss: 0.7283400893211365\n",
      "Epoch 35: Train Loss: 0.6484678149223327, Validation Loss: 0.7403533458709717\n",
      "Epoch 36: Train Loss: 0.6450109004974365, Validation Loss: 0.7267969250679016\n",
      "Epoch 37: Train Loss: 0.6046400427818298, Validation Loss: 0.7371270060539246\n",
      "Epoch 38: Train Loss: 0.6785956859588623, Validation Loss: 0.7256057858467102\n",
      "Epoch 39: Train Loss: 0.6607393145561218, Validation Loss: 0.7279639840126038\n",
      "Epoch 40: Train Loss: 0.6337721943855286, Validation Loss: 0.7218682169914246\n",
      "Epoch 41: Train Loss: 0.6524804472923279, Validation Loss: 0.7194989323616028\n",
      "Epoch 42: Train Loss: 0.6229682564735413, Validation Loss: 0.7264005541801453\n",
      "Epoch 43: Train Loss: 0.598199999332428, Validation Loss: 0.7411597967147827\n",
      "Epoch 44: Train Loss: 0.622944176197052, Validation Loss: 0.7396775484085083\n",
      "Epoch 45: Train Loss: 0.6851833939552308, Validation Loss: 0.7400763630867004\n",
      "Epoch 46: Train Loss: 0.6478039503097535, Validation Loss: 0.7352744936943054\n",
      "Epoch 47: Train Loss: 0.6065756440162658, Validation Loss: 0.7409061193466187\n",
      "Epoch 48: Train Loss: 0.6566500782966613, Validation Loss: 0.7377305030822754\n",
      "Epoch 49: Train Loss: 0.6732549905776978, Validation Loss: 0.7490971684455872\n",
      "Epoch 50: Train Loss: 0.6607894182205201, Validation Loss: 0.7553812861442566\n",
      "Epoch 51: Train Loss: 0.6246251106262207, Validation Loss: 0.7566531300544739\n",
      "Epoch 52: Train Loss: 0.5876644134521485, Validation Loss: 0.753532350063324\n",
      "Epoch 53: Train Loss: 0.6453156709671021, Validation Loss: 0.7324551939964294\n",
      "Epoch 54: Train Loss: 0.5958252668380737, Validation Loss: 0.7361642718315125\n",
      "Epoch 55: Train Loss: 0.6612759709358216, Validation Loss: 0.7466323375701904\n",
      "Epoch 56: Train Loss: 0.6687393903732299, Validation Loss: 0.7500637173652649\n",
      "Epoch 57: Train Loss: 0.6518456220626831, Validation Loss: 0.7647147178649902\n",
      "Epoch 58: Train Loss: 0.5997332453727722, Validation Loss: 0.7531726956367493\n",
      "Epoch 59: Train Loss: 0.632105803489685, Validation Loss: 0.7509022951126099\n",
      "Epoch 60: Train Loss: 0.5987321734428406, Validation Loss: 0.7723509669303894\n",
      "Epoch 61: Train Loss: 0.6494081735610961, Validation Loss: 0.7783688902854919\n",
      "Epoch 62: Train Loss: 0.61183180809021, Validation Loss: 0.7838965654373169\n",
      "Epoch 63: Train Loss: 0.6146721124649048, Validation Loss: 0.7898106575012207\n",
      "Epoch 64: Train Loss: 0.5780963718891143, Validation Loss: 0.7874464988708496\n",
      "Epoch 65: Train Loss: 0.6698270320892334, Validation Loss: 0.7922157049179077\n",
      "Epoch 66: Train Loss: 0.6515613198280334, Validation Loss: 0.7903925180435181\n",
      "Epoch 67: Train Loss: 0.647376298904419, Validation Loss: 0.7885847091674805\n",
      "Epoch 68: Train Loss: 0.577426278591156, Validation Loss: 0.788453221321106\n",
      "Epoch 69: Train Loss: 0.5723499298095703, Validation Loss: 0.7824732065200806\n",
      "Epoch 70: Train Loss: 0.5495068252086639, Validation Loss: 0.7748709917068481\n",
      "Epoch 71: Train Loss: 0.5923821687698364, Validation Loss: 0.7867246866226196\n",
      "Epoch 72: Train Loss: 0.6047508835792541, Validation Loss: 0.7790366411209106\n",
      "Epoch 73: Train Loss: 0.5852225303649903, Validation Loss: 0.7828683853149414\n",
      "Epoch 74: Train Loss: 0.6166805505752564, Validation Loss: 0.7789878845214844\n",
      "Epoch 75: Train Loss: 0.5572768330574036, Validation Loss: 0.7885782122612\n",
      "Epoch 76: Train Loss: 0.6252664685249328, Validation Loss: 0.7869008779525757\n",
      "Epoch 77: Train Loss: 0.5700193881988526, Validation Loss: 0.7854659557342529\n",
      "Epoch 78: Train Loss: 0.5523981213569641, Validation Loss: 0.783350944519043\n",
      "Epoch 79: Train Loss: 0.6067790269851685, Validation Loss: 0.7672179341316223\n",
      "Epoch 80: Train Loss: 0.582372784614563, Validation Loss: 0.782711386680603\n",
      "Epoch 81: Train Loss: 0.5953752040863037, Validation Loss: 0.7910457253456116\n",
      "Epoch 82: Train Loss: 0.5887502312660218, Validation Loss: 0.7769271731376648\n",
      "Epoch 83: Train Loss: 0.5426965355873108, Validation Loss: 0.7742122411727905\n",
      "Epoch 84: Train Loss: 0.5545232236385346, Validation Loss: 0.7667446136474609\n",
      "Epoch 85: Train Loss: 0.5763302922248841, Validation Loss: 0.7674651145935059\n",
      "Epoch 86: Train Loss: 0.5650927305221558, Validation Loss: 0.7644760012626648\n",
      "Epoch 87: Train Loss: 0.5676694512367249, Validation Loss: 0.7558197975158691\n",
      "Epoch 88: Train Loss: 0.5460890352725982, Validation Loss: 0.7454724907875061\n",
      "Epoch 89: Train Loss: 0.6367160201072692, Validation Loss: 0.7456262111663818\n",
      "Epoch 90: Train Loss: 0.6091998934745788, Validation Loss: 0.767238974571228\n",
      "Epoch 91: Train Loss: 0.5839829802513122, Validation Loss: 0.7806382775306702\n",
      "Epoch 92: Train Loss: 0.5758836805820465, Validation Loss: 0.8063308000564575\n",
      "Epoch 93: Train Loss: 0.5108889698982239, Validation Loss: 0.8039584755897522\n",
      "Epoch 94: Train Loss: 0.5956602692604065, Validation Loss: 0.8142898082733154\n",
      "Epoch 95: Train Loss: 0.5544493079185486, Validation Loss: 0.8020314574241638\n",
      "Epoch 96: Train Loss: 0.6047010540962219, Validation Loss: 0.7918608784675598\n",
      "Epoch 97: Train Loss: 0.6009336113929749, Validation Loss: 0.8062564730644226\n",
      "Epoch 98: Train Loss: 0.5482102572917938, Validation Loss: 0.8011012077331543\n",
      "Epoch 99: Train Loss: 0.55790393948555, Validation Loss: 0.7988086938858032\n",
      "Epoch 100: Train Loss: 0.5270529329776764, Validation Loss: 0.793806254863739\n",
      "Epoch 101: Train Loss: 0.5835517287254334, Validation Loss: 0.7862003445625305\n",
      "Epoch 102: Train Loss: 0.5685215592384338, Validation Loss: 0.7878507375717163\n",
      "Epoch 103: Train Loss: 0.5617900907993316, Validation Loss: 0.7774240374565125\n",
      "Epoch 104: Train Loss: 0.5575931191444397, Validation Loss: 0.7690993547439575\n",
      "Epoch 105: Train Loss: 0.5137681007385254, Validation Loss: 0.7677538990974426\n",
      "Epoch 106: Train Loss: 0.5706839442253113, Validation Loss: 0.778631329536438\n",
      "Epoch 107: Train Loss: 0.5719315469264984, Validation Loss: 0.777170717716217\n",
      "Epoch 108: Train Loss: 0.6307826519012452, Validation Loss: 0.7786458134651184\n",
      "Epoch 109: Train Loss: 0.625810420513153, Validation Loss: 0.7964457869529724\n",
      "Epoch 110: Train Loss: 0.5504533350467682, Validation Loss: 0.8201595544815063\n",
      "Epoch 111: Train Loss: 0.5173646748065949, Validation Loss: 0.8324565291404724\n",
      "Epoch 112: Train Loss: 0.5323374629020691, Validation Loss: 0.8473189473152161\n",
      "Epoch 113: Train Loss: 0.6079629182815551, Validation Loss: 0.8400692939758301\n",
      "Epoch 114: Train Loss: 0.592837393283844, Validation Loss: 0.838762640953064\n",
      "Epoch 115: Train Loss: 0.5370509445667266, Validation Loss: 0.8438934683799744\n",
      "Epoch 116: Train Loss: 0.5964368939399719, Validation Loss: 0.8445232510566711\n",
      "Epoch 117: Train Loss: 0.5407059192657471, Validation Loss: 0.8515415191650391\n",
      "Epoch 118: Train Loss: 0.5651691794395447, Validation Loss: 0.8456538319587708\n",
      "Epoch 119: Train Loss: 0.5860789656639099, Validation Loss: 0.8442742824554443\n",
      "Epoch 120: Train Loss: 0.5857173800468445, Validation Loss: 0.8338180184364319\n",
      "Epoch 121: Train Loss: 0.5302184343338012, Validation Loss: 0.8235886693000793\n",
      "Epoch 122: Train Loss: 0.5390151023864747, Validation Loss: 0.8005478978157043\n",
      "Epoch 123: Train Loss: 0.5185221374034882, Validation Loss: 0.8023014068603516\n",
      "Epoch 124: Train Loss: 0.536689305305481, Validation Loss: 0.8177913427352905\n",
      "Epoch 125: Train Loss: 0.5577923059463501, Validation Loss: 0.8321908116340637\n",
      "Epoch 126: Train Loss: 0.5089689314365387, Validation Loss: 0.8523610830307007\n",
      "Epoch 127: Train Loss: 0.6082637250423432, Validation Loss: 0.8616670966148376\n",
      "Epoch 128: Train Loss: 0.5654736399650574, Validation Loss: 0.8420150279998779\n",
      "Epoch 129: Train Loss: 0.5333719372749328, Validation Loss: 0.8383895754814148\n",
      "Epoch 130: Train Loss: 0.5231136202812194, Validation Loss: 0.8395724892616272\n",
      "Epoch 131: Train Loss: 0.5075329720973969, Validation Loss: 0.8538846969604492\n",
      "Epoch 132: Train Loss: 0.5776301264762879, Validation Loss: 0.8509185314178467\n",
      "Epoch 133: Train Loss: 0.6033212184906006, Validation Loss: 0.8869994282722473\n",
      "Epoch 134: Train Loss: 0.5481433868408203, Validation Loss: 0.8967334032058716\n",
      "Epoch 135: Train Loss: 0.5461631715297699, Validation Loss: 0.907701313495636\n",
      "Epoch 136: Train Loss: 0.5264032363891602, Validation Loss: 0.8999775052070618\n",
      "Epoch 137: Train Loss: 0.5003822326660157, Validation Loss: 0.894955039024353\n",
      "Epoch 138: Train Loss: 0.539077365398407, Validation Loss: 0.9162286520004272\n",
      "Epoch 139: Train Loss: 0.598829984664917, Validation Loss: 0.9115918874740601\n",
      "Epoch 140: Train Loss: 0.4752508819103241, Validation Loss: 0.9040943384170532\n",
      "Epoch 141: Train Loss: 0.49068915843963623, Validation Loss: 0.9108527898788452\n",
      "Epoch 142: Train Loss: 0.5079765260219574, Validation Loss: 0.8900464773178101\n",
      "Epoch 143: Train Loss: 0.513200443983078, Validation Loss: 0.8863552212715149\n",
      "Epoch 144: Train Loss: 0.541795802116394, Validation Loss: 0.8764014840126038\n",
      "Epoch 145: Train Loss: 0.5079650104045867, Validation Loss: 0.8866966366767883\n",
      "Epoch 146: Train Loss: 0.5557613968849182, Validation Loss: 0.9189771413803101\n",
      "Epoch 147: Train Loss: 0.541497528553009, Validation Loss: 0.9246279001235962\n",
      "Epoch 148: Train Loss: 0.46283523440361024, Validation Loss: 0.9203783273696899\n",
      "Epoch 149: Train Loss: 0.5154868960380554, Validation Loss: 0.899204671382904\n",
      "Epoch 150: Train Loss: 0.48366867303848265, Validation Loss: 0.9029174447059631\n",
      "Epoch 151: Train Loss: 0.48846819400787356, Validation Loss: 0.9163827300071716\n",
      "Epoch 152: Train Loss: 0.46455695629119875, Validation Loss: 0.9204031229019165\n",
      "Epoch 153: Train Loss: 0.5305215001106263, Validation Loss: 0.943230390548706\n",
      "Epoch 154: Train Loss: 0.49518402814865115, Validation Loss: 0.9274490475654602\n",
      "Epoch 155: Train Loss: 0.4952165484428406, Validation Loss: 0.9260866045951843\n",
      "Epoch 156: Train Loss: 0.5062394678592682, Validation Loss: 0.9288881421089172\n",
      "Epoch 157: Train Loss: 0.5034237861633301, Validation Loss: 0.9226285219192505\n",
      "Epoch 158: Train Loss: 0.4409271001815796, Validation Loss: 0.9455039501190186\n",
      "Epoch 159: Train Loss: 0.4912100911140442, Validation Loss: 0.9431411623954773\n",
      "Epoch 160: Train Loss: 0.4298287957906723, Validation Loss: 0.9433456659317017\n",
      "Epoch 161: Train Loss: 0.45493940711021424, Validation Loss: 0.9540588855743408\n",
      "Epoch 162: Train Loss: 0.48184972405433657, Validation Loss: 0.9567283987998962\n",
      "Epoch 163: Train Loss: 0.5176740765571595, Validation Loss: 0.9573345184326172\n",
      "Epoch 164: Train Loss: 0.5083946943283081, Validation Loss: 0.9377095699310303\n",
      "Epoch 165: Train Loss: 0.5235724925994873, Validation Loss: 0.9412221908569336\n",
      "Epoch 166: Train Loss: 0.4384062230587006, Validation Loss: 0.9606310129165649\n",
      "Epoch 167: Train Loss: 0.4939707100391388, Validation Loss: 0.9841472506523132\n",
      "Epoch 168: Train Loss: 0.4597862631082535, Validation Loss: 0.9689098000526428\n",
      "Epoch 169: Train Loss: 0.4796465516090393, Validation Loss: 0.9597399234771729\n",
      "Epoch 170: Train Loss: 0.5595475614070893, Validation Loss: 0.9468711614608765\n",
      "Epoch 171: Train Loss: 0.5019830822944641, Validation Loss: 0.9467584490776062\n",
      "Epoch 172: Train Loss: 0.4959384143352509, Validation Loss: 0.9721751809120178\n",
      "Epoch 173: Train Loss: 0.47349089980125425, Validation Loss: 0.966202974319458\n",
      "Epoch 174: Train Loss: 0.4415497422218323, Validation Loss: 0.9569661617279053\n",
      "Epoch 175: Train Loss: 0.4774370491504669, Validation Loss: 0.958385705947876\n",
      "Epoch 176: Train Loss: 0.4732041537761688, Validation Loss: 0.9562274217605591\n",
      "Epoch 177: Train Loss: 0.4431230902671814, Validation Loss: 0.9548003673553467\n",
      "Epoch 178: Train Loss: 0.4443632483482361, Validation Loss: 0.9470040798187256\n",
      "Epoch 179: Train Loss: 0.43844578266143797, Validation Loss: 0.9345513582229614\n",
      "Epoch 180: Train Loss: 0.49093306064605713, Validation Loss: 0.9306881427764893\n",
      "Epoch 181: Train Loss: 0.4279292166233063, Validation Loss: 0.9279359579086304\n",
      "Epoch 182: Train Loss: 0.48359026312828063, Validation Loss: 0.9462345242500305\n",
      "Epoch 183: Train Loss: 0.4627164304256439, Validation Loss: 0.9432763457298279\n",
      "Epoch 184: Train Loss: 0.45209723711013794, Validation Loss: 0.9507810473442078\n",
      "Epoch 185: Train Loss: 0.5115702092647553, Validation Loss: 0.977486789226532\n",
      "Epoch 186: Train Loss: 0.46950716376304624, Validation Loss: 0.9909889698028564\n",
      "Epoch 187: Train Loss: 0.4295588731765747, Validation Loss: 0.9946288466453552\n",
      "Epoch 188: Train Loss: 0.5146895825862885, Validation Loss: 0.9800593852996826\n",
      "Epoch 189: Train Loss: 0.47065806984901426, Validation Loss: 0.9754924178123474\n",
      "Epoch 190: Train Loss: 0.47090943455696105, Validation Loss: 0.9559127688407898\n",
      "Epoch 191: Train Loss: 0.4688819110393524, Validation Loss: 0.9645705819129944\n",
      "Epoch 192: Train Loss: 0.4236301124095917, Validation Loss: 0.9926626682281494\n",
      "Epoch 193: Train Loss: 0.4065278351306915, Validation Loss: 1.0272116661071777\n",
      "Epoch 194: Train Loss: 0.38934738636016847, Validation Loss: 1.010900855064392\n",
      "Epoch 195: Train Loss: 0.44834454655647277, Validation Loss: 0.9963794350624084\n",
      "Epoch 196: Train Loss: 0.48043504953384397, Validation Loss: 0.9895966053009033\n",
      "Epoch 197: Train Loss: 0.45496222376823425, Validation Loss: 0.983241856098175\n",
      "Epoch 198: Train Loss: 0.4482937753200531, Validation Loss: 0.9926719665527344\n",
      "Epoch 199: Train Loss: 0.4355810880661011, Validation Loss: 0.9751882553100586\n",
      "Epoch 200: Train Loss: 0.5053860008716583, Validation Loss: 1.0049629211425781\n",
      "Epoch 201: Train Loss: 0.4768406391143799, Validation Loss: 1.004925012588501\n",
      "Epoch 202: Train Loss: 0.5153455257415771, Validation Loss: 0.990159273147583\n",
      "Epoch 203: Train Loss: 0.4237824440002441, Validation Loss: 1.0346856117248535\n",
      "Epoch 204: Train Loss: 0.41473093032836916, Validation Loss: 1.0441572666168213\n",
      "Epoch 205: Train Loss: 0.4336509585380554, Validation Loss: 1.0449739694595337\n",
      "Epoch 206: Train Loss: 0.5033225655555725, Validation Loss: 1.0605555772781372\n",
      "Epoch 207: Train Loss: 0.4392396926879883, Validation Loss: 1.020084261894226\n",
      "Epoch 208: Train Loss: 0.38017740845680237, Validation Loss: 0.9980739951133728\n",
      "Epoch 209: Train Loss: 0.4580008327960968, Validation Loss: 1.0185517072677612\n",
      "Epoch 210: Train Loss: 0.4107101023197174, Validation Loss: 1.0033879280090332\n",
      "Epoch 211: Train Loss: 0.44896717071533204, Validation Loss: 0.9995517134666443\n",
      "Epoch 212: Train Loss: 0.492589408159256, Validation Loss: 0.9918372631072998\n",
      "Epoch 213: Train Loss: 0.5106586515903473, Validation Loss: 1.022724986076355\n",
      "Epoch 214: Train Loss: 0.43869980573654177, Validation Loss: 1.0200444459915161\n",
      "Epoch 215: Train Loss: 0.4403459131717682, Validation Loss: 1.0235930681228638\n",
      "Epoch 216: Train Loss: 0.4088829457759857, Validation Loss: 1.0319448709487915\n",
      "Epoch 217: Train Loss: 0.3943822145462036, Validation Loss: 1.03847074508667\n",
      "Epoch 218: Train Loss: 0.4665694713592529, Validation Loss: 1.0428239107131958\n",
      "Epoch 219: Train Loss: 0.46888747811317444, Validation Loss: 1.0663150548934937\n",
      "Epoch 220: Train Loss: 0.4829087495803833, Validation Loss: 1.061641812324524\n",
      "Epoch 221: Train Loss: 0.40040884613990785, Validation Loss: 1.1154170036315918\n",
      "Epoch 222: Train Loss: 0.40922781825065613, Validation Loss: 1.1055852174758911\n",
      "Epoch 223: Train Loss: 0.44157843589782714, Validation Loss: 1.100184679031372\n",
      "Epoch 224: Train Loss: 0.3989263832569122, Validation Loss: 1.0962495803833008\n",
      "Epoch 225: Train Loss: 0.4431454539299011, Validation Loss: 1.1019253730773926\n",
      "Epoch 226: Train Loss: 0.461026656627655, Validation Loss: 1.1165181398391724\n",
      "Epoch 227: Train Loss: 0.3961500108242035, Validation Loss: 1.1149598360061646\n",
      "Epoch 228: Train Loss: 0.4033676564693451, Validation Loss: 1.1094250679016113\n",
      "Epoch 229: Train Loss: 0.38680065274238584, Validation Loss: 1.1108156442642212\n",
      "Epoch 230: Train Loss: 0.40986103415489195, Validation Loss: 1.113700032234192\n",
      "Epoch 231: Train Loss: 0.4139706134796143, Validation Loss: 1.123547077178955\n",
      "Epoch 232: Train Loss: 0.41469565629959104, Validation Loss: 1.1279335021972656\n",
      "Epoch 233: Train Loss: 0.40418354868888856, Validation Loss: 1.0967926979064941\n",
      "Epoch 234: Train Loss: 0.41112558245658876, Validation Loss: 1.1048365831375122\n",
      "Epoch 235: Train Loss: 0.4179037034511566, Validation Loss: 1.0502891540527344\n",
      "Epoch 236: Train Loss: 0.37949320673942566, Validation Loss: 1.018182635307312\n",
      "Epoch 237: Train Loss: 0.4458462357521057, Validation Loss: 1.0239709615707397\n",
      "Epoch 238: Train Loss: 0.380494749546051, Validation Loss: 1.0215449333190918\n",
      "Epoch 239: Train Loss: 0.4235854625701904, Validation Loss: 1.0341191291809082\n",
      "Epoch 240: Train Loss: 0.440416157245636, Validation Loss: 1.0454453229904175\n",
      "Epoch 241: Train Loss: 0.4590215146541595, Validation Loss: 1.0490126609802246\n",
      "Epoch 242: Train Loss: 0.4165353298187256, Validation Loss: 1.013445258140564\n",
      "Epoch 243: Train Loss: 0.3996737837791443, Validation Loss: 1.01686429977417\n",
      "Epoch 244: Train Loss: 0.3379326671361923, Validation Loss: 1.0290412902832031\n",
      "Epoch 245: Train Loss: 0.4140888750553131, Validation Loss: 1.0123282670974731\n",
      "Epoch 246: Train Loss: 0.4001722991466522, Validation Loss: 1.0297170877456665\n",
      "Epoch 247: Train Loss: 0.45054837465286257, Validation Loss: 1.0367066860198975\n",
      "Epoch 248: Train Loss: 0.38453041315078734, Validation Loss: 1.0376909971237183\n",
      "Epoch 249: Train Loss: 0.37066359519958497, Validation Loss: 1.0395941734313965\n",
      "Epoch 250: Train Loss: 0.38249113857746125, Validation Loss: 1.0250264406204224\n",
      "Epoch 251: Train Loss: 0.36871581673622134, Validation Loss: 1.0446230173110962\n",
      "Epoch 252: Train Loss: 0.37404727935791016, Validation Loss: 1.059360384941101\n",
      "Epoch 253: Train Loss: 0.4370842456817627, Validation Loss: 1.035598874092102\n",
      "Epoch 254: Train Loss: 0.3702173113822937, Validation Loss: 1.0556267499923706\n",
      "Epoch 255: Train Loss: 0.3491034358739853, Validation Loss: 1.0429257154464722\n",
      "Epoch 256: Train Loss: 0.4005724906921387, Validation Loss: 1.063758134841919\n",
      "Epoch 257: Train Loss: 0.32252555191516874, Validation Loss: 1.091650128364563\n",
      "Epoch 258: Train Loss: 0.3892528474330902, Validation Loss: 1.0456830263137817\n",
      "Epoch 259: Train Loss: 0.4470826625823975, Validation Loss: 1.063592553138733\n",
      "Epoch 260: Train Loss: 0.3909121096134186, Validation Loss: 1.1066052913665771\n",
      "Epoch 261: Train Loss: 0.37364801168441775, Validation Loss: 1.1008533239364624\n",
      "Epoch 262: Train Loss: 0.4794234037399292, Validation Loss: 1.1441552639007568\n",
      "Epoch 263: Train Loss: 0.359800922870636, Validation Loss: 1.1165026426315308\n",
      "Epoch 264: Train Loss: 0.47474480867385865, Validation Loss: 1.0907756090164185\n",
      "Epoch 265: Train Loss: 0.4244182169437408, Validation Loss: 1.0898847579956055\n",
      "Epoch 266: Train Loss: 0.3685566604137421, Validation Loss: 1.0556608438491821\n",
      "Epoch 267: Train Loss: 0.33465989530086515, Validation Loss: 1.0630606412887573\n",
      "Epoch 268: Train Loss: 0.3615905702114105, Validation Loss: 1.0160837173461914\n",
      "Epoch 269: Train Loss: 0.368845134973526, Validation Loss: 1.0403224229812622\n",
      "Epoch 270: Train Loss: 0.35980838239192964, Validation Loss: 1.0188332796096802\n",
      "Epoch 271: Train Loss: 0.3964207828044891, Validation Loss: 1.0235953330993652\n",
      "Epoch 272: Train Loss: 0.3457001864910126, Validation Loss: 1.050662875175476\n",
      "Epoch 273: Train Loss: 0.4397706925868988, Validation Loss: 1.083643913269043\n",
      "Epoch 274: Train Loss: 0.3496703505516052, Validation Loss: 1.0328394174575806\n",
      "Epoch 275: Train Loss: 0.30420724451541903, Validation Loss: 1.0649313926696777\n",
      "Epoch 276: Train Loss: 0.34226945638656614, Validation Loss: 1.0550484657287598\n",
      "Epoch 277: Train Loss: 0.3419120192527771, Validation Loss: 1.032691478729248\n",
      "Epoch 278: Train Loss: 0.37311973571777346, Validation Loss: 1.0276083946228027\n",
      "Epoch 279: Train Loss: 0.41492841243743894, Validation Loss: 1.0796773433685303\n",
      "Epoch 280: Train Loss: 0.36308605074882505, Validation Loss: 1.0704026222229004\n",
      "Epoch 281: Train Loss: 0.330223149061203, Validation Loss: 1.0378516912460327\n",
      "Epoch 282: Train Loss: 0.37404751777648926, Validation Loss: 1.076831579208374\n",
      "Epoch 283: Train Loss: 0.3965504586696625, Validation Loss: 1.0864373445510864\n",
      "Epoch 284: Train Loss: 0.3250162720680237, Validation Loss: 1.0682822465896606\n",
      "Epoch 285: Train Loss: 0.3758434772491455, Validation Loss: 1.0796808004379272\n",
      "Epoch 286: Train Loss: 0.35912017822265624, Validation Loss: 1.1245983839035034\n",
      "Epoch 287: Train Loss: 0.4268538236618042, Validation Loss: 1.1286537647247314\n",
      "Epoch 288: Train Loss: 0.3902123987674713, Validation Loss: 1.1587382555007935\n",
      "Epoch 289: Train Loss: 0.3562535524368286, Validation Loss: 1.1505722999572754\n",
      "Epoch 290: Train Loss: 0.37610580325126647, Validation Loss: 1.1339478492736816\n",
      "Epoch 291: Train Loss: 0.35373757481575013, Validation Loss: 1.129101276397705\n",
      "Epoch 292: Train Loss: 0.3422714710235596, Validation Loss: 1.1333681344985962\n",
      "Epoch 293: Train Loss: 0.3662164151668549, Validation Loss: 1.0678290128707886\n",
      "Epoch 294: Train Loss: 0.3292719185352325, Validation Loss: 1.0552582740783691\n",
      "Epoch 295: Train Loss: 0.3394106090068817, Validation Loss: 1.093212604522705\n",
      "Epoch 296: Train Loss: 0.36770285069942477, Validation Loss: 1.1018824577331543\n",
      "Epoch 297: Train Loss: 0.3567812144756317, Validation Loss: 1.112729549407959\n",
      "Epoch 298: Train Loss: 0.40621947646141054, Validation Loss: 1.1015536785125732\n",
      "Epoch 299: Train Loss: 0.3257793068885803, Validation Loss: 1.1289349794387817\n",
      "Epoch 300: Train Loss: 0.27504169940948486, Validation Loss: 1.1118781566619873\n",
      "Epoch 301: Train Loss: 0.34826419353485105, Validation Loss: 1.0953645706176758\n",
      "Epoch 302: Train Loss: 0.3716724157333374, Validation Loss: 1.0923234224319458\n",
      "Epoch 303: Train Loss: 0.4515853404998779, Validation Loss: 1.1344646215438843\n",
      "Epoch 304: Train Loss: 0.3426628351211548, Validation Loss: 1.1834074258804321\n",
      "Epoch 305: Train Loss: 0.3311471700668335, Validation Loss: 1.1696244478225708\n",
      "Epoch 306: Train Loss: 0.3502082943916321, Validation Loss: 1.1510354280471802\n",
      "Epoch 307: Train Loss: 0.36242947578430174, Validation Loss: 1.169938564300537\n",
      "Epoch 308: Train Loss: 0.42611443996429443, Validation Loss: 1.2342530488967896\n",
      "Epoch 309: Train Loss: 0.336806458234787, Validation Loss: 1.2058624029159546\n",
      "Epoch 310: Train Loss: 0.3179128587245941, Validation Loss: 1.2637012004852295\n",
      "Epoch 311: Train Loss: 0.3335794687271118, Validation Loss: 1.2309600114822388\n",
      "Epoch 312: Train Loss: 0.35344082713127134, Validation Loss: 1.2455253601074219\n",
      "Epoch 313: Train Loss: 0.33468757271766664, Validation Loss: 1.1671054363250732\n",
      "Epoch 314: Train Loss: 0.3019548147916794, Validation Loss: 1.1849271059036255\n",
      "Epoch 315: Train Loss: 0.3181084841489792, Validation Loss: 1.1225453615188599\n",
      "Epoch 316: Train Loss: 0.34238373935222627, Validation Loss: 1.1379870176315308\n",
      "Epoch 317: Train Loss: 0.2882301241159439, Validation Loss: 1.161910891532898\n",
      "Epoch 318: Train Loss: 0.36238414645195005, Validation Loss: 1.1370046138763428\n",
      "Epoch 319: Train Loss: 0.3148920118808746, Validation Loss: 1.1716983318328857\n",
      "Epoch 320: Train Loss: 0.3390780746936798, Validation Loss: 1.1618674993515015\n",
      "Epoch 321: Train Loss: 0.3446197211742401, Validation Loss: 1.191373348236084\n",
      "Epoch 322: Train Loss: 0.28420162200927734, Validation Loss: 1.1697726249694824\n",
      "Epoch 323: Train Loss: 0.3670214056968689, Validation Loss: 1.1601381301879883\n",
      "Epoch 324: Train Loss: 0.31746068596839905, Validation Loss: 1.1522387266159058\n",
      "Epoch 325: Train Loss: 0.3051678478717804, Validation Loss: 1.1062384843826294\n",
      "Epoch 326: Train Loss: 0.30337191820144654, Validation Loss: 1.1674315929412842\n",
      "Epoch 327: Train Loss: 0.3652382642030716, Validation Loss: 1.1529252529144287\n",
      "Epoch 328: Train Loss: 0.31080428659915926, Validation Loss: 1.159895658493042\n",
      "Epoch 329: Train Loss: 0.41627908051013945, Validation Loss: 1.1869927644729614\n",
      "Epoch 330: Train Loss: 0.33385968804359434, Validation Loss: 1.177721619606018\n",
      "Epoch 331: Train Loss: 0.3352597296237946, Validation Loss: 1.2114522457122803\n",
      "Epoch 332: Train Loss: 0.30324290990829467, Validation Loss: 1.2523491382598877\n",
      "Epoch 333: Train Loss: 0.2760983735322952, Validation Loss: 1.2442466020584106\n",
      "Epoch 334: Train Loss: 0.27250663936138153, Validation Loss: 1.2521569728851318\n",
      "Epoch 335: Train Loss: 0.3357908546924591, Validation Loss: 1.204666256904602\n",
      "Epoch 336: Train Loss: 0.2542699009180069, Validation Loss: 1.215858817100525\n",
      "Epoch 337: Train Loss: 0.2715775489807129, Validation Loss: 1.2565850019454956\n",
      "Epoch 338: Train Loss: 0.40640842318534853, Validation Loss: 1.3037469387054443\n",
      "Epoch 339: Train Loss: 0.31365281641483306, Validation Loss: 1.274588942527771\n",
      "Epoch 340: Train Loss: 0.29607545137405394, Validation Loss: 1.2166268825531006\n",
      "Epoch 341: Train Loss: 0.2815431296825409, Validation Loss: 1.2010711431503296\n",
      "Epoch 342: Train Loss: 0.27701793015003207, Validation Loss: 1.2124346494674683\n",
      "Epoch 343: Train Loss: 0.32431635558605193, Validation Loss: 1.2214511632919312\n",
      "Epoch 344: Train Loss: 0.2987512320280075, Validation Loss: 1.2273893356323242\n",
      "Epoch 345: Train Loss: 0.2873352378606796, Validation Loss: 1.183307409286499\n",
      "Epoch 346: Train Loss: 0.2775018483400345, Validation Loss: 1.1597968339920044\n",
      "Epoch 347: Train Loss: 0.24778779745101928, Validation Loss: 1.1348447799682617\n",
      "Epoch 348: Train Loss: 0.30711228847503663, Validation Loss: 1.1559572219848633\n",
      "Epoch 349: Train Loss: 0.34995090663433076, Validation Loss: 1.187099814414978\n",
      "Epoch 350: Train Loss: 0.36127220690250395, Validation Loss: 1.2525386810302734\n",
      "Epoch 351: Train Loss: 0.25390601605176927, Validation Loss: 1.2153998613357544\n",
      "Epoch 352: Train Loss: 0.26528704166412354, Validation Loss: 1.187497854232788\n",
      "Epoch 353: Train Loss: 0.30283492505550386, Validation Loss: 1.1817097663879395\n",
      "Epoch 354: Train Loss: 0.24429832100868226, Validation Loss: 1.2218235731124878\n",
      "Epoch 355: Train Loss: 0.3306656539440155, Validation Loss: 1.2866734266281128\n",
      "Epoch 356: Train Loss: 0.2437230244278908, Validation Loss: 1.270350694656372\n",
      "Epoch 357: Train Loss: 0.3296333968639374, Validation Loss: 1.2519519329071045\n",
      "Epoch 358: Train Loss: 0.27524666786193847, Validation Loss: 1.2596763372421265\n",
      "Epoch 359: Train Loss: 0.26573555171489716, Validation Loss: 1.2751609086990356\n",
      "Epoch 360: Train Loss: 0.2725388199090958, Validation Loss: 1.2836207151412964\n",
      "Epoch 361: Train Loss: 0.2777888745069504, Validation Loss: 1.2735689878463745\n",
      "Epoch 362: Train Loss: 0.31282707750797273, Validation Loss: 1.2023379802703857\n",
      "Epoch 363: Train Loss: 0.3216636061668396, Validation Loss: 1.290688157081604\n",
      "Epoch 364: Train Loss: 0.24874102920293809, Validation Loss: 1.2919570207595825\n",
      "Epoch 365: Train Loss: 0.31897200345993043, Validation Loss: 1.2994282245635986\n",
      "Epoch 366: Train Loss: 0.29488863348960875, Validation Loss: 1.3389357328414917\n",
      "Epoch 367: Train Loss: 0.2424187481403351, Validation Loss: 1.2897166013717651\n",
      "Epoch 368: Train Loss: 0.27240965366363523, Validation Loss: 1.2877023220062256\n",
      "Epoch 369: Train Loss: 0.3723071992397308, Validation Loss: 1.3284600973129272\n",
      "Epoch 370: Train Loss: 0.34517781138420106, Validation Loss: 1.335447072982788\n",
      "Epoch 371: Train Loss: 0.26615109741687776, Validation Loss: 1.2458817958831787\n",
      "Epoch 372: Train Loss: 0.2606291800737381, Validation Loss: 1.307690143585205\n",
      "Epoch 373: Train Loss: 0.3088618993759155, Validation Loss: 1.2773650884628296\n",
      "Epoch 374: Train Loss: 0.24498583972454072, Validation Loss: 1.2476344108581543\n",
      "Epoch 375: Train Loss: 0.23369138836860656, Validation Loss: 1.2375352382659912\n",
      "Epoch 376: Train Loss: 0.28002784550189974, Validation Loss: 1.3297408819198608\n",
      "Epoch 377: Train Loss: 0.2567817509174347, Validation Loss: 1.301723837852478\n",
      "Epoch 378: Train Loss: 0.27979896664619447, Validation Loss: 1.3146026134490967\n",
      "Epoch 379: Train Loss: 0.33171587288379667, Validation Loss: 1.301792025566101\n",
      "Epoch 380: Train Loss: 0.3211496829986572, Validation Loss: 1.2768043279647827\n",
      "Epoch 381: Train Loss: 0.31827881634235383, Validation Loss: 1.334959864616394\n",
      "Epoch 382: Train Loss: 0.2737305790185928, Validation Loss: 1.3433773517608643\n",
      "Epoch 383: Train Loss: 0.31219395995140076, Validation Loss: 1.3190646171569824\n",
      "Epoch 384: Train Loss: 0.2727814167737961, Validation Loss: 1.3556876182556152\n",
      "Epoch 385: Train Loss: 0.40993031561374665, Validation Loss: 1.3348932266235352\n",
      "Epoch 386: Train Loss: 0.2923482835292816, Validation Loss: 1.3337234258651733\n",
      "Epoch 387: Train Loss: 0.2982482522726059, Validation Loss: 1.2731822729110718\n",
      "Epoch 388: Train Loss: 0.20136551484465598, Validation Loss: 1.259460210800171\n",
      "Epoch 389: Train Loss: 0.23018496632575988, Validation Loss: 1.2885873317718506\n",
      "Epoch 390: Train Loss: 0.2630670517683029, Validation Loss: 1.331801414489746\n",
      "Epoch 391: Train Loss: 0.2523333072662354, Validation Loss: 1.3334461450576782\n",
      "Epoch 392: Train Loss: 0.23994505107402803, Validation Loss: 1.3280645608901978\n",
      "Epoch 393: Train Loss: 0.24495759904384612, Validation Loss: 1.2743536233901978\n",
      "Epoch 394: Train Loss: 0.2433108866214752, Validation Loss: 1.3186036348342896\n",
      "Epoch 395: Train Loss: 0.2517904728651047, Validation Loss: 1.3083128929138184\n",
      "Epoch 396: Train Loss: 0.26225918233394624, Validation Loss: 1.3268280029296875\n",
      "Epoch 397: Train Loss: 0.26031733751297, Validation Loss: 1.3365145921707153\n",
      "Epoch 398: Train Loss: 0.32765201330184934, Validation Loss: 1.4053733348846436\n",
      "Epoch 399: Train Loss: 0.25174411237239835, Validation Loss: 1.3783172369003296\n",
      "Epoch 400: Train Loss: 0.31402176320552827, Validation Loss: 1.3361053466796875\n",
      "Epoch 401: Train Loss: 0.2849660784006119, Validation Loss: 1.273633360862732\n",
      "Epoch 402: Train Loss: 0.21685990393161775, Validation Loss: 1.2551453113555908\n",
      "Epoch 403: Train Loss: 0.29299269914627074, Validation Loss: 1.3385162353515625\n",
      "Epoch 404: Train Loss: 0.24676453769207002, Validation Loss: 1.27743399143219\n",
      "Epoch 405: Train Loss: 0.25105359852313996, Validation Loss: 1.303333044052124\n",
      "Epoch 406: Train Loss: 0.37169474363327026, Validation Loss: 1.2263298034667969\n",
      "Epoch 407: Train Loss: 0.23086829036474227, Validation Loss: 1.2032897472381592\n",
      "Epoch 408: Train Loss: 0.2604581415653229, Validation Loss: 1.2464388608932495\n",
      "Epoch 409: Train Loss: 0.26234798431396483, Validation Loss: 1.2048661708831787\n",
      "Epoch 410: Train Loss: 0.22653368413448333, Validation Loss: 1.2087385654449463\n",
      "Epoch 411: Train Loss: 0.32057062089443206, Validation Loss: 1.2070045471191406\n",
      "Epoch 412: Train Loss: 0.2891697853803635, Validation Loss: 1.2141321897506714\n",
      "Epoch 413: Train Loss: 0.3527258038520813, Validation Loss: 1.2357804775238037\n",
      "Epoch 414: Train Loss: 0.24714497923851014, Validation Loss: 1.2997907400131226\n",
      "Epoch 415: Train Loss: 0.2548683613538742, Validation Loss: 1.2910147905349731\n",
      "Epoch 416: Train Loss: 0.2982961177825928, Validation Loss: 1.22806715965271\n",
      "Epoch 417: Train Loss: 0.19155823290348054, Validation Loss: 1.2231401205062866\n",
      "Epoch 418: Train Loss: 0.2167477086186409, Validation Loss: 1.215440273284912\n",
      "Epoch 419: Train Loss: 0.25358161330223083, Validation Loss: 1.1591007709503174\n",
      "Epoch 420: Train Loss: 0.27690814435482025, Validation Loss: 1.1515448093414307\n",
      "Epoch 421: Train Loss: 0.2885060727596283, Validation Loss: 1.2284489870071411\n",
      "Epoch 422: Train Loss: 0.24833512604236602, Validation Loss: 1.2223260402679443\n",
      "Epoch 423: Train Loss: 0.23026348054409027, Validation Loss: 1.2776212692260742\n",
      "Epoch 424: Train Loss: 0.2178385764360428, Validation Loss: 1.2277189493179321\n",
      "Epoch 425: Train Loss: 0.33787767589092255, Validation Loss: 1.2576180696487427\n",
      "Epoch 426: Train Loss: 0.2907242089509964, Validation Loss: 1.2611947059631348\n",
      "Epoch 427: Train Loss: 0.21066175401210785, Validation Loss: 1.2903448343276978\n",
      "Epoch 428: Train Loss: 0.23409585952758788, Validation Loss: 1.3170051574707031\n",
      "Epoch 429: Train Loss: 0.2721523195505142, Validation Loss: 1.2623088359832764\n",
      "Epoch 430: Train Loss: 0.19905991554260255, Validation Loss: 1.2715210914611816\n",
      "Epoch 431: Train Loss: 0.22821802496910096, Validation Loss: 1.2597638368606567\n",
      "Epoch 432: Train Loss: 0.19065462946891784, Validation Loss: 1.3196243047714233\n",
      "Epoch 433: Train Loss: 0.22828377783298492, Validation Loss: 1.3728305101394653\n",
      "Epoch 434: Train Loss: 0.22225168347358704, Validation Loss: 1.3528608083724976\n",
      "Epoch 435: Train Loss: 0.2999798387289047, Validation Loss: 1.4290997982025146\n",
      "Epoch 436: Train Loss: 0.21816725730895997, Validation Loss: 1.4032487869262695\n",
      "Epoch 437: Train Loss: 0.19726695716381074, Validation Loss: 1.337180495262146\n",
      "Epoch 438: Train Loss: 0.21495017409324646, Validation Loss: 1.3586726188659668\n",
      "Epoch 439: Train Loss: 0.18612824529409408, Validation Loss: 1.3445981740951538\n",
      "Epoch 440: Train Loss: 0.19930092096328736, Validation Loss: 1.3367146253585815\n",
      "Epoch 441: Train Loss: 0.2618221044540405, Validation Loss: 1.3748953342437744\n",
      "Epoch 442: Train Loss: 0.21657635271549225, Validation Loss: 1.4042807817459106\n",
      "Epoch 443: Train Loss: 0.20152414441108704, Validation Loss: 1.377624750137329\n",
      "Epoch 444: Train Loss: 0.2393294095993042, Validation Loss: 1.3832365274429321\n",
      "Epoch 445: Train Loss: 0.19797178655862807, Validation Loss: 1.4153541326522827\n",
      "Epoch 446: Train Loss: 0.2570338636636734, Validation Loss: 1.3891490697860718\n",
      "Epoch 447: Train Loss: 0.19044457972049714, Validation Loss: 1.4155633449554443\n",
      "Epoch 448: Train Loss: 0.21504984200000762, Validation Loss: 1.358191728591919\n",
      "Epoch 449: Train Loss: 0.18483979403972625, Validation Loss: 1.3141337633132935\n",
      "Epoch 450: Train Loss: 0.18027884587645532, Validation Loss: 1.2879067659378052\n",
      "Epoch 451: Train Loss: 0.19049564599990845, Validation Loss: 1.315285086631775\n",
      "Epoch 452: Train Loss: 0.2326447159051895, Validation Loss: 1.339958906173706\n",
      "Epoch 453: Train Loss: 0.2005649909377098, Validation Loss: 1.366100788116455\n",
      "Epoch 454: Train Loss: 0.2605695754289627, Validation Loss: 1.3453824520111084\n",
      "Epoch 455: Train Loss: 0.19346189200878144, Validation Loss: 1.3558740615844727\n",
      "Epoch 456: Train Loss: 0.23838368356227874, Validation Loss: 1.3836854696273804\n",
      "Epoch 457: Train Loss: 0.24334957003593444, Validation Loss: 1.43693208694458\n",
      "Epoch 458: Train Loss: 0.20638233721256255, Validation Loss: 1.4175488948822021\n",
      "Epoch 459: Train Loss: 0.1993528887629509, Validation Loss: 1.4081107378005981\n",
      "Epoch 460: Train Loss: 0.1985826313495636, Validation Loss: 1.3914772272109985\n",
      "Epoch 461: Train Loss: 0.2030970960855484, Validation Loss: 1.4488940238952637\n",
      "Epoch 462: Train Loss: 0.29137179255485535, Validation Loss: 1.436809778213501\n",
      "Epoch 463: Train Loss: 0.2280178487300873, Validation Loss: 1.4257428646087646\n",
      "Epoch 464: Train Loss: 0.2813459873199463, Validation Loss: 1.4972590208053589\n",
      "Epoch 465: Train Loss: 0.2424382358789444, Validation Loss: 1.5204120874404907\n",
      "Epoch 466: Train Loss: 0.20313084423542022, Validation Loss: 1.5267577171325684\n",
      "Epoch 467: Train Loss: 0.2331637680530548, Validation Loss: 1.534550428390503\n",
      "Epoch 468: Train Loss: 0.2328100472688675, Validation Loss: 1.480005145072937\n",
      "Epoch 469: Train Loss: 0.22642284333705903, Validation Loss: 1.486657977104187\n",
      "Epoch 470: Train Loss: 0.19499972462654114, Validation Loss: 1.517944574356079\n",
      "Epoch 471: Train Loss: 0.27288082242012024, Validation Loss: 1.5613549947738647\n",
      "Epoch 472: Train Loss: 0.17740560472011566, Validation Loss: 1.6092383861541748\n",
      "Epoch 473: Train Loss: 0.18908957988023758, Validation Loss: 1.6477642059326172\n",
      "Epoch 474: Train Loss: 0.17191347628831863, Validation Loss: 1.567677617073059\n",
      "Epoch 475: Train Loss: 0.2075193405151367, Validation Loss: 1.530690312385559\n",
      "Epoch 476: Train Loss: 0.1739320322871208, Validation Loss: 1.5803747177124023\n",
      "Epoch 477: Train Loss: 0.19001129567623137, Validation Loss: 1.5921945571899414\n",
      "Epoch 478: Train Loss: 0.24068041741847992, Validation Loss: 1.607139229774475\n",
      "Epoch 479: Train Loss: 0.14785987734794617, Validation Loss: 1.6065982580184937\n",
      "Epoch 480: Train Loss: 0.24466592669487, Validation Loss: 1.5719648599624634\n",
      "Epoch 481: Train Loss: 0.21171829402446746, Validation Loss: 1.5296682119369507\n",
      "Epoch 482: Train Loss: 0.181952965259552, Validation Loss: 1.618253231048584\n",
      "Epoch 483: Train Loss: 0.27460033297538755, Validation Loss: 1.6619356870651245\n",
      "Epoch 484: Train Loss: 0.24258543848991393, Validation Loss: 1.5912786722183228\n",
      "Epoch 485: Train Loss: 0.26154744178056716, Validation Loss: 1.4662718772888184\n",
      "Epoch 486: Train Loss: 0.16563089415431023, Validation Loss: 1.4351412057876587\n",
      "Epoch 487: Train Loss: 0.1816260561347008, Validation Loss: 1.4182027578353882\n",
      "Epoch 488: Train Loss: 0.22465515583753587, Validation Loss: 1.4390536546707153\n",
      "Epoch 489: Train Loss: 0.25922472923994067, Validation Loss: 1.4729048013687134\n",
      "Epoch 490: Train Loss: 0.15464260280132294, Validation Loss: 1.5592138767242432\n",
      "Epoch 491: Train Loss: 0.19476632475852967, Validation Loss: 1.5088449716567993\n",
      "Epoch 492: Train Loss: 0.17277713716030121, Validation Loss: 1.5054439306259155\n",
      "Epoch 493: Train Loss: 0.16760348230600358, Validation Loss: 1.5458154678344727\n",
      "Epoch 494: Train Loss: 0.18770388066768645, Validation Loss: 1.569481611251831\n",
      "Epoch 495: Train Loss: 0.24415611624717712, Validation Loss: 1.4778398275375366\n",
      "Epoch 496: Train Loss: 0.21812669336795806, Validation Loss: 1.4412667751312256\n",
      "Epoch 497: Train Loss: 0.19017048478126525, Validation Loss: 1.4674975872039795\n",
      "Epoch 498: Train Loss: 0.16474059522151946, Validation Loss: 1.543553113937378\n",
      "Epoch 499: Train Loss: 0.19533568620681763, Validation Loss: 1.5543338060379028\n",
      "Fold 4 Metrics:\n",
      "Accuracy: 0.36363636363636365, Precision: 0.4444444444444444, Recall: 0.6666666666666666, F1-score: 0.5333333333333333, AUC: 0.3333333333333333\n",
      "Confusion Matrix:\n",
      "[[0 5]\n",
      " [2 4]]\n",
      "Completed fold 4\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples from subject 13 to test set\n",
      "Adding 6 truth samples from subject 13 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 250)\n",
      "(132, 65, 250)\n",
      "Epoch 0: Train Loss: 0.7138859272003174, Validation Loss: 0.7212328314781189\n",
      "Epoch 1: Train Loss: 0.7481175661087036, Validation Loss: 0.7379496693611145\n",
      "Epoch 2: Train Loss: 0.708773958683014, Validation Loss: 0.7590338587760925\n",
      "Epoch 3: Train Loss: 0.7087496161460877, Validation Loss: 0.7917144894599915\n",
      "Epoch 4: Train Loss: 0.6823771238327027, Validation Loss: 0.8208116292953491\n",
      "Epoch 5: Train Loss: 0.7001851439476013, Validation Loss: 0.8458973169326782\n",
      "Epoch 6: Train Loss: 0.6646673440933227, Validation Loss: 0.8700332641601562\n",
      "Epoch 7: Train Loss: 0.6645251631736755, Validation Loss: 0.8948481678962708\n",
      "Epoch 8: Train Loss: 0.631153666973114, Validation Loss: 0.9119886755943298\n",
      "Epoch 9: Train Loss: 0.6346542596817016, Validation Loss: 0.9239957332611084\n",
      "Epoch 10: Train Loss: 0.6639042735099793, Validation Loss: 0.9415726065635681\n",
      "Epoch 11: Train Loss: 0.7237332582473754, Validation Loss: 0.9728847146034241\n",
      "Epoch 12: Train Loss: 0.654385221004486, Validation Loss: 0.9653903841972351\n",
      "Epoch 13: Train Loss: 0.6659999012947082, Validation Loss: 0.9632347226142883\n",
      "Epoch 14: Train Loss: 0.6629369139671326, Validation Loss: 0.969404399394989\n",
      "Epoch 15: Train Loss: 0.6669572949409485, Validation Loss: 0.9924926161766052\n",
      "Epoch 16: Train Loss: 0.6253582596778869, Validation Loss: 0.9764279723167419\n",
      "Epoch 17: Train Loss: 0.6773391246795655, Validation Loss: 0.964073896408081\n",
      "Epoch 18: Train Loss: 0.6912681221961975, Validation Loss: 0.9743562936782837\n",
      "Epoch 19: Train Loss: 0.6895018696784974, Validation Loss: 0.9876800179481506\n",
      "Epoch 20: Train Loss: 0.7168899416923523, Validation Loss: 1.011497974395752\n",
      "Epoch 21: Train Loss: 0.6912140846252441, Validation Loss: 1.0306580066680908\n",
      "Epoch 22: Train Loss: 0.642008900642395, Validation Loss: 1.061564564704895\n",
      "Epoch 23: Train Loss: 0.643323564529419, Validation Loss: 1.0688406229019165\n",
      "Epoch 24: Train Loss: 0.5966022968292236, Validation Loss: 1.0710065364837646\n",
      "Epoch 25: Train Loss: 0.631416380405426, Validation Loss: 1.0745784044265747\n",
      "Epoch 26: Train Loss: 0.6522100806236267, Validation Loss: 1.0850045680999756\n",
      "Epoch 27: Train Loss: 0.6581241250038147, Validation Loss: 1.0745930671691895\n",
      "Epoch 28: Train Loss: 0.5919975399971008, Validation Loss: 1.0564830303192139\n",
      "Epoch 29: Train Loss: 0.6619675040245057, Validation Loss: 1.0612452030181885\n",
      "Epoch 30: Train Loss: 0.6905433893203735, Validation Loss: 1.080315351486206\n",
      "Epoch 31: Train Loss: 0.6401386260986328, Validation Loss: 1.0812536478042603\n",
      "Epoch 32: Train Loss: 0.6504923105239868, Validation Loss: 1.0868502855300903\n",
      "Epoch 33: Train Loss: 0.6217025756835938, Validation Loss: 1.112808346748352\n",
      "Epoch 34: Train Loss: 0.5873795747756958, Validation Loss: 1.1287962198257446\n",
      "Epoch 35: Train Loss: 0.6635210275650024, Validation Loss: 1.1297043561935425\n",
      "Epoch 36: Train Loss: 0.5854463100433349, Validation Loss: 1.1429907083511353\n",
      "Epoch 37: Train Loss: 0.6682196497917176, Validation Loss: 1.1393263339996338\n",
      "Epoch 38: Train Loss: 0.5873796939849854, Validation Loss: 1.1367331743240356\n",
      "Epoch 39: Train Loss: 0.655941104888916, Validation Loss: 1.1314377784729004\n",
      "Epoch 40: Train Loss: 0.6199269652366638, Validation Loss: 1.1244670152664185\n",
      "Epoch 41: Train Loss: 0.6321659088134766, Validation Loss: 1.128354549407959\n",
      "Epoch 42: Train Loss: 0.6094184756278992, Validation Loss: 1.119399905204773\n",
      "Epoch 43: Train Loss: 0.6148637056350708, Validation Loss: 1.1493171453475952\n",
      "Epoch 44: Train Loss: 0.6605640769004821, Validation Loss: 1.1622779369354248\n",
      "Epoch 45: Train Loss: 0.5823750734329224, Validation Loss: 1.1446934938430786\n",
      "Epoch 46: Train Loss: 0.5853892624378204, Validation Loss: 1.144906997680664\n",
      "Epoch 47: Train Loss: 0.5914517521858216, Validation Loss: 1.1659293174743652\n",
      "Epoch 48: Train Loss: 0.6052479147911072, Validation Loss: 1.173563838005066\n",
      "Epoch 49: Train Loss: 0.6137389183044434, Validation Loss: 1.1964343786239624\n",
      "Epoch 50: Train Loss: 0.5841468811035156, Validation Loss: 1.1919541358947754\n",
      "Epoch 51: Train Loss: 0.5762037873268128, Validation Loss: 1.1787056922912598\n",
      "Epoch 52: Train Loss: 0.5952536344528199, Validation Loss: 1.2003295421600342\n",
      "Epoch 53: Train Loss: 0.5339780509471893, Validation Loss: 1.2187234163284302\n",
      "Epoch 54: Train Loss: 0.6095133662223816, Validation Loss: 1.226725697517395\n",
      "Epoch 55: Train Loss: 0.558352530002594, Validation Loss: 1.2003769874572754\n",
      "Epoch 56: Train Loss: 0.5459849834442139, Validation Loss: 1.216057300567627\n",
      "Epoch 57: Train Loss: 0.5472088813781738, Validation Loss: 1.1909321546554565\n",
      "Epoch 58: Train Loss: 0.6115056157112122, Validation Loss: 1.187812328338623\n",
      "Epoch 59: Train Loss: 0.5958109617233276, Validation Loss: 1.2029359340667725\n",
      "Epoch 60: Train Loss: 0.5547752380371094, Validation Loss: 1.2389135360717773\n",
      "Epoch 61: Train Loss: 0.5814213991165161, Validation Loss: 1.2529982328414917\n",
      "Epoch 62: Train Loss: 0.5993080496788025, Validation Loss: 1.2649918794631958\n",
      "Epoch 63: Train Loss: 0.5613226413726806, Validation Loss: 1.2534133195877075\n",
      "Epoch 64: Train Loss: 0.6268862009048461, Validation Loss: 1.244370460510254\n",
      "Epoch 65: Train Loss: 0.5400365829467774, Validation Loss: 1.2755416631698608\n",
      "Epoch 66: Train Loss: 0.7517632484436035, Validation Loss: 1.2920383214950562\n",
      "Epoch 67: Train Loss: 0.5917802929878235, Validation Loss: 1.2974836826324463\n",
      "Epoch 68: Train Loss: 0.592273473739624, Validation Loss: 1.277927041053772\n",
      "Epoch 69: Train Loss: 0.6129308223724366, Validation Loss: 1.2906811237335205\n",
      "Epoch 70: Train Loss: 0.49056708812713623, Validation Loss: 1.278217077255249\n",
      "Epoch 71: Train Loss: 0.5436405837535858, Validation Loss: 1.2826979160308838\n",
      "Epoch 72: Train Loss: 0.563869035243988, Validation Loss: 1.3148002624511719\n",
      "Epoch 73: Train Loss: 0.5771647214889526, Validation Loss: 1.3402141332626343\n",
      "Epoch 74: Train Loss: 0.5536482334136963, Validation Loss: 1.3384647369384766\n",
      "Epoch 75: Train Loss: 0.5427483975887298, Validation Loss: 1.3331849575042725\n",
      "Epoch 76: Train Loss: 0.5784556090831756, Validation Loss: 1.3338122367858887\n",
      "Epoch 77: Train Loss: 0.5852070093154907, Validation Loss: 1.3561571836471558\n",
      "Epoch 78: Train Loss: 0.579500925540924, Validation Loss: 1.3748102188110352\n",
      "Epoch 79: Train Loss: 0.6447946548461914, Validation Loss: 1.3757600784301758\n",
      "Epoch 80: Train Loss: 0.5870638847351074, Validation Loss: 1.3102117776870728\n",
      "Epoch 81: Train Loss: 0.5488126575946808, Validation Loss: 1.3095215559005737\n",
      "Epoch 82: Train Loss: 0.5620965480804443, Validation Loss: 1.3291711807250977\n",
      "Epoch 83: Train Loss: 0.6137101173400878, Validation Loss: 1.3441754579544067\n",
      "Epoch 84: Train Loss: 0.60239879488945, Validation Loss: 1.3548794984817505\n",
      "Epoch 85: Train Loss: 0.5689609766006469, Validation Loss: 1.363608479499817\n",
      "Epoch 86: Train Loss: 0.6427167892456055, Validation Loss: 1.3429362773895264\n",
      "Epoch 87: Train Loss: 0.5350376844406128, Validation Loss: 1.363835096359253\n",
      "Epoch 88: Train Loss: 0.5401084661483765, Validation Loss: 1.319838523864746\n",
      "Epoch 89: Train Loss: 0.5143402993679047, Validation Loss: 1.327099084854126\n",
      "Epoch 90: Train Loss: 0.5148614466190338, Validation Loss: 1.3556036949157715\n",
      "Epoch 91: Train Loss: 0.623318326473236, Validation Loss: 1.3981409072875977\n",
      "Epoch 92: Train Loss: 0.5425125479698181, Validation Loss: 1.410529613494873\n",
      "Epoch 93: Train Loss: 0.5337743520736694, Validation Loss: 1.404536247253418\n",
      "Epoch 94: Train Loss: 0.6309035658836365, Validation Loss: 1.4066524505615234\n",
      "Epoch 95: Train Loss: 0.6016719341278076, Validation Loss: 1.419621467590332\n",
      "Epoch 96: Train Loss: 0.5367175221443177, Validation Loss: 1.4104247093200684\n",
      "Epoch 97: Train Loss: 0.49137994050979616, Validation Loss: 1.4007714986801147\n",
      "Epoch 98: Train Loss: 0.5407856583595276, Validation Loss: 1.3590022325515747\n",
      "Epoch 99: Train Loss: 0.5436957716941834, Validation Loss: 1.3851505517959595\n",
      "Epoch 100: Train Loss: 0.5601541519165039, Validation Loss: 1.3933717012405396\n",
      "Epoch 101: Train Loss: 0.5838495910167694, Validation Loss: 1.409118890762329\n",
      "Epoch 102: Train Loss: 0.5969434976577759, Validation Loss: 1.4204576015472412\n",
      "Epoch 103: Train Loss: 0.5264664411544799, Validation Loss: 1.4372279644012451\n",
      "Epoch 104: Train Loss: 0.5387917697429657, Validation Loss: 1.4637465476989746\n",
      "Epoch 105: Train Loss: 0.5185075402259827, Validation Loss: 1.4860666990280151\n",
      "Epoch 106: Train Loss: 0.516256183385849, Validation Loss: 1.4728106260299683\n",
      "Epoch 107: Train Loss: 0.48795983791351316, Validation Loss: 1.4592818021774292\n",
      "Epoch 108: Train Loss: 0.5095894515514374, Validation Loss: 1.4473778009414673\n",
      "Epoch 109: Train Loss: 0.5993334293365479, Validation Loss: 1.4296188354492188\n",
      "Epoch 110: Train Loss: 0.6221275568008423, Validation Loss: 1.473549485206604\n",
      "Epoch 111: Train Loss: 0.5377387881278992, Validation Loss: 1.5075571537017822\n",
      "Epoch 112: Train Loss: 0.5695076763629914, Validation Loss: 1.5310033559799194\n",
      "Epoch 113: Train Loss: 0.45580669343471525, Validation Loss: 1.4948230981826782\n",
      "Epoch 114: Train Loss: 0.5274568796157837, Validation Loss: 1.529748558998108\n",
      "Epoch 115: Train Loss: 0.5219975590705872, Validation Loss: 1.5264772176742554\n",
      "Epoch 116: Train Loss: 0.5246697902679444, Validation Loss: 1.5455527305603027\n",
      "Epoch 117: Train Loss: 0.4771625816822052, Validation Loss: 1.5541038513183594\n",
      "Epoch 118: Train Loss: 0.5016301155090332, Validation Loss: 1.537841796875\n",
      "Epoch 119: Train Loss: 0.5570392966270447, Validation Loss: 1.455034613609314\n",
      "Epoch 120: Train Loss: 0.53702113032341, Validation Loss: 1.460026502609253\n",
      "Epoch 121: Train Loss: 0.5242066442966461, Validation Loss: 1.4667482376098633\n",
      "Epoch 122: Train Loss: 0.4940622568130493, Validation Loss: 1.5084186792373657\n",
      "Epoch 123: Train Loss: 0.4820109844207764, Validation Loss: 1.5271037817001343\n",
      "Epoch 124: Train Loss: 0.4961579442024231, Validation Loss: 1.4941219091415405\n",
      "Epoch 125: Train Loss: 0.5436080038547516, Validation Loss: 1.4778434038162231\n",
      "Epoch 126: Train Loss: 0.5423007607460022, Validation Loss: 1.51919686794281\n",
      "Epoch 127: Train Loss: 0.47975826263427734, Validation Loss: 1.5521200895309448\n",
      "Epoch 128: Train Loss: 0.5040297150611878, Validation Loss: 1.5299619436264038\n",
      "Epoch 129: Train Loss: 0.5372623205184937, Validation Loss: 1.5485515594482422\n",
      "Epoch 130: Train Loss: 0.5361988842487335, Validation Loss: 1.543580412864685\n",
      "Epoch 131: Train Loss: 0.5001408100128174, Validation Loss: 1.553774118423462\n",
      "Epoch 132: Train Loss: 0.5764739513397217, Validation Loss: 1.590874433517456\n",
      "Epoch 133: Train Loss: 0.49728780388832095, Validation Loss: 1.6221781969070435\n",
      "Epoch 134: Train Loss: 0.5511003613471985, Validation Loss: 1.6508381366729736\n",
      "Epoch 135: Train Loss: 0.4915182709693909, Validation Loss: 1.6447969675064087\n",
      "Epoch 136: Train Loss: 0.5515671253204346, Validation Loss: 1.611311912536621\n",
      "Epoch 137: Train Loss: 0.5394969820976258, Validation Loss: 1.6049305200576782\n",
      "Epoch 138: Train Loss: 0.4899975657463074, Validation Loss: 1.5709773302078247\n",
      "Epoch 139: Train Loss: 0.5276055693626404, Validation Loss: 1.573095679283142\n",
      "Epoch 140: Train Loss: 0.5040851652622222, Validation Loss: 1.5471887588500977\n",
      "Epoch 141: Train Loss: 0.4767630875110626, Validation Loss: 1.5864276885986328\n",
      "Epoch 142: Train Loss: 0.5741842985153198, Validation Loss: 1.6317516565322876\n",
      "Epoch 143: Train Loss: 0.4845248758792877, Validation Loss: 1.576509714126587\n",
      "Epoch 144: Train Loss: 0.5399354457855224, Validation Loss: 1.6128822565078735\n",
      "Epoch 145: Train Loss: 0.5340363442897796, Validation Loss: 1.6076360940933228\n",
      "Epoch 146: Train Loss: 0.49497339129447937, Validation Loss: 1.6283186674118042\n",
      "Epoch 147: Train Loss: 0.42618879675865173, Validation Loss: 1.5963317155838013\n",
      "Epoch 148: Train Loss: 0.5253176510334014, Validation Loss: 1.537292718887329\n",
      "Epoch 149: Train Loss: 0.547499829530716, Validation Loss: 1.554741621017456\n",
      "Epoch 150: Train Loss: 0.47191237211227416, Validation Loss: 1.5919313430786133\n",
      "Epoch 151: Train Loss: 0.46690388917922976, Validation Loss: 1.6236540079116821\n",
      "Epoch 152: Train Loss: 0.4952329158782959, Validation Loss: 1.618286371231079\n",
      "Epoch 153: Train Loss: 0.4864916861057281, Validation Loss: 1.6225249767303467\n",
      "Epoch 154: Train Loss: 0.462439751625061, Validation Loss: 1.6472716331481934\n",
      "Epoch 155: Train Loss: 0.5276053667068481, Validation Loss: 1.670403242111206\n",
      "Epoch 156: Train Loss: 0.47481865882873536, Validation Loss: 1.6822341680526733\n",
      "Epoch 157: Train Loss: 0.492856502532959, Validation Loss: 1.6789802312850952\n",
      "Epoch 158: Train Loss: 0.45391640067100525, Validation Loss: 1.653266191482544\n",
      "Epoch 159: Train Loss: 0.4760352075099945, Validation Loss: 1.6284263134002686\n",
      "Epoch 160: Train Loss: 0.4830354332923889, Validation Loss: 1.6690573692321777\n",
      "Epoch 161: Train Loss: 0.5143881976604462, Validation Loss: 1.6901615858078003\n",
      "Epoch 162: Train Loss: 0.4661234259605408, Validation Loss: 1.6500834226608276\n",
      "Epoch 163: Train Loss: 0.4903852343559265, Validation Loss: 1.646113634109497\n",
      "Epoch 164: Train Loss: 0.44654111862182616, Validation Loss: 1.6508384943008423\n",
      "Epoch 165: Train Loss: 0.4804945528507233, Validation Loss: 1.636991024017334\n",
      "Epoch 166: Train Loss: 0.5634683310985565, Validation Loss: 1.659226417541504\n",
      "Epoch 167: Train Loss: 0.5554048299789429, Validation Loss: 1.6909373998641968\n",
      "Epoch 168: Train Loss: 0.5084137141704559, Validation Loss: 1.6727591753005981\n",
      "Epoch 169: Train Loss: 0.5053916990756988, Validation Loss: 1.6783839464187622\n",
      "Epoch 170: Train Loss: 0.5009527027606964, Validation Loss: 1.6806185245513916\n",
      "Epoch 171: Train Loss: 0.4968314468860626, Validation Loss: 1.6517078876495361\n",
      "Epoch 172: Train Loss: 0.4969086587429047, Validation Loss: 1.6840178966522217\n",
      "Epoch 173: Train Loss: 0.5811235249042511, Validation Loss: 1.693610429763794\n",
      "Epoch 174: Train Loss: 0.4580019474029541, Validation Loss: 1.685533881187439\n",
      "Epoch 175: Train Loss: 0.5109079778194427, Validation Loss: 1.6761280298233032\n",
      "Epoch 176: Train Loss: 0.46946183443069456, Validation Loss: 1.6668193340301514\n",
      "Epoch 177: Train Loss: 0.5266681492328644, Validation Loss: 1.6109156608581543\n",
      "Epoch 178: Train Loss: 0.43301960825920105, Validation Loss: 1.5969667434692383\n",
      "Epoch 179: Train Loss: 0.5306380569934845, Validation Loss: 1.6320867538452148\n",
      "Epoch 180: Train Loss: 0.4168400287628174, Validation Loss: 1.5929423570632935\n",
      "Epoch 181: Train Loss: 0.5038197219371796, Validation Loss: 1.6226569414138794\n",
      "Epoch 182: Train Loss: 0.44559335708618164, Validation Loss: 1.640584945678711\n",
      "Epoch 183: Train Loss: 0.5003384709358215, Validation Loss: 1.674195408821106\n",
      "Epoch 184: Train Loss: 0.5609745919704437, Validation Loss: 1.6889206171035767\n",
      "Epoch 185: Train Loss: 0.47189358472824094, Validation Loss: 1.6814296245574951\n",
      "Epoch 186: Train Loss: 0.4274133563041687, Validation Loss: 1.6991537809371948\n",
      "Epoch 187: Train Loss: 0.4122783899307251, Validation Loss: 1.7380359172821045\n",
      "Epoch 188: Train Loss: 0.485723215341568, Validation Loss: 1.743140697479248\n",
      "Epoch 189: Train Loss: 0.4765618860721588, Validation Loss: 1.7259997129440308\n",
      "Epoch 190: Train Loss: 0.4465389013290405, Validation Loss: 1.71822988986969\n",
      "Epoch 191: Train Loss: 0.4497445225715637, Validation Loss: 1.6485157012939453\n",
      "Epoch 192: Train Loss: 0.40201722383499144, Validation Loss: 1.6729576587677002\n",
      "Epoch 193: Train Loss: 0.49178858399391173, Validation Loss: 1.7162269353866577\n",
      "Epoch 194: Train Loss: 0.4303196549415588, Validation Loss: 1.719675898551941\n",
      "Epoch 195: Train Loss: 0.4712405502796173, Validation Loss: 1.750818133354187\n",
      "Epoch 196: Train Loss: 0.45368351340293883, Validation Loss: 1.7822372913360596\n",
      "Epoch 197: Train Loss: 0.40004082322120665, Validation Loss: 1.7836658954620361\n",
      "Epoch 198: Train Loss: 0.47357909083366395, Validation Loss: 1.77354896068573\n",
      "Epoch 199: Train Loss: 0.5028397738933563, Validation Loss: 1.791586995124817\n",
      "Epoch 200: Train Loss: 0.4076352477073669, Validation Loss: 1.794481873512268\n",
      "Epoch 201: Train Loss: 0.45142298340797427, Validation Loss: 1.8008918762207031\n",
      "Epoch 202: Train Loss: 0.41964600086212156, Validation Loss: 1.7965986728668213\n",
      "Epoch 203: Train Loss: 0.41684882044792176, Validation Loss: 1.7799266576766968\n",
      "Epoch 204: Train Loss: 0.42367254495620726, Validation Loss: 1.7496589422225952\n",
      "Epoch 205: Train Loss: 0.41140342950820924, Validation Loss: 1.7491801977157593\n",
      "Epoch 206: Train Loss: 0.46662846207618713, Validation Loss: 1.7786941528320312\n",
      "Epoch 207: Train Loss: 0.4681155025959015, Validation Loss: 1.7492533922195435\n",
      "Epoch 208: Train Loss: 0.4329249083995819, Validation Loss: 1.769790530204773\n",
      "Epoch 209: Train Loss: 0.5069278120994568, Validation Loss: 1.7250463962554932\n",
      "Epoch 210: Train Loss: 0.4654115676879883, Validation Loss: 1.7587627172470093\n",
      "Epoch 211: Train Loss: 0.45538870692253114, Validation Loss: 1.7053228616714478\n",
      "Epoch 212: Train Loss: 0.38852640986442566, Validation Loss: 1.746484398841858\n",
      "Epoch 213: Train Loss: 0.4735648512840271, Validation Loss: 1.8001495599746704\n",
      "Epoch 214: Train Loss: 0.46557403802871705, Validation Loss: 1.7295985221862793\n",
      "Epoch 215: Train Loss: 0.4024456024169922, Validation Loss: 1.7833770513534546\n",
      "Epoch 216: Train Loss: 0.38749395608901976, Validation Loss: 1.789390206336975\n",
      "Epoch 217: Train Loss: 0.4130322575569153, Validation Loss: 1.7999244928359985\n",
      "Epoch 218: Train Loss: 0.387550488114357, Validation Loss: 1.7982676029205322\n",
      "Epoch 219: Train Loss: 0.3617581158876419, Validation Loss: 1.8131003379821777\n",
      "Epoch 220: Train Loss: 0.41632758975028994, Validation Loss: 1.806372046470642\n",
      "Epoch 221: Train Loss: 0.3997578561306, Validation Loss: 1.8466659784317017\n",
      "Epoch 222: Train Loss: 0.4102249950170517, Validation Loss: 1.881094217300415\n",
      "Epoch 223: Train Loss: 0.42659040093421935, Validation Loss: 1.8453303575515747\n",
      "Epoch 224: Train Loss: 0.4811441481113434, Validation Loss: 1.8770790100097656\n",
      "Epoch 225: Train Loss: 0.47932112216949463, Validation Loss: 1.9218806028366089\n",
      "Epoch 226: Train Loss: 0.5048845827579498, Validation Loss: 1.9288239479064941\n",
      "Epoch 227: Train Loss: 0.4304969310760498, Validation Loss: 1.8484634160995483\n",
      "Epoch 228: Train Loss: 0.4666164696216583, Validation Loss: 1.8961691856384277\n",
      "Epoch 229: Train Loss: 0.4303323566913605, Validation Loss: 1.9147251844406128\n",
      "Epoch 230: Train Loss: 0.41881983280181884, Validation Loss: 1.8314963579177856\n",
      "Epoch 231: Train Loss: 0.4570757389068604, Validation Loss: 1.918681263923645\n",
      "Epoch 232: Train Loss: 0.5270904421806335, Validation Loss: 1.9364721775054932\n",
      "Epoch 233: Train Loss: 0.4052975296974182, Validation Loss: 1.939954400062561\n",
      "Epoch 234: Train Loss: 0.3942443490028381, Validation Loss: 1.945900797843933\n",
      "Epoch 235: Train Loss: 0.37966942191123965, Validation Loss: 2.012707471847534\n",
      "Epoch 236: Train Loss: 0.3555652126669884, Validation Loss: 1.9611573219299316\n",
      "Epoch 237: Train Loss: 0.36346727013587954, Validation Loss: 1.955493450164795\n",
      "Epoch 238: Train Loss: 0.4459028899669647, Validation Loss: 1.9716384410858154\n",
      "Epoch 239: Train Loss: 0.386414510011673, Validation Loss: 1.948777675628662\n",
      "Epoch 240: Train Loss: 0.38493654131889343, Validation Loss: 2.019826889038086\n",
      "Epoch 241: Train Loss: 0.3763492614030838, Validation Loss: 1.9838579893112183\n",
      "Epoch 242: Train Loss: 0.47222687005996705, Validation Loss: 1.9752167463302612\n",
      "Epoch 243: Train Loss: 0.45951384902000425, Validation Loss: 1.995097279548645\n",
      "Epoch 244: Train Loss: 0.4806079626083374, Validation Loss: 2.019639015197754\n",
      "Epoch 245: Train Loss: 0.40959245562553404, Validation Loss: 1.984939694404602\n",
      "Epoch 246: Train Loss: 0.3464387893676758, Validation Loss: 1.98296320438385\n",
      "Epoch 247: Train Loss: 0.41984668374061584, Validation Loss: 1.9527260065078735\n",
      "Epoch 248: Train Loss: 0.36921332478523256, Validation Loss: 1.9677166938781738\n",
      "Epoch 249: Train Loss: 0.4126110255718231, Validation Loss: 1.9619749784469604\n",
      "Epoch 250: Train Loss: 0.4079721510410309, Validation Loss: 1.822424292564392\n",
      "Epoch 251: Train Loss: 0.39300852417945864, Validation Loss: 1.879259467124939\n",
      "Epoch 252: Train Loss: 0.3704466879367828, Validation Loss: 1.9175947904586792\n",
      "Epoch 253: Train Loss: 0.46238287091255187, Validation Loss: 1.9991555213928223\n",
      "Epoch 254: Train Loss: 0.44401679635047914, Validation Loss: 2.0256614685058594\n",
      "Epoch 255: Train Loss: 0.4124559462070465, Validation Loss: 2.032233476638794\n",
      "Epoch 256: Train Loss: 0.4392676591873169, Validation Loss: 2.0706634521484375\n",
      "Epoch 257: Train Loss: 0.3421169638633728, Validation Loss: 2.1217339038848877\n",
      "Epoch 258: Train Loss: 0.38089250922203066, Validation Loss: 2.059955596923828\n",
      "Epoch 259: Train Loss: 0.3764812171459198, Validation Loss: 2.050278425216675\n",
      "Epoch 260: Train Loss: 0.41212413311004636, Validation Loss: 2.07710862159729\n",
      "Epoch 261: Train Loss: 0.3862201988697052, Validation Loss: 2.0750057697296143\n",
      "Epoch 262: Train Loss: 0.44893903732299806, Validation Loss: 2.0826990604400635\n",
      "Epoch 263: Train Loss: 0.4021350979804993, Validation Loss: 2.061586856842041\n",
      "Epoch 264: Train Loss: 0.37154549956321714, Validation Loss: 2.1604740619659424\n",
      "Epoch 265: Train Loss: 0.41491780877113343, Validation Loss: 2.166929244995117\n",
      "Epoch 266: Train Loss: 0.4582318663597107, Validation Loss: 2.063027858734131\n",
      "Epoch 267: Train Loss: 0.35279629826545716, Validation Loss: 2.0993764400482178\n",
      "Epoch 268: Train Loss: 0.4071833729743958, Validation Loss: 2.1382906436920166\n",
      "Epoch 269: Train Loss: 0.38059598207473755, Validation Loss: 2.126354932785034\n",
      "Epoch 270: Train Loss: 0.37169135808944703, Validation Loss: 2.1486799716949463\n",
      "Epoch 271: Train Loss: 0.3685271620750427, Validation Loss: 2.0473074913024902\n",
      "Epoch 272: Train Loss: 0.3416348934173584, Validation Loss: 2.0744218826293945\n",
      "Epoch 273: Train Loss: 0.42590181827545165, Validation Loss: 2.052459239959717\n",
      "Epoch 274: Train Loss: 0.43569728136062624, Validation Loss: 2.0896458625793457\n",
      "Epoch 275: Train Loss: 0.3818123400211334, Validation Loss: 2.053208589553833\n",
      "Epoch 276: Train Loss: 0.38701658844947817, Validation Loss: 2.1247739791870117\n",
      "Epoch 277: Train Loss: 0.3796298742294312, Validation Loss: 2.1559078693389893\n",
      "Epoch 278: Train Loss: 0.419014173746109, Validation Loss: 2.0956108570098877\n",
      "Epoch 279: Train Loss: 0.39373204708099363, Validation Loss: 2.1155803203582764\n",
      "Epoch 280: Train Loss: 0.3600641965866089, Validation Loss: 2.026848077774048\n",
      "Epoch 281: Train Loss: 0.34505481123924253, Validation Loss: 2.0519659519195557\n",
      "Epoch 282: Train Loss: 0.4362021565437317, Validation Loss: 2.0088136196136475\n",
      "Epoch 283: Train Loss: 0.3824016809463501, Validation Loss: 2.11944842338562\n",
      "Epoch 284: Train Loss: 0.4949475646018982, Validation Loss: 2.168731689453125\n",
      "Epoch 285: Train Loss: 0.3021780550479889, Validation Loss: 2.13720703125\n",
      "Epoch 286: Train Loss: 0.3150308519601822, Validation Loss: 2.0866925716400146\n",
      "Epoch 287: Train Loss: 0.34375456273555755, Validation Loss: 2.081915855407715\n",
      "Epoch 288: Train Loss: 0.5111968100070954, Validation Loss: 2.1438143253326416\n",
      "Epoch 289: Train Loss: 0.41595605611801145, Validation Loss: 2.1450791358947754\n",
      "Epoch 290: Train Loss: 0.35448219776153567, Validation Loss: 2.163071393966675\n",
      "Epoch 291: Train Loss: 0.37620466351509096, Validation Loss: 2.1362476348876953\n",
      "Epoch 292: Train Loss: 0.3389697581529617, Validation Loss: 2.1715478897094727\n",
      "Epoch 293: Train Loss: 0.42623255848884584, Validation Loss: 2.2474331855773926\n",
      "Epoch 294: Train Loss: 0.3437000513076782, Validation Loss: 2.2413599491119385\n",
      "Epoch 295: Train Loss: 0.39177835583686826, Validation Loss: 2.2064454555511475\n",
      "Epoch 296: Train Loss: 0.36179768443107607, Validation Loss: 2.1325526237487793\n",
      "Epoch 297: Train Loss: 0.4547178506851196, Validation Loss: 2.1975367069244385\n",
      "Epoch 298: Train Loss: 0.388031667470932, Validation Loss: 2.1918811798095703\n",
      "Epoch 299: Train Loss: 0.39296003580093386, Validation Loss: 2.198052167892456\n",
      "Epoch 300: Train Loss: 0.4402651429176331, Validation Loss: 2.161298990249634\n",
      "Epoch 301: Train Loss: 0.4186472177505493, Validation Loss: 2.1756515502929688\n",
      "Epoch 302: Train Loss: 0.3362850546836853, Validation Loss: 2.1178476810455322\n",
      "Epoch 303: Train Loss: 0.41165324449539187, Validation Loss: 2.1487410068511963\n",
      "Epoch 304: Train Loss: 0.30681494176387786, Validation Loss: 2.0496416091918945\n",
      "Epoch 305: Train Loss: 0.3409707546234131, Validation Loss: 2.0612826347351074\n",
      "Epoch 306: Train Loss: 0.3603322982788086, Validation Loss: 2.0898423194885254\n",
      "Epoch 307: Train Loss: 0.3743847131729126, Validation Loss: 2.117330312728882\n",
      "Epoch 308: Train Loss: 0.3452021270990372, Validation Loss: 2.178879976272583\n",
      "Epoch 309: Train Loss: 0.36589713096618653, Validation Loss: 2.097681999206543\n",
      "Epoch 310: Train Loss: 0.397617769241333, Validation Loss: 2.130244255065918\n",
      "Epoch 311: Train Loss: 0.33189351856708527, Validation Loss: 2.107970952987671\n",
      "Epoch 312: Train Loss: 0.32003431022167206, Validation Loss: 2.132641315460205\n",
      "Epoch 313: Train Loss: 0.35784240663051603, Validation Loss: 2.165302038192749\n",
      "Epoch 314: Train Loss: 0.3984417498111725, Validation Loss: 2.2107057571411133\n",
      "Epoch 315: Train Loss: 0.37593162059783936, Validation Loss: 2.219473361968994\n",
      "Epoch 316: Train Loss: 0.3734151363372803, Validation Loss: 2.1837799549102783\n",
      "Epoch 317: Train Loss: 0.36301971077919004, Validation Loss: 2.1334383487701416\n",
      "Epoch 318: Train Loss: 0.3867945671081543, Validation Loss: 2.188586473464966\n",
      "Epoch 319: Train Loss: 0.35782511830329894, Validation Loss: 2.237658739089966\n",
      "Epoch 320: Train Loss: 0.38752928376197815, Validation Loss: 2.2159335613250732\n",
      "Epoch 321: Train Loss: 0.3480016589164734, Validation Loss: 2.2390527725219727\n",
      "Epoch 322: Train Loss: 0.36361523866653445, Validation Loss: 2.2386884689331055\n",
      "Epoch 323: Train Loss: 0.35600371956825255, Validation Loss: 2.2427966594696045\n",
      "Epoch 324: Train Loss: 0.2892365872859955, Validation Loss: 2.2082064151763916\n",
      "Epoch 325: Train Loss: 0.33620010018348695, Validation Loss: 2.233614683151245\n",
      "Epoch 326: Train Loss: 0.31428677439689634, Validation Loss: 2.258626699447632\n",
      "Epoch 327: Train Loss: 0.4855023920536041, Validation Loss: 2.2630372047424316\n",
      "Epoch 328: Train Loss: 0.4157870948314667, Validation Loss: 2.061418056488037\n",
      "Epoch 329: Train Loss: 0.33208409547805784, Validation Loss: 2.152035713195801\n",
      "Epoch 330: Train Loss: 0.32924277782440187, Validation Loss: 2.2686052322387695\n",
      "Epoch 331: Train Loss: 0.344305157661438, Validation Loss: 2.166445016860962\n",
      "Epoch 332: Train Loss: 0.31117220818996427, Validation Loss: 2.250789165496826\n",
      "Epoch 333: Train Loss: 0.29248867034912107, Validation Loss: 2.245509386062622\n",
      "Epoch 334: Train Loss: 0.3441513478755951, Validation Loss: 2.3223683834075928\n",
      "Epoch 335: Train Loss: 0.35027897357940674, Validation Loss: 2.2224268913269043\n",
      "Epoch 336: Train Loss: 0.39039781093597414, Validation Loss: 2.2393791675567627\n",
      "Epoch 337: Train Loss: 0.38259785771369936, Validation Loss: 2.3264148235321045\n",
      "Epoch 338: Train Loss: 0.30477593243122103, Validation Loss: 2.2997782230377197\n",
      "Epoch 339: Train Loss: 0.3600191354751587, Validation Loss: 2.342376470565796\n",
      "Epoch 340: Train Loss: 0.32036020159721373, Validation Loss: 2.3449573516845703\n",
      "Epoch 341: Train Loss: 0.3678166836500168, Validation Loss: 2.349984884262085\n",
      "Epoch 342: Train Loss: 0.34187306761741637, Validation Loss: 2.1852707862854004\n",
      "Epoch 343: Train Loss: 0.37321906089782714, Validation Loss: 2.1817617416381836\n",
      "Epoch 344: Train Loss: 0.3933226943016052, Validation Loss: 2.16374135017395\n",
      "Epoch 345: Train Loss: 0.3742338478565216, Validation Loss: 2.205979347229004\n",
      "Epoch 346: Train Loss: 0.38378127217292785, Validation Loss: 2.3513171672821045\n",
      "Epoch 347: Train Loss: 0.40901486575603485, Validation Loss: 2.4326910972595215\n",
      "Epoch 348: Train Loss: 0.39342735409736634, Validation Loss: 2.332850694656372\n",
      "Epoch 349: Train Loss: 0.29711945056915284, Validation Loss: 2.2632181644439697\n",
      "Epoch 350: Train Loss: 0.384848028421402, Validation Loss: 2.278777837753296\n",
      "Epoch 351: Train Loss: 0.34084304571151736, Validation Loss: 2.2038321495056152\n",
      "Epoch 352: Train Loss: 0.37087456583976747, Validation Loss: 2.2540411949157715\n",
      "Epoch 353: Train Loss: 0.3263836860656738, Validation Loss: 2.3236188888549805\n",
      "Epoch 354: Train Loss: 0.28726046681404116, Validation Loss: 2.3438632488250732\n",
      "Epoch 355: Train Loss: 0.29860185384750365, Validation Loss: 2.3814780712127686\n",
      "Epoch 356: Train Loss: 0.28636823296546937, Validation Loss: 2.3145642280578613\n",
      "Epoch 357: Train Loss: 0.3208152651786804, Validation Loss: 2.2598373889923096\n",
      "Epoch 358: Train Loss: 0.30419217348098754, Validation Loss: 2.3093247413635254\n",
      "Epoch 359: Train Loss: 0.3291188806295395, Validation Loss: 2.323025941848755\n",
      "Epoch 360: Train Loss: 0.2932016283273697, Validation Loss: 2.2049412727355957\n",
      "Epoch 361: Train Loss: 0.3104328989982605, Validation Loss: 2.252237558364868\n",
      "Epoch 362: Train Loss: 0.27885671257972716, Validation Loss: 2.272639274597168\n",
      "Epoch 363: Train Loss: 0.3852922260761261, Validation Loss: 2.2557432651519775\n",
      "Epoch 364: Train Loss: 0.26388842761516573, Validation Loss: 2.3315958976745605\n",
      "Epoch 365: Train Loss: 0.3373544096946716, Validation Loss: 2.286424160003662\n",
      "Epoch 366: Train Loss: 0.31840783953666685, Validation Loss: 2.238464593887329\n",
      "Epoch 367: Train Loss: 0.2698754593729973, Validation Loss: 2.2655699253082275\n",
      "Epoch 368: Train Loss: 0.308688348531723, Validation Loss: 2.270768165588379\n",
      "Epoch 369: Train Loss: 0.3380557656288147, Validation Loss: 2.1517457962036133\n",
      "Epoch 370: Train Loss: 0.29267129898071287, Validation Loss: 2.2508554458618164\n",
      "Epoch 371: Train Loss: 0.3380886733531952, Validation Loss: 2.2693591117858887\n",
      "Epoch 372: Train Loss: 0.31462820768356325, Validation Loss: 2.3539836406707764\n",
      "Epoch 373: Train Loss: 0.2794911816716194, Validation Loss: 2.351386308670044\n",
      "Epoch 374: Train Loss: 0.32048711478710173, Validation Loss: 2.317272186279297\n",
      "Epoch 375: Train Loss: 0.29223738312721254, Validation Loss: 2.243694543838501\n",
      "Epoch 376: Train Loss: 0.3248869597911835, Validation Loss: 2.2894606590270996\n",
      "Epoch 377: Train Loss: 0.37744455337524413, Validation Loss: 2.3052821159362793\n",
      "Epoch 378: Train Loss: 0.28601743280887604, Validation Loss: 2.3227949142456055\n",
      "Epoch 379: Train Loss: 0.3494062960147858, Validation Loss: 2.3687453269958496\n",
      "Epoch 380: Train Loss: 0.3204279035329819, Validation Loss: 2.3988142013549805\n",
      "Epoch 381: Train Loss: 0.3040271282196045, Validation Loss: 2.4735326766967773\n",
      "Epoch 382: Train Loss: 0.3993247389793396, Validation Loss: 2.5252699851989746\n",
      "Epoch 383: Train Loss: 0.27318131625652314, Validation Loss: 2.4227912425994873\n",
      "Epoch 384: Train Loss: 0.28526017963886263, Validation Loss: 2.3775477409362793\n",
      "Epoch 385: Train Loss: 0.2575314432382584, Validation Loss: 2.4209482669830322\n",
      "Epoch 386: Train Loss: 0.4865467309951782, Validation Loss: 2.4093520641326904\n",
      "Epoch 387: Train Loss: 0.2997504651546478, Validation Loss: 2.468010902404785\n",
      "Epoch 388: Train Loss: 0.27227252423763276, Validation Loss: 2.4914565086364746\n",
      "Epoch 389: Train Loss: 0.36688015758991244, Validation Loss: 2.3905506134033203\n",
      "Epoch 390: Train Loss: 0.3069076657295227, Validation Loss: 2.268599271774292\n",
      "Epoch 391: Train Loss: 0.36686624586582184, Validation Loss: 2.333883285522461\n",
      "Epoch 392: Train Loss: 0.3322405219078064, Validation Loss: 2.291025161743164\n",
      "Epoch 393: Train Loss: 0.34668020009994505, Validation Loss: 2.357161045074463\n",
      "Epoch 394: Train Loss: 0.26002461165189744, Validation Loss: 2.417450189590454\n",
      "Epoch 395: Train Loss: 0.2955463737249374, Validation Loss: 2.3821139335632324\n",
      "Epoch 396: Train Loss: 0.3142908692359924, Validation Loss: 2.4473586082458496\n",
      "Epoch 397: Train Loss: 0.38184713423252103, Validation Loss: 2.4898908138275146\n",
      "Epoch 398: Train Loss: 0.3887789219617844, Validation Loss: 2.549534559249878\n",
      "Epoch 399: Train Loss: 0.2766189843416214, Validation Loss: 2.455408811569214\n",
      "Epoch 400: Train Loss: 0.2583803042769432, Validation Loss: 2.460150957107544\n",
      "Epoch 401: Train Loss: 0.27840364873409273, Validation Loss: 2.4851183891296387\n",
      "Epoch 402: Train Loss: 0.31829885840415956, Validation Loss: 2.395468235015869\n",
      "Epoch 403: Train Loss: 0.26450336575508115, Validation Loss: 2.4582643508911133\n",
      "Epoch 404: Train Loss: 0.334575355052948, Validation Loss: 2.3946030139923096\n",
      "Epoch 405: Train Loss: 0.338266858458519, Validation Loss: 2.4088938236236572\n",
      "Epoch 406: Train Loss: 0.2388061836361885, Validation Loss: 2.3802177906036377\n",
      "Epoch 407: Train Loss: 0.2680426359176636, Validation Loss: 2.414031744003296\n",
      "Epoch 408: Train Loss: 0.33229617178440096, Validation Loss: 2.2663779258728027\n",
      "Epoch 409: Train Loss: 0.22857190370559693, Validation Loss: 2.3023202419281006\n",
      "Epoch 410: Train Loss: 0.30666128396987913, Validation Loss: 2.34228777885437\n",
      "Epoch 411: Train Loss: 0.3398913085460663, Validation Loss: 2.3783271312713623\n",
      "Epoch 412: Train Loss: 0.32895071506500245, Validation Loss: 2.3845839500427246\n",
      "Epoch 413: Train Loss: 0.25846333205699923, Validation Loss: 2.479700803756714\n",
      "Epoch 414: Train Loss: 0.415572863817215, Validation Loss: 2.4695918560028076\n",
      "Epoch 415: Train Loss: 0.331220281124115, Validation Loss: 2.5142874717712402\n",
      "Epoch 416: Train Loss: 0.24422124028205872, Validation Loss: 2.525207757949829\n",
      "Epoch 417: Train Loss: 0.28477299213409424, Validation Loss: 2.520794153213501\n",
      "Epoch 418: Train Loss: 0.3381957232952118, Validation Loss: 2.5443296432495117\n",
      "Epoch 419: Train Loss: 0.28053785264492037, Validation Loss: 2.615605354309082\n",
      "Epoch 420: Train Loss: 0.28898573517799375, Validation Loss: 2.6225242614746094\n",
      "Epoch 421: Train Loss: 0.2843364655971527, Validation Loss: 2.65141224861145\n",
      "Epoch 422: Train Loss: 0.3334951430559158, Validation Loss: 2.573513984680176\n",
      "Epoch 423: Train Loss: 0.273184210062027, Validation Loss: 2.354735851287842\n",
      "Epoch 424: Train Loss: 0.3022193521261215, Validation Loss: 2.4609663486480713\n",
      "Epoch 425: Train Loss: 0.25946183502674103, Validation Loss: 2.4935970306396484\n",
      "Epoch 426: Train Loss: 0.28082490563392637, Validation Loss: 2.6088645458221436\n",
      "Epoch 427: Train Loss: 0.2952173948287964, Validation Loss: 2.62396502494812\n",
      "Epoch 428: Train Loss: 0.24428158700466157, Validation Loss: 2.606783628463745\n",
      "Epoch 429: Train Loss: 0.2650517076253891, Validation Loss: 2.549457550048828\n",
      "Epoch 430: Train Loss: 0.3214911222457886, Validation Loss: 2.5485410690307617\n",
      "Epoch 431: Train Loss: 0.2990141659975052, Validation Loss: 2.6154680252075195\n",
      "Epoch 432: Train Loss: 0.33678366243839264, Validation Loss: 2.679363250732422\n",
      "Epoch 433: Train Loss: 0.23848422169685363, Validation Loss: 2.7245452404022217\n",
      "Epoch 434: Train Loss: 0.23877403140068054, Validation Loss: 2.698694944381714\n",
      "Epoch 435: Train Loss: 0.23899585902690887, Validation Loss: 2.598552703857422\n",
      "Epoch 436: Train Loss: 0.2837983012199402, Validation Loss: 2.478302478790283\n",
      "Epoch 437: Train Loss: 0.27782038748264315, Validation Loss: 2.4376773834228516\n",
      "Epoch 438: Train Loss: 0.2988519728183746, Validation Loss: 2.560636043548584\n",
      "Epoch 439: Train Loss: 0.257281631231308, Validation Loss: 2.5606067180633545\n",
      "Epoch 440: Train Loss: 0.39150779545307157, Validation Loss: 2.6015403270721436\n",
      "Epoch 441: Train Loss: 0.30429701805114745, Validation Loss: 2.568901538848877\n",
      "Epoch 442: Train Loss: 0.2468345731496811, Validation Loss: 2.4959042072296143\n",
      "Epoch 443: Train Loss: 0.27219362258911134, Validation Loss: 2.573021650314331\n",
      "Epoch 444: Train Loss: 0.2778137505054474, Validation Loss: 2.5921077728271484\n",
      "Epoch 445: Train Loss: 0.28275887072086336, Validation Loss: 2.6662979125976562\n",
      "Epoch 446: Train Loss: 0.2387725055217743, Validation Loss: 2.5336225032806396\n",
      "Epoch 447: Train Loss: 0.2590020179748535, Validation Loss: 2.5726404190063477\n",
      "Epoch 448: Train Loss: 0.30150369703769686, Validation Loss: 2.634260892868042\n",
      "Epoch 449: Train Loss: 0.22715083807706832, Validation Loss: 2.496377468109131\n",
      "Epoch 450: Train Loss: 0.29508586823940275, Validation Loss: 2.5575640201568604\n",
      "Epoch 451: Train Loss: 0.30092624127864837, Validation Loss: 2.6092121601104736\n",
      "Epoch 452: Train Loss: 0.2524659842252731, Validation Loss: 2.5275752544403076\n",
      "Epoch 453: Train Loss: 0.29752507209777834, Validation Loss: 2.373039722442627\n",
      "Epoch 454: Train Loss: 0.2756163001060486, Validation Loss: 2.348142147064209\n",
      "Epoch 455: Train Loss: 0.22393519431352615, Validation Loss: 2.4231269359588623\n",
      "Epoch 456: Train Loss: 0.24718374013900757, Validation Loss: 2.521014451980591\n",
      "Epoch 457: Train Loss: 0.40691377520561217, Validation Loss: 2.6202871799468994\n",
      "Epoch 458: Train Loss: 0.24746241867542268, Validation Loss: 2.6381492614746094\n",
      "Epoch 459: Train Loss: 0.29191798269748687, Validation Loss: 2.722041368484497\n",
      "Epoch 460: Train Loss: 0.24321011900901796, Validation Loss: 2.7328906059265137\n",
      "Epoch 461: Train Loss: 0.2899824231863022, Validation Loss: 2.7381367683410645\n",
      "Epoch 462: Train Loss: 0.3096027076244354, Validation Loss: 2.7381279468536377\n",
      "Epoch 463: Train Loss: 0.41304500699043273, Validation Loss: 2.7185065746307373\n",
      "Epoch 464: Train Loss: 0.20683725476264953, Validation Loss: 2.637143850326538\n",
      "Epoch 465: Train Loss: 0.28835950791835785, Validation Loss: 2.564020872116089\n",
      "Epoch 466: Train Loss: 0.35719567239284516, Validation Loss: 2.651181697845459\n",
      "Epoch 467: Train Loss: 0.20901707112789153, Validation Loss: 2.608804941177368\n",
      "Epoch 468: Train Loss: 0.23869494050741197, Validation Loss: 2.5711870193481445\n",
      "Epoch 469: Train Loss: 0.27636116147041323, Validation Loss: 2.5726325511932373\n",
      "Epoch 470: Train Loss: 0.21696716248989106, Validation Loss: 2.60367751121521\n",
      "Epoch 471: Train Loss: 0.24941335022449493, Validation Loss: 2.570796489715576\n",
      "Epoch 472: Train Loss: 0.2599209725856781, Validation Loss: 2.6422412395477295\n",
      "Epoch 473: Train Loss: 0.3007231026887894, Validation Loss: 2.6003377437591553\n",
      "Epoch 474: Train Loss: 0.2287570659071207, Validation Loss: 2.5623106956481934\n",
      "Epoch 475: Train Loss: 0.21952907145023345, Validation Loss: 2.622286558151245\n",
      "Epoch 476: Train Loss: 0.41662211418151857, Validation Loss: 2.7013537883758545\n",
      "Epoch 477: Train Loss: 0.2751900225877762, Validation Loss: 2.713116407394409\n",
      "Epoch 478: Train Loss: 0.23577668368816376, Validation Loss: 2.62258243560791\n",
      "Epoch 479: Train Loss: 0.28341309130191805, Validation Loss: 2.6766085624694824\n",
      "Epoch 480: Train Loss: 0.2598675101995468, Validation Loss: 2.702037811279297\n",
      "Epoch 481: Train Loss: 0.24378037750720977, Validation Loss: 2.6909587383270264\n",
      "Epoch 482: Train Loss: 0.2703752279281616, Validation Loss: 2.614542245864868\n",
      "Epoch 483: Train Loss: 0.2639886438846588, Validation Loss: 2.6819064617156982\n",
      "Epoch 484: Train Loss: 0.24663052707910538, Validation Loss: 2.7401905059814453\n",
      "Epoch 485: Train Loss: 0.23658990859985352, Validation Loss: 2.623655080795288\n",
      "Epoch 486: Train Loss: 0.3083504199981689, Validation Loss: 2.6889262199401855\n",
      "Epoch 487: Train Loss: 0.27258113622665403, Validation Loss: 2.660592794418335\n",
      "Epoch 488: Train Loss: 0.36153556406497955, Validation Loss: 2.7393898963928223\n",
      "Epoch 489: Train Loss: 0.33486237525939944, Validation Loss: 2.6475276947021484\n",
      "Epoch 490: Train Loss: 0.2169015735387802, Validation Loss: 2.4776530265808105\n",
      "Epoch 491: Train Loss: 0.24543238580226898, Validation Loss: 2.5273611545562744\n",
      "Epoch 492: Train Loss: 0.31856057047843933, Validation Loss: 2.551663875579834\n",
      "Epoch 493: Train Loss: 0.26047668755054476, Validation Loss: 2.5297513008117676\n",
      "Epoch 494: Train Loss: 0.22087860703468323, Validation Loss: 2.546935796737671\n",
      "Epoch 495: Train Loss: 0.2519080191850662, Validation Loss: 2.5986618995666504\n",
      "Epoch 496: Train Loss: 0.22049390524625778, Validation Loss: 2.554506301879883\n",
      "Epoch 497: Train Loss: 0.2500944554805756, Validation Loss: 2.5827512741088867\n",
      "Epoch 498: Train Loss: 0.276509752869606, Validation Loss: 2.4827682971954346\n",
      "Epoch 499: Train Loss: 0.2496593713760376, Validation Loss: 2.5183091163635254\n",
      "Fold 5 Metrics:\n",
      "Accuracy: 0.18181818181818182, Precision: 0.2, Recall: 0.16666666666666666, F1-score: 0.18181818181818182, AUC: 0.18333333333333332\n",
      "Confusion Matrix:\n",
      "[[1 4]\n",
      " [5 1]]\n",
      "Completed fold 5\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples from subject 6 to test set\n",
      "Adding 6 truth samples from subject 6 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 250)\n",
      "(132, 65, 250)\n",
      "Epoch 0: Train Loss: 0.7408512353897094, Validation Loss: 0.6938786506652832\n",
      "Epoch 1: Train Loss: 0.7909900546073914, Validation Loss: 0.693965494632721\n",
      "Epoch 2: Train Loss: 0.7126543641090393, Validation Loss: 0.693708062171936\n",
      "Epoch 3: Train Loss: 0.6928599596023559, Validation Loss: 0.6927449703216553\n",
      "Epoch 4: Train Loss: 0.7453648805618286, Validation Loss: 0.6910223960876465\n",
      "Epoch 5: Train Loss: 0.7865788102149963, Validation Loss: 0.6920132040977478\n",
      "Epoch 6: Train Loss: 0.7085007667541504, Validation Loss: 0.6932905912399292\n",
      "Epoch 7: Train Loss: 0.7276350378990173, Validation Loss: 0.6884249448776245\n",
      "Epoch 8: Train Loss: 0.7069622159004212, Validation Loss: 0.6882548332214355\n",
      "Epoch 9: Train Loss: 0.7191387176513672, Validation Loss: 0.6883134245872498\n",
      "Epoch 10: Train Loss: 0.73426833152771, Validation Loss: 0.6926717758178711\n",
      "Epoch 11: Train Loss: 0.7180504679679871, Validation Loss: 0.6949049234390259\n",
      "Epoch 12: Train Loss: 0.6524652957916259, Validation Loss: 0.6958107948303223\n",
      "Epoch 13: Train Loss: 0.6549372971057892, Validation Loss: 0.6940097212791443\n",
      "Epoch 14: Train Loss: 0.7046569585800171, Validation Loss: 0.6954374313354492\n",
      "Epoch 15: Train Loss: 0.671398150920868, Validation Loss: 0.6937348246574402\n",
      "Epoch 16: Train Loss: 0.7161842584609985, Validation Loss: 0.6939613819122314\n",
      "Epoch 17: Train Loss: 0.6747202277183533, Validation Loss: 0.6948550939559937\n",
      "Epoch 18: Train Loss: 0.7119295358657837, Validation Loss: 0.6972078084945679\n",
      "Epoch 19: Train Loss: 0.728084146976471, Validation Loss: 0.6966358423233032\n",
      "Epoch 20: Train Loss: 0.7099637389183044, Validation Loss: 0.6950963735580444\n",
      "Epoch 21: Train Loss: 0.739135468006134, Validation Loss: 0.6975429654121399\n",
      "Epoch 22: Train Loss: 0.6585035920143127, Validation Loss: 0.7028810977935791\n",
      "Epoch 23: Train Loss: 0.6867181420326233, Validation Loss: 0.7025987505912781\n",
      "Epoch 24: Train Loss: 0.6620293736457825, Validation Loss: 0.7103816270828247\n",
      "Epoch 25: Train Loss: 0.6274500250816345, Validation Loss: 0.7133365273475647\n",
      "Epoch 26: Train Loss: 0.5881417214870452, Validation Loss: 0.7169168591499329\n",
      "Epoch 27: Train Loss: 0.7236112475395202, Validation Loss: 0.7150838375091553\n",
      "Epoch 28: Train Loss: 0.6362810373306275, Validation Loss: 0.7149243354797363\n",
      "Epoch 29: Train Loss: 0.6850577235221863, Validation Loss: 0.7135499715805054\n",
      "Epoch 30: Train Loss: 0.6980033993721009, Validation Loss: 0.7168370485305786\n",
      "Epoch 31: Train Loss: 0.6187122702598572, Validation Loss: 0.7225735783576965\n",
      "Epoch 32: Train Loss: 0.6141768574714661, Validation Loss: 0.7257189750671387\n",
      "Epoch 33: Train Loss: 0.7232321619987487, Validation Loss: 0.7275761961936951\n",
      "Epoch 34: Train Loss: 0.67992182970047, Validation Loss: 0.7318965196609497\n",
      "Epoch 35: Train Loss: 0.6241421341896057, Validation Loss: 0.729773223400116\n",
      "Epoch 36: Train Loss: 0.6646498560905456, Validation Loss: 0.7354393005371094\n",
      "Epoch 37: Train Loss: 0.7208006858825684, Validation Loss: 0.7382581233978271\n",
      "Epoch 38: Train Loss: 0.7218161463737488, Validation Loss: 0.7408271431922913\n",
      "Epoch 39: Train Loss: 0.6711107492446899, Validation Loss: 0.741474449634552\n",
      "Epoch 40: Train Loss: 0.6966406226158142, Validation Loss: 0.7369203567504883\n",
      "Epoch 41: Train Loss: 0.6391910195350647, Validation Loss: 0.7355040907859802\n",
      "Epoch 42: Train Loss: 0.6497727274894715, Validation Loss: 0.7342364192008972\n",
      "Epoch 43: Train Loss: 0.6587207674980163, Validation Loss: 0.7314710021018982\n",
      "Epoch 44: Train Loss: 0.6030901789665222, Validation Loss: 0.7337625622749329\n",
      "Epoch 45: Train Loss: 0.6426407933235169, Validation Loss: 0.7366677522659302\n",
      "Epoch 46: Train Loss: 0.6487259864807129, Validation Loss: 0.7371038198471069\n",
      "Epoch 47: Train Loss: 0.6662760734558105, Validation Loss: 0.7343629598617554\n",
      "Epoch 48: Train Loss: 0.6228121042251586, Validation Loss: 0.734258770942688\n",
      "Epoch 49: Train Loss: 0.6455031633377075, Validation Loss: 0.7360213994979858\n",
      "Epoch 50: Train Loss: 0.6832094073295594, Validation Loss: 0.7389480471611023\n",
      "Epoch 51: Train Loss: 0.6069575428962708, Validation Loss: 0.7367313504219055\n",
      "Epoch 52: Train Loss: 0.6701899647712708, Validation Loss: 0.7339383363723755\n",
      "Epoch 53: Train Loss: 0.6255029678344727, Validation Loss: 0.7380198836326599\n",
      "Epoch 54: Train Loss: 0.6387423038482666, Validation Loss: 0.7375251054763794\n",
      "Epoch 55: Train Loss: 0.5686750054359436, Validation Loss: 0.737494945526123\n",
      "Epoch 56: Train Loss: 0.6447761178016662, Validation Loss: 0.7352609038352966\n",
      "Epoch 57: Train Loss: 0.6351017713546753, Validation Loss: 0.7347224950790405\n",
      "Epoch 58: Train Loss: 0.639877712726593, Validation Loss: 0.7386398315429688\n",
      "Epoch 59: Train Loss: 0.5838599562644958, Validation Loss: 0.7399722933769226\n",
      "Epoch 60: Train Loss: 0.6113363265991211, Validation Loss: 0.7337042689323425\n",
      "Epoch 61: Train Loss: 0.6335166096687317, Validation Loss: 0.7364009022712708\n",
      "Epoch 62: Train Loss: 0.6635896921157837, Validation Loss: 0.7380545735359192\n",
      "Epoch 63: Train Loss: 0.6245824813842773, Validation Loss: 0.730012834072113\n",
      "Epoch 64: Train Loss: 0.6729222655296325, Validation Loss: 0.7240334153175354\n",
      "Epoch 65: Train Loss: 0.6007555365562439, Validation Loss: 0.724931538105011\n",
      "Epoch 66: Train Loss: 0.625124704837799, Validation Loss: 0.72491055727005\n",
      "Epoch 67: Train Loss: 0.6235808730125427, Validation Loss: 0.7212616205215454\n",
      "Epoch 68: Train Loss: 0.6073399901390075, Validation Loss: 0.7222831845283508\n",
      "Epoch 69: Train Loss: 0.6454654693603515, Validation Loss: 0.7235915064811707\n",
      "Epoch 70: Train Loss: 0.5942303538322449, Validation Loss: 0.722533106803894\n",
      "Epoch 71: Train Loss: 0.6323922514915467, Validation Loss: 0.7251020073890686\n",
      "Epoch 72: Train Loss: 0.6135497570037842, Validation Loss: 0.7284377217292786\n",
      "Epoch 73: Train Loss: 0.6305732607841492, Validation Loss: 0.7266258597373962\n",
      "Epoch 74: Train Loss: 0.6464950323104859, Validation Loss: 0.7286229729652405\n",
      "Epoch 75: Train Loss: 0.5980071544647216, Validation Loss: 0.7326229214668274\n",
      "Epoch 76: Train Loss: 0.603459644317627, Validation Loss: 0.7324765920639038\n",
      "Epoch 77: Train Loss: 0.6091872155666351, Validation Loss: 0.7359679341316223\n",
      "Epoch 78: Train Loss: 0.5960597634315491, Validation Loss: 0.7310360074043274\n",
      "Epoch 79: Train Loss: 0.5983119010925293, Validation Loss: 0.733710765838623\n",
      "Epoch 80: Train Loss: 0.6045991778373718, Validation Loss: 0.7376189231872559\n",
      "Epoch 81: Train Loss: 0.6189979314804077, Validation Loss: 0.743464469909668\n",
      "Epoch 82: Train Loss: 0.6577194452285766, Validation Loss: 0.7419350743293762\n",
      "Epoch 83: Train Loss: 0.6380515456199646, Validation Loss: 0.7430197596549988\n",
      "Epoch 84: Train Loss: 0.6154901623725891, Validation Loss: 0.7473710775375366\n",
      "Epoch 85: Train Loss: 0.6395549535751343, Validation Loss: 0.7551438212394714\n",
      "Epoch 86: Train Loss: 0.5789949595928192, Validation Loss: 0.7536529302597046\n",
      "Epoch 87: Train Loss: 0.5619592428207397, Validation Loss: 0.7538514137268066\n",
      "Epoch 88: Train Loss: 0.6304592490196228, Validation Loss: 0.749014675617218\n",
      "Epoch 89: Train Loss: 0.5971875429153443, Validation Loss: 0.7512803077697754\n",
      "Epoch 90: Train Loss: 0.6169800400733948, Validation Loss: 0.7548723220825195\n",
      "Epoch 91: Train Loss: 0.6218358635902405, Validation Loss: 0.7496172785758972\n",
      "Epoch 92: Train Loss: 0.5664241135120391, Validation Loss: 0.7594043016433716\n",
      "Epoch 93: Train Loss: 0.58302121758461, Validation Loss: 0.7619420886039734\n",
      "Epoch 94: Train Loss: 0.5773085117340088, Validation Loss: 0.7627602815628052\n",
      "Epoch 95: Train Loss: 0.6048587322235107, Validation Loss: 0.7674874663352966\n",
      "Epoch 96: Train Loss: 0.6073703646659852, Validation Loss: 0.75592440366745\n",
      "Epoch 97: Train Loss: 0.6103003978729248, Validation Loss: 0.7616153359413147\n",
      "Epoch 98: Train Loss: 0.580600130558014, Validation Loss: 0.7617161870002747\n",
      "Epoch 99: Train Loss: 0.5952450513839722, Validation Loss: 0.761601448059082\n",
      "Epoch 100: Train Loss: 0.5699247479438782, Validation Loss: 0.7664439678192139\n",
      "Epoch 101: Train Loss: 0.59555504322052, Validation Loss: 0.7778993248939514\n",
      "Epoch 102: Train Loss: 0.5861079573631287, Validation Loss: 0.7864343523979187\n",
      "Epoch 103: Train Loss: 0.5498721957206726, Validation Loss: 0.7803869247436523\n",
      "Epoch 104: Train Loss: 0.5671300053596496, Validation Loss: 0.778220534324646\n",
      "Epoch 105: Train Loss: 0.6021079301834107, Validation Loss: 0.7806103825569153\n",
      "Epoch 106: Train Loss: 0.5492274641990662, Validation Loss: 0.7790093421936035\n",
      "Epoch 107: Train Loss: 0.5641431093215943, Validation Loss: 0.7815214395523071\n",
      "Epoch 108: Train Loss: 0.5522720098495484, Validation Loss: 0.7762253880500793\n",
      "Epoch 109: Train Loss: 0.5907386422157288, Validation Loss: 0.776474118232727\n",
      "Epoch 110: Train Loss: 0.5855916380882263, Validation Loss: 0.775963544845581\n",
      "Epoch 111: Train Loss: 0.5906983137130737, Validation Loss: 0.7760656476020813\n",
      "Epoch 112: Train Loss: 0.5773837566375732, Validation Loss: 0.7763616442680359\n",
      "Epoch 113: Train Loss: 0.5887503623962402, Validation Loss: 0.7839276790618896\n",
      "Epoch 114: Train Loss: 0.5351898074150085, Validation Loss: 0.7843259572982788\n",
      "Epoch 115: Train Loss: 0.5590552926063538, Validation Loss: 0.7784280776977539\n",
      "Epoch 116: Train Loss: 0.5652155101299285, Validation Loss: 0.7723878026008606\n",
      "Epoch 117: Train Loss: 0.5648232340812683, Validation Loss: 0.7725465893745422\n",
      "Epoch 118: Train Loss: 0.5742913246154785, Validation Loss: 0.7798299789428711\n",
      "Epoch 119: Train Loss: 0.5269826114177704, Validation Loss: 0.7850064635276794\n",
      "Epoch 120: Train Loss: 0.6173880457878113, Validation Loss: 0.7743002772331238\n",
      "Epoch 121: Train Loss: 0.5932359457015991, Validation Loss: 0.7656314969062805\n",
      "Epoch 122: Train Loss: 0.6119492650032043, Validation Loss: 0.7624492049217224\n",
      "Epoch 123: Train Loss: 0.5361069560050964, Validation Loss: 0.764889121055603\n",
      "Epoch 124: Train Loss: 0.6046088457107544, Validation Loss: 0.7694374918937683\n",
      "Epoch 125: Train Loss: 0.6073738217353821, Validation Loss: 0.7735860347747803\n",
      "Epoch 126: Train Loss: 0.581470274925232, Validation Loss: 0.768642008304596\n",
      "Epoch 127: Train Loss: 0.49769704341888427, Validation Loss: 0.7676756978034973\n",
      "Epoch 128: Train Loss: 0.5575668334960937, Validation Loss: 0.7659368515014648\n",
      "Epoch 129: Train Loss: 0.5352775275707244, Validation Loss: 0.7695159912109375\n",
      "Epoch 130: Train Loss: 0.565621292591095, Validation Loss: 0.7677916884422302\n",
      "Epoch 131: Train Loss: 0.5556941747665405, Validation Loss: 0.7715142369270325\n",
      "Epoch 132: Train Loss: 0.580479484796524, Validation Loss: 0.7767138481140137\n",
      "Epoch 133: Train Loss: 0.5183176815509796, Validation Loss: 0.7806805968284607\n",
      "Epoch 134: Train Loss: 0.5369665622711182, Validation Loss: 0.791926920413971\n",
      "Epoch 135: Train Loss: 0.5166965901851654, Validation Loss: 0.7952897548675537\n",
      "Epoch 136: Train Loss: 0.5290440142154693, Validation Loss: 0.8040922284126282\n",
      "Epoch 137: Train Loss: 0.5264484167099, Validation Loss: 0.8035445213317871\n",
      "Epoch 138: Train Loss: 0.5841910302639007, Validation Loss: 0.8045821785926819\n",
      "Epoch 139: Train Loss: 0.5073080956935883, Validation Loss: 0.8031619191169739\n",
      "Epoch 140: Train Loss: 0.5383755803108216, Validation Loss: 0.8099958300590515\n",
      "Epoch 141: Train Loss: 0.5227434635162354, Validation Loss: 0.8064720034599304\n",
      "Epoch 142: Train Loss: 0.5391631126403809, Validation Loss: 0.7979711294174194\n",
      "Epoch 143: Train Loss: 0.5712302803993226, Validation Loss: 0.7962709069252014\n",
      "Epoch 144: Train Loss: 0.5618220806121826, Validation Loss: 0.8003638386726379\n",
      "Epoch 145: Train Loss: 0.5432493448257446, Validation Loss: 0.804521918296814\n",
      "Epoch 146: Train Loss: 0.5603021681308746, Validation Loss: 0.7984457015991211\n",
      "Epoch 147: Train Loss: 0.5259638071060181, Validation Loss: 0.8028011322021484\n",
      "Epoch 148: Train Loss: 0.5698109030723572, Validation Loss: 0.7954443097114563\n",
      "Epoch 149: Train Loss: 0.563340163230896, Validation Loss: 0.795364499092102\n",
      "Epoch 150: Train Loss: 0.4986075282096863, Validation Loss: 0.8002078533172607\n",
      "Epoch 151: Train Loss: 0.5084454417228699, Validation Loss: 0.8010428547859192\n",
      "Epoch 152: Train Loss: 0.5340740740299225, Validation Loss: 0.8009658455848694\n",
      "Epoch 153: Train Loss: 0.5455956876277923, Validation Loss: 0.7943919897079468\n",
      "Epoch 154: Train Loss: 0.5507691919803619, Validation Loss: 0.8032208681106567\n",
      "Epoch 155: Train Loss: 0.5478356778621674, Validation Loss: 0.8081323504447937\n",
      "Epoch 156: Train Loss: 0.5552444398403168, Validation Loss: 0.8176101446151733\n",
      "Epoch 157: Train Loss: 0.5325278401374817, Validation Loss: 0.8089702725410461\n",
      "Epoch 158: Train Loss: 0.5615575790405274, Validation Loss: 0.8130735158920288\n",
      "Epoch 159: Train Loss: 0.5924189984798431, Validation Loss: 0.820099413394928\n",
      "Epoch 160: Train Loss: 0.5307417213916779, Validation Loss: 0.8420760631561279\n",
      "Epoch 161: Train Loss: 0.5016084611415863, Validation Loss: 0.8325355648994446\n",
      "Epoch 162: Train Loss: 0.5560289740562439, Validation Loss: 0.8276610970497131\n",
      "Epoch 163: Train Loss: 0.5022236168384552, Validation Loss: 0.8208775520324707\n",
      "Epoch 164: Train Loss: 0.5274920165538788, Validation Loss: 0.8321536779403687\n",
      "Epoch 165: Train Loss: 0.5087557077407837, Validation Loss: 0.8241409063339233\n",
      "Epoch 166: Train Loss: 0.4989636898040771, Validation Loss: 0.8337820172309875\n",
      "Epoch 167: Train Loss: 0.5403158247470856, Validation Loss: 0.8309146761894226\n",
      "Epoch 168: Train Loss: 0.4953655958175659, Validation Loss: 0.8313974142074585\n",
      "Epoch 169: Train Loss: 0.5824640691280365, Validation Loss: 0.8375563621520996\n",
      "Epoch 170: Train Loss: 0.5634766101837159, Validation Loss: 0.8314074873924255\n",
      "Epoch 171: Train Loss: 0.47210187315940855, Validation Loss: 0.8249627947807312\n",
      "Epoch 172: Train Loss: 0.5390224814414978, Validation Loss: 0.8199338912963867\n",
      "Epoch 173: Train Loss: 0.5088072001934052, Validation Loss: 0.8148255348205566\n",
      "Epoch 174: Train Loss: 0.4842370688915253, Validation Loss: 0.8142591714859009\n",
      "Epoch 175: Train Loss: 0.5172379851341248, Validation Loss: 0.8122672438621521\n",
      "Epoch 176: Train Loss: 0.5361344516277313, Validation Loss: 0.8216598629951477\n",
      "Epoch 177: Train Loss: 0.5110581040382385, Validation Loss: 0.820868968963623\n",
      "Epoch 178: Train Loss: 0.49183858633041383, Validation Loss: 0.8235720992088318\n",
      "Epoch 179: Train Loss: 0.5172409296035767, Validation Loss: 0.814930260181427\n",
      "Epoch 180: Train Loss: 0.5677848339080811, Validation Loss: 0.8330357670783997\n",
      "Epoch 181: Train Loss: 0.5139357924461365, Validation Loss: 0.8338738679885864\n",
      "Epoch 182: Train Loss: 0.5292861640453339, Validation Loss: 0.8309566378593445\n",
      "Epoch 183: Train Loss: 0.5727179229259491, Validation Loss: 0.8390624523162842\n",
      "Epoch 184: Train Loss: 0.49753684997558595, Validation Loss: 0.8454403281211853\n",
      "Epoch 185: Train Loss: 0.5123422861099243, Validation Loss: 0.8496599197387695\n",
      "Epoch 186: Train Loss: 0.534153413772583, Validation Loss: 0.8396091461181641\n",
      "Epoch 187: Train Loss: 0.491493946313858, Validation Loss: 0.8311800956726074\n",
      "Epoch 188: Train Loss: 0.5145256459712982, Validation Loss: 0.8343467116355896\n",
      "Epoch 189: Train Loss: 0.5278572380542755, Validation Loss: 0.8437015414237976\n",
      "Epoch 190: Train Loss: 0.4993912100791931, Validation Loss: 0.8401716351509094\n",
      "Epoch 191: Train Loss: 0.48578296303749086, Validation Loss: 0.8430726528167725\n",
      "Epoch 192: Train Loss: 0.461749005317688, Validation Loss: 0.8391332626342773\n",
      "Epoch 193: Train Loss: 0.4493710160255432, Validation Loss: 0.837347149848938\n",
      "Epoch 194: Train Loss: 0.5155168950557709, Validation Loss: 0.8546420335769653\n",
      "Epoch 195: Train Loss: 0.4520548164844513, Validation Loss: 0.8462973833084106\n",
      "Epoch 196: Train Loss: 0.6065200805664063, Validation Loss: 0.8466857075691223\n",
      "Epoch 197: Train Loss: 0.5037579536437988, Validation Loss: 0.8560975790023804\n",
      "Epoch 198: Train Loss: 0.4784453272819519, Validation Loss: 0.8599244356155396\n",
      "Epoch 199: Train Loss: 0.46752004623413085, Validation Loss: 0.8615677952766418\n",
      "Epoch 200: Train Loss: 0.512488067150116, Validation Loss: 0.8575753569602966\n",
      "Epoch 201: Train Loss: 0.5201058387756348, Validation Loss: 0.8755490779876709\n",
      "Epoch 202: Train Loss: 0.4663925886154175, Validation Loss: 0.8875802159309387\n",
      "Epoch 203: Train Loss: 0.45153277516365053, Validation Loss: 0.8896562457084656\n",
      "Epoch 204: Train Loss: 0.4981038749217987, Validation Loss: 0.8920137286186218\n",
      "Epoch 205: Train Loss: 0.49949645400047304, Validation Loss: 0.8929929733276367\n",
      "Epoch 206: Train Loss: 0.5269577383995057, Validation Loss: 0.8846703767776489\n",
      "Epoch 207: Train Loss: 0.45125861167907716, Validation Loss: 0.8743486404418945\n",
      "Epoch 208: Train Loss: 0.45446675419807436, Validation Loss: 0.8748242855072021\n",
      "Epoch 209: Train Loss: 0.7161103308200836, Validation Loss: 0.8801769614219666\n",
      "Epoch 210: Train Loss: 0.45657168626785277, Validation Loss: 0.8615367412567139\n",
      "Epoch 211: Train Loss: 0.4564878880977631, Validation Loss: 0.8682617545127869\n",
      "Epoch 212: Train Loss: 0.4390533983707428, Validation Loss: 0.8606519103050232\n",
      "Epoch 213: Train Loss: 0.5419434428215026, Validation Loss: 0.8562828302383423\n",
      "Epoch 214: Train Loss: 0.4791311681270599, Validation Loss: 0.8626466989517212\n",
      "Epoch 215: Train Loss: 0.48397589325904844, Validation Loss: 0.8767083287239075\n",
      "Epoch 216: Train Loss: 0.47556808590888977, Validation Loss: 0.8604327440261841\n",
      "Epoch 217: Train Loss: 0.42361109852790835, Validation Loss: 0.8570775985717773\n",
      "Epoch 218: Train Loss: 0.4657454967498779, Validation Loss: 0.8554988503456116\n",
      "Epoch 219: Train Loss: 0.569760537147522, Validation Loss: 0.8667840361595154\n",
      "Epoch 220: Train Loss: 0.48979464173316956, Validation Loss: 0.8735082745552063\n",
      "Epoch 221: Train Loss: 0.4412975311279297, Validation Loss: 0.866181492805481\n",
      "Epoch 222: Train Loss: 0.4481673359870911, Validation Loss: 0.8580783009529114\n",
      "Epoch 223: Train Loss: 0.5364223301410675, Validation Loss: 0.8612380623817444\n",
      "Epoch 224: Train Loss: 0.49238385558128356, Validation Loss: 0.8640471696853638\n",
      "Epoch 225: Train Loss: 0.4539212226867676, Validation Loss: 0.8583923578262329\n",
      "Epoch 226: Train Loss: 0.44242467880249026, Validation Loss: 0.863551914691925\n",
      "Epoch 227: Train Loss: 0.4875787138938904, Validation Loss: 0.859929084777832\n",
      "Epoch 228: Train Loss: 0.46465708017349244, Validation Loss: 0.8611404299736023\n",
      "Epoch 229: Train Loss: 0.4783462405204773, Validation Loss: 0.8703758120536804\n",
      "Epoch 230: Train Loss: 0.4549429178237915, Validation Loss: 0.8903972506523132\n",
      "Epoch 231: Train Loss: 0.5052826642990113, Validation Loss: 0.8851640224456787\n",
      "Epoch 232: Train Loss: 0.4341883957386017, Validation Loss: 0.8774735927581787\n",
      "Epoch 233: Train Loss: 0.5094688534736633, Validation Loss: 0.8723925948143005\n",
      "Epoch 234: Train Loss: 0.4403402626514435, Validation Loss: 0.8964697122573853\n",
      "Epoch 235: Train Loss: 0.4442528784275055, Validation Loss: 0.888288140296936\n",
      "Epoch 236: Train Loss: 0.4829174816608429, Validation Loss: 0.9038426876068115\n",
      "Epoch 237: Train Loss: 0.4472545921802521, Validation Loss: 0.9080379605293274\n",
      "Epoch 238: Train Loss: 0.45896435976028443, Validation Loss: 0.9114786386489868\n",
      "Epoch 239: Train Loss: 0.46945286393165586, Validation Loss: 0.9172194004058838\n",
      "Epoch 240: Train Loss: 0.42145723700523374, Validation Loss: 0.8974790573120117\n",
      "Epoch 241: Train Loss: 0.459852397441864, Validation Loss: 0.8832949995994568\n",
      "Epoch 242: Train Loss: 0.48135672211647035, Validation Loss: 0.8816247582435608\n",
      "Epoch 243: Train Loss: 0.5077835619449615, Validation Loss: 0.8956369757652283\n",
      "Epoch 244: Train Loss: 0.4392451703548431, Validation Loss: 0.9029468894004822\n",
      "Epoch 245: Train Loss: 0.42894303798675537, Validation Loss: 0.8892604112625122\n",
      "Epoch 246: Train Loss: 0.49530682563781736, Validation Loss: 0.8854331374168396\n",
      "Epoch 247: Train Loss: 0.4794838011264801, Validation Loss: 0.8867595195770264\n",
      "Epoch 248: Train Loss: 0.4175994634628296, Validation Loss: 0.8914440274238586\n",
      "Epoch 249: Train Loss: 0.47396573424339294, Validation Loss: 0.8764281868934631\n",
      "Epoch 250: Train Loss: 0.4712716281414032, Validation Loss: 0.9028985500335693\n",
      "Epoch 251: Train Loss: 0.45382060408592223, Validation Loss: 0.903870701789856\n",
      "Epoch 252: Train Loss: 0.4473807990550995, Validation Loss: 0.9252191185951233\n",
      "Epoch 253: Train Loss: 0.4283431887626648, Validation Loss: 0.9327975511550903\n",
      "Epoch 254: Train Loss: 0.4261963188648224, Validation Loss: 0.909917414188385\n",
      "Epoch 255: Train Loss: 0.40407615900039673, Validation Loss: 0.9213945865631104\n",
      "Epoch 256: Train Loss: 0.4231166929006577, Validation Loss: 0.928763210773468\n",
      "Epoch 257: Train Loss: 0.47013981342315675, Validation Loss: 0.9119327068328857\n",
      "Epoch 258: Train Loss: 0.4373736083507538, Validation Loss: 0.9145632982254028\n",
      "Epoch 259: Train Loss: 0.41965746879577637, Validation Loss: 0.9228177666664124\n",
      "Epoch 260: Train Loss: 0.5568627119064331, Validation Loss: 0.9152507185935974\n",
      "Epoch 261: Train Loss: 0.39183721542358396, Validation Loss: 0.8902090191841125\n",
      "Epoch 262: Train Loss: 0.4343079626560211, Validation Loss: 0.8928254842758179\n",
      "Epoch 263: Train Loss: 0.4672341287136078, Validation Loss: 0.8920470476150513\n",
      "Epoch 264: Train Loss: 0.376658296585083, Validation Loss: 0.9008525609970093\n",
      "Epoch 265: Train Loss: 0.45137829780578614, Validation Loss: 0.910491406917572\n",
      "Epoch 266: Train Loss: 0.47418941259384156, Validation Loss: 0.9212196469306946\n",
      "Epoch 267: Train Loss: 0.41209822297096255, Validation Loss: 0.9281022548675537\n",
      "Epoch 268: Train Loss: 0.4332258403301239, Validation Loss: 0.9325288534164429\n",
      "Epoch 269: Train Loss: 0.4381074905395508, Validation Loss: 0.9203423857688904\n",
      "Epoch 270: Train Loss: 0.43265002965927124, Validation Loss: 0.9521442651748657\n",
      "Epoch 271: Train Loss: 0.4379830837249756, Validation Loss: 0.9484556317329407\n",
      "Epoch 272: Train Loss: 0.445436155796051, Validation Loss: 0.9479544162750244\n",
      "Epoch 273: Train Loss: 0.5086329996585846, Validation Loss: 0.9370381832122803\n",
      "Epoch 274: Train Loss: 0.4946558833122253, Validation Loss: 0.9413921236991882\n",
      "Epoch 275: Train Loss: 0.4555471658706665, Validation Loss: 0.9370401501655579\n",
      "Epoch 276: Train Loss: 0.40934569835662843, Validation Loss: 0.9359475374221802\n",
      "Epoch 277: Train Loss: 0.43089415431022643, Validation Loss: 0.9346468448638916\n",
      "Epoch 278: Train Loss: 0.4454615652561188, Validation Loss: 0.91864013671875\n",
      "Epoch 279: Train Loss: 0.417877072095871, Validation Loss: 0.9294149875640869\n",
      "Epoch 280: Train Loss: 0.4716191291809082, Validation Loss: 0.9396140575408936\n",
      "Epoch 281: Train Loss: 0.4166912019252777, Validation Loss: 0.9521676898002625\n",
      "Epoch 282: Train Loss: 0.5254863023757934, Validation Loss: 0.9393289089202881\n",
      "Epoch 283: Train Loss: 0.39542382657527925, Validation Loss: 0.9153367280960083\n",
      "Epoch 284: Train Loss: 0.5482920944690705, Validation Loss: 0.9034585952758789\n",
      "Epoch 285: Train Loss: 0.43840742111206055, Validation Loss: 0.9258843660354614\n",
      "Epoch 286: Train Loss: 0.38602367639541624, Validation Loss: 0.9264315962791443\n",
      "Epoch 287: Train Loss: 0.37230598032474516, Validation Loss: 0.9495296478271484\n",
      "Epoch 288: Train Loss: 0.3925868809223175, Validation Loss: 0.9218486547470093\n",
      "Epoch 289: Train Loss: 0.38601829409599303, Validation Loss: 0.9406416416168213\n",
      "Epoch 290: Train Loss: 0.4342593789100647, Validation Loss: 0.9335778951644897\n",
      "Epoch 291: Train Loss: 0.4336238741874695, Validation Loss: 0.9517390727996826\n",
      "Epoch 292: Train Loss: 0.44335312247276304, Validation Loss: 0.9670474529266357\n",
      "Epoch 293: Train Loss: 0.4506402492523193, Validation Loss: 0.9760494232177734\n",
      "Epoch 294: Train Loss: 0.37327846586704255, Validation Loss: 0.9633734226226807\n",
      "Epoch 295: Train Loss: 0.4171180069446564, Validation Loss: 0.9877386093139648\n",
      "Epoch 296: Train Loss: 0.4004308819770813, Validation Loss: 0.9971359372138977\n",
      "Epoch 297: Train Loss: 0.507872074842453, Validation Loss: 0.9878913760185242\n",
      "Epoch 298: Train Loss: 0.4554709196090698, Validation Loss: 0.9649177193641663\n",
      "Epoch 299: Train Loss: 0.4212010085582733, Validation Loss: 0.9686488509178162\n",
      "Epoch 300: Train Loss: 0.4499504566192627, Validation Loss: 0.9505426287651062\n",
      "Epoch 301: Train Loss: 0.4206986963748932, Validation Loss: 0.9305986762046814\n",
      "Epoch 302: Train Loss: 0.37730035483837127, Validation Loss: 0.9458490014076233\n",
      "Epoch 303: Train Loss: 0.44243172407150266, Validation Loss: 0.9619964957237244\n",
      "Epoch 304: Train Loss: 0.47820993065834044, Validation Loss: 0.9788126349449158\n",
      "Epoch 305: Train Loss: 0.4357177376747131, Validation Loss: 0.9895566701889038\n",
      "Epoch 306: Train Loss: 0.4152876853942871, Validation Loss: 0.998818039894104\n",
      "Epoch 307: Train Loss: 0.4065609872341156, Validation Loss: 0.9829342365264893\n",
      "Epoch 308: Train Loss: 0.37546897530555723, Validation Loss: 0.9803538918495178\n",
      "Epoch 309: Train Loss: 0.35663957595825196, Validation Loss: 0.9840506911277771\n",
      "Epoch 310: Train Loss: 0.482648640871048, Validation Loss: 0.9766889810562134\n",
      "Epoch 311: Train Loss: 0.5022154211997986, Validation Loss: 0.9798644781112671\n",
      "Epoch 312: Train Loss: 0.40207828879356383, Validation Loss: 0.988325297832489\n",
      "Epoch 313: Train Loss: 0.3639658272266388, Validation Loss: 0.9819789528846741\n",
      "Epoch 314: Train Loss: 0.3791337847709656, Validation Loss: 0.9738077521324158\n",
      "Epoch 315: Train Loss: 0.44994903206825254, Validation Loss: 0.9682016372680664\n",
      "Epoch 316: Train Loss: 0.40129217505455017, Validation Loss: 0.9791105389595032\n",
      "Epoch 317: Train Loss: 0.38679133653640746, Validation Loss: 0.9829308986663818\n",
      "Epoch 318: Train Loss: 0.3597238928079605, Validation Loss: 0.992555558681488\n",
      "Epoch 319: Train Loss: 0.36020949482917786, Validation Loss: 0.9833978414535522\n",
      "Epoch 320: Train Loss: 0.35224262475967405, Validation Loss: 1.0007529258728027\n",
      "Epoch 321: Train Loss: 0.4181062340736389, Validation Loss: 0.993931233882904\n",
      "Epoch 322: Train Loss: 0.4058374047279358, Validation Loss: 1.0078577995300293\n",
      "Epoch 323: Train Loss: 0.4032219171524048, Validation Loss: 0.9966418743133545\n",
      "Epoch 324: Train Loss: 0.4074756920337677, Validation Loss: 1.0123339891433716\n",
      "Epoch 325: Train Loss: 0.39721145629882815, Validation Loss: 0.9835008382797241\n",
      "Epoch 326: Train Loss: 0.39958255290985106, Validation Loss: 0.993450403213501\n",
      "Epoch 327: Train Loss: 0.37994964718818663, Validation Loss: 0.9784063696861267\n",
      "Epoch 328: Train Loss: 0.4421439111232758, Validation Loss: 0.9816386699676514\n",
      "Epoch 329: Train Loss: 0.3785382568836212, Validation Loss: 1.0015442371368408\n",
      "Epoch 330: Train Loss: 0.3749996185302734, Validation Loss: 1.027543544769287\n",
      "Epoch 331: Train Loss: 0.4861822843551636, Validation Loss: 1.04824697971344\n",
      "Epoch 332: Train Loss: 0.3872641086578369, Validation Loss: 1.0096163749694824\n",
      "Epoch 333: Train Loss: 0.32065201699733736, Validation Loss: 1.01258385181427\n",
      "Epoch 334: Train Loss: 0.3490747660398483, Validation Loss: 1.0149118900299072\n",
      "Epoch 335: Train Loss: 0.4425514996051788, Validation Loss: 1.0177274942398071\n",
      "Epoch 336: Train Loss: 0.3962034940719604, Validation Loss: 1.0051182508468628\n",
      "Epoch 337: Train Loss: 0.3808533728122711, Validation Loss: 0.9879976511001587\n",
      "Epoch 338: Train Loss: 0.3416903167963028, Validation Loss: 0.997497022151947\n",
      "Epoch 339: Train Loss: 0.35625865757465364, Validation Loss: 0.9612497687339783\n",
      "Epoch 340: Train Loss: 0.3529300570487976, Validation Loss: 1.0019664764404297\n",
      "Epoch 341: Train Loss: 0.34665267765522, Validation Loss: 1.0134869813919067\n",
      "Epoch 342: Train Loss: 0.41058582067489624, Validation Loss: 1.012695074081421\n",
      "Epoch 343: Train Loss: 0.4430135846138, Validation Loss: 1.0007035732269287\n",
      "Epoch 344: Train Loss: 0.40866516828536986, Validation Loss: 0.9988307356834412\n",
      "Epoch 345: Train Loss: 0.41058477759361267, Validation Loss: 0.9763163924217224\n",
      "Epoch 346: Train Loss: 0.4093062937259674, Validation Loss: 0.9647454619407654\n",
      "Epoch 347: Train Loss: 0.44855008721351625, Validation Loss: 0.9772361516952515\n",
      "Epoch 348: Train Loss: 0.3765790700912476, Validation Loss: 0.99276202917099\n",
      "Epoch 349: Train Loss: 0.3640242040157318, Validation Loss: 0.9997492432594299\n",
      "Epoch 350: Train Loss: 0.37014214396476747, Validation Loss: 0.9908603429794312\n",
      "Epoch 351: Train Loss: 0.4016261756420135, Validation Loss: 1.0050188302993774\n",
      "Epoch 352: Train Loss: 0.32115737050771714, Validation Loss: 0.9886207580566406\n",
      "Epoch 353: Train Loss: 0.43455045223236083, Validation Loss: 0.9847719669342041\n",
      "Epoch 354: Train Loss: 0.3609577476978302, Validation Loss: 1.0078003406524658\n",
      "Epoch 355: Train Loss: 0.37144623398780824, Validation Loss: 1.0429434776306152\n",
      "Epoch 356: Train Loss: 0.35175979137420654, Validation Loss: 1.054807186126709\n",
      "Epoch 357: Train Loss: 0.36014100909233093, Validation Loss: 1.0455560684204102\n",
      "Epoch 358: Train Loss: 0.3638236284255981, Validation Loss: 1.0426020622253418\n",
      "Epoch 359: Train Loss: 0.4302006781101227, Validation Loss: 1.049539566040039\n",
      "Epoch 360: Train Loss: 0.3795927405357361, Validation Loss: 1.0500298738479614\n",
      "Epoch 361: Train Loss: 0.32930341064929963, Validation Loss: 1.0457934141159058\n",
      "Epoch 362: Train Loss: 0.36459546685218813, Validation Loss: 1.0302424430847168\n",
      "Epoch 363: Train Loss: 0.33997468948364257, Validation Loss: 1.0588101148605347\n",
      "Epoch 364: Train Loss: 0.37123662829399107, Validation Loss: 1.0819756984710693\n",
      "Epoch 365: Train Loss: 0.3985304653644562, Validation Loss: 1.040266513824463\n",
      "Epoch 366: Train Loss: 0.34457472562789915, Validation Loss: 1.0488287210464478\n",
      "Epoch 367: Train Loss: 0.3498896241188049, Validation Loss: 1.0483120679855347\n",
      "Epoch 368: Train Loss: 0.3520986378192902, Validation Loss: 1.0551979541778564\n",
      "Epoch 369: Train Loss: 0.3167413711547852, Validation Loss: 1.041275978088379\n",
      "Epoch 370: Train Loss: 0.3250357761979103, Validation Loss: 1.0739110708236694\n",
      "Epoch 371: Train Loss: 0.4384190320968628, Validation Loss: 1.0594007968902588\n",
      "Epoch 372: Train Loss: 0.38237274885177613, Validation Loss: 1.0662869215011597\n",
      "Epoch 373: Train Loss: 0.3462697625160217, Validation Loss: 1.063596487045288\n",
      "Epoch 374: Train Loss: 0.35005351305007937, Validation Loss: 1.0832549333572388\n",
      "Epoch 375: Train Loss: 0.4111586570739746, Validation Loss: 1.0911160707473755\n",
      "Epoch 376: Train Loss: 0.3361883223056793, Validation Loss: 1.0957716703414917\n",
      "Epoch 377: Train Loss: 0.32117181420326235, Validation Loss: 1.099278211593628\n",
      "Epoch 378: Train Loss: 0.3599755585193634, Validation Loss: 1.1071622371673584\n",
      "Epoch 379: Train Loss: 0.3919323146343231, Validation Loss: 1.0932847261428833\n",
      "Epoch 380: Train Loss: 0.463528448343277, Validation Loss: 1.071617603302002\n",
      "Epoch 381: Train Loss: 0.34370180368423464, Validation Loss: 1.0637688636779785\n",
      "Epoch 382: Train Loss: 0.3630209624767303, Validation Loss: 1.09547758102417\n",
      "Epoch 383: Train Loss: 0.3565187990665436, Validation Loss: 1.108752727508545\n",
      "Epoch 384: Train Loss: 0.3924078106880188, Validation Loss: 1.092942714691162\n",
      "Epoch 385: Train Loss: 0.3555528402328491, Validation Loss: 1.0962263345718384\n",
      "Epoch 386: Train Loss: 0.3941202998161316, Validation Loss: 1.0884816646575928\n",
      "Epoch 387: Train Loss: 0.3126054108142853, Validation Loss: 1.1070315837860107\n",
      "Epoch 388: Train Loss: 0.35991805195808413, Validation Loss: 1.1263813972473145\n",
      "Epoch 389: Train Loss: 0.32306470572948454, Validation Loss: 1.1390622854232788\n",
      "Epoch 390: Train Loss: 0.3404252231121063, Validation Loss: 1.1311553716659546\n",
      "Epoch 391: Train Loss: 0.33653790652751925, Validation Loss: 1.1107113361358643\n",
      "Epoch 392: Train Loss: 0.3695801913738251, Validation Loss: 1.0960687398910522\n",
      "Epoch 393: Train Loss: 0.3714149296283722, Validation Loss: 1.070401906967163\n",
      "Epoch 394: Train Loss: 0.3691140949726105, Validation Loss: 1.072166919708252\n",
      "Epoch 395: Train Loss: 0.3170390039682388, Validation Loss: 1.0741513967514038\n",
      "Epoch 396: Train Loss: 0.3374190151691437, Validation Loss: 1.064442753791809\n",
      "Epoch 397: Train Loss: 0.32396422028541566, Validation Loss: 1.072512149810791\n",
      "Epoch 398: Train Loss: 0.33867045044898986, Validation Loss: 1.059911847114563\n",
      "Epoch 399: Train Loss: 0.3429389834403992, Validation Loss: 1.088697910308838\n",
      "Epoch 400: Train Loss: 0.37712287306785586, Validation Loss: 1.1100598573684692\n",
      "Epoch 401: Train Loss: 0.3852819621562958, Validation Loss: 1.1407614946365356\n",
      "Epoch 402: Train Loss: 0.34717024564743043, Validation Loss: 1.1622034311294556\n",
      "Epoch 403: Train Loss: 0.31213618218898775, Validation Loss: 1.1407369375228882\n",
      "Epoch 404: Train Loss: 0.4264479160308838, Validation Loss: 1.1518861055374146\n",
      "Epoch 405: Train Loss: 0.38341537714004514, Validation Loss: 1.1665709018707275\n",
      "Epoch 406: Train Loss: 0.33255906105041505, Validation Loss: 1.1632648706436157\n",
      "Epoch 407: Train Loss: 0.40249075889587405, Validation Loss: 1.157302975654602\n",
      "Epoch 408: Train Loss: 0.4074324667453766, Validation Loss: 1.15137779712677\n",
      "Epoch 409: Train Loss: 0.2990871071815491, Validation Loss: 1.1582565307617188\n",
      "Epoch 410: Train Loss: 0.3751397728919983, Validation Loss: 1.1546695232391357\n",
      "Epoch 411: Train Loss: 0.30137611031532285, Validation Loss: 1.154415488243103\n",
      "Epoch 412: Train Loss: 0.35949366092681884, Validation Loss: 1.1670140027999878\n",
      "Epoch 413: Train Loss: 0.35501866936683657, Validation Loss: 1.1774461269378662\n",
      "Epoch 414: Train Loss: 0.3747047007083893, Validation Loss: 1.1739739179611206\n",
      "Epoch 415: Train Loss: 0.3789623141288757, Validation Loss: 1.1972342729568481\n",
      "Epoch 416: Train Loss: 0.3398197054862976, Validation Loss: 1.1875685453414917\n",
      "Epoch 417: Train Loss: 0.357112455368042, Validation Loss: 1.1971760988235474\n",
      "Epoch 418: Train Loss: 0.5284181654453277, Validation Loss: 1.1948941946029663\n",
      "Epoch 419: Train Loss: 0.3633086264133453, Validation Loss: 1.20589280128479\n",
      "Epoch 420: Train Loss: 0.3027042329311371, Validation Loss: 1.1799967288970947\n",
      "Epoch 421: Train Loss: 0.4422225534915924, Validation Loss: 1.1697124242782593\n",
      "Epoch 422: Train Loss: 0.333874836564064, Validation Loss: 1.148913860321045\n",
      "Epoch 423: Train Loss: 0.33346809148788453, Validation Loss: 1.1341599225997925\n",
      "Epoch 424: Train Loss: 0.28097891956567767, Validation Loss: 1.1145297288894653\n",
      "Epoch 425: Train Loss: 0.28915040493011473, Validation Loss: 1.1400508880615234\n",
      "Epoch 426: Train Loss: 0.3113304406404495, Validation Loss: 1.1261866092681885\n",
      "Epoch 427: Train Loss: 0.4645207583904266, Validation Loss: 1.1245810985565186\n",
      "Epoch 428: Train Loss: 0.41938539147377013, Validation Loss: 1.1267611980438232\n",
      "Epoch 429: Train Loss: 0.30655552446842194, Validation Loss: 1.1453824043273926\n",
      "Epoch 430: Train Loss: 0.2852004081010818, Validation Loss: 1.1894770860671997\n",
      "Epoch 431: Train Loss: 0.2998058915138245, Validation Loss: 1.175587773323059\n",
      "Epoch 432: Train Loss: 0.33119002282619475, Validation Loss: 1.1660996675491333\n",
      "Epoch 433: Train Loss: 0.31817454397678374, Validation Loss: 1.1875160932540894\n",
      "Epoch 434: Train Loss: 0.363044998049736, Validation Loss: 1.161969780921936\n",
      "Epoch 435: Train Loss: 0.3675422966480255, Validation Loss: 1.1823722124099731\n",
      "Epoch 436: Train Loss: 0.30934057533741, Validation Loss: 1.188031554222107\n",
      "Epoch 437: Train Loss: 0.3574018329381943, Validation Loss: 1.2067807912826538\n",
      "Epoch 438: Train Loss: 0.2825104981660843, Validation Loss: 1.1911252737045288\n",
      "Epoch 439: Train Loss: 0.29317121505737304, Validation Loss: 1.1996310949325562\n",
      "Epoch 440: Train Loss: 0.29526602327823637, Validation Loss: 1.2241144180297852\n",
      "Epoch 441: Train Loss: 0.3383458435535431, Validation Loss: 1.1997520923614502\n",
      "Epoch 442: Train Loss: 0.38436196744441986, Validation Loss: 1.1800000667572021\n",
      "Epoch 443: Train Loss: 0.41807891726493834, Validation Loss: 1.1803268194198608\n",
      "Epoch 444: Train Loss: 0.39184610843658446, Validation Loss: 1.1730728149414062\n",
      "Epoch 445: Train Loss: 0.38289187252521517, Validation Loss: 1.143813967704773\n",
      "Epoch 446: Train Loss: 0.40579673647880554, Validation Loss: 1.1277563571929932\n",
      "Epoch 447: Train Loss: 0.27667681872844696, Validation Loss: 1.122934341430664\n",
      "Epoch 448: Train Loss: 0.3226769953966141, Validation Loss: 1.128689169883728\n",
      "Epoch 449: Train Loss: 0.34131149053573606, Validation Loss: 1.1579896211624146\n",
      "Epoch 450: Train Loss: 0.28688467144966123, Validation Loss: 1.1833336353302002\n",
      "Epoch 451: Train Loss: 0.33979383707046507, Validation Loss: 1.1616950035095215\n",
      "Epoch 452: Train Loss: 0.27744138836860655, Validation Loss: 1.1941145658493042\n",
      "Epoch 453: Train Loss: 0.3109194815158844, Validation Loss: 1.1898025274276733\n",
      "Epoch 454: Train Loss: 0.40015741586685183, Validation Loss: 1.1622252464294434\n",
      "Epoch 455: Train Loss: 0.3602802097797394, Validation Loss: 1.1362624168395996\n",
      "Epoch 456: Train Loss: 0.352344012260437, Validation Loss: 1.139984369277954\n",
      "Epoch 457: Train Loss: 0.3848878890275955, Validation Loss: 1.1784073114395142\n",
      "Epoch 458: Train Loss: 0.3228751480579376, Validation Loss: 1.1366550922393799\n",
      "Epoch 459: Train Loss: 0.29154735505580903, Validation Loss: 1.1510707139968872\n",
      "Epoch 460: Train Loss: 0.29035410583019255, Validation Loss: 1.1502596139907837\n",
      "Epoch 461: Train Loss: 0.3176903486251831, Validation Loss: 1.173102617263794\n",
      "Epoch 462: Train Loss: 0.3099536418914795, Validation Loss: 1.1776251792907715\n",
      "Epoch 463: Train Loss: 0.40454995036125185, Validation Loss: 1.1768851280212402\n",
      "Epoch 464: Train Loss: 0.3518274545669556, Validation Loss: 1.1656866073608398\n",
      "Epoch 465: Train Loss: 0.2840616077184677, Validation Loss: 1.1693816184997559\n",
      "Epoch 466: Train Loss: 0.39835495352745054, Validation Loss: 1.1852126121520996\n",
      "Epoch 467: Train Loss: 0.3545114636421204, Validation Loss: 1.1544605493545532\n",
      "Epoch 468: Train Loss: 0.31155866384506226, Validation Loss: 1.1568716764450073\n",
      "Epoch 469: Train Loss: 0.31219848394393923, Validation Loss: 1.1358007192611694\n",
      "Epoch 470: Train Loss: 0.3108707368373871, Validation Loss: 1.1505070924758911\n",
      "Epoch 471: Train Loss: 0.2959446310997009, Validation Loss: 1.1448147296905518\n",
      "Epoch 472: Train Loss: 0.3671816289424896, Validation Loss: 1.1480698585510254\n",
      "Epoch 473: Train Loss: 0.2610552966594696, Validation Loss: 1.178731083869934\n",
      "Epoch 474: Train Loss: 0.3738776445388794, Validation Loss: 1.1877509355545044\n",
      "Epoch 475: Train Loss: 0.2981876850128174, Validation Loss: 1.172999382019043\n",
      "Epoch 476: Train Loss: 0.36065969467163084, Validation Loss: 1.1958898305892944\n",
      "Epoch 477: Train Loss: 0.2817862331867218, Validation Loss: 1.2094602584838867\n",
      "Epoch 478: Train Loss: 0.31063615083694457, Validation Loss: 1.172756314277649\n",
      "Epoch 479: Train Loss: 0.3021702170372009, Validation Loss: 1.2089024782180786\n",
      "Epoch 480: Train Loss: 0.2571337580680847, Validation Loss: 1.2052686214447021\n",
      "Epoch 481: Train Loss: 0.26769989579916, Validation Loss: 1.1975317001342773\n",
      "Epoch 482: Train Loss: 0.2671249508857727, Validation Loss: 1.2028244733810425\n",
      "Epoch 483: Train Loss: 0.28609524965286254, Validation Loss: 1.1925612688064575\n",
      "Epoch 484: Train Loss: 0.37161638736724856, Validation Loss: 1.2191493511199951\n",
      "Epoch 485: Train Loss: 0.29050427973270415, Validation Loss: 1.2263894081115723\n",
      "Epoch 486: Train Loss: 0.33541795015335085, Validation Loss: 1.2270909547805786\n",
      "Epoch 487: Train Loss: 0.3124690651893616, Validation Loss: 1.2275935411453247\n",
      "Epoch 488: Train Loss: 0.2734876751899719, Validation Loss: 1.252148151397705\n",
      "Epoch 489: Train Loss: 0.27070175409317015, Validation Loss: 1.2422034740447998\n",
      "Epoch 490: Train Loss: 0.3249298334121704, Validation Loss: 1.2346301078796387\n",
      "Epoch 491: Train Loss: 0.2647227972745895, Validation Loss: 1.2068415880203247\n",
      "Epoch 492: Train Loss: 0.24410177692770957, Validation Loss: 1.2027456760406494\n",
      "Epoch 493: Train Loss: 0.29976686239242556, Validation Loss: 1.221096396446228\n",
      "Epoch 494: Train Loss: 0.3169927328824997, Validation Loss: 1.2231223583221436\n",
      "Epoch 495: Train Loss: 0.2554078012704849, Validation Loss: 1.2397475242614746\n",
      "Epoch 496: Train Loss: 0.27051693499088286, Validation Loss: 1.25275719165802\n",
      "Epoch 497: Train Loss: 0.2837221771478653, Validation Loss: 1.2180918455123901\n",
      "Epoch 498: Train Loss: 0.36649749279022215, Validation Loss: 1.2554728984832764\n",
      "Epoch 499: Train Loss: 0.2597556710243225, Validation Loss: 1.255773663520813\n",
      "Fold 6 Metrics:\n",
      "Accuracy: 0.45454545454545453, Precision: 0.5, Recall: 0.16666666666666666, F1-score: 0.25, AUC: 0.4833333333333334\n",
      "Confusion Matrix:\n",
      "[[4 1]\n",
      " [5 1]]\n",
      "Completed fold 6\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples from subject 12 to test set\n",
      "Adding 6 truth samples from subject 12 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 250)\n",
      "(132, 65, 250)\n",
      "Epoch 0: Train Loss: 0.7027640342712402, Validation Loss: 0.7144946455955505\n",
      "Epoch 1: Train Loss: 0.7328841924667359, Validation Loss: 0.7301549315452576\n",
      "Epoch 2: Train Loss: 0.7203818559646606, Validation Loss: 0.7580363750457764\n",
      "Epoch 3: Train Loss: 0.774238920211792, Validation Loss: 0.7879428863525391\n",
      "Epoch 4: Train Loss: 0.7343711376190185, Validation Loss: 0.8192868232727051\n",
      "Epoch 5: Train Loss: 0.6645623803138733, Validation Loss: 0.8555165529251099\n",
      "Epoch 6: Train Loss: 0.7038815140724182, Validation Loss: 0.8822246789932251\n",
      "Epoch 7: Train Loss: 0.7271597266197205, Validation Loss: 0.8945220112800598\n",
      "Epoch 8: Train Loss: 0.6607664704322815, Validation Loss: 0.8976096510887146\n",
      "Epoch 9: Train Loss: 0.6882356166839599, Validation Loss: 0.8833569884300232\n",
      "Epoch 10: Train Loss: 0.7793037176132203, Validation Loss: 0.9266223907470703\n",
      "Epoch 11: Train Loss: 0.6798195481300354, Validation Loss: 0.9222787618637085\n",
      "Epoch 12: Train Loss: 0.6903448820114135, Validation Loss: 0.9297106266021729\n",
      "Epoch 13: Train Loss: 0.673663318157196, Validation Loss: 0.9454370141029358\n",
      "Epoch 14: Train Loss: 0.7027649879455566, Validation Loss: 0.9492172598838806\n",
      "Epoch 15: Train Loss: 0.7389295101165771, Validation Loss: 0.9379041194915771\n",
      "Epoch 16: Train Loss: 0.6773108005523681, Validation Loss: 0.941053032875061\n",
      "Epoch 17: Train Loss: 0.6592066287994385, Validation Loss: 0.9614932537078857\n",
      "Epoch 18: Train Loss: 0.6693466424942016, Validation Loss: 0.9549806714057922\n",
      "Epoch 19: Train Loss: 0.6854973554611206, Validation Loss: 0.9564203023910522\n",
      "Epoch 20: Train Loss: 0.643088412284851, Validation Loss: 0.9633685946464539\n",
      "Epoch 21: Train Loss: 0.6833256483078003, Validation Loss: 0.9907947778701782\n",
      "Epoch 22: Train Loss: 0.6386106967926025, Validation Loss: 1.006189227104187\n",
      "Epoch 23: Train Loss: 0.6571760892868042, Validation Loss: 1.002234697341919\n",
      "Epoch 24: Train Loss: 0.6489907383918763, Validation Loss: 1.002705454826355\n",
      "Epoch 25: Train Loss: 0.6225849032402039, Validation Loss: 1.0257359743118286\n",
      "Epoch 26: Train Loss: 0.6246185302734375, Validation Loss: 1.019760012626648\n",
      "Epoch 27: Train Loss: 0.6503587245941163, Validation Loss: 1.016506314277649\n",
      "Epoch 28: Train Loss: 0.6697019338607788, Validation Loss: 1.0105875730514526\n",
      "Epoch 29: Train Loss: 0.6503539443016052, Validation Loss: 1.0161705017089844\n",
      "Epoch 30: Train Loss: 0.5749925553798676, Validation Loss: 1.0184842348098755\n",
      "Epoch 31: Train Loss: 0.6414224028587341, Validation Loss: 1.0295639038085938\n",
      "Epoch 32: Train Loss: 0.646512758731842, Validation Loss: 1.0379998683929443\n",
      "Epoch 33: Train Loss: 0.6575788021087646, Validation Loss: 1.0422568321228027\n",
      "Epoch 34: Train Loss: 0.6670366048812866, Validation Loss: 1.033833622932434\n",
      "Epoch 35: Train Loss: 0.7155871748924255, Validation Loss: 1.045906901359558\n",
      "Epoch 36: Train Loss: 0.6823556184768677, Validation Loss: 1.054254412651062\n",
      "Epoch 37: Train Loss: 0.6278718829154968, Validation Loss: 1.0504428148269653\n",
      "Epoch 38: Train Loss: 0.6965483784675598, Validation Loss: 1.0445746183395386\n",
      "Epoch 39: Train Loss: 0.645525085926056, Validation Loss: 0.999500036239624\n",
      "Epoch 40: Train Loss: 0.6112207293510437, Validation Loss: 1.0143799781799316\n",
      "Epoch 41: Train Loss: 0.6340368866920472, Validation Loss: 1.0209051370620728\n",
      "Epoch 42: Train Loss: 0.6770615458488465, Validation Loss: 1.0518797636032104\n",
      "Epoch 43: Train Loss: 0.6599081754684448, Validation Loss: 1.0867254734039307\n",
      "Epoch 44: Train Loss: 0.6079395651817322, Validation Loss: 1.1149346828460693\n",
      "Epoch 45: Train Loss: 0.6302488803863525, Validation Loss: 1.110026478767395\n",
      "Epoch 46: Train Loss: 0.6479894518852234, Validation Loss: 1.0883195400238037\n",
      "Epoch 47: Train Loss: 0.6190008759498596, Validation Loss: 1.117252230644226\n",
      "Epoch 48: Train Loss: 0.6298008799552918, Validation Loss: 1.1277481317520142\n",
      "Epoch 49: Train Loss: 0.6479905247688293, Validation Loss: 1.0902165174484253\n",
      "Epoch 50: Train Loss: 0.6050928711891175, Validation Loss: 1.117572546005249\n",
      "Epoch 51: Train Loss: 0.591477119922638, Validation Loss: 1.1501519680023193\n",
      "Epoch 52: Train Loss: 0.7034009337425232, Validation Loss: 1.171736478805542\n",
      "Epoch 53: Train Loss: 0.6149279832839966, Validation Loss: 1.1163352727890015\n",
      "Epoch 54: Train Loss: 0.6220759749412537, Validation Loss: 1.1208401918411255\n",
      "Epoch 55: Train Loss: 0.6477296829223633, Validation Loss: 1.130144476890564\n",
      "Epoch 56: Train Loss: 0.6180070281028748, Validation Loss: 1.1416032314300537\n",
      "Epoch 57: Train Loss: 0.5835044860839844, Validation Loss: 1.1694796085357666\n",
      "Epoch 58: Train Loss: 0.6048478245735168, Validation Loss: 1.1244407892227173\n",
      "Epoch 59: Train Loss: 0.6129501223564148, Validation Loss: 1.1469000577926636\n",
      "Epoch 60: Train Loss: 0.5976807475090027, Validation Loss: 1.1768593788146973\n",
      "Epoch 61: Train Loss: 0.5931233167648315, Validation Loss: 1.1642582416534424\n",
      "Epoch 62: Train Loss: 0.6043682098388672, Validation Loss: 1.1834334135055542\n",
      "Epoch 63: Train Loss: 0.5911490678787231, Validation Loss: 1.200208067893982\n",
      "Epoch 64: Train Loss: 0.5925734400749206, Validation Loss: 1.2174488306045532\n",
      "Epoch 65: Train Loss: 0.5999183893203736, Validation Loss: 1.2202075719833374\n",
      "Epoch 66: Train Loss: 0.6159155368804932, Validation Loss: 1.1840792894363403\n",
      "Epoch 67: Train Loss: 0.5926772832870484, Validation Loss: 1.191083550453186\n",
      "Epoch 68: Train Loss: 0.6045009016990661, Validation Loss: 1.211265206336975\n",
      "Epoch 69: Train Loss: 0.6469954967498779, Validation Loss: 1.1964155435562134\n",
      "Epoch 70: Train Loss: 0.5802773535251617, Validation Loss: 1.2527004480361938\n",
      "Epoch 71: Train Loss: 0.6322435736656189, Validation Loss: 1.2835216522216797\n",
      "Epoch 72: Train Loss: 0.6410232901573181, Validation Loss: 1.2504993677139282\n",
      "Epoch 73: Train Loss: 0.5586676836013794, Validation Loss: 1.2502108812332153\n",
      "Epoch 74: Train Loss: 0.5716083884239197, Validation Loss: 1.1878604888916016\n",
      "Epoch 75: Train Loss: 0.6776683330535889, Validation Loss: 1.2207536697387695\n",
      "Epoch 76: Train Loss: 0.6376169323921204, Validation Loss: 1.2375867366790771\n",
      "Epoch 77: Train Loss: 0.5897607445716858, Validation Loss: 1.2388455867767334\n",
      "Epoch 78: Train Loss: 0.549606204032898, Validation Loss: 1.2475907802581787\n",
      "Epoch 79: Train Loss: 0.5980037808418274, Validation Loss: 1.2493683099746704\n",
      "Epoch 80: Train Loss: 0.5566100001335144, Validation Loss: 1.2967561483383179\n",
      "Epoch 81: Train Loss: 0.5568608522415162, Validation Loss: 1.306706428527832\n",
      "Epoch 82: Train Loss: 0.5835318803787232, Validation Loss: 1.3483800888061523\n",
      "Epoch 83: Train Loss: 0.6352018117904663, Validation Loss: 1.3492032289505005\n",
      "Epoch 84: Train Loss: 0.5597277045249939, Validation Loss: 1.3318815231323242\n",
      "Epoch 85: Train Loss: 0.5761254668235779, Validation Loss: 1.2809354066848755\n",
      "Epoch 86: Train Loss: 0.5972654819488525, Validation Loss: 1.2839667797088623\n",
      "Epoch 87: Train Loss: 0.5661103010177613, Validation Loss: 1.3194074630737305\n",
      "Epoch 88: Train Loss: 0.5393275678157806, Validation Loss: 1.33729088306427\n",
      "Epoch 89: Train Loss: 0.5251153945922852, Validation Loss: 1.3067551851272583\n",
      "Epoch 90: Train Loss: 0.5941065073013305, Validation Loss: 1.3304435014724731\n",
      "Epoch 91: Train Loss: 0.5701071143150329, Validation Loss: 1.337162733078003\n",
      "Epoch 92: Train Loss: 0.5784654259681702, Validation Loss: 1.3330230712890625\n",
      "Epoch 93: Train Loss: 0.5767684936523437, Validation Loss: 1.3194938898086548\n",
      "Epoch 94: Train Loss: 0.5657660603523255, Validation Loss: 1.3419697284698486\n",
      "Epoch 95: Train Loss: 0.5322816610336304, Validation Loss: 1.3478271961212158\n",
      "Epoch 96: Train Loss: 0.5714427947998046, Validation Loss: 1.2956326007843018\n",
      "Epoch 97: Train Loss: 0.6058947205543518, Validation Loss: 1.348182201385498\n",
      "Epoch 98: Train Loss: 0.5647823691368103, Validation Loss: 1.383662223815918\n",
      "Epoch 99: Train Loss: 0.6043602108955384, Validation Loss: 1.3795942068099976\n",
      "Epoch 100: Train Loss: 0.5205844640731812, Validation Loss: 1.4150798320770264\n",
      "Epoch 101: Train Loss: 0.5183706700801849, Validation Loss: 1.3926219940185547\n",
      "Epoch 102: Train Loss: 0.5838651537895203, Validation Loss: 1.4292546510696411\n",
      "Epoch 103: Train Loss: 0.5669066667556762, Validation Loss: 1.3864585161209106\n",
      "Epoch 104: Train Loss: 0.5455145001411438, Validation Loss: 1.394950270652771\n",
      "Epoch 105: Train Loss: 0.5316792726516724, Validation Loss: 1.3777910470962524\n",
      "Epoch 106: Train Loss: 0.5217679977416992, Validation Loss: 1.4044978618621826\n",
      "Epoch 107: Train Loss: 0.5422190368175507, Validation Loss: 1.3912618160247803\n",
      "Epoch 108: Train Loss: 0.6153762936592102, Validation Loss: 1.4520783424377441\n",
      "Epoch 109: Train Loss: 0.5427733302116394, Validation Loss: 1.4740897417068481\n",
      "Epoch 110: Train Loss: 0.5421571910381318, Validation Loss: 1.5378704071044922\n",
      "Epoch 111: Train Loss: 0.5204814314842224, Validation Loss: 1.5380442142486572\n",
      "Epoch 112: Train Loss: 0.5745805442333222, Validation Loss: 1.511269211769104\n",
      "Epoch 113: Train Loss: 0.54283447265625, Validation Loss: 1.5708080530166626\n",
      "Epoch 114: Train Loss: 0.5885250329971313, Validation Loss: 1.5418403148651123\n",
      "Epoch 115: Train Loss: 0.5102679014205933, Validation Loss: 1.5453391075134277\n",
      "Epoch 116: Train Loss: 0.5530233979225159, Validation Loss: 1.5963213443756104\n",
      "Epoch 117: Train Loss: 0.531414532661438, Validation Loss: 1.632602572441101\n",
      "Epoch 118: Train Loss: 0.5735667049884796, Validation Loss: 1.6294262409210205\n",
      "Epoch 119: Train Loss: 0.5755218744277955, Validation Loss: 1.5906769037246704\n",
      "Epoch 120: Train Loss: 0.5560452222824097, Validation Loss: 1.5852015018463135\n",
      "Epoch 121: Train Loss: 0.5567662835121154, Validation Loss: 1.558008074760437\n",
      "Epoch 122: Train Loss: 0.518575781583786, Validation Loss: 1.5490608215332031\n",
      "Epoch 123: Train Loss: 0.544936865568161, Validation Loss: 1.5737398862838745\n",
      "Epoch 124: Train Loss: 0.596970671415329, Validation Loss: 1.6279703378677368\n",
      "Epoch 125: Train Loss: 0.47291060090065, Validation Loss: 1.6627570390701294\n",
      "Epoch 126: Train Loss: 0.5314311921596527, Validation Loss: 1.5263172388076782\n",
      "Epoch 127: Train Loss: 0.5362144231796264, Validation Loss: 1.6086113452911377\n",
      "Epoch 128: Train Loss: 0.456656688451767, Validation Loss: 1.6505991220474243\n",
      "Epoch 129: Train Loss: 0.5158674657344818, Validation Loss: 1.6585428714752197\n",
      "Epoch 130: Train Loss: 0.5049717962741852, Validation Loss: 1.735775351524353\n",
      "Epoch 131: Train Loss: 0.5368163108825683, Validation Loss: 1.7281492948532104\n",
      "Epoch 132: Train Loss: 0.5155993402004242, Validation Loss: 1.6833704710006714\n",
      "Epoch 133: Train Loss: 0.5454279184341431, Validation Loss: 1.6988972425460815\n",
      "Epoch 134: Train Loss: 0.5001375615596771, Validation Loss: 1.7092692852020264\n",
      "Epoch 135: Train Loss: 0.5206097364425659, Validation Loss: 1.6716558933258057\n",
      "Epoch 136: Train Loss: 0.4876417815685272, Validation Loss: 1.655562400817871\n",
      "Epoch 137: Train Loss: 0.5150964498519898, Validation Loss: 1.6090731620788574\n",
      "Epoch 138: Train Loss: 0.5354939997196198, Validation Loss: 1.5760611295700073\n",
      "Epoch 139: Train Loss: 0.5268408954143524, Validation Loss: 1.5857352018356323\n",
      "Epoch 140: Train Loss: 0.4736626923084259, Validation Loss: 1.6711466312408447\n",
      "Epoch 141: Train Loss: 0.5350331246852875, Validation Loss: 1.717905879020691\n",
      "Epoch 142: Train Loss: 0.5389602303504943, Validation Loss: 1.7729765176773071\n",
      "Epoch 143: Train Loss: 0.5244965493679047, Validation Loss: 1.8158855438232422\n",
      "Epoch 144: Train Loss: 0.5262400686740876, Validation Loss: 1.7582935094833374\n",
      "Epoch 145: Train Loss: 0.47121686339378355, Validation Loss: 1.7416926622390747\n",
      "Epoch 146: Train Loss: 0.5632514595985413, Validation Loss: 1.8073363304138184\n",
      "Epoch 147: Train Loss: 0.5884849309921265, Validation Loss: 1.8811957836151123\n",
      "Epoch 148: Train Loss: 0.48349780440330503, Validation Loss: 1.7809994220733643\n",
      "Epoch 149: Train Loss: 0.48794327974319457, Validation Loss: 1.7934112548828125\n",
      "Epoch 150: Train Loss: 0.48953222632408144, Validation Loss: 1.7834776639938354\n",
      "Epoch 151: Train Loss: 0.5060126185417175, Validation Loss: 1.7839587926864624\n",
      "Epoch 152: Train Loss: 0.5359963595867157, Validation Loss: 1.8118937015533447\n",
      "Epoch 153: Train Loss: 0.5486729562282562, Validation Loss: 1.937124252319336\n",
      "Epoch 154: Train Loss: 0.4879101812839508, Validation Loss: 1.995131015777588\n",
      "Epoch 155: Train Loss: 0.539278119802475, Validation Loss: 2.000887870788574\n",
      "Epoch 156: Train Loss: 0.47936923503875734, Validation Loss: 2.00791072845459\n",
      "Epoch 157: Train Loss: 0.49826533198356626, Validation Loss: 2.0625360012054443\n",
      "Epoch 158: Train Loss: 0.4799026370048523, Validation Loss: 2.076526403427124\n",
      "Epoch 159: Train Loss: 0.4716186225414276, Validation Loss: 2.1131539344787598\n",
      "Epoch 160: Train Loss: 0.4936161756515503, Validation Loss: 2.014230728149414\n",
      "Epoch 161: Train Loss: 0.493979275226593, Validation Loss: 2.035806655883789\n",
      "Epoch 162: Train Loss: 0.47202548384666443, Validation Loss: 1.9269856214523315\n",
      "Epoch 163: Train Loss: 0.4733611226081848, Validation Loss: 1.9773859977722168\n",
      "Epoch 164: Train Loss: 0.4848843336105347, Validation Loss: 1.9905661344528198\n",
      "Epoch 165: Train Loss: 0.5004859507083893, Validation Loss: 1.867282748222351\n",
      "Epoch 166: Train Loss: 0.4779023051261902, Validation Loss: 1.9145801067352295\n",
      "Epoch 167: Train Loss: 0.5041723310947418, Validation Loss: 2.0121865272521973\n",
      "Epoch 168: Train Loss: 0.46926994919776915, Validation Loss: 1.9308863878250122\n",
      "Epoch 169: Train Loss: 0.4557389199733734, Validation Loss: 2.0115721225738525\n",
      "Epoch 170: Train Loss: 0.514162677526474, Validation Loss: 2.0478110313415527\n",
      "Epoch 171: Train Loss: 0.49064165353775024, Validation Loss: 2.0014586448669434\n",
      "Epoch 172: Train Loss: 0.44058105945587156, Validation Loss: 1.9077818393707275\n",
      "Epoch 173: Train Loss: 0.5551403403282166, Validation Loss: 1.9674420356750488\n",
      "Epoch 174: Train Loss: 0.5084221124649048, Validation Loss: 1.9745904207229614\n",
      "Epoch 175: Train Loss: 0.4740326166152954, Validation Loss: 2.0126049518585205\n",
      "Epoch 176: Train Loss: 0.4296533465385437, Validation Loss: 2.0097618103027344\n",
      "Epoch 177: Train Loss: 0.5506363093852997, Validation Loss: 2.0486719608306885\n",
      "Epoch 178: Train Loss: 0.5251839280128479, Validation Loss: 2.063088893890381\n",
      "Epoch 179: Train Loss: 0.4452158033847809, Validation Loss: 2.1247880458831787\n",
      "Epoch 180: Train Loss: 0.5134056091308594, Validation Loss: 2.0319011211395264\n",
      "Epoch 181: Train Loss: 0.48752973079681394, Validation Loss: 2.0873734951019287\n",
      "Epoch 182: Train Loss: 0.5040606141090394, Validation Loss: 2.0815160274505615\n",
      "Epoch 183: Train Loss: 0.5453941643238067, Validation Loss: 2.096487522125244\n",
      "Epoch 184: Train Loss: 0.4406140625476837, Validation Loss: 2.121776580810547\n",
      "Epoch 185: Train Loss: 0.4427782356739044, Validation Loss: 2.1173834800720215\n",
      "Epoch 186: Train Loss: 0.5206348180770874, Validation Loss: 2.2108511924743652\n",
      "Epoch 187: Train Loss: 0.4468905687332153, Validation Loss: 2.3199479579925537\n",
      "Epoch 188: Train Loss: 0.5157534301280975, Validation Loss: 2.308797836303711\n",
      "Epoch 189: Train Loss: 0.49753636717796323, Validation Loss: 2.3451457023620605\n",
      "Epoch 190: Train Loss: 0.43347467482089996, Validation Loss: 2.2104790210723877\n",
      "Epoch 191: Train Loss: 0.44738306403160094, Validation Loss: 2.1810696125030518\n",
      "Epoch 192: Train Loss: 0.512284916639328, Validation Loss: 2.229058265686035\n",
      "Epoch 193: Train Loss: 0.5331359922885894, Validation Loss: 2.3103415966033936\n",
      "Epoch 194: Train Loss: 0.43977624773979185, Validation Loss: 2.39493465423584\n",
      "Epoch 195: Train Loss: 0.4582282304763794, Validation Loss: 2.3874499797821045\n",
      "Epoch 196: Train Loss: 0.4930158793926239, Validation Loss: 2.323315143585205\n",
      "Epoch 197: Train Loss: 0.47400394082069397, Validation Loss: 2.3888752460479736\n",
      "Epoch 198: Train Loss: 0.4463150858879089, Validation Loss: 2.2571187019348145\n",
      "Epoch 199: Train Loss: 0.5014724195003509, Validation Loss: 2.304043769836426\n",
      "Epoch 200: Train Loss: 0.4646647751331329, Validation Loss: 2.2048678398132324\n",
      "Epoch 201: Train Loss: 0.4559417188167572, Validation Loss: 2.1990418434143066\n",
      "Epoch 202: Train Loss: 0.47643088102340697, Validation Loss: 2.123689651489258\n",
      "Epoch 203: Train Loss: 0.5005633354187011, Validation Loss: 2.2124545574188232\n",
      "Epoch 204: Train Loss: 0.5054933845996856, Validation Loss: 2.274601697921753\n",
      "Epoch 205: Train Loss: 0.4107480049133301, Validation Loss: 2.218543291091919\n",
      "Epoch 206: Train Loss: 0.49279332160949707, Validation Loss: 2.246922254562378\n",
      "Epoch 207: Train Loss: 0.53684042096138, Validation Loss: 2.2796127796173096\n",
      "Epoch 208: Train Loss: 0.4354655921459198, Validation Loss: 2.2738962173461914\n",
      "Epoch 209: Train Loss: 0.4893625795841217, Validation Loss: 2.3069746494293213\n",
      "Epoch 210: Train Loss: 0.4698402345180511, Validation Loss: 2.414663553237915\n",
      "Epoch 211: Train Loss: 0.45254462361335757, Validation Loss: 2.4021384716033936\n",
      "Epoch 212: Train Loss: 0.48597550988197324, Validation Loss: 2.366201877593994\n",
      "Epoch 213: Train Loss: 0.5087286591529846, Validation Loss: 2.3274075984954834\n",
      "Epoch 214: Train Loss: 0.5025579631328583, Validation Loss: 2.410640239715576\n",
      "Epoch 215: Train Loss: 0.43386298418045044, Validation Loss: 2.4858720302581787\n",
      "Epoch 216: Train Loss: 0.5402456939220428, Validation Loss: 2.485713481903076\n",
      "Epoch 217: Train Loss: 0.43613134026527406, Validation Loss: 2.434372901916504\n",
      "Epoch 218: Train Loss: 0.5168476521968841, Validation Loss: 2.506897211074829\n",
      "Epoch 219: Train Loss: 0.47365847826004026, Validation Loss: 2.3584346771240234\n",
      "Epoch 220: Train Loss: 0.42358359694480896, Validation Loss: 2.2532522678375244\n",
      "Epoch 221: Train Loss: 0.5237595677375794, Validation Loss: 2.3972628116607666\n",
      "Epoch 222: Train Loss: 0.4598540186882019, Validation Loss: 2.4606144428253174\n",
      "Epoch 223: Train Loss: 0.404101687669754, Validation Loss: 2.5210061073303223\n",
      "Epoch 224: Train Loss: 0.4551680266857147, Validation Loss: 2.4960904121398926\n",
      "Epoch 225: Train Loss: 0.3684199169278145, Validation Loss: 2.5348169803619385\n",
      "Epoch 226: Train Loss: 0.4274936497211456, Validation Loss: 2.462202548980713\n",
      "Epoch 227: Train Loss: 0.4294233381748199, Validation Loss: 2.4282195568084717\n",
      "Epoch 228: Train Loss: 0.480508553981781, Validation Loss: 2.5807948112487793\n",
      "Epoch 229: Train Loss: 0.438357412815094, Validation Loss: 2.5937340259552\n",
      "Epoch 230: Train Loss: 0.4329404354095459, Validation Loss: 2.538004159927368\n",
      "Epoch 231: Train Loss: 0.4339684784412384, Validation Loss: 2.476574659347534\n",
      "Epoch 232: Train Loss: 0.44566685557365415, Validation Loss: 2.4005448818206787\n",
      "Epoch 233: Train Loss: 0.4053073853254318, Validation Loss: 2.3758654594421387\n",
      "Epoch 234: Train Loss: 0.438854318857193, Validation Loss: 2.385594129562378\n",
      "Epoch 235: Train Loss: 0.40568100214004515, Validation Loss: 2.3889710903167725\n",
      "Epoch 236: Train Loss: 0.4723154306411743, Validation Loss: 2.414473056793213\n",
      "Epoch 237: Train Loss: 0.3834379404783249, Validation Loss: 2.3427138328552246\n",
      "Epoch 238: Train Loss: 0.4181506812572479, Validation Loss: 2.364351511001587\n",
      "Epoch 239: Train Loss: 0.4203405261039734, Validation Loss: 2.3756141662597656\n",
      "Epoch 240: Train Loss: 0.3952303737401962, Validation Loss: 2.393280506134033\n",
      "Epoch 241: Train Loss: 0.40480844378471376, Validation Loss: 2.467381238937378\n",
      "Epoch 242: Train Loss: 0.43255354166030885, Validation Loss: 2.402451992034912\n",
      "Epoch 243: Train Loss: 0.44989281296730044, Validation Loss: 2.461581230163574\n",
      "Epoch 244: Train Loss: 0.4513878107070923, Validation Loss: 2.537445306777954\n",
      "Epoch 245: Train Loss: 0.4468397915363312, Validation Loss: 2.6309332847595215\n",
      "Epoch 246: Train Loss: 0.4542525470256805, Validation Loss: 2.6158812046051025\n",
      "Epoch 247: Train Loss: 0.388575068116188, Validation Loss: 2.6707828044891357\n",
      "Epoch 248: Train Loss: 0.4712474584579468, Validation Loss: 2.5666818618774414\n",
      "Epoch 249: Train Loss: 0.509409898519516, Validation Loss: 2.6201326847076416\n",
      "Epoch 250: Train Loss: 0.4273613214492798, Validation Loss: 2.66508412361145\n",
      "Epoch 251: Train Loss: 0.44463720321655276, Validation Loss: 2.6454293727874756\n",
      "Epoch 252: Train Loss: 0.40187557339668273, Validation Loss: 2.708759069442749\n",
      "Epoch 253: Train Loss: 0.40342905521392824, Validation Loss: 2.6773738861083984\n",
      "Epoch 254: Train Loss: 0.38981378078460693, Validation Loss: 2.723540782928467\n",
      "Epoch 255: Train Loss: 0.4014290511608124, Validation Loss: 2.562326192855835\n",
      "Epoch 256: Train Loss: 0.4504318594932556, Validation Loss: 2.533688545227051\n",
      "Epoch 257: Train Loss: 0.40153816938400266, Validation Loss: 2.5885086059570312\n",
      "Epoch 258: Train Loss: 0.45343818664550783, Validation Loss: 2.5331428050994873\n",
      "Epoch 259: Train Loss: 0.3881914675235748, Validation Loss: 2.645678997039795\n",
      "Epoch 260: Train Loss: 0.44186673164367674, Validation Loss: 2.6445958614349365\n",
      "Epoch 261: Train Loss: 0.4183704972267151, Validation Loss: 2.655378818511963\n",
      "Epoch 262: Train Loss: 0.39851110577583315, Validation Loss: 2.506291151046753\n",
      "Epoch 263: Train Loss: 0.4154557943344116, Validation Loss: 2.4568581581115723\n",
      "Epoch 264: Train Loss: 0.3889564037322998, Validation Loss: 2.4756057262420654\n",
      "Epoch 265: Train Loss: 0.378338497877121, Validation Loss: 2.4403045177459717\n",
      "Epoch 266: Train Loss: 0.43082951903343203, Validation Loss: 2.5570456981658936\n",
      "Epoch 267: Train Loss: 0.38540257811546325, Validation Loss: 2.6320087909698486\n",
      "Epoch 268: Train Loss: 0.4350286900997162, Validation Loss: 2.7092912197113037\n",
      "Epoch 269: Train Loss: 0.4155464291572571, Validation Loss: 2.665656089782715\n",
      "Epoch 270: Train Loss: 0.4094423234462738, Validation Loss: 2.7982430458068848\n",
      "Epoch 271: Train Loss: 0.3455679267644882, Validation Loss: 2.67958402633667\n",
      "Epoch 272: Train Loss: 0.44170163869857787, Validation Loss: 2.541794538497925\n",
      "Epoch 273: Train Loss: 0.3784060120582581, Validation Loss: 2.4248805046081543\n",
      "Epoch 274: Train Loss: 0.3729835212230682, Validation Loss: 2.5245137214660645\n",
      "Epoch 275: Train Loss: 0.3845395505428314, Validation Loss: 2.6118645668029785\n",
      "Epoch 276: Train Loss: 0.460569429397583, Validation Loss: 2.721522331237793\n",
      "Epoch 277: Train Loss: 0.42366445660591123, Validation Loss: 2.728774309158325\n",
      "Epoch 278: Train Loss: 0.3941942572593689, Validation Loss: 2.8259477615356445\n",
      "Epoch 279: Train Loss: 0.3840339958667755, Validation Loss: 2.885443687438965\n",
      "Epoch 280: Train Loss: 0.3981863260269165, Validation Loss: 2.9329516887664795\n",
      "Epoch 281: Train Loss: 0.3930608808994293, Validation Loss: 2.7873191833496094\n",
      "Epoch 282: Train Loss: 0.3830697238445282, Validation Loss: 2.912245512008667\n",
      "Epoch 283: Train Loss: 0.3462787538766861, Validation Loss: 3.010302782058716\n",
      "Epoch 284: Train Loss: 0.3870461404323578, Validation Loss: 2.9710440635681152\n",
      "Epoch 285: Train Loss: 0.5049766540527344, Validation Loss: 3.0465259552001953\n",
      "Epoch 286: Train Loss: 0.33012850806117056, Validation Loss: 3.0235824584960938\n",
      "Epoch 287: Train Loss: 0.43560532927513124, Validation Loss: 2.8861656188964844\n",
      "Epoch 288: Train Loss: 0.39397152066230773, Validation Loss: 2.840632915496826\n",
      "Epoch 289: Train Loss: 0.39185982942581177, Validation Loss: 2.8702609539031982\n",
      "Epoch 290: Train Loss: 0.3426417350769043, Validation Loss: 2.996839761734009\n",
      "Epoch 291: Train Loss: 0.3862443149089813, Validation Loss: 2.793236255645752\n",
      "Epoch 292: Train Loss: 0.38735103607177734, Validation Loss: 2.6977107524871826\n",
      "Epoch 293: Train Loss: 0.3279355376958847, Validation Loss: 2.8682963848114014\n",
      "Epoch 294: Train Loss: 0.45858638286590575, Validation Loss: 2.9730231761932373\n",
      "Epoch 295: Train Loss: 0.436211895942688, Validation Loss: 3.0002262592315674\n",
      "Epoch 296: Train Loss: 0.36206152439117434, Validation Loss: 3.039559841156006\n",
      "Epoch 297: Train Loss: 0.3949898660182953, Validation Loss: 2.953810930252075\n",
      "Epoch 298: Train Loss: 0.3682362973690033, Validation Loss: 2.963998317718506\n",
      "Epoch 299: Train Loss: 0.4112653970718384, Validation Loss: 2.8836300373077393\n",
      "Epoch 300: Train Loss: 0.34896319508552553, Validation Loss: 2.9324538707733154\n",
      "Epoch 301: Train Loss: 0.34451570808887483, Validation Loss: 3.114515542984009\n",
      "Epoch 302: Train Loss: 0.3773433446884155, Validation Loss: 2.9784185886383057\n",
      "Epoch 303: Train Loss: 0.42176607847213743, Validation Loss: 2.946063280105591\n",
      "Epoch 304: Train Loss: 0.4236446976661682, Validation Loss: 2.994009256362915\n",
      "Epoch 305: Train Loss: 0.35192973017692564, Validation Loss: 2.9170172214508057\n",
      "Epoch 306: Train Loss: 0.4621497094631195, Validation Loss: 2.9581425189971924\n",
      "Epoch 307: Train Loss: 0.35456493198871614, Validation Loss: 2.9541068077087402\n",
      "Epoch 308: Train Loss: 0.3545752078294754, Validation Loss: 3.0335853099823\n",
      "Epoch 309: Train Loss: 0.35300871133804324, Validation Loss: 2.9666588306427\n",
      "Epoch 310: Train Loss: 0.4183506786823273, Validation Loss: 2.9127025604248047\n",
      "Epoch 311: Train Loss: 0.3887665271759033, Validation Loss: 3.045132875442505\n",
      "Epoch 312: Train Loss: 0.36574546694755555, Validation Loss: 2.997164487838745\n",
      "Epoch 313: Train Loss: 0.3658937633037567, Validation Loss: 2.9118194580078125\n",
      "Epoch 314: Train Loss: 0.4247923016548157, Validation Loss: 2.879696846008301\n",
      "Epoch 315: Train Loss: 0.355258983373642, Validation Loss: 2.8311917781829834\n",
      "Epoch 316: Train Loss: 0.4320213317871094, Validation Loss: 2.9809257984161377\n",
      "Epoch 317: Train Loss: 0.3230441063642502, Validation Loss: 3.0567679405212402\n",
      "Epoch 318: Train Loss: 0.3567990779876709, Validation Loss: 3.1485488414764404\n",
      "Epoch 319: Train Loss: 0.3532867044210434, Validation Loss: 3.1850712299346924\n",
      "Epoch 320: Train Loss: 0.39151450991630554, Validation Loss: 3.0772383213043213\n",
      "Epoch 321: Train Loss: 0.35090451836586, Validation Loss: 3.1271519660949707\n",
      "Epoch 322: Train Loss: 0.34056603610515596, Validation Loss: 3.1795241832733154\n",
      "Epoch 323: Train Loss: 0.41456280946731566, Validation Loss: 3.217160701751709\n",
      "Epoch 324: Train Loss: 0.39440027475357053, Validation Loss: 3.2338852882385254\n",
      "Epoch 325: Train Loss: 0.3500791132450104, Validation Loss: 3.209538221359253\n",
      "Epoch 326: Train Loss: 0.35143412947654723, Validation Loss: 3.1067569255828857\n",
      "Epoch 327: Train Loss: 0.3921519100666046, Validation Loss: 3.0403196811676025\n",
      "Epoch 328: Train Loss: 0.33040195405483247, Validation Loss: 3.0602378845214844\n",
      "Epoch 329: Train Loss: 0.37056723833084104, Validation Loss: 3.012531280517578\n",
      "Epoch 330: Train Loss: 0.42431735396385195, Validation Loss: 2.93135142326355\n",
      "Epoch 331: Train Loss: 0.31576659679412844, Validation Loss: 3.0734457969665527\n",
      "Epoch 332: Train Loss: 0.32356037199497223, Validation Loss: 3.070622682571411\n",
      "Epoch 333: Train Loss: 0.3438915014266968, Validation Loss: 3.1857616901397705\n",
      "Epoch 334: Train Loss: 0.38370056748390197, Validation Loss: 3.2390975952148438\n",
      "Epoch 335: Train Loss: 0.4044231355190277, Validation Loss: 3.207343339920044\n",
      "Epoch 336: Train Loss: 0.3671978533267975, Validation Loss: 3.2176735401153564\n",
      "Epoch 337: Train Loss: 0.39367952942848206, Validation Loss: 3.2208809852600098\n",
      "Epoch 338: Train Loss: 0.4850057363510132, Validation Loss: 3.03727650642395\n",
      "Epoch 339: Train Loss: 0.329997318983078, Validation Loss: 3.1442246437072754\n",
      "Epoch 340: Train Loss: 0.35242571234703063, Validation Loss: 3.1339962482452393\n",
      "Epoch 341: Train Loss: 0.3562465846538544, Validation Loss: 3.246612548828125\n",
      "Epoch 342: Train Loss: 0.39798101782798767, Validation Loss: 3.2287044525146484\n",
      "Epoch 343: Train Loss: 0.41141948103904724, Validation Loss: 3.277754306793213\n",
      "Epoch 344: Train Loss: 0.3970037817955017, Validation Loss: 3.1222312450408936\n",
      "Epoch 345: Train Loss: 0.3653711318969727, Validation Loss: 3.2566781044006348\n",
      "Epoch 346: Train Loss: 0.3718803107738495, Validation Loss: 3.090916872024536\n",
      "Epoch 347: Train Loss: 0.35542644262313844, Validation Loss: 3.241095542907715\n",
      "Epoch 348: Train Loss: 0.41347719728946686, Validation Loss: 3.1525535583496094\n",
      "Epoch 349: Train Loss: 0.3852165281772614, Validation Loss: 3.2451651096343994\n",
      "Epoch 350: Train Loss: 0.3353638261556625, Validation Loss: 3.3262059688568115\n",
      "Epoch 351: Train Loss: 0.3388900697231293, Validation Loss: 3.2608108520507812\n",
      "Epoch 352: Train Loss: 0.4146999180316925, Validation Loss: 3.3231472969055176\n",
      "Epoch 353: Train Loss: 0.35459715127944946, Validation Loss: 3.233449935913086\n",
      "Epoch 354: Train Loss: 0.4182327687740326, Validation Loss: 3.3065366744995117\n",
      "Epoch 355: Train Loss: 0.33701876401901243, Validation Loss: 3.2814693450927734\n",
      "Epoch 356: Train Loss: 0.3508186548948288, Validation Loss: 3.24776291847229\n",
      "Epoch 357: Train Loss: 0.429683119058609, Validation Loss: 3.3054451942443848\n",
      "Epoch 358: Train Loss: 0.3476505637168884, Validation Loss: 3.237834930419922\n",
      "Epoch 359: Train Loss: 0.4430208444595337, Validation Loss: 3.3149001598358154\n",
      "Epoch 360: Train Loss: 0.35617594718933104, Validation Loss: 3.3759217262268066\n",
      "Epoch 361: Train Loss: 0.33892581462860105, Validation Loss: 3.164504051208496\n",
      "Epoch 362: Train Loss: 0.45668659210205076, Validation Loss: 3.293034076690674\n",
      "Epoch 363: Train Loss: 0.3460987150669098, Validation Loss: 3.1506855487823486\n",
      "Epoch 364: Train Loss: 0.3175855815410614, Validation Loss: 3.260439872741699\n",
      "Epoch 365: Train Loss: 0.33064375519752504, Validation Loss: 3.150191307067871\n",
      "Epoch 366: Train Loss: 0.2827301621437073, Validation Loss: 3.1915676593780518\n",
      "Epoch 367: Train Loss: 0.3160136193037033, Validation Loss: 3.1810734272003174\n",
      "Epoch 368: Train Loss: 0.33726794719696046, Validation Loss: 3.245037794113159\n",
      "Epoch 369: Train Loss: 0.35445212125778197, Validation Loss: 3.135232925415039\n",
      "Epoch 370: Train Loss: 0.37279399037361144, Validation Loss: 3.183014392852783\n",
      "Epoch 371: Train Loss: 0.3266367793083191, Validation Loss: 3.1445233821868896\n",
      "Epoch 372: Train Loss: 0.3265134960412979, Validation Loss: 3.2703778743743896\n",
      "Epoch 373: Train Loss: 0.3405495584011078, Validation Loss: 3.3211891651153564\n",
      "Epoch 374: Train Loss: 0.3099363803863525, Validation Loss: 3.335878849029541\n",
      "Epoch 375: Train Loss: 0.3256707310676575, Validation Loss: 3.5043575763702393\n",
      "Epoch 376: Train Loss: 0.3132211729884148, Validation Loss: 3.3994193077087402\n",
      "Epoch 377: Train Loss: 0.3518531620502472, Validation Loss: 3.1485509872436523\n",
      "Epoch 378: Train Loss: 0.31669763922691346, Validation Loss: 3.129688024520874\n",
      "Epoch 379: Train Loss: 0.31732420325279237, Validation Loss: 3.1891655921936035\n",
      "Epoch 380: Train Loss: 0.3108623266220093, Validation Loss: 3.172640085220337\n",
      "Epoch 381: Train Loss: 0.31046555638313295, Validation Loss: 3.2612133026123047\n",
      "Epoch 382: Train Loss: 0.2998789370059967, Validation Loss: 3.3840527534484863\n",
      "Epoch 383: Train Loss: 0.32070232629776, Validation Loss: 3.2621357440948486\n",
      "Epoch 384: Train Loss: 0.4281815648078918, Validation Loss: 3.4434690475463867\n",
      "Epoch 385: Train Loss: 0.4153507471084595, Validation Loss: 3.433350086212158\n",
      "Epoch 386: Train Loss: 0.2815925538539886, Validation Loss: 3.3557775020599365\n",
      "Epoch 387: Train Loss: 0.2787085697054863, Validation Loss: 3.4084603786468506\n",
      "Epoch 388: Train Loss: 0.3771107614040375, Validation Loss: 3.4399795532226562\n",
      "Epoch 389: Train Loss: 0.3165135055780411, Validation Loss: 3.6159684658050537\n",
      "Epoch 390: Train Loss: 0.2785515069961548, Validation Loss: 3.4908204078674316\n",
      "Epoch 391: Train Loss: 0.3073631823062897, Validation Loss: 3.407069206237793\n",
      "Epoch 392: Train Loss: 0.3134163111448288, Validation Loss: 3.563265085220337\n",
      "Epoch 393: Train Loss: 0.3209619104862213, Validation Loss: 3.586921453475952\n",
      "Epoch 394: Train Loss: 0.2835564076900482, Validation Loss: 3.6118714809417725\n",
      "Epoch 395: Train Loss: 0.41691716611385343, Validation Loss: 3.59124755859375\n",
      "Epoch 396: Train Loss: 0.2855923712253571, Validation Loss: 3.601909637451172\n",
      "Epoch 397: Train Loss: 0.3216219305992126, Validation Loss: 3.6130166053771973\n",
      "Epoch 398: Train Loss: 0.3928522229194641, Validation Loss: 3.6263952255249023\n",
      "Epoch 399: Train Loss: 0.3059914767742157, Validation Loss: 3.6400885581970215\n",
      "Epoch 400: Train Loss: 0.3139046609401703, Validation Loss: 3.5112342834472656\n",
      "Epoch 401: Train Loss: 0.30243727564811707, Validation Loss: 3.3864309787750244\n",
      "Epoch 402: Train Loss: 0.36913188099861144, Validation Loss: 3.373737335205078\n",
      "Epoch 403: Train Loss: 0.359688001871109, Validation Loss: 3.6759748458862305\n",
      "Epoch 404: Train Loss: 0.2699127420783043, Validation Loss: 3.650752305984497\n",
      "Epoch 405: Train Loss: 0.30821946263313293, Validation Loss: 3.5276801586151123\n",
      "Epoch 406: Train Loss: 0.26549433916807175, Validation Loss: 3.6692051887512207\n",
      "Epoch 407: Train Loss: 0.30749478936195374, Validation Loss: 3.698246479034424\n",
      "Epoch 408: Train Loss: 0.32122704684734343, Validation Loss: 3.66054368019104\n",
      "Epoch 409: Train Loss: 0.3057443290948868, Validation Loss: 3.6507461071014404\n",
      "Epoch 410: Train Loss: 0.352006334066391, Validation Loss: 3.6028060913085938\n",
      "Epoch 411: Train Loss: 0.3074042946100235, Validation Loss: 3.5788276195526123\n",
      "Epoch 412: Train Loss: 0.4213063895702362, Validation Loss: 3.69150972366333\n",
      "Epoch 413: Train Loss: 0.2946865826845169, Validation Loss: 3.7702131271362305\n",
      "Epoch 414: Train Loss: 0.35681597590446473, Validation Loss: 3.920541524887085\n",
      "Epoch 415: Train Loss: 0.3088927686214447, Validation Loss: 3.799027442932129\n",
      "Epoch 416: Train Loss: 0.2921008288860321, Validation Loss: 3.8442933559417725\n",
      "Epoch 417: Train Loss: 0.3450066149234772, Validation Loss: 3.9241278171539307\n",
      "Epoch 418: Train Loss: 0.29812687933444976, Validation Loss: 3.8477683067321777\n",
      "Epoch 419: Train Loss: 0.33124211728572844, Validation Loss: 3.771205186843872\n",
      "Epoch 420: Train Loss: 0.34132817983627317, Validation Loss: 3.6478841304779053\n",
      "Epoch 421: Train Loss: 0.4126858800649643, Validation Loss: 3.6621787548065186\n",
      "Epoch 422: Train Loss: 0.4313043266534805, Validation Loss: 3.6563918590545654\n",
      "Epoch 423: Train Loss: 0.2991623431444168, Validation Loss: 3.77921462059021\n",
      "Epoch 424: Train Loss: 0.31587445437908174, Validation Loss: 3.7364301681518555\n",
      "Epoch 425: Train Loss: 0.3001668930053711, Validation Loss: 3.666325330734253\n",
      "Epoch 426: Train Loss: 0.29805418848991394, Validation Loss: 3.6506028175354004\n",
      "Epoch 427: Train Loss: 0.407481050491333, Validation Loss: 3.700453042984009\n",
      "Epoch 428: Train Loss: 0.3998360186815262, Validation Loss: 3.877946376800537\n",
      "Epoch 429: Train Loss: 0.38373009860515594, Validation Loss: 3.9363274574279785\n",
      "Epoch 430: Train Loss: 0.34882678985595705, Validation Loss: 3.7632644176483154\n",
      "Epoch 431: Train Loss: 0.26079643070697783, Validation Loss: 3.8544485569000244\n",
      "Epoch 432: Train Loss: 0.30660060942173006, Validation Loss: 3.9757540225982666\n",
      "Epoch 433: Train Loss: 0.2875264257192612, Validation Loss: 3.9854941368103027\n",
      "Epoch 434: Train Loss: 0.3052935630083084, Validation Loss: 3.943214178085327\n",
      "Epoch 435: Train Loss: 0.373570054769516, Validation Loss: 3.886270761489868\n",
      "Epoch 436: Train Loss: 0.314259472489357, Validation Loss: 3.7650365829467773\n",
      "Epoch 437: Train Loss: 0.31295873820781706, Validation Loss: 3.7372686862945557\n",
      "Epoch 438: Train Loss: 0.29763284623622893, Validation Loss: 3.7397477626800537\n",
      "Epoch 439: Train Loss: 0.2860680788755417, Validation Loss: 3.79646372795105\n",
      "Epoch 440: Train Loss: 0.30821074843406676, Validation Loss: 3.8600664138793945\n",
      "Epoch 441: Train Loss: 0.31181435883045194, Validation Loss: 3.7779388427734375\n",
      "Epoch 442: Train Loss: 0.29979148507118225, Validation Loss: 3.6582860946655273\n",
      "Epoch 443: Train Loss: 0.26966017186641694, Validation Loss: 3.7440688610076904\n",
      "Epoch 444: Train Loss: 0.2878848433494568, Validation Loss: 3.9009759426116943\n",
      "Epoch 445: Train Loss: 0.26814581751823424, Validation Loss: 3.8481802940368652\n",
      "Epoch 446: Train Loss: 0.31496813893318176, Validation Loss: 4.012971878051758\n",
      "Epoch 447: Train Loss: 0.2958961397409439, Validation Loss: 3.9274260997772217\n",
      "Epoch 448: Train Loss: 0.35731894671916964, Validation Loss: 3.872920036315918\n",
      "Epoch 449: Train Loss: 0.30418199598789214, Validation Loss: 4.022773265838623\n",
      "Epoch 450: Train Loss: 0.31333889365196227, Validation Loss: 3.9982972145080566\n",
      "Epoch 451: Train Loss: 0.2697628825902939, Validation Loss: 3.9719560146331787\n",
      "Epoch 452: Train Loss: 0.34620946943759917, Validation Loss: 3.8344123363494873\n",
      "Epoch 453: Train Loss: 0.3300494283437729, Validation Loss: 3.7145309448242188\n",
      "Epoch 454: Train Loss: 0.2777504697442055, Validation Loss: 3.897169351577759\n",
      "Epoch 455: Train Loss: 0.3128874659538269, Validation Loss: 3.796529531478882\n",
      "Epoch 456: Train Loss: 0.3399572789669037, Validation Loss: 3.8556604385375977\n",
      "Epoch 457: Train Loss: 0.3446018099784851, Validation Loss: 3.75974702835083\n",
      "Epoch 458: Train Loss: 0.3870590180158615, Validation Loss: 3.9178850650787354\n",
      "Epoch 459: Train Loss: 0.25930386781692505, Validation Loss: 3.824230194091797\n",
      "Epoch 460: Train Loss: 0.29044331312179567, Validation Loss: 3.952519178390503\n",
      "Epoch 461: Train Loss: 0.29172243475914, Validation Loss: 3.8142027854919434\n",
      "Epoch 462: Train Loss: 0.2555474817752838, Validation Loss: 3.999990224838257\n",
      "Epoch 463: Train Loss: 0.28589830696582796, Validation Loss: 4.1200480461120605\n",
      "Epoch 464: Train Loss: 0.2686705857515335, Validation Loss: 4.30750846862793\n",
      "Epoch 465: Train Loss: 0.30411275923252107, Validation Loss: 4.024959087371826\n",
      "Epoch 466: Train Loss: 0.34088465869426726, Validation Loss: 4.187697410583496\n",
      "Epoch 467: Train Loss: 0.3068189978599548, Validation Loss: 4.267506122589111\n",
      "Epoch 468: Train Loss: 0.2627955794334412, Validation Loss: 4.254787921905518\n",
      "Epoch 469: Train Loss: 0.32171562910079954, Validation Loss: 4.2526750564575195\n",
      "Epoch 470: Train Loss: 0.2578605443239212, Validation Loss: 4.101524353027344\n",
      "Epoch 471: Train Loss: 0.3043249070644379, Validation Loss: 4.249493598937988\n",
      "Epoch 472: Train Loss: 0.2625412613153458, Validation Loss: 4.333346366882324\n",
      "Epoch 473: Train Loss: 0.35494994521141054, Validation Loss: 4.1328444480896\n",
      "Epoch 474: Train Loss: 0.3548588126897812, Validation Loss: 3.8955962657928467\n",
      "Epoch 475: Train Loss: 0.24561211913824083, Validation Loss: 3.9417784214019775\n",
      "Epoch 476: Train Loss: 0.33679760098457334, Validation Loss: 3.811244249343872\n",
      "Epoch 477: Train Loss: 0.22733277902007104, Validation Loss: 3.7638654708862305\n",
      "Epoch 478: Train Loss: 0.26904999613761904, Validation Loss: 3.847334861755371\n",
      "Epoch 479: Train Loss: 0.2976549655199051, Validation Loss: 4.016900062561035\n",
      "Epoch 480: Train Loss: 0.29320370554924013, Validation Loss: 4.168254375457764\n",
      "Epoch 481: Train Loss: 0.2458137109875679, Validation Loss: 4.00880241394043\n",
      "Epoch 482: Train Loss: 0.2754645079374313, Validation Loss: 3.9465882778167725\n",
      "Epoch 483: Train Loss: 0.28062591552734373, Validation Loss: 3.8557915687561035\n",
      "Epoch 484: Train Loss: 0.2631386578083038, Validation Loss: 3.9954988956451416\n",
      "Epoch 485: Train Loss: 0.3681495636701584, Validation Loss: 4.118506908416748\n",
      "Epoch 486: Train Loss: 0.2879730314016342, Validation Loss: 3.995821952819824\n",
      "Epoch 487: Train Loss: 0.2344890609383583, Validation Loss: 4.0903000831604\n",
      "Epoch 488: Train Loss: 0.2762641340494156, Validation Loss: 4.137198448181152\n",
      "Epoch 489: Train Loss: 0.37677283883094786, Validation Loss: 4.01155424118042\n",
      "Epoch 490: Train Loss: 0.2647460728883743, Validation Loss: 3.886805534362793\n",
      "Epoch 491: Train Loss: 0.2602380484342575, Validation Loss: 3.8672332763671875\n",
      "Epoch 492: Train Loss: 0.2605800896883011, Validation Loss: 4.067470073699951\n",
      "Epoch 493: Train Loss: 0.2971364438533783, Validation Loss: 4.175488471984863\n",
      "Epoch 494: Train Loss: 0.2782342553138733, Validation Loss: 4.225893020629883\n",
      "Epoch 495: Train Loss: 0.23208794444799424, Validation Loss: 4.0015668869018555\n",
      "Epoch 496: Train Loss: 0.29295319616794585, Validation Loss: 4.14509391784668\n",
      "Epoch 497: Train Loss: 0.2690983474254608, Validation Loss: 4.101961135864258\n",
      "Epoch 498: Train Loss: 0.24112788438796998, Validation Loss: 4.130388259887695\n",
      "Epoch 499: Train Loss: 0.2724072337150574, Validation Loss: 4.145045280456543\n",
      "Fold 7 Metrics:\n",
      "Accuracy: 0.0, Precision: 0.0, Recall: 0.0, F1-score: 0.0, AUC: 0.0\n",
      "Confusion Matrix:\n",
      "[[0 5]\n",
      " [6 0]]\n",
      "Completed fold 7\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples from subject 9 to test set\n",
      "Adding 6 truth samples from subject 9 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 250)\n",
      "(132, 65, 250)\n",
      "Epoch 0: Train Loss: 0.6824420034885407, Validation Loss: 0.6951210498809814\n",
      "Epoch 1: Train Loss: 0.7344815969467163, Validation Loss: 0.6964610815048218\n",
      "Epoch 2: Train Loss: 0.7004165530204773, Validation Loss: 0.6977804899215698\n",
      "Epoch 3: Train Loss: 0.6710338711738586, Validation Loss: 0.7007935047149658\n",
      "Epoch 4: Train Loss: 0.6901202201843262, Validation Loss: 0.7065987586975098\n",
      "Epoch 5: Train Loss: 0.6993978381156921, Validation Loss: 0.7140842080116272\n",
      "Epoch 6: Train Loss: 0.6652973294258118, Validation Loss: 0.71893310546875\n",
      "Epoch 7: Train Loss: 0.7615192174911499, Validation Loss: 0.7223384976387024\n",
      "Epoch 8: Train Loss: 0.7204180002212525, Validation Loss: 0.7246591448783875\n",
      "Epoch 9: Train Loss: 0.7177037000656128, Validation Loss: 0.7294868230819702\n",
      "Epoch 10: Train Loss: 0.6682239174842834, Validation Loss: 0.7360402345657349\n",
      "Epoch 11: Train Loss: 0.6656078934669495, Validation Loss: 0.7404218316078186\n",
      "Epoch 12: Train Loss: 0.6678344011306763, Validation Loss: 0.7415686845779419\n",
      "Epoch 13: Train Loss: 0.702575421333313, Validation Loss: 0.7417254447937012\n",
      "Epoch 14: Train Loss: 0.7251616597175599, Validation Loss: 0.7387824058532715\n",
      "Epoch 15: Train Loss: 0.7002229809761047, Validation Loss: 0.7364665865898132\n",
      "Epoch 16: Train Loss: 0.7048818111419678, Validation Loss: 0.7437871694564819\n",
      "Epoch 17: Train Loss: 0.6026729762554168, Validation Loss: 0.740962564945221\n",
      "Epoch 18: Train Loss: 0.6498730301856994, Validation Loss: 0.7389035224914551\n",
      "Epoch 19: Train Loss: 0.619249963760376, Validation Loss: 0.7417434453964233\n",
      "Epoch 20: Train Loss: 0.6222857534885406, Validation Loss: 0.7422447204589844\n",
      "Epoch 21: Train Loss: 0.6468236446380615, Validation Loss: 0.7447953224182129\n",
      "Epoch 22: Train Loss: 0.7513147234916687, Validation Loss: 0.7460614442825317\n",
      "Epoch 23: Train Loss: 0.65454261302948, Validation Loss: 0.7524651885032654\n",
      "Epoch 24: Train Loss: 0.653979218006134, Validation Loss: 0.7582047581672668\n",
      "Epoch 25: Train Loss: 0.659166407585144, Validation Loss: 0.7654147148132324\n",
      "Epoch 26: Train Loss: 0.6101934432983398, Validation Loss: 0.7688714265823364\n",
      "Epoch 27: Train Loss: 0.6989803314208984, Validation Loss: 0.7647549510002136\n",
      "Epoch 28: Train Loss: 0.6892841935157776, Validation Loss: 0.7576828002929688\n",
      "Epoch 29: Train Loss: 0.6407731413841248, Validation Loss: 0.7551543712615967\n",
      "Epoch 30: Train Loss: 0.6038012504577637, Validation Loss: 0.7551480531692505\n",
      "Epoch 31: Train Loss: 0.6686890602111817, Validation Loss: 0.7646781206130981\n",
      "Epoch 32: Train Loss: 0.663839316368103, Validation Loss: 0.7598686218261719\n",
      "Epoch 33: Train Loss: 0.6043660998344421, Validation Loss: 0.7640668153762817\n",
      "Epoch 34: Train Loss: 0.6447368025779724, Validation Loss: 0.7619353532791138\n",
      "Epoch 35: Train Loss: 0.7020920395851136, Validation Loss: 0.7683825492858887\n",
      "Epoch 36: Train Loss: 0.6397504806518555, Validation Loss: 0.7658107876777649\n",
      "Epoch 37: Train Loss: 0.644594144821167, Validation Loss: 0.7741716504096985\n",
      "Epoch 38: Train Loss: 0.6524997472763061, Validation Loss: 0.7685620188713074\n",
      "Epoch 39: Train Loss: 0.6572501182556152, Validation Loss: 0.7694549560546875\n",
      "Epoch 40: Train Loss: 0.6366061210632324, Validation Loss: 0.7782256007194519\n",
      "Epoch 41: Train Loss: 0.6604941248893738, Validation Loss: 0.7827130556106567\n",
      "Epoch 42: Train Loss: 0.6849830746650696, Validation Loss: 0.7821927666664124\n",
      "Epoch 43: Train Loss: 0.6042780041694641, Validation Loss: 0.7774607539176941\n",
      "Epoch 44: Train Loss: 0.6547163724899292, Validation Loss: 0.7765887379646301\n",
      "Epoch 45: Train Loss: 0.6067295074462891, Validation Loss: 0.779241681098938\n",
      "Epoch 46: Train Loss: 0.5834801912307739, Validation Loss: 0.7809005379676819\n",
      "Epoch 47: Train Loss: 0.650964331626892, Validation Loss: 0.774602472782135\n",
      "Epoch 48: Train Loss: 0.6176517844200134, Validation Loss: 0.7828099727630615\n",
      "Epoch 49: Train Loss: 0.6684093713760376, Validation Loss: 0.7802171111106873\n",
      "Epoch 50: Train Loss: 0.6233991384506226, Validation Loss: 0.7887959480285645\n",
      "Epoch 51: Train Loss: 0.6082883238792419, Validation Loss: 0.7854997515678406\n",
      "Epoch 52: Train Loss: 0.5662712633609772, Validation Loss: 0.7955072522163391\n",
      "Epoch 53: Train Loss: 0.6095203042030335, Validation Loss: 0.7998242378234863\n",
      "Epoch 54: Train Loss: 0.6993462681770325, Validation Loss: 0.8119046688079834\n",
      "Epoch 55: Train Loss: 0.5714184045791626, Validation Loss: 0.8168919682502747\n",
      "Epoch 56: Train Loss: 0.6014968991279602, Validation Loss: 0.8109879493713379\n",
      "Epoch 57: Train Loss: 0.5949004769325257, Validation Loss: 0.8065348863601685\n",
      "Epoch 58: Train Loss: 0.6299355387687683, Validation Loss: 0.8148447275161743\n",
      "Epoch 59: Train Loss: 0.5651532232761383, Validation Loss: 0.821833610534668\n",
      "Epoch 60: Train Loss: 0.5782325565814972, Validation Loss: 0.8289331197738647\n",
      "Epoch 61: Train Loss: 0.5767301499843598, Validation Loss: 0.8397019505500793\n",
      "Epoch 62: Train Loss: 0.5766711592674255, Validation Loss: 0.8273286819458008\n",
      "Epoch 63: Train Loss: 0.5728592872619629, Validation Loss: 0.8327987194061279\n",
      "Epoch 64: Train Loss: 0.5790719747543335, Validation Loss: 0.8389779925346375\n",
      "Epoch 65: Train Loss: 0.6274908781051636, Validation Loss: 0.8441802859306335\n",
      "Epoch 66: Train Loss: 0.564365702867508, Validation Loss: 0.8355193138122559\n",
      "Epoch 67: Train Loss: 0.5972515344619751, Validation Loss: 0.8328614830970764\n",
      "Epoch 68: Train Loss: 0.5834881901741028, Validation Loss: 0.8393470644950867\n",
      "Epoch 69: Train Loss: 0.5431035041809082, Validation Loss: 0.8352192044258118\n",
      "Epoch 70: Train Loss: 0.6035973310470581, Validation Loss: 0.8470330834388733\n",
      "Epoch 71: Train Loss: 0.5463114619255066, Validation Loss: 0.851772665977478\n",
      "Epoch 72: Train Loss: 0.5707492232322693, Validation Loss: 0.8501468896865845\n",
      "Epoch 73: Train Loss: 0.5359511494636535, Validation Loss: 0.8585622310638428\n",
      "Epoch 74: Train Loss: 0.5858177959918975, Validation Loss: 0.8698760271072388\n",
      "Epoch 75: Train Loss: 0.5685755252838135, Validation Loss: 0.8587466478347778\n",
      "Epoch 76: Train Loss: 0.5452665567398072, Validation Loss: 0.8674190640449524\n",
      "Epoch 77: Train Loss: 0.595216166973114, Validation Loss: 0.8672715425491333\n",
      "Epoch 78: Train Loss: 0.5353096067905426, Validation Loss: 0.8666493892669678\n",
      "Epoch 79: Train Loss: 0.5940938949584961, Validation Loss: 0.8754971623420715\n",
      "Epoch 80: Train Loss: 0.6033877372741699, Validation Loss: 0.8771321773529053\n",
      "Epoch 81: Train Loss: 0.5443726539611816, Validation Loss: 0.8878753185272217\n",
      "Epoch 82: Train Loss: 0.5749912261962891, Validation Loss: 0.8897784948348999\n",
      "Epoch 83: Train Loss: 0.6127983212471009, Validation Loss: 0.8884300589561462\n",
      "Epoch 84: Train Loss: 0.5304125785827637, Validation Loss: 0.8976888060569763\n",
      "Epoch 85: Train Loss: 0.5784993886947631, Validation Loss: 0.8908658623695374\n",
      "Epoch 86: Train Loss: 0.49086466431617737, Validation Loss: 0.9014694094657898\n",
      "Epoch 87: Train Loss: 0.5203184962272644, Validation Loss: 0.8978671431541443\n",
      "Epoch 88: Train Loss: 0.5890363335609436, Validation Loss: 0.8949514627456665\n",
      "Epoch 89: Train Loss: 0.5066300153732299, Validation Loss: 0.8962351083755493\n",
      "Epoch 90: Train Loss: 0.5321086823940278, Validation Loss: 0.8957023024559021\n",
      "Epoch 91: Train Loss: 0.5710148811340332, Validation Loss: 0.9000712633132935\n",
      "Epoch 92: Train Loss: 0.5622478723526001, Validation Loss: 0.921663224697113\n",
      "Epoch 93: Train Loss: 0.48186381459236144, Validation Loss: 0.9142234325408936\n",
      "Epoch 94: Train Loss: 0.48396337032318115, Validation Loss: 0.8968899846076965\n",
      "Epoch 95: Train Loss: 0.5217963755130768, Validation Loss: 0.9066705107688904\n",
      "Epoch 96: Train Loss: 0.5961798310279847, Validation Loss: 0.915518581867218\n",
      "Epoch 97: Train Loss: 0.5205656290054321, Validation Loss: 0.9181791543960571\n",
      "Epoch 98: Train Loss: 0.510165560245514, Validation Loss: 0.9230077862739563\n",
      "Epoch 99: Train Loss: 0.5681875944137573, Validation Loss: 0.9219061136245728\n",
      "Epoch 100: Train Loss: 0.5432060956954956, Validation Loss: 0.9143708944320679\n",
      "Epoch 101: Train Loss: 0.5917502999305725, Validation Loss: 0.9273389577865601\n",
      "Epoch 102: Train Loss: 0.5512098908424378, Validation Loss: 0.9326034784317017\n",
      "Epoch 103: Train Loss: 0.5495518922805787, Validation Loss: 0.9303666949272156\n",
      "Epoch 104: Train Loss: 0.5470577836036682, Validation Loss: 0.941298246383667\n",
      "Epoch 105: Train Loss: 0.5050378143787384, Validation Loss: 0.9476258754730225\n",
      "Epoch 106: Train Loss: 0.5145759224891663, Validation Loss: 0.9398794174194336\n",
      "Epoch 107: Train Loss: 0.5430176496505738, Validation Loss: 0.9446229338645935\n",
      "Epoch 108: Train Loss: 0.5161302328109741, Validation Loss: 0.9407810568809509\n",
      "Epoch 109: Train Loss: 0.5442986965179444, Validation Loss: 0.9366411566734314\n",
      "Epoch 110: Train Loss: 0.5360158085823059, Validation Loss: 0.933089017868042\n",
      "Epoch 111: Train Loss: 0.5321602642536163, Validation Loss: 0.9345710873603821\n",
      "Epoch 112: Train Loss: 0.5331406354904175, Validation Loss: 0.9386745095252991\n",
      "Epoch 113: Train Loss: 0.5613031804561615, Validation Loss: 0.9392820596694946\n",
      "Epoch 114: Train Loss: 0.518820583820343, Validation Loss: 0.9461688995361328\n",
      "Epoch 115: Train Loss: 0.5322959303855896, Validation Loss: 0.9296057224273682\n",
      "Epoch 116: Train Loss: 0.5103136003017426, Validation Loss: 0.9410111904144287\n",
      "Epoch 117: Train Loss: 0.5005516588687897, Validation Loss: 0.9385335445404053\n",
      "Epoch 118: Train Loss: 0.5068823456764221, Validation Loss: 0.9469945430755615\n",
      "Epoch 119: Train Loss: 0.47291712164878846, Validation Loss: 0.950490415096283\n",
      "Epoch 120: Train Loss: 0.45832070112228396, Validation Loss: 0.9651996493339539\n",
      "Epoch 121: Train Loss: 0.5009219110012054, Validation Loss: 0.9527080655097961\n",
      "Epoch 122: Train Loss: 0.449508410692215, Validation Loss: 0.966160774230957\n",
      "Epoch 123: Train Loss: 0.49328393340110777, Validation Loss: 0.9672120809555054\n",
      "Epoch 124: Train Loss: 0.5550069034099578, Validation Loss: 0.9677412509918213\n",
      "Epoch 125: Train Loss: 0.5550159633159637, Validation Loss: 0.9661163091659546\n",
      "Epoch 126: Train Loss: 0.5130448281764984, Validation Loss: 0.9702590703964233\n",
      "Epoch 127: Train Loss: 0.5787968337535858, Validation Loss: 0.9933658838272095\n",
      "Epoch 128: Train Loss: 0.5081623196601868, Validation Loss: 0.9641961455345154\n",
      "Epoch 129: Train Loss: 0.53571115732193, Validation Loss: 0.97152179479599\n",
      "Epoch 130: Train Loss: 0.5067520856857299, Validation Loss: 0.9705271124839783\n",
      "Epoch 131: Train Loss: 0.4893042862415314, Validation Loss: 0.9678089618682861\n",
      "Epoch 132: Train Loss: 0.46920338869094846, Validation Loss: 0.97127366065979\n",
      "Epoch 133: Train Loss: 0.46729844212532046, Validation Loss: 0.9861055016517639\n",
      "Epoch 134: Train Loss: 0.49320794343948365, Validation Loss: 0.9866028428077698\n",
      "Epoch 135: Train Loss: 0.49714393019676206, Validation Loss: 0.9950244426727295\n",
      "Epoch 136: Train Loss: 0.5220531940460205, Validation Loss: 0.982976496219635\n",
      "Epoch 137: Train Loss: 0.4438261568546295, Validation Loss: 0.9819765686988831\n",
      "Epoch 138: Train Loss: 0.4660855710506439, Validation Loss: 0.97664874792099\n",
      "Epoch 139: Train Loss: 0.5626927733421325, Validation Loss: 0.9977302551269531\n",
      "Epoch 140: Train Loss: 0.46490576267242434, Validation Loss: 1.011772871017456\n",
      "Epoch 141: Train Loss: 0.5105654776096344, Validation Loss: 0.9904933571815491\n",
      "Epoch 142: Train Loss: 0.4680133581161499, Validation Loss: 1.004828929901123\n",
      "Epoch 143: Train Loss: 0.49819462895393374, Validation Loss: 1.0135397911071777\n",
      "Epoch 144: Train Loss: 0.4931219756603241, Validation Loss: 1.0118422508239746\n",
      "Epoch 145: Train Loss: 0.5027494370937348, Validation Loss: 1.021574854850769\n",
      "Epoch 146: Train Loss: 0.4456368386745453, Validation Loss: 1.0256463289260864\n",
      "Epoch 147: Train Loss: 0.4734616756439209, Validation Loss: 1.0454792976379395\n",
      "Epoch 148: Train Loss: 0.5138461530208588, Validation Loss: 1.0556753873825073\n",
      "Epoch 149: Train Loss: 0.5078729629516602, Validation Loss: 1.0712358951568604\n",
      "Epoch 150: Train Loss: 0.46838732361793517, Validation Loss: 1.047939419746399\n",
      "Epoch 151: Train Loss: 0.5017611861228943, Validation Loss: 1.0528746843338013\n",
      "Epoch 152: Train Loss: 0.4533843398094177, Validation Loss: 1.041822910308838\n",
      "Epoch 153: Train Loss: 0.44448246955871584, Validation Loss: 1.0480767488479614\n",
      "Epoch 154: Train Loss: 0.44562179446220396, Validation Loss: 1.0328470468521118\n",
      "Epoch 155: Train Loss: 0.4912839651107788, Validation Loss: 1.0579267740249634\n",
      "Epoch 156: Train Loss: 0.5244315564632416, Validation Loss: 1.0655055046081543\n",
      "Epoch 157: Train Loss: 0.4641400992870331, Validation Loss: 1.0485122203826904\n",
      "Epoch 158: Train Loss: 0.5317249476909638, Validation Loss: 1.0489068031311035\n",
      "Epoch 159: Train Loss: 0.4323038518428802, Validation Loss: 1.0424920320510864\n",
      "Epoch 160: Train Loss: 0.44461760520935056, Validation Loss: 1.0663371086120605\n",
      "Epoch 161: Train Loss: 0.5284575760364533, Validation Loss: 1.0466042757034302\n",
      "Epoch 162: Train Loss: 0.4322233617305756, Validation Loss: 1.053714632987976\n",
      "Epoch 163: Train Loss: 0.4256655007600784, Validation Loss: 1.0592045783996582\n",
      "Epoch 164: Train Loss: 0.4537302076816559, Validation Loss: 1.0499883890151978\n",
      "Epoch 165: Train Loss: 0.5148684799671173, Validation Loss: 1.0530227422714233\n",
      "Epoch 166: Train Loss: 0.46112443804740905, Validation Loss: 1.0602010488510132\n",
      "Epoch 167: Train Loss: 0.5077466309070587, Validation Loss: 1.0289537906646729\n",
      "Epoch 168: Train Loss: 0.41927247047424315, Validation Loss: 1.027150273323059\n",
      "Epoch 169: Train Loss: 0.5336101293563843, Validation Loss: 1.041406512260437\n",
      "Epoch 170: Train Loss: 0.5481920421123505, Validation Loss: 1.0324269533157349\n",
      "Epoch 171: Train Loss: 0.604507440328598, Validation Loss: 1.032706379890442\n",
      "Epoch 172: Train Loss: 0.4444027662277222, Validation Loss: 1.03526771068573\n",
      "Epoch 173: Train Loss: 0.47602368593215943, Validation Loss: 1.0169345140457153\n",
      "Epoch 174: Train Loss: 0.45479140281677244, Validation Loss: 1.0347199440002441\n",
      "Epoch 175: Train Loss: 0.49280489087104795, Validation Loss: 1.0413286685943604\n",
      "Epoch 176: Train Loss: 0.45163328647613527, Validation Loss: 1.0563230514526367\n",
      "Epoch 177: Train Loss: 0.4450934946537018, Validation Loss: 1.0385416746139526\n",
      "Epoch 178: Train Loss: 0.42127643823623656, Validation Loss: 1.0515879392623901\n",
      "Epoch 179: Train Loss: 0.44707874655723573, Validation Loss: 1.0317928791046143\n",
      "Epoch 180: Train Loss: 0.40527451038360596, Validation Loss: 1.0557793378829956\n",
      "Epoch 181: Train Loss: 0.5449654281139373, Validation Loss: 1.0945067405700684\n",
      "Epoch 182: Train Loss: 0.4112086117267609, Validation Loss: 1.1258893013000488\n",
      "Epoch 183: Train Loss: 0.44267246723175047, Validation Loss: 1.1278080940246582\n",
      "Epoch 184: Train Loss: 0.4469658136367798, Validation Loss: 1.1360912322998047\n",
      "Epoch 185: Train Loss: 0.43752545714378355, Validation Loss: 1.1157252788543701\n",
      "Epoch 186: Train Loss: 0.5067573547363281, Validation Loss: 1.0953917503356934\n",
      "Epoch 187: Train Loss: 0.45585463643074037, Validation Loss: 1.0902268886566162\n",
      "Epoch 188: Train Loss: 0.438532418012619, Validation Loss: 1.1039599180221558\n",
      "Epoch 189: Train Loss: 0.4952016890048981, Validation Loss: 1.1177852153778076\n",
      "Epoch 190: Train Loss: 0.4321856081485748, Validation Loss: 1.1111246347427368\n",
      "Epoch 191: Train Loss: 0.4362762153148651, Validation Loss: 1.1170146465301514\n",
      "Epoch 192: Train Loss: 0.40092883408069613, Validation Loss: 1.1297892332077026\n",
      "Epoch 193: Train Loss: 0.507910531759262, Validation Loss: 1.1519441604614258\n",
      "Epoch 194: Train Loss: 0.43562266826629636, Validation Loss: 1.1617106199264526\n",
      "Epoch 195: Train Loss: 0.4556079626083374, Validation Loss: 1.1225228309631348\n",
      "Epoch 196: Train Loss: 0.4153048753738403, Validation Loss: 1.1448228359222412\n",
      "Epoch 197: Train Loss: 0.43205506801605226, Validation Loss: 1.1576911211013794\n",
      "Epoch 198: Train Loss: 0.5133653700351715, Validation Loss: 1.160771369934082\n",
      "Epoch 199: Train Loss: 0.47132885456085205, Validation Loss: 1.1335052251815796\n",
      "Epoch 200: Train Loss: 0.43063579201698304, Validation Loss: 1.1722646951675415\n",
      "Epoch 201: Train Loss: 0.4472895681858063, Validation Loss: 1.1707367897033691\n",
      "Epoch 202: Train Loss: 0.41575218439102174, Validation Loss: 1.1673310995101929\n",
      "Epoch 203: Train Loss: 0.4255423188209534, Validation Loss: 1.182639241218567\n",
      "Epoch 204: Train Loss: 0.36490934491157534, Validation Loss: 1.1847071647644043\n",
      "Epoch 205: Train Loss: 0.4435857057571411, Validation Loss: 1.1804933547973633\n",
      "Epoch 206: Train Loss: 0.4299479901790619, Validation Loss: 1.1704589128494263\n",
      "Epoch 207: Train Loss: 0.4273634493350983, Validation Loss: 1.1848489046096802\n",
      "Epoch 208: Train Loss: 0.4343271732330322, Validation Loss: 1.2054178714752197\n",
      "Epoch 209: Train Loss: 0.38009893894195557, Validation Loss: 1.2123498916625977\n",
      "Epoch 210: Train Loss: 0.41896108984947206, Validation Loss: 1.2194212675094604\n",
      "Epoch 211: Train Loss: 0.42935836911201475, Validation Loss: 1.2183958292007446\n",
      "Epoch 212: Train Loss: 0.46446395516395567, Validation Loss: 1.2202790975570679\n",
      "Epoch 213: Train Loss: 0.38313413262367246, Validation Loss: 1.2357909679412842\n",
      "Epoch 214: Train Loss: 0.4106377065181732, Validation Loss: 1.1845831871032715\n",
      "Epoch 215: Train Loss: 0.40564579963684083, Validation Loss: 1.177353858947754\n",
      "Epoch 216: Train Loss: 0.4702491521835327, Validation Loss: 1.1774823665618896\n",
      "Epoch 217: Train Loss: 0.38494077920913694, Validation Loss: 1.1977628469467163\n",
      "Epoch 218: Train Loss: 0.3862208783626556, Validation Loss: 1.1801799535751343\n",
      "Epoch 219: Train Loss: 0.3912427306175232, Validation Loss: 1.1679507493972778\n",
      "Epoch 220: Train Loss: 0.4623239994049072, Validation Loss: 1.1748782396316528\n",
      "Epoch 221: Train Loss: 0.3927586257457733, Validation Loss: 1.197600245475769\n",
      "Epoch 222: Train Loss: 0.4106967210769653, Validation Loss: 1.2006348371505737\n",
      "Epoch 223: Train Loss: 0.365197080373764, Validation Loss: 1.222602367401123\n",
      "Epoch 224: Train Loss: 0.415545380115509, Validation Loss: 1.2444535493850708\n",
      "Epoch 225: Train Loss: 0.43002825379371645, Validation Loss: 1.2601542472839355\n",
      "Epoch 226: Train Loss: 0.46879451870918276, Validation Loss: 1.2348487377166748\n",
      "Epoch 227: Train Loss: 0.3775743544101715, Validation Loss: 1.1773027181625366\n",
      "Epoch 228: Train Loss: 0.43954300284385683, Validation Loss: 1.2044779062271118\n",
      "Epoch 229: Train Loss: 0.39265053272247313, Validation Loss: 1.2041133642196655\n",
      "Epoch 230: Train Loss: 0.39631468057632446, Validation Loss: 1.230008602142334\n",
      "Epoch 231: Train Loss: 0.41454133987426756, Validation Loss: 1.2321157455444336\n",
      "Epoch 232: Train Loss: 0.368956932425499, Validation Loss: 1.2169736623764038\n",
      "Epoch 233: Train Loss: 0.4066775381565094, Validation Loss: 1.2062270641326904\n",
      "Epoch 234: Train Loss: 0.4378067433834076, Validation Loss: 1.2479891777038574\n",
      "Epoch 235: Train Loss: 0.48068174719810486, Validation Loss: 1.2757413387298584\n",
      "Epoch 236: Train Loss: 0.3689501523971558, Validation Loss: 1.2387564182281494\n",
      "Epoch 237: Train Loss: 0.41024749875068667, Validation Loss: 1.2590535879135132\n",
      "Epoch 238: Train Loss: 0.43031640648841857, Validation Loss: 1.2469764947891235\n",
      "Epoch 239: Train Loss: 0.418230676651001, Validation Loss: 1.2522755861282349\n",
      "Epoch 240: Train Loss: 0.3953726053237915, Validation Loss: 1.2527776956558228\n",
      "Epoch 241: Train Loss: 0.3561912953853607, Validation Loss: 1.2607543468475342\n",
      "Epoch 242: Train Loss: 0.39115601778030396, Validation Loss: 1.2889448404312134\n",
      "Epoch 243: Train Loss: 0.45431833267211913, Validation Loss: 1.297489881515503\n",
      "Epoch 244: Train Loss: 0.374796587228775, Validation Loss: 1.2397576570510864\n",
      "Epoch 245: Train Loss: 0.42010743618011476, Validation Loss: 1.268800139427185\n",
      "Epoch 246: Train Loss: 0.4200452148914337, Validation Loss: 1.2350915670394897\n",
      "Epoch 247: Train Loss: 0.3669974207878113, Validation Loss: 1.2567822933197021\n",
      "Epoch 248: Train Loss: 0.3839771032333374, Validation Loss: 1.2659039497375488\n",
      "Epoch 249: Train Loss: 0.4224728763103485, Validation Loss: 1.259321689605713\n",
      "Epoch 250: Train Loss: 0.40445658564567566, Validation Loss: 1.2487331628799438\n",
      "Epoch 251: Train Loss: 0.41721194982528687, Validation Loss: 1.2500884532928467\n",
      "Epoch 252: Train Loss: 0.38806367516517637, Validation Loss: 1.2510873079299927\n",
      "Epoch 253: Train Loss: 0.38693876266479493, Validation Loss: 1.2819494009017944\n",
      "Epoch 254: Train Loss: 0.3568975031375885, Validation Loss: 1.2973566055297852\n",
      "Epoch 255: Train Loss: 0.3506101369857788, Validation Loss: 1.296683669090271\n",
      "Epoch 256: Train Loss: 0.3421940803527832, Validation Loss: 1.2746920585632324\n",
      "Epoch 257: Train Loss: 0.39396911263465884, Validation Loss: 1.2764623165130615\n",
      "Epoch 258: Train Loss: 0.34864846169948577, Validation Loss: 1.2896493673324585\n",
      "Epoch 259: Train Loss: 0.3133280426263809, Validation Loss: 1.299139380455017\n",
      "Epoch 260: Train Loss: 0.3407150059938431, Validation Loss: 1.3004634380340576\n",
      "Epoch 261: Train Loss: 0.35281681418418886, Validation Loss: 1.2964650392532349\n",
      "Epoch 262: Train Loss: 0.41695229411125184, Validation Loss: 1.3301523923873901\n",
      "Epoch 263: Train Loss: 0.3968265175819397, Validation Loss: 1.281181812286377\n",
      "Epoch 264: Train Loss: 0.3581624150276184, Validation Loss: 1.2819230556488037\n",
      "Epoch 265: Train Loss: 0.4032946050167084, Validation Loss: 1.2869882583618164\n",
      "Epoch 266: Train Loss: 0.42794435620307925, Validation Loss: 1.2830175161361694\n",
      "Epoch 267: Train Loss: 0.4196825444698334, Validation Loss: 1.3108103275299072\n",
      "Epoch 268: Train Loss: 0.34831578731536866, Validation Loss: 1.2761249542236328\n",
      "Epoch 269: Train Loss: 0.3715791702270508, Validation Loss: 1.3039220571517944\n",
      "Epoch 270: Train Loss: 0.39246432185173036, Validation Loss: 1.2863038778305054\n",
      "Epoch 271: Train Loss: 0.43040561079978945, Validation Loss: 1.248081922531128\n",
      "Epoch 272: Train Loss: 0.35630578398704527, Validation Loss: 1.2501975297927856\n",
      "Epoch 273: Train Loss: 0.3362167000770569, Validation Loss: 1.2537330389022827\n",
      "Epoch 274: Train Loss: 0.368262255191803, Validation Loss: 1.2682844400405884\n",
      "Epoch 275: Train Loss: 0.3484447836875916, Validation Loss: 1.253745198249817\n",
      "Epoch 276: Train Loss: 0.40881494283676145, Validation Loss: 1.2586524486541748\n",
      "Epoch 277: Train Loss: 0.3709241569042206, Validation Loss: 1.298986554145813\n",
      "Epoch 278: Train Loss: 0.39587379693984986, Validation Loss: 1.321253776550293\n",
      "Epoch 279: Train Loss: 0.3423666894435883, Validation Loss: 1.3133174180984497\n",
      "Epoch 280: Train Loss: 0.41799221038818357, Validation Loss: 1.330428123474121\n",
      "Epoch 281: Train Loss: 0.4103917717933655, Validation Loss: 1.3584623336791992\n",
      "Epoch 282: Train Loss: 0.43631134629249574, Validation Loss: 1.3519753217697144\n",
      "Epoch 283: Train Loss: 0.37754846811294557, Validation Loss: 1.3849619626998901\n",
      "Epoch 284: Train Loss: 0.4022130787372589, Validation Loss: 1.3713767528533936\n",
      "Epoch 285: Train Loss: 0.3548231482505798, Validation Loss: 1.3756028413772583\n",
      "Epoch 286: Train Loss: 0.3512055814266205, Validation Loss: 1.379040002822876\n",
      "Epoch 287: Train Loss: 0.4292327105998993, Validation Loss: 1.3588138818740845\n",
      "Epoch 288: Train Loss: 0.3090779662132263, Validation Loss: 1.3875652551651\n",
      "Epoch 289: Train Loss: 0.43160015940666197, Validation Loss: 1.3893945217132568\n",
      "Epoch 290: Train Loss: 0.45164549350738525, Validation Loss: 1.409751296043396\n",
      "Epoch 291: Train Loss: 0.3676668882369995, Validation Loss: 1.37357759475708\n",
      "Epoch 292: Train Loss: 0.31822538673877715, Validation Loss: 1.3422075510025024\n",
      "Epoch 293: Train Loss: 0.40420445799827576, Validation Loss: 1.367392897605896\n",
      "Epoch 294: Train Loss: 0.3217193424701691, Validation Loss: 1.3050404787063599\n",
      "Epoch 295: Train Loss: 0.33765188455581663, Validation Loss: 1.3401527404785156\n",
      "Epoch 296: Train Loss: 0.3087104171514511, Validation Loss: 1.3520509004592896\n",
      "Epoch 297: Train Loss: 0.3231500208377838, Validation Loss: 1.3471266031265259\n",
      "Epoch 298: Train Loss: 0.3286789834499359, Validation Loss: 1.3567235469818115\n",
      "Epoch 299: Train Loss: 0.3459201276302338, Validation Loss: 1.36833655834198\n",
      "Epoch 300: Train Loss: 0.34936497211456297, Validation Loss: 1.331468939781189\n",
      "Epoch 301: Train Loss: 0.3201982617378235, Validation Loss: 1.3875843286514282\n",
      "Epoch 302: Train Loss: 0.33775150775909424, Validation Loss: 1.3751857280731201\n",
      "Epoch 303: Train Loss: 0.3155754148960114, Validation Loss: 1.4108667373657227\n",
      "Epoch 304: Train Loss: 0.3660241663455963, Validation Loss: 1.3993581533432007\n",
      "Epoch 305: Train Loss: 0.3985571593046188, Validation Loss: 1.4545748233795166\n",
      "Epoch 306: Train Loss: 0.319742488861084, Validation Loss: 1.4374456405639648\n",
      "Epoch 307: Train Loss: 0.3289720952510834, Validation Loss: 1.4414467811584473\n",
      "Epoch 308: Train Loss: 0.32464869916439054, Validation Loss: 1.4473823308944702\n",
      "Epoch 309: Train Loss: 0.29573714137077334, Validation Loss: 1.4550132751464844\n",
      "Epoch 310: Train Loss: 0.32519901990890504, Validation Loss: 1.441368818283081\n",
      "Epoch 311: Train Loss: 0.4267900586128235, Validation Loss: 1.4857877492904663\n",
      "Epoch 312: Train Loss: 0.28675269782543183, Validation Loss: 1.4461153745651245\n",
      "Epoch 313: Train Loss: 0.32843523621559145, Validation Loss: 1.4512990713119507\n",
      "Epoch 314: Train Loss: 0.3112674355506897, Validation Loss: 1.4338698387145996\n",
      "Epoch 315: Train Loss: 0.2995195001363754, Validation Loss: 1.4287290573120117\n",
      "Epoch 316: Train Loss: 0.3124637633562088, Validation Loss: 1.453338623046875\n",
      "Epoch 317: Train Loss: 0.46788687705993653, Validation Loss: 1.44974684715271\n",
      "Epoch 318: Train Loss: 0.30274049043655393, Validation Loss: 1.474901556968689\n",
      "Epoch 319: Train Loss: 0.34592517614364626, Validation Loss: 1.453505516052246\n",
      "Epoch 320: Train Loss: 0.3469159245491028, Validation Loss: 1.4554522037506104\n",
      "Epoch 321: Train Loss: 0.30116295218467715, Validation Loss: 1.3922595977783203\n",
      "Epoch 322: Train Loss: 0.34115243554115293, Validation Loss: 1.4526243209838867\n",
      "Epoch 323: Train Loss: 0.33512906432151796, Validation Loss: 1.4612191915512085\n",
      "Epoch 324: Train Loss: 0.28416165709495544, Validation Loss: 1.4888486862182617\n",
      "Epoch 325: Train Loss: 0.2934111148118973, Validation Loss: 1.5247206687927246\n",
      "Epoch 326: Train Loss: 0.37186025381088256, Validation Loss: 1.5456348657608032\n",
      "Epoch 327: Train Loss: 0.3679324328899384, Validation Loss: 1.536413550376892\n",
      "Epoch 328: Train Loss: 0.3690100222826004, Validation Loss: 1.5326067209243774\n",
      "Epoch 329: Train Loss: 0.3732367157936096, Validation Loss: 1.4848624467849731\n",
      "Epoch 330: Train Loss: 0.340877366065979, Validation Loss: 1.4468059539794922\n",
      "Epoch 331: Train Loss: 0.33503402173519137, Validation Loss: 1.3810466527938843\n",
      "Epoch 332: Train Loss: 0.3291201710700989, Validation Loss: 1.4101381301879883\n",
      "Epoch 333: Train Loss: 0.2803045570850372, Validation Loss: 1.4236865043640137\n",
      "Epoch 334: Train Loss: 0.402886363863945, Validation Loss: 1.4581084251403809\n",
      "Epoch 335: Train Loss: 0.33194093108177186, Validation Loss: 1.4824302196502686\n",
      "Epoch 336: Train Loss: 0.2810484513640404, Validation Loss: 1.5127272605895996\n",
      "Epoch 337: Train Loss: 0.2622684001922607, Validation Loss: 1.5153557062149048\n",
      "Epoch 338: Train Loss: 0.32325305342674254, Validation Loss: 1.5060447454452515\n",
      "Epoch 339: Train Loss: 0.3096087336540222, Validation Loss: 1.4824068546295166\n",
      "Epoch 340: Train Loss: 0.27821520566940305, Validation Loss: 1.4744243621826172\n",
      "Epoch 341: Train Loss: 0.3379206657409668, Validation Loss: 1.5286165475845337\n",
      "Epoch 342: Train Loss: 0.3055979013442993, Validation Loss: 1.5560251474380493\n",
      "Epoch 343: Train Loss: 0.3459067285060883, Validation Loss: 1.4904239177703857\n",
      "Epoch 344: Train Loss: 0.2956467866897583, Validation Loss: 1.5141102075576782\n",
      "Epoch 345: Train Loss: 0.2814031600952148, Validation Loss: 1.4958593845367432\n",
      "Epoch 346: Train Loss: 0.27452883720397947, Validation Loss: 1.5042600631713867\n",
      "Epoch 347: Train Loss: 0.33446912467479706, Validation Loss: 1.4870964288711548\n",
      "Epoch 348: Train Loss: 0.27120943665504454, Validation Loss: 1.4398887157440186\n",
      "Epoch 349: Train Loss: 0.38209846019744875, Validation Loss: 1.4691457748413086\n",
      "Epoch 350: Train Loss: 0.320293527841568, Validation Loss: 1.474331259727478\n",
      "Epoch 351: Train Loss: 0.2574600875377655, Validation Loss: 1.5225156545639038\n",
      "Epoch 352: Train Loss: 0.29709585905075075, Validation Loss: 1.5276685953140259\n",
      "Epoch 353: Train Loss: 0.3499669909477234, Validation Loss: 1.5795496702194214\n",
      "Epoch 354: Train Loss: 0.278739920258522, Validation Loss: 1.533057689666748\n",
      "Epoch 355: Train Loss: 0.25287153422832487, Validation Loss: 1.5858725309371948\n",
      "Epoch 356: Train Loss: 0.293554750084877, Validation Loss: 1.6313831806182861\n",
      "Epoch 357: Train Loss: 0.32999414801597593, Validation Loss: 1.5966567993164062\n",
      "Epoch 358: Train Loss: 0.28252305686473844, Validation Loss: 1.6076478958129883\n",
      "Epoch 359: Train Loss: 0.30199731290340426, Validation Loss: 1.5874682664871216\n",
      "Epoch 360: Train Loss: 0.2721641302108765, Validation Loss: 1.6079436540603638\n",
      "Epoch 361: Train Loss: 0.25598383843898775, Validation Loss: 1.5593734979629517\n",
      "Epoch 362: Train Loss: 0.28771595358848573, Validation Loss: 1.595592737197876\n",
      "Epoch 363: Train Loss: 0.3560280740261078, Validation Loss: 1.5728607177734375\n",
      "Epoch 364: Train Loss: 0.2650997698307037, Validation Loss: 1.535362958908081\n",
      "Epoch 365: Train Loss: 0.27513477206230164, Validation Loss: 1.557407021522522\n",
      "Epoch 366: Train Loss: 0.2727579176425934, Validation Loss: 1.5543261766433716\n",
      "Epoch 367: Train Loss: 0.26611825823783875, Validation Loss: 1.6002074480056763\n",
      "Epoch 368: Train Loss: 0.2646558225154877, Validation Loss: 1.5862032175064087\n",
      "Epoch 369: Train Loss: 0.24831652343273164, Validation Loss: 1.580161213874817\n",
      "Epoch 370: Train Loss: 0.44411928951740265, Validation Loss: 1.568810224533081\n",
      "Epoch 371: Train Loss: 0.3500933349132538, Validation Loss: 1.5341399908065796\n",
      "Epoch 372: Train Loss: 0.2594076842069626, Validation Loss: 1.4868520498275757\n",
      "Epoch 373: Train Loss: 0.2557463526725769, Validation Loss: 1.4676192998886108\n",
      "Epoch 374: Train Loss: 0.2715966284275055, Validation Loss: 1.5226030349731445\n",
      "Epoch 375: Train Loss: 0.25255225151777266, Validation Loss: 1.5785826444625854\n",
      "Epoch 376: Train Loss: 0.27535001933574677, Validation Loss: 1.5891739130020142\n",
      "Epoch 377: Train Loss: 0.24145630300045012, Validation Loss: 1.563806414604187\n",
      "Epoch 378: Train Loss: 0.310807991027832, Validation Loss: 1.506792664527893\n",
      "Epoch 379: Train Loss: 0.2777511656284332, Validation Loss: 1.4883626699447632\n",
      "Epoch 380: Train Loss: 0.3440736889839172, Validation Loss: 1.5050883293151855\n",
      "Epoch 381: Train Loss: 0.2780404180288315, Validation Loss: 1.5341590642929077\n",
      "Epoch 382: Train Loss: 0.3001385420560837, Validation Loss: 1.581390619277954\n",
      "Epoch 383: Train Loss: 0.22067374959588051, Validation Loss: 1.5877727270126343\n",
      "Epoch 384: Train Loss: 0.24239850267767907, Validation Loss: 1.5682094097137451\n",
      "Epoch 385: Train Loss: 0.20749636888504028, Validation Loss: 1.5962611436843872\n",
      "Epoch 386: Train Loss: 0.26491294503211976, Validation Loss: 1.5621566772460938\n",
      "Epoch 387: Train Loss: 0.3490646779537201, Validation Loss: 1.557807445526123\n",
      "Epoch 388: Train Loss: 0.2880696922540665, Validation Loss: 1.6127070188522339\n",
      "Epoch 389: Train Loss: 0.2690862476825714, Validation Loss: 1.5905948877334595\n",
      "Epoch 390: Train Loss: 0.31056922674179077, Validation Loss: 1.5842067003250122\n",
      "Epoch 391: Train Loss: 0.23748189210891724, Validation Loss: 1.616356611251831\n",
      "Epoch 392: Train Loss: 0.26895259916782377, Validation Loss: 1.6180238723754883\n",
      "Epoch 393: Train Loss: 0.38057093024253846, Validation Loss: 1.6544220447540283\n",
      "Epoch 394: Train Loss: 0.25324890315532683, Validation Loss: 1.708178162574768\n",
      "Epoch 395: Train Loss: 0.2690118461847305, Validation Loss: 1.725251317024231\n",
      "Epoch 396: Train Loss: 0.2540806919336319, Validation Loss: 1.6473017930984497\n",
      "Epoch 397: Train Loss: 0.2707770109176636, Validation Loss: 1.6247825622558594\n",
      "Epoch 398: Train Loss: 0.3074999898672104, Validation Loss: 1.5940691232681274\n",
      "Epoch 399: Train Loss: 0.2670339852571487, Validation Loss: 1.6423075199127197\n",
      "Epoch 400: Train Loss: 0.23598865568637847, Validation Loss: 1.6506727933883667\n",
      "Epoch 401: Train Loss: 0.2860946774482727, Validation Loss: 1.7083637714385986\n",
      "Epoch 402: Train Loss: 0.2642016619443893, Validation Loss: 1.6803617477416992\n",
      "Epoch 403: Train Loss: 0.2605270564556122, Validation Loss: 1.6318469047546387\n",
      "Epoch 404: Train Loss: 0.20972828269004823, Validation Loss: 1.5547846555709839\n",
      "Epoch 405: Train Loss: 0.2305043250322342, Validation Loss: 1.6008148193359375\n",
      "Epoch 406: Train Loss: 0.30876208245754244, Validation Loss: 1.6132316589355469\n",
      "Epoch 407: Train Loss: 0.27807243168354034, Validation Loss: 1.6345618963241577\n",
      "Epoch 408: Train Loss: 0.24501124322414397, Validation Loss: 1.641225814819336\n",
      "Epoch 409: Train Loss: 0.25649591982364656, Validation Loss: 1.603196144104004\n",
      "Epoch 410: Train Loss: 0.23615662455558778, Validation Loss: 1.6006183624267578\n",
      "Epoch 411: Train Loss: 0.24565005600452422, Validation Loss: 1.6794325113296509\n",
      "Epoch 412: Train Loss: 0.31958909928798673, Validation Loss: 1.6021548509597778\n",
      "Epoch 413: Train Loss: 0.3342883139848709, Validation Loss: 1.6274641752243042\n",
      "Epoch 414: Train Loss: 0.223453027009964, Validation Loss: 1.6694408655166626\n",
      "Epoch 415: Train Loss: 0.21768995821475984, Validation Loss: 1.5806708335876465\n",
      "Epoch 416: Train Loss: 0.2255725681781769, Validation Loss: 1.6168574094772339\n",
      "Epoch 417: Train Loss: 0.21779985576868058, Validation Loss: 1.6726824045181274\n",
      "Epoch 418: Train Loss: 0.3164555847644806, Validation Loss: 1.706944227218628\n",
      "Epoch 419: Train Loss: 0.23718541711568833, Validation Loss: 1.7075674533843994\n",
      "Epoch 420: Train Loss: 0.21965291798114778, Validation Loss: 1.6753273010253906\n",
      "Epoch 421: Train Loss: 0.23255842924118042, Validation Loss: 1.6499207019805908\n",
      "Epoch 422: Train Loss: 0.2287539780139923, Validation Loss: 1.656103491783142\n",
      "Epoch 423: Train Loss: 0.2649887800216675, Validation Loss: 1.7034450769424438\n",
      "Epoch 424: Train Loss: 0.2660723298788071, Validation Loss: 1.724969744682312\n",
      "Epoch 425: Train Loss: 0.23002120554447175, Validation Loss: 1.711595058441162\n",
      "Epoch 426: Train Loss: 0.18793156743049622, Validation Loss: 1.7135037183761597\n",
      "Epoch 427: Train Loss: 0.30126664936542513, Validation Loss: 1.7262706756591797\n",
      "Epoch 428: Train Loss: 0.3030106693506241, Validation Loss: 1.7617496252059937\n",
      "Epoch 429: Train Loss: 0.24656572043895722, Validation Loss: 1.7252116203308105\n",
      "Epoch 430: Train Loss: 0.22598266899585723, Validation Loss: 1.6973806619644165\n",
      "Epoch 431: Train Loss: 0.2518178552389145, Validation Loss: 1.6410871744155884\n",
      "Epoch 432: Train Loss: 0.2207875818014145, Validation Loss: 1.6723737716674805\n",
      "Epoch 433: Train Loss: 0.3331036537885666, Validation Loss: 1.688772201538086\n",
      "Epoch 434: Train Loss: 0.22368772476911544, Validation Loss: 1.751112699508667\n",
      "Epoch 435: Train Loss: 0.21820171177387238, Validation Loss: 1.7652870416641235\n",
      "Epoch 436: Train Loss: 0.2558646023273468, Validation Loss: 1.778246521949768\n",
      "Epoch 437: Train Loss: 0.33760771453380584, Validation Loss: 1.794972538948059\n",
      "Epoch 438: Train Loss: 0.18660275712609292, Validation Loss: 1.7641242742538452\n",
      "Epoch 439: Train Loss: 0.3288838267326355, Validation Loss: 1.7803699970245361\n",
      "Epoch 440: Train Loss: 0.3827455252408981, Validation Loss: 1.7668297290802002\n",
      "Epoch 441: Train Loss: 0.1942674219608307, Validation Loss: 1.7891833782196045\n",
      "Epoch 442: Train Loss: 0.21046306192874908, Validation Loss: 1.7248928546905518\n",
      "Epoch 443: Train Loss: 0.2369699478149414, Validation Loss: 1.7805510759353638\n",
      "Epoch 444: Train Loss: 0.25445683002471925, Validation Loss: 1.8283445835113525\n",
      "Epoch 445: Train Loss: 0.21801483631134033, Validation Loss: 1.852929949760437\n",
      "Epoch 446: Train Loss: 0.2653944194316864, Validation Loss: 1.815478801727295\n",
      "Epoch 447: Train Loss: 0.2812419146299362, Validation Loss: 1.8222346305847168\n",
      "Epoch 448: Train Loss: 0.19452450424432755, Validation Loss: 1.8002995252609253\n",
      "Epoch 449: Train Loss: 0.3167974889278412, Validation Loss: 1.8554561138153076\n",
      "Epoch 450: Train Loss: 0.2447115182876587, Validation Loss: 1.822269320487976\n",
      "Epoch 451: Train Loss: 0.32800151109695436, Validation Loss: 1.770288348197937\n",
      "Epoch 452: Train Loss: 0.21516506969928742, Validation Loss: 1.784275770187378\n",
      "Epoch 453: Train Loss: 0.28129159808158877, Validation Loss: 1.7920219898223877\n",
      "Epoch 454: Train Loss: 0.23775120973587036, Validation Loss: 1.728332757949829\n",
      "Epoch 455: Train Loss: 0.23881557285785676, Validation Loss: 1.7840149402618408\n",
      "Epoch 456: Train Loss: 0.3032973498106003, Validation Loss: 1.8076997995376587\n",
      "Epoch 457: Train Loss: 0.29978731870651243, Validation Loss: 1.8348445892333984\n",
      "Epoch 458: Train Loss: 0.22971462607383727, Validation Loss: 1.8247120380401611\n",
      "Epoch 459: Train Loss: 0.19018294364213945, Validation Loss: 1.8426200151443481\n",
      "Epoch 460: Train Loss: 0.21089809834957124, Validation Loss: 1.7671282291412354\n",
      "Epoch 461: Train Loss: 0.23239847719669343, Validation Loss: 1.7894396781921387\n",
      "Epoch 462: Train Loss: 0.21598716378211974, Validation Loss: 1.82566499710083\n",
      "Epoch 463: Train Loss: 0.2456710547208786, Validation Loss: 1.7594534158706665\n",
      "Epoch 464: Train Loss: 0.23541053533554077, Validation Loss: 1.7727465629577637\n",
      "Epoch 465: Train Loss: 0.1884717106819153, Validation Loss: 1.7448668479919434\n",
      "Epoch 466: Train Loss: 0.1750331774353981, Validation Loss: 1.742673397064209\n",
      "Epoch 467: Train Loss: 0.19260618537664415, Validation Loss: 1.7873400449752808\n",
      "Epoch 468: Train Loss: 0.19698776453733444, Validation Loss: 1.8188384771347046\n",
      "Epoch 469: Train Loss: 0.22244610488414765, Validation Loss: 1.889862298965454\n",
      "Epoch 470: Train Loss: 0.2885799705982208, Validation Loss: 1.9319558143615723\n",
      "Epoch 471: Train Loss: 0.2360278695821762, Validation Loss: 1.9656333923339844\n",
      "Epoch 472: Train Loss: 0.24786331951618196, Validation Loss: 1.7956494092941284\n",
      "Epoch 473: Train Loss: 0.20410520881414412, Validation Loss: 1.8005908727645874\n",
      "Epoch 474: Train Loss: 0.1742730811238289, Validation Loss: 1.8242785930633545\n",
      "Epoch 475: Train Loss: 0.2720657497644424, Validation Loss: 1.8067091703414917\n",
      "Epoch 476: Train Loss: 0.24845650494098664, Validation Loss: 1.7122870683670044\n",
      "Epoch 477: Train Loss: 0.206846284866333, Validation Loss: 1.784058690071106\n",
      "Epoch 478: Train Loss: 0.21792328655719756, Validation Loss: 1.8436732292175293\n",
      "Epoch 479: Train Loss: 0.20413819402456285, Validation Loss: 1.8569961786270142\n",
      "Epoch 480: Train Loss: 0.3102790355682373, Validation Loss: 1.8738166093826294\n",
      "Epoch 481: Train Loss: 0.18658625818789004, Validation Loss: 1.8392256498336792\n",
      "Epoch 482: Train Loss: 0.2369242489337921, Validation Loss: 1.8084720373153687\n",
      "Epoch 483: Train Loss: 0.21590778827667237, Validation Loss: 1.760934829711914\n",
      "Epoch 484: Train Loss: 0.18599035292863847, Validation Loss: 1.8496466875076294\n",
      "Epoch 485: Train Loss: 0.4108498692512512, Validation Loss: 1.8619933128356934\n",
      "Epoch 486: Train Loss: 0.17293906062841416, Validation Loss: 1.8702852725982666\n",
      "Epoch 487: Train Loss: 0.2592476695775986, Validation Loss: 1.9414775371551514\n",
      "Epoch 488: Train Loss: 0.20479261577129365, Validation Loss: 1.962561011314392\n",
      "Epoch 489: Train Loss: 0.24802341163158417, Validation Loss: 1.9577217102050781\n",
      "Epoch 490: Train Loss: 0.18492831885814667, Validation Loss: 1.9897799491882324\n",
      "Epoch 491: Train Loss: 0.2866846203804016, Validation Loss: 1.9439697265625\n",
      "Epoch 492: Train Loss: 0.2153356373310089, Validation Loss: 1.8473525047302246\n",
      "Epoch 493: Train Loss: 0.17621397227048874, Validation Loss: 1.8646540641784668\n",
      "Epoch 494: Train Loss: 0.27811104357242583, Validation Loss: 1.8449580669403076\n",
      "Epoch 495: Train Loss: 0.18055042624473572, Validation Loss: 1.9068529605865479\n",
      "Epoch 496: Train Loss: 0.24141394793987275, Validation Loss: 1.9752124547958374\n",
      "Epoch 497: Train Loss: 0.186494317650795, Validation Loss: 1.9471248388290405\n",
      "Epoch 498: Train Loss: 0.1869244784116745, Validation Loss: 1.9420993328094482\n",
      "Epoch 499: Train Loss: 0.24358786940574645, Validation Loss: 1.920568823814392\n",
      "Fold 8 Metrics:\n",
      "Accuracy: 0.09090909090909091, Precision: 0.0, Recall: 0.0, F1-score: 0.0, AUC: 0.09999999999999998\n",
      "Confusion Matrix:\n",
      "[[1 4]\n",
      " [6 0]]\n",
      "Completed fold 8\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples from subject 4 to test set\n",
      "Adding 6 truth samples from subject 4 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 250)\n",
      "(132, 65, 250)\n",
      "Epoch 0: Train Loss: 0.7267539739608765, Validation Loss: 0.6976959109306335\n",
      "Epoch 1: Train Loss: 0.6743750691413879, Validation Loss: 0.7045459151268005\n",
      "Epoch 2: Train Loss: 0.700054943561554, Validation Loss: 0.7136449813842773\n",
      "Epoch 3: Train Loss: 0.6416445195674896, Validation Loss: 0.723038375377655\n",
      "Epoch 4: Train Loss: 0.6206789255142212, Validation Loss: 0.7379636764526367\n",
      "Epoch 5: Train Loss: 0.6813618183135987, Validation Loss: 0.7415686845779419\n",
      "Epoch 6: Train Loss: 0.6391818284988403, Validation Loss: 0.7447344660758972\n",
      "Epoch 7: Train Loss: 0.6793468117713928, Validation Loss: 0.7474162578582764\n",
      "Epoch 8: Train Loss: 0.6615014433860779, Validation Loss: 0.7445686459541321\n",
      "Epoch 9: Train Loss: 0.6195655584335327, Validation Loss: 0.7454793453216553\n",
      "Epoch 10: Train Loss: 0.7310758471488953, Validation Loss: 0.7582347989082336\n",
      "Epoch 11: Train Loss: 0.6536043405532836, Validation Loss: 0.7682309150695801\n",
      "Epoch 12: Train Loss: 0.6493669748306274, Validation Loss: 0.7687903642654419\n",
      "Epoch 13: Train Loss: 0.612762176990509, Validation Loss: 0.7709914445877075\n",
      "Epoch 14: Train Loss: 0.7005040764808654, Validation Loss: 0.7771857976913452\n",
      "Epoch 15: Train Loss: 0.6995767712593078, Validation Loss: 0.7767360806465149\n",
      "Epoch 16: Train Loss: 0.6826688885688782, Validation Loss: 0.7759563326835632\n",
      "Epoch 17: Train Loss: 0.6684265971183777, Validation Loss: 0.7792689800262451\n",
      "Epoch 18: Train Loss: 0.6377451658248902, Validation Loss: 0.7830918431282043\n",
      "Epoch 19: Train Loss: 0.654532253742218, Validation Loss: 0.7742660045623779\n",
      "Epoch 20: Train Loss: 0.6574799299240113, Validation Loss: 0.7786957621574402\n",
      "Epoch 21: Train Loss: 0.6159184694290161, Validation Loss: 0.7985559701919556\n",
      "Epoch 22: Train Loss: 0.6313611388206481, Validation Loss: 0.8097209930419922\n",
      "Epoch 23: Train Loss: 0.6201147258281707, Validation Loss: 0.8084705471992493\n",
      "Epoch 24: Train Loss: 0.6140585899353027, Validation Loss: 0.808939516544342\n",
      "Epoch 25: Train Loss: 0.625172245502472, Validation Loss: 0.8070119619369507\n",
      "Epoch 26: Train Loss: 0.6812274575233459, Validation Loss: 0.8212887048721313\n",
      "Epoch 27: Train Loss: 0.6029600858688354, Validation Loss: 0.8230302929878235\n",
      "Epoch 28: Train Loss: 0.6199146151542664, Validation Loss: 0.8240809440612793\n",
      "Epoch 29: Train Loss: 0.6561152696609497, Validation Loss: 0.814606785774231\n",
      "Epoch 30: Train Loss: 0.638973593711853, Validation Loss: 0.8198308348655701\n",
      "Epoch 31: Train Loss: 0.6356298089027405, Validation Loss: 0.8306156396865845\n",
      "Epoch 32: Train Loss: 0.6360666155815125, Validation Loss: 0.8315803408622742\n",
      "Epoch 33: Train Loss: 0.6141299128532409, Validation Loss: 0.8284551501274109\n",
      "Epoch 34: Train Loss: 0.6142918586730957, Validation Loss: 0.8347507119178772\n",
      "Epoch 35: Train Loss: 0.5962026357650757, Validation Loss: 0.8341721892356873\n",
      "Epoch 36: Train Loss: 0.6380326390266419, Validation Loss: 0.8401345610618591\n",
      "Epoch 37: Train Loss: 0.6289835572242737, Validation Loss: 0.8463916778564453\n",
      "Epoch 38: Train Loss: 0.598668348789215, Validation Loss: 0.849419891834259\n",
      "Epoch 39: Train Loss: 0.6656072854995727, Validation Loss: 0.8500750660896301\n",
      "Epoch 40: Train Loss: 0.6279015302658081, Validation Loss: 0.8658151030540466\n",
      "Epoch 41: Train Loss: 0.6291984438896179, Validation Loss: 0.8807719349861145\n",
      "Epoch 42: Train Loss: 0.588040566444397, Validation Loss: 0.878838300704956\n",
      "Epoch 43: Train Loss: 0.6296968817710876, Validation Loss: 0.8910138607025146\n",
      "Epoch 44: Train Loss: 0.6097832322120667, Validation Loss: 0.8879092931747437\n",
      "Epoch 45: Train Loss: 0.6433532476425171, Validation Loss: 0.8926264047622681\n",
      "Epoch 46: Train Loss: 0.5960851669311523, Validation Loss: 0.8973569869995117\n",
      "Epoch 47: Train Loss: 0.5866257429122925, Validation Loss: 0.8810266256332397\n",
      "Epoch 48: Train Loss: 0.6194633841514587, Validation Loss: 0.8890126347541809\n",
      "Epoch 49: Train Loss: 0.6097306132316589, Validation Loss: 0.892782986164093\n",
      "Epoch 50: Train Loss: 0.6291615605354309, Validation Loss: 0.894099235534668\n",
      "Epoch 51: Train Loss: 0.5664827942848205, Validation Loss: 0.8853217959403992\n",
      "Epoch 52: Train Loss: 0.5679083347320557, Validation Loss: 0.9089645743370056\n",
      "Epoch 53: Train Loss: 0.5469664692878723, Validation Loss: 0.9091205596923828\n",
      "Epoch 54: Train Loss: 0.662259829044342, Validation Loss: 0.9268512725830078\n",
      "Epoch 55: Train Loss: 0.6172854423522949, Validation Loss: 0.9321041107177734\n",
      "Epoch 56: Train Loss: 0.5423706710338593, Validation Loss: 0.9312336444854736\n",
      "Epoch 57: Train Loss: 0.5410724103450775, Validation Loss: 0.9435614943504333\n",
      "Epoch 58: Train Loss: 0.5540497601032257, Validation Loss: 0.9491612911224365\n",
      "Epoch 59: Train Loss: 0.6137633264064789, Validation Loss: 0.9384006857872009\n",
      "Epoch 60: Train Loss: 0.5565721988677979, Validation Loss: 0.9543624520301819\n",
      "Epoch 61: Train Loss: 0.592102336883545, Validation Loss: 0.9503122568130493\n",
      "Epoch 62: Train Loss: 0.5511460781097413, Validation Loss: 0.9529353976249695\n",
      "Epoch 63: Train Loss: 0.5208777189254761, Validation Loss: 0.9717671871185303\n",
      "Epoch 64: Train Loss: 0.6163548231124878, Validation Loss: 0.9750064611434937\n",
      "Epoch 65: Train Loss: 0.5914613604545593, Validation Loss: 0.990404486656189\n",
      "Epoch 66: Train Loss: 0.5753819942474365, Validation Loss: 0.9753665924072266\n",
      "Epoch 67: Train Loss: 0.5647166967391968, Validation Loss: 0.9666094183921814\n",
      "Epoch 68: Train Loss: 0.5409426689147949, Validation Loss: 0.9668216109275818\n",
      "Epoch 69: Train Loss: 0.5467188239097596, Validation Loss: 0.9778628945350647\n",
      "Epoch 70: Train Loss: 0.5390397369861603, Validation Loss: 0.9837602972984314\n",
      "Epoch 71: Train Loss: 0.5914925694465637, Validation Loss: 1.0015721321105957\n",
      "Epoch 72: Train Loss: 0.5500645518302918, Validation Loss: 1.00003981590271\n",
      "Epoch 73: Train Loss: 0.5665770649909974, Validation Loss: 1.0166836977005005\n",
      "Epoch 74: Train Loss: 0.5673908531665802, Validation Loss: 1.0277626514434814\n",
      "Epoch 75: Train Loss: 0.5599171161651612, Validation Loss: 1.0166163444519043\n",
      "Epoch 76: Train Loss: 0.5167259931564331, Validation Loss: 1.0431772470474243\n",
      "Epoch 77: Train Loss: 0.5377858936786651, Validation Loss: 1.0580235719680786\n",
      "Epoch 78: Train Loss: 0.5627848625183105, Validation Loss: 1.0656859874725342\n",
      "Epoch 79: Train Loss: 0.5724855065345764, Validation Loss: 1.039352297782898\n",
      "Epoch 80: Train Loss: 0.6113159358501434, Validation Loss: 1.0244359970092773\n",
      "Epoch 81: Train Loss: 0.5296844363212585, Validation Loss: 1.0088071823120117\n",
      "Epoch 82: Train Loss: 0.5562104463577271, Validation Loss: 1.0175725221633911\n",
      "Epoch 83: Train Loss: 0.5613053262233734, Validation Loss: 1.0076740980148315\n",
      "Epoch 84: Train Loss: 0.5068779230117798, Validation Loss: 1.010995626449585\n",
      "Epoch 85: Train Loss: 0.5360248506069183, Validation Loss: 0.9919949173927307\n",
      "Epoch 86: Train Loss: 0.563890916109085, Validation Loss: 1.0168524980545044\n",
      "Epoch 87: Train Loss: 0.6077399551868439, Validation Loss: 1.0345454216003418\n",
      "Epoch 88: Train Loss: 0.5727610766887665, Validation Loss: 1.04794442653656\n",
      "Epoch 89: Train Loss: 0.5575209200382233, Validation Loss: 1.0467514991760254\n",
      "Epoch 90: Train Loss: 0.5310942709445954, Validation Loss: 1.0599346160888672\n",
      "Epoch 91: Train Loss: 0.5812838733196258, Validation Loss: 1.0716233253479004\n",
      "Epoch 92: Train Loss: 0.5531854033470154, Validation Loss: 1.0961562395095825\n",
      "Epoch 93: Train Loss: 0.5257140040397644, Validation Loss: 1.0708693265914917\n",
      "Epoch 94: Train Loss: 0.5385287821292877, Validation Loss: 1.0796157121658325\n",
      "Epoch 95: Train Loss: 0.4706800997257233, Validation Loss: 1.0838239192962646\n",
      "Epoch 96: Train Loss: 0.4905845046043396, Validation Loss: 1.1094473600387573\n",
      "Epoch 97: Train Loss: 0.48352659344673155, Validation Loss: 1.128971815109253\n",
      "Epoch 98: Train Loss: 0.4822660028934479, Validation Loss: 1.1345025300979614\n",
      "Epoch 99: Train Loss: 0.4789933502674103, Validation Loss: 1.1403851509094238\n",
      "Epoch 100: Train Loss: 0.512290495634079, Validation Loss: 1.1632567644119263\n",
      "Epoch 101: Train Loss: 0.4771444797515869, Validation Loss: 1.17032790184021\n",
      "Epoch 102: Train Loss: 0.4871467113494873, Validation Loss: 1.1899129152297974\n",
      "Epoch 103: Train Loss: 0.5154093980789185, Validation Loss: 1.2146459817886353\n",
      "Epoch 104: Train Loss: 0.48792786002159116, Validation Loss: 1.2155985832214355\n",
      "Epoch 105: Train Loss: 0.49645426869392395, Validation Loss: 1.2351868152618408\n",
      "Epoch 106: Train Loss: 0.5295595526695251, Validation Loss: 1.1999261379241943\n",
      "Epoch 107: Train Loss: 0.5564715147018433, Validation Loss: 1.2004587650299072\n",
      "Epoch 108: Train Loss: 0.4837333977222443, Validation Loss: 1.202858328819275\n",
      "Epoch 109: Train Loss: 0.46878171563148496, Validation Loss: 1.225252389907837\n",
      "Epoch 110: Train Loss: 0.47800015211105346, Validation Loss: 1.233420491218567\n",
      "Epoch 111: Train Loss: 0.5330656051635743, Validation Loss: 1.2545802593231201\n",
      "Epoch 112: Train Loss: 0.5184453129768372, Validation Loss: 1.1937676668167114\n",
      "Epoch 113: Train Loss: 0.5342161357402802, Validation Loss: 1.1822739839553833\n",
      "Epoch 114: Train Loss: 0.48554991483688353, Validation Loss: 1.1841901540756226\n",
      "Epoch 115: Train Loss: 0.4540257453918457, Validation Loss: 1.1956658363342285\n",
      "Epoch 116: Train Loss: 0.4199369788169861, Validation Loss: 1.1944620609283447\n",
      "Epoch 117: Train Loss: 0.5282791554927826, Validation Loss: 1.228293776512146\n",
      "Epoch 118: Train Loss: 0.5632523894309998, Validation Loss: 1.2564576864242554\n",
      "Epoch 119: Train Loss: 0.5090014934539795, Validation Loss: 1.2174874544143677\n",
      "Epoch 120: Train Loss: 0.4917763650417328, Validation Loss: 1.2412118911743164\n",
      "Epoch 121: Train Loss: 0.45561565160751344, Validation Loss: 1.2616618871688843\n",
      "Epoch 122: Train Loss: 0.44882657527923586, Validation Loss: 1.271798849105835\n",
      "Epoch 123: Train Loss: 0.42751853466033934, Validation Loss: 1.258810043334961\n",
      "Epoch 124: Train Loss: 0.44919835925102236, Validation Loss: 1.3180674314498901\n",
      "Epoch 125: Train Loss: 0.5347288370132446, Validation Loss: 1.308549165725708\n",
      "Epoch 126: Train Loss: 0.43840688467025757, Validation Loss: 1.3341212272644043\n",
      "Epoch 127: Train Loss: 0.44994441866874696, Validation Loss: 1.355703353881836\n",
      "Epoch 128: Train Loss: 0.46885234117507935, Validation Loss: 1.3610291481018066\n",
      "Epoch 129: Train Loss: 0.4481815218925476, Validation Loss: 1.368492841720581\n",
      "Epoch 130: Train Loss: 0.4879122138023376, Validation Loss: 1.3545737266540527\n",
      "Epoch 131: Train Loss: 0.47015249729156494, Validation Loss: 1.3606096506118774\n",
      "Epoch 132: Train Loss: 0.4699189782142639, Validation Loss: 1.348272442817688\n",
      "Epoch 133: Train Loss: 0.45222455859184263, Validation Loss: 1.357082724571228\n",
      "Epoch 134: Train Loss: 0.4468266785144806, Validation Loss: 1.3912839889526367\n",
      "Epoch 135: Train Loss: 0.46009629368782046, Validation Loss: 1.4284331798553467\n",
      "Epoch 136: Train Loss: 0.4506212711334229, Validation Loss: 1.4063729047775269\n",
      "Epoch 137: Train Loss: 0.4389820396900177, Validation Loss: 1.4124253988265991\n",
      "Epoch 138: Train Loss: 0.441395103931427, Validation Loss: 1.3875160217285156\n",
      "Epoch 139: Train Loss: 0.42785062193870543, Validation Loss: 1.3853896856307983\n",
      "Epoch 140: Train Loss: 0.48096917271614076, Validation Loss: 1.4400207996368408\n",
      "Epoch 141: Train Loss: 0.4767626583576202, Validation Loss: 1.4797576665878296\n",
      "Epoch 142: Train Loss: 0.4119265854358673, Validation Loss: 1.5182358026504517\n",
      "Epoch 143: Train Loss: 0.430083167552948, Validation Loss: 1.5739054679870605\n",
      "Epoch 144: Train Loss: 0.394822895526886, Validation Loss: 1.573799729347229\n",
      "Epoch 145: Train Loss: 0.4691938817501068, Validation Loss: 1.5554167032241821\n",
      "Epoch 146: Train Loss: 0.4237644195556641, Validation Loss: 1.5376399755477905\n",
      "Epoch 147: Train Loss: 0.4178589105606079, Validation Loss: 1.5554217100143433\n",
      "Epoch 148: Train Loss: 0.41368595957756044, Validation Loss: 1.563884973526001\n",
      "Epoch 149: Train Loss: 0.4136465549468994, Validation Loss: 1.589328646659851\n",
      "Epoch 150: Train Loss: 0.4340850591659546, Validation Loss: 1.5683867931365967\n",
      "Epoch 151: Train Loss: 0.4148561358451843, Validation Loss: 1.5854822397232056\n",
      "Epoch 152: Train Loss: 0.4716485500335693, Validation Loss: 1.5799978971481323\n",
      "Epoch 153: Train Loss: 0.5180671572685241, Validation Loss: 1.5790274143218994\n",
      "Epoch 154: Train Loss: 0.3795695424079895, Validation Loss: 1.5966554880142212\n",
      "Epoch 155: Train Loss: 0.4600484251976013, Validation Loss: 1.6348001956939697\n",
      "Epoch 156: Train Loss: 0.41455326080322263, Validation Loss: 1.6536742448806763\n",
      "Epoch 157: Train Loss: 0.4138805866241455, Validation Loss: 1.6285419464111328\n",
      "Epoch 158: Train Loss: 0.4686769127845764, Validation Loss: 1.6290560960769653\n",
      "Epoch 159: Train Loss: 0.417798113822937, Validation Loss: 1.6429293155670166\n",
      "Epoch 160: Train Loss: 0.44279085397720336, Validation Loss: 1.6999648809432983\n",
      "Epoch 161: Train Loss: 0.4675833165645599, Validation Loss: 1.711526870727539\n",
      "Epoch 162: Train Loss: 0.42947455644607546, Validation Loss: 1.7224797010421753\n",
      "Epoch 163: Train Loss: 0.4030942678451538, Validation Loss: 1.7411036491394043\n",
      "Epoch 164: Train Loss: 0.40425158143043516, Validation Loss: 1.6568052768707275\n",
      "Epoch 165: Train Loss: 0.38386912941932677, Validation Loss: 1.6828358173370361\n",
      "Epoch 166: Train Loss: 0.4278454124927521, Validation Loss: 1.706157922744751\n",
      "Epoch 167: Train Loss: 0.5178529739379882, Validation Loss: 1.7277255058288574\n",
      "Epoch 168: Train Loss: 0.4659359097480774, Validation Loss: 1.7585877180099487\n",
      "Epoch 169: Train Loss: 0.41952950954437257, Validation Loss: 1.71729576587677\n",
      "Epoch 170: Train Loss: 0.41466503739356997, Validation Loss: 1.7288830280303955\n",
      "Epoch 171: Train Loss: 0.40842620134353635, Validation Loss: 1.666346788406372\n",
      "Epoch 172: Train Loss: 0.4143642783164978, Validation Loss: 1.6227390766143799\n",
      "Epoch 173: Train Loss: 0.4833780825138092, Validation Loss: 1.6621702909469604\n",
      "Epoch 174: Train Loss: 0.38777499198913573, Validation Loss: 1.6717503070831299\n",
      "Epoch 175: Train Loss: 0.4617233633995056, Validation Loss: 1.710783839225769\n",
      "Epoch 176: Train Loss: 0.36473295986652376, Validation Loss: 1.7559400796890259\n",
      "Epoch 177: Train Loss: 0.42153151631355285, Validation Loss: 1.7866495847702026\n",
      "Epoch 178: Train Loss: 0.39903306365013125, Validation Loss: 1.8156490325927734\n",
      "Epoch 179: Train Loss: 0.41610087752342223, Validation Loss: 1.8024195432662964\n",
      "Epoch 180: Train Loss: 0.39527949690818787, Validation Loss: 1.7574249505996704\n",
      "Epoch 181: Train Loss: 0.4551104247570038, Validation Loss: 1.7852565050125122\n",
      "Epoch 182: Train Loss: 0.34951217770576476, Validation Loss: 1.8292770385742188\n",
      "Epoch 183: Train Loss: 0.3630345046520233, Validation Loss: 1.816046953201294\n",
      "Epoch 184: Train Loss: 0.36555970311164854, Validation Loss: 1.8659590482711792\n",
      "Epoch 185: Train Loss: 0.3848012387752533, Validation Loss: 1.8488869667053223\n",
      "Epoch 186: Train Loss: 0.3931497156620026, Validation Loss: 1.8883659839630127\n",
      "Epoch 187: Train Loss: 0.3876387417316437, Validation Loss: 1.9106563329696655\n",
      "Epoch 188: Train Loss: 0.3972979962825775, Validation Loss: 1.8620593547821045\n",
      "Epoch 189: Train Loss: 0.3914319396018982, Validation Loss: 1.8893917798995972\n",
      "Epoch 190: Train Loss: 0.36696645617485046, Validation Loss: 1.8889544010162354\n",
      "Epoch 191: Train Loss: 0.3584639966487885, Validation Loss: 1.9516361951828003\n",
      "Epoch 192: Train Loss: 0.37428549528121946, Validation Loss: 2.024385690689087\n",
      "Epoch 193: Train Loss: 0.37918405532836913, Validation Loss: 1.9616211652755737\n",
      "Epoch 194: Train Loss: 0.39919676780700686, Validation Loss: 1.9263312816619873\n",
      "Epoch 195: Train Loss: 0.3071844905614853, Validation Loss: 1.9378501176834106\n",
      "Epoch 196: Train Loss: 0.34106466770172117, Validation Loss: 1.9533833265304565\n",
      "Epoch 197: Train Loss: 0.3969912171363831, Validation Loss: 1.9988635778427124\n",
      "Epoch 198: Train Loss: 0.3908836007118225, Validation Loss: 1.936804175376892\n",
      "Epoch 199: Train Loss: 0.37133116722106935, Validation Loss: 2.012948751449585\n",
      "Epoch 200: Train Loss: 0.3595882892608643, Validation Loss: 2.0479636192321777\n",
      "Epoch 201: Train Loss: 0.36252725720405576, Validation Loss: 2.017200231552124\n",
      "Epoch 202: Train Loss: 0.43155495524406434, Validation Loss: 1.999531626701355\n",
      "Epoch 203: Train Loss: 0.3290281116962433, Validation Loss: 2.0319535732269287\n",
      "Epoch 204: Train Loss: 0.3652401089668274, Validation Loss: 2.084787607192993\n",
      "Epoch 205: Train Loss: 0.3734623193740845, Validation Loss: 2.0438966751098633\n",
      "Epoch 206: Train Loss: 0.3649482786655426, Validation Loss: 2.0597925186157227\n",
      "Epoch 207: Train Loss: 0.3338063210248947, Validation Loss: 2.070688009262085\n",
      "Epoch 208: Train Loss: 0.358572793006897, Validation Loss: 1.9917327165603638\n",
      "Epoch 209: Train Loss: 0.38180782794952395, Validation Loss: 2.0527515411376953\n",
      "Epoch 210: Train Loss: 0.4076172709465027, Validation Loss: 2.161043167114258\n",
      "Epoch 211: Train Loss: 0.32724239528179166, Validation Loss: 2.221649408340454\n",
      "Epoch 212: Train Loss: 0.3182738542556763, Validation Loss: 2.2669312953948975\n",
      "Epoch 213: Train Loss: 0.3333704799413681, Validation Loss: 2.273709535598755\n",
      "Epoch 214: Train Loss: 0.3994948774576187, Validation Loss: 2.2673802375793457\n",
      "Epoch 215: Train Loss: 0.34411032795906066, Validation Loss: 2.172515869140625\n",
      "Epoch 216: Train Loss: 0.3823374330997467, Validation Loss: 2.2124650478363037\n",
      "Epoch 217: Train Loss: 0.3306459248065948, Validation Loss: 2.1853251457214355\n",
      "Epoch 218: Train Loss: 0.31095497608184813, Validation Loss: 2.190561532974243\n",
      "Epoch 219: Train Loss: 0.3123145967721939, Validation Loss: 2.2097525596618652\n",
      "Epoch 220: Train Loss: 0.3665110766887665, Validation Loss: 2.183478355407715\n",
      "Epoch 221: Train Loss: 0.38554903864860535, Validation Loss: 2.1402575969696045\n",
      "Epoch 222: Train Loss: 0.2922244369983673, Validation Loss: 2.1988654136657715\n",
      "Epoch 223: Train Loss: 0.39404577016830444, Validation Loss: 2.100701093673706\n",
      "Epoch 224: Train Loss: 0.3215807467699051, Validation Loss: 2.159989833831787\n",
      "Epoch 225: Train Loss: 0.3345376044511795, Validation Loss: 2.1882641315460205\n",
      "Epoch 226: Train Loss: 0.3102480858564377, Validation Loss: 2.2695376873016357\n",
      "Epoch 227: Train Loss: 0.30505053102970126, Validation Loss: 2.2806904315948486\n",
      "Epoch 228: Train Loss: 0.31824623346328734, Validation Loss: 2.2644665241241455\n",
      "Epoch 229: Train Loss: 0.4462498962879181, Validation Loss: 2.275362253189087\n",
      "Epoch 230: Train Loss: 0.3951419711112976, Validation Loss: 2.3026647567749023\n",
      "Epoch 231: Train Loss: 0.3338169395923615, Validation Loss: 2.299905776977539\n",
      "Epoch 232: Train Loss: 0.374764746427536, Validation Loss: 2.326718807220459\n",
      "Epoch 233: Train Loss: 0.330398952960968, Validation Loss: 2.2977135181427\n",
      "Epoch 234: Train Loss: 0.2921193212270737, Validation Loss: 2.318419933319092\n",
      "Epoch 235: Train Loss: 0.3423071801662445, Validation Loss: 2.361931324005127\n",
      "Epoch 236: Train Loss: 0.3371525526046753, Validation Loss: 2.29889178276062\n",
      "Epoch 237: Train Loss: 0.4395761042833328, Validation Loss: 2.3022971153259277\n",
      "Epoch 238: Train Loss: 0.2655663400888443, Validation Loss: 2.303431510925293\n",
      "Epoch 239: Train Loss: 0.35990113615989683, Validation Loss: 2.3583223819732666\n",
      "Epoch 240: Train Loss: 0.29605980813503263, Validation Loss: 2.391436815261841\n",
      "Epoch 241: Train Loss: 0.3308315873146057, Validation Loss: 2.432711362838745\n",
      "Epoch 242: Train Loss: 0.3286894917488098, Validation Loss: 2.4613513946533203\n",
      "Epoch 243: Train Loss: 0.31525089144706725, Validation Loss: 2.365891933441162\n",
      "Epoch 244: Train Loss: 0.29649306237697604, Validation Loss: 2.336561679840088\n",
      "Epoch 245: Train Loss: 0.28028895556926725, Validation Loss: 2.380316734313965\n",
      "Epoch 246: Train Loss: 0.35599455833435056, Validation Loss: 2.4237401485443115\n",
      "Epoch 247: Train Loss: 0.32454258799552915, Validation Loss: 2.4296743869781494\n",
      "Epoch 248: Train Loss: 0.3256575226783752, Validation Loss: 2.371504306793213\n",
      "Epoch 249: Train Loss: 0.45172632932662965, Validation Loss: 2.3117895126342773\n",
      "Epoch 250: Train Loss: 0.3220338225364685, Validation Loss: 2.2972662448883057\n",
      "Epoch 251: Train Loss: 0.28831092417240145, Validation Loss: 2.3063173294067383\n",
      "Epoch 252: Train Loss: 0.3757770538330078, Validation Loss: 2.3769783973693848\n",
      "Epoch 253: Train Loss: 0.3892432987689972, Validation Loss: 2.4339754581451416\n",
      "Epoch 254: Train Loss: 0.26645777374505997, Validation Loss: 2.4171526432037354\n",
      "Epoch 255: Train Loss: 0.3188562631607056, Validation Loss: 2.3917336463928223\n",
      "Epoch 256: Train Loss: 0.2934734344482422, Validation Loss: 2.4793310165405273\n",
      "Epoch 257: Train Loss: 0.25488888174295427, Validation Loss: 2.556976079940796\n",
      "Epoch 258: Train Loss: 0.35156853795051574, Validation Loss: 2.5927932262420654\n",
      "Epoch 259: Train Loss: 0.3764990001916885, Validation Loss: 2.465899705886841\n",
      "Epoch 260: Train Loss: 0.3693933427333832, Validation Loss: 2.4365527629852295\n",
      "Epoch 261: Train Loss: 0.3315082311630249, Validation Loss: 2.3612911701202393\n",
      "Epoch 262: Train Loss: 0.4244928896427155, Validation Loss: 2.518853187561035\n",
      "Epoch 263: Train Loss: 0.3954629421234131, Validation Loss: 2.5653021335601807\n",
      "Epoch 264: Train Loss: 0.3363540768623352, Validation Loss: 2.598201036453247\n",
      "Epoch 265: Train Loss: 0.2785564333200455, Validation Loss: 2.493378162384033\n",
      "Epoch 266: Train Loss: 0.32212752401828765, Validation Loss: 2.5441079139709473\n",
      "Epoch 267: Train Loss: 0.35631170868873596, Validation Loss: 2.5699424743652344\n",
      "Epoch 268: Train Loss: 0.28187925815582277, Validation Loss: 2.6038572788238525\n",
      "Epoch 269: Train Loss: 0.2984064668416977, Validation Loss: 2.6557140350341797\n",
      "Epoch 270: Train Loss: 0.3431980967521667, Validation Loss: 2.7055861949920654\n",
      "Epoch 271: Train Loss: 0.28219946622848513, Validation Loss: 2.6992805004119873\n",
      "Epoch 272: Train Loss: 0.35638431310653684, Validation Loss: 2.658588171005249\n",
      "Epoch 273: Train Loss: 0.2842062324285507, Validation Loss: 2.581226348876953\n",
      "Epoch 274: Train Loss: 0.24626104682683944, Validation Loss: 2.618018388748169\n",
      "Epoch 275: Train Loss: 0.2656536281108856, Validation Loss: 2.684142827987671\n",
      "Epoch 276: Train Loss: 0.29839830100536346, Validation Loss: 2.751239538192749\n",
      "Epoch 277: Train Loss: 0.3237802445888519, Validation Loss: 2.6753458976745605\n",
      "Epoch 278: Train Loss: 0.2824021130800247, Validation Loss: 2.6755008697509766\n",
      "Epoch 279: Train Loss: 0.27666967213153837, Validation Loss: 2.72505521774292\n",
      "Epoch 280: Train Loss: 0.24302889108657838, Validation Loss: 2.7990994453430176\n",
      "Epoch 281: Train Loss: 0.3289014369249344, Validation Loss: 2.6770999431610107\n",
      "Epoch 282: Train Loss: 0.24654763340950012, Validation Loss: 2.634273052215576\n",
      "Epoch 283: Train Loss: 0.22117102295160293, Validation Loss: 2.7059383392333984\n",
      "Epoch 284: Train Loss: 0.2779744386672974, Validation Loss: 2.785167932510376\n",
      "Epoch 285: Train Loss: 0.275399512052536, Validation Loss: 2.857403516769409\n",
      "Epoch 286: Train Loss: 0.3488009124994278, Validation Loss: 2.7588915824890137\n",
      "Epoch 287: Train Loss: 0.3358353465795517, Validation Loss: 2.819622039794922\n",
      "Epoch 288: Train Loss: 0.2898234724998474, Validation Loss: 2.919525623321533\n",
      "Epoch 289: Train Loss: 0.3572978496551514, Validation Loss: 2.9022862911224365\n",
      "Epoch 290: Train Loss: 0.2999004065990448, Validation Loss: 2.8210532665252686\n",
      "Epoch 291: Train Loss: 0.24206435084342956, Validation Loss: 2.8721530437469482\n",
      "Epoch 292: Train Loss: 0.34090695679187777, Validation Loss: 2.918635129928589\n",
      "Epoch 293: Train Loss: 0.3391005575656891, Validation Loss: 2.966832399368286\n",
      "Epoch 294: Train Loss: 0.22126299813389777, Validation Loss: 3.0203444957733154\n",
      "Epoch 295: Train Loss: 0.33077569901943205, Validation Loss: 2.956244707107544\n",
      "Epoch 296: Train Loss: 0.3038485497236252, Validation Loss: 2.840359926223755\n",
      "Epoch 297: Train Loss: 0.3299755334854126, Validation Loss: 3.0084731578826904\n",
      "Epoch 298: Train Loss: 0.2805434316396713, Validation Loss: 3.057325601577759\n",
      "Epoch 299: Train Loss: 0.2845857381820679, Validation Loss: 3.022585868835449\n",
      "Epoch 300: Train Loss: 0.3132737159729004, Validation Loss: 3.003268241882324\n",
      "Epoch 301: Train Loss: 0.22595016956329345, Validation Loss: 3.028331756591797\n",
      "Epoch 302: Train Loss: 0.2659385234117508, Validation Loss: 3.003788709640503\n",
      "Epoch 303: Train Loss: 0.4909521877765656, Validation Loss: 2.988257646560669\n",
      "Epoch 304: Train Loss: 0.2549749851226807, Validation Loss: 3.0148673057556152\n",
      "Epoch 305: Train Loss: 0.3326617658138275, Validation Loss: 2.947387456893921\n",
      "Epoch 306: Train Loss: 0.2507123351097107, Validation Loss: 3.0420010089874268\n",
      "Epoch 307: Train Loss: 0.3191789507865906, Validation Loss: 3.1209423542022705\n",
      "Epoch 308: Train Loss: 0.28787766098976136, Validation Loss: 3.0911734104156494\n",
      "Epoch 309: Train Loss: 0.329569348692894, Validation Loss: 3.092080593109131\n",
      "Epoch 310: Train Loss: 0.25732738673686983, Validation Loss: 3.038782835006714\n",
      "Epoch 311: Train Loss: 0.3223224878311157, Validation Loss: 3.0688703060150146\n",
      "Epoch 312: Train Loss: 0.28590177893638613, Validation Loss: 3.079559087753296\n",
      "Epoch 313: Train Loss: 0.32868425250053407, Validation Loss: 3.076127529144287\n",
      "Epoch 314: Train Loss: 0.2973024517297745, Validation Loss: 2.985474109649658\n",
      "Epoch 315: Train Loss: 0.23831721693277358, Validation Loss: 2.972412109375\n",
      "Epoch 316: Train Loss: 0.29322474598884585, Validation Loss: 3.0456435680389404\n",
      "Epoch 317: Train Loss: 0.2340848043560982, Validation Loss: 3.0248374938964844\n",
      "Epoch 318: Train Loss: 0.4155284583568573, Validation Loss: 2.9469025135040283\n",
      "Epoch 319: Train Loss: 0.244230917096138, Validation Loss: 2.9581165313720703\n",
      "Epoch 320: Train Loss: 0.3135055869817734, Validation Loss: 2.998077154159546\n",
      "Epoch 321: Train Loss: 0.2568099141120911, Validation Loss: 3.0207931995391846\n",
      "Epoch 322: Train Loss: 0.26967052817344667, Validation Loss: 3.061047077178955\n",
      "Epoch 323: Train Loss: 0.2193324789404869, Validation Loss: 3.0043752193450928\n",
      "Epoch 324: Train Loss: 0.22475546300411225, Validation Loss: 3.0814297199249268\n",
      "Epoch 325: Train Loss: 0.27934685349464417, Validation Loss: 3.161756753921509\n",
      "Epoch 326: Train Loss: 0.2766347736120224, Validation Loss: 3.255049228668213\n",
      "Epoch 327: Train Loss: 0.300952610373497, Validation Loss: 3.1106114387512207\n",
      "Epoch 328: Train Loss: 0.24246651828289031, Validation Loss: 3.05612850189209\n",
      "Epoch 329: Train Loss: 0.23177210092544556, Validation Loss: 3.1161863803863525\n",
      "Epoch 330: Train Loss: 0.23425171673297882, Validation Loss: 3.1777000427246094\n",
      "Epoch 331: Train Loss: 0.24052459001541138, Validation Loss: 3.1352314949035645\n",
      "Epoch 332: Train Loss: 0.31217829287052157, Validation Loss: 3.2421092987060547\n",
      "Epoch 333: Train Loss: 0.23353505730628968, Validation Loss: 3.3039052486419678\n",
      "Epoch 334: Train Loss: 0.28534261882305145, Validation Loss: 3.1158039569854736\n",
      "Epoch 335: Train Loss: 0.21550157517194748, Validation Loss: 3.158613681793213\n",
      "Epoch 336: Train Loss: 0.22315161526203156, Validation Loss: 3.178858995437622\n",
      "Epoch 337: Train Loss: 0.3261259853839874, Validation Loss: 3.2746214866638184\n",
      "Epoch 338: Train Loss: 0.2163992553949356, Validation Loss: 3.249728202819824\n",
      "Epoch 339: Train Loss: 0.2023425281047821, Validation Loss: 3.1804275512695312\n",
      "Epoch 340: Train Loss: 0.2349597066640854, Validation Loss: 3.2186107635498047\n",
      "Epoch 341: Train Loss: 0.29317406117916106, Validation Loss: 3.203953504562378\n",
      "Epoch 342: Train Loss: 0.2283224105834961, Validation Loss: 3.2578155994415283\n",
      "Epoch 343: Train Loss: 0.37836002111434935, Validation Loss: 3.3166069984436035\n",
      "Epoch 344: Train Loss: 0.2200960785150528, Validation Loss: 3.3622753620147705\n",
      "Epoch 345: Train Loss: 0.18984383717179298, Validation Loss: 3.3392491340637207\n",
      "Epoch 346: Train Loss: 0.2345975458621979, Validation Loss: 3.3763434886932373\n",
      "Epoch 347: Train Loss: 0.35206203162670135, Validation Loss: 3.347790479660034\n",
      "Epoch 348: Train Loss: 0.20883543491363527, Validation Loss: 3.4237866401672363\n",
      "Epoch 349: Train Loss: 0.24617383182048796, Validation Loss: 3.4388105869293213\n",
      "Epoch 350: Train Loss: 0.2541980236768723, Validation Loss: 3.257695436477661\n",
      "Epoch 351: Train Loss: 0.22904525697231293, Validation Loss: 3.2451889514923096\n",
      "Epoch 352: Train Loss: 0.21037429571151733, Validation Loss: 3.228909730911255\n",
      "Epoch 353: Train Loss: 0.20195539817214012, Validation Loss: 3.2788732051849365\n",
      "Epoch 354: Train Loss: 0.34145624935626984, Validation Loss: 3.3360366821289062\n",
      "Epoch 355: Train Loss: 0.21098847985267638, Validation Loss: 3.257859468460083\n",
      "Epoch 356: Train Loss: 0.22048733234405518, Validation Loss: 3.2765374183654785\n",
      "Epoch 357: Train Loss: 0.19655362218618394, Validation Loss: 3.3570098876953125\n",
      "Epoch 358: Train Loss: 0.21664566099643706, Validation Loss: 3.301582098007202\n",
      "Epoch 359: Train Loss: 0.22289476692676544, Validation Loss: 3.3753347396850586\n",
      "Epoch 360: Train Loss: 0.23425899147987367, Validation Loss: 3.3847711086273193\n",
      "Epoch 361: Train Loss: 0.2401726245880127, Validation Loss: 3.4341726303100586\n",
      "Epoch 362: Train Loss: 0.25231092274188993, Validation Loss: 3.519503593444824\n",
      "Epoch 363: Train Loss: 0.22309925854206086, Validation Loss: 3.579777479171753\n",
      "Epoch 364: Train Loss: 0.36107388138771057, Validation Loss: 3.6005260944366455\n",
      "Epoch 365: Train Loss: 0.24882891178131103, Validation Loss: 3.545577049255371\n",
      "Epoch 366: Train Loss: 0.3352251350879669, Validation Loss: 3.5570454597473145\n",
      "Epoch 367: Train Loss: 0.223377525806427, Validation Loss: 3.631702423095703\n",
      "Epoch 368: Train Loss: 0.2591026991605759, Validation Loss: 3.6059231758117676\n",
      "Epoch 369: Train Loss: 0.2740565091371536, Validation Loss: 3.4684808254241943\n",
      "Epoch 370: Train Loss: 0.33211165964603423, Validation Loss: 3.506502389907837\n",
      "Epoch 371: Train Loss: 0.2389975219964981, Validation Loss: 3.3456974029541016\n",
      "Epoch 372: Train Loss: 0.20222654938697815, Validation Loss: 3.2711453437805176\n",
      "Epoch 373: Train Loss: 0.19096014201641082, Validation Loss: 3.390641450881958\n",
      "Epoch 374: Train Loss: 0.20394720435142516, Validation Loss: 3.434757947921753\n",
      "Epoch 375: Train Loss: 0.26063673198223114, Validation Loss: 3.4419476985931396\n",
      "Epoch 376: Train Loss: 0.27778375446796416, Validation Loss: 3.4001214504241943\n",
      "Epoch 377: Train Loss: 0.21740692257881164, Validation Loss: 3.4297890663146973\n",
      "Epoch 378: Train Loss: 0.29452240765094756, Validation Loss: 3.3207037448883057\n",
      "Epoch 379: Train Loss: 0.2887845605611801, Validation Loss: 3.474120855331421\n",
      "Epoch 380: Train Loss: 0.28868130743503573, Validation Loss: 3.345337390899658\n",
      "Epoch 381: Train Loss: 0.30985057055950166, Validation Loss: 3.3794000148773193\n",
      "Epoch 382: Train Loss: 0.2764445811510086, Validation Loss: 3.4252545833587646\n",
      "Epoch 383: Train Loss: 0.24703034460544587, Validation Loss: 3.430650472640991\n",
      "Epoch 384: Train Loss: 0.1999729335308075, Validation Loss: 3.374422550201416\n",
      "Epoch 385: Train Loss: 0.2663382291793823, Validation Loss: 3.3814260959625244\n",
      "Epoch 386: Train Loss: 0.2496940851211548, Validation Loss: 3.3341901302337646\n",
      "Epoch 387: Train Loss: 0.21603947579860688, Validation Loss: 3.3297760486602783\n",
      "Epoch 388: Train Loss: 0.20750342607498168, Validation Loss: 3.364506244659424\n",
      "Epoch 389: Train Loss: 0.21865638494491577, Validation Loss: 3.452361822128296\n",
      "Epoch 390: Train Loss: 0.22378555834293365, Validation Loss: 3.4490342140197754\n",
      "Epoch 391: Train Loss: 0.18749177157878877, Validation Loss: 3.505138397216797\n",
      "Epoch 392: Train Loss: 0.2165715366601944, Validation Loss: 3.4368598461151123\n",
      "Epoch 393: Train Loss: 0.25316339135169985, Validation Loss: 3.3995413780212402\n",
      "Epoch 394: Train Loss: 0.244830858707428, Validation Loss: 3.4371047019958496\n",
      "Epoch 395: Train Loss: 0.29049059748649597, Validation Loss: 3.500828504562378\n",
      "Epoch 396: Train Loss: 0.2872277557849884, Validation Loss: 3.50164794921875\n",
      "Epoch 397: Train Loss: 0.19878318011760712, Validation Loss: 3.3522613048553467\n",
      "Epoch 398: Train Loss: 0.21030315160751342, Validation Loss: 3.4941303730010986\n",
      "Epoch 399: Train Loss: 0.2637732237577438, Validation Loss: 3.5630886554718018\n",
      "Epoch 400: Train Loss: 0.22589735090732574, Validation Loss: 3.6101667881011963\n",
      "Epoch 401: Train Loss: 0.21604444980621337, Validation Loss: 3.6959760189056396\n",
      "Epoch 402: Train Loss: 0.2967989295721054, Validation Loss: 3.6828489303588867\n",
      "Epoch 403: Train Loss: 0.19483949840068818, Validation Loss: 3.679614305496216\n",
      "Epoch 404: Train Loss: 0.24602806866168975, Validation Loss: 3.70345139503479\n",
      "Epoch 405: Train Loss: 0.20508793592453003, Validation Loss: 3.5752601623535156\n",
      "Epoch 406: Train Loss: 0.2351955160498619, Validation Loss: 3.618661403656006\n",
      "Epoch 407: Train Loss: 0.1885291963815689, Validation Loss: 3.5183792114257812\n",
      "Epoch 408: Train Loss: 0.25678141713142394, Validation Loss: 3.5343017578125\n",
      "Epoch 409: Train Loss: 0.19738809466362, Validation Loss: 3.4712538719177246\n",
      "Epoch 410: Train Loss: 0.33824672400951383, Validation Loss: 3.588630437850952\n",
      "Epoch 411: Train Loss: 0.203434494137764, Validation Loss: 3.7861297130584717\n",
      "Epoch 412: Train Loss: 0.21334999203681945, Validation Loss: 3.734616756439209\n",
      "Epoch 413: Train Loss: 0.27827762961387636, Validation Loss: 3.8445045948028564\n",
      "Epoch 414: Train Loss: 0.21044419407844545, Validation Loss: 3.8810768127441406\n",
      "Epoch 415: Train Loss: 0.2531017929315567, Validation Loss: 3.8092336654663086\n",
      "Epoch 416: Train Loss: 0.19408186227083207, Validation Loss: 3.851513385772705\n",
      "Epoch 417: Train Loss: 0.20068923234939576, Validation Loss: 3.7443768978118896\n",
      "Epoch 418: Train Loss: 0.2998047560453415, Validation Loss: 3.563767671585083\n",
      "Epoch 419: Train Loss: 0.2193926155567169, Validation Loss: 3.695136785507202\n",
      "Epoch 420: Train Loss: 0.17795900106430054, Validation Loss: 3.5957717895507812\n",
      "Epoch 421: Train Loss: 0.1657061830163002, Validation Loss: 3.4541006088256836\n",
      "Epoch 422: Train Loss: 0.19662101790308953, Validation Loss: 3.5216190814971924\n",
      "Epoch 423: Train Loss: 0.2377209335565567, Validation Loss: 3.6665618419647217\n",
      "Epoch 424: Train Loss: 0.25224312841892244, Validation Loss: 3.7353436946868896\n",
      "Epoch 425: Train Loss: 0.19816533029079436, Validation Loss: 3.861382007598877\n",
      "Epoch 426: Train Loss: 0.19545659571886062, Validation Loss: 3.7857093811035156\n",
      "Epoch 427: Train Loss: 0.22567912340164184, Validation Loss: 3.5997374057769775\n",
      "Epoch 428: Train Loss: 0.1816243976354599, Validation Loss: 3.567615270614624\n",
      "Epoch 429: Train Loss: 0.21911197900772095, Validation Loss: 3.777977705001831\n",
      "Epoch 430: Train Loss: 0.19205943942070008, Validation Loss: 3.7227089405059814\n",
      "Epoch 431: Train Loss: 0.17972312569618226, Validation Loss: 3.6791915893554688\n",
      "Epoch 432: Train Loss: 0.2568646132946014, Validation Loss: 3.799591541290283\n",
      "Epoch 433: Train Loss: 0.3194225639104843, Validation Loss: 3.89217472076416\n",
      "Epoch 434: Train Loss: 0.1880837857723236, Validation Loss: 3.8970611095428467\n",
      "Epoch 435: Train Loss: 0.45939632058143615, Validation Loss: 3.8312790393829346\n",
      "Epoch 436: Train Loss: 0.3373743146657944, Validation Loss: 3.7437078952789307\n",
      "Epoch 437: Train Loss: 0.2536554515361786, Validation Loss: 3.7959752082824707\n",
      "Epoch 438: Train Loss: 0.32185833156108856, Validation Loss: 3.8042211532592773\n",
      "Epoch 439: Train Loss: 0.2339347094297409, Validation Loss: 3.6879873275756836\n",
      "Epoch 440: Train Loss: 0.20935975909233093, Validation Loss: 3.659315586090088\n",
      "Epoch 441: Train Loss: 0.2700363129377365, Validation Loss: 3.671971082687378\n",
      "Epoch 442: Train Loss: 0.1888222087174654, Validation Loss: 3.670657157897949\n",
      "Epoch 443: Train Loss: 0.17690583020448686, Validation Loss: 3.713198184967041\n",
      "Epoch 444: Train Loss: 0.2690875560045242, Validation Loss: 3.8310203552246094\n",
      "Epoch 445: Train Loss: 0.22647719383239745, Validation Loss: 3.7597603797912598\n",
      "Epoch 446: Train Loss: 0.18509264290332794, Validation Loss: 3.842395067214966\n",
      "Epoch 447: Train Loss: 0.19747380018234253, Validation Loss: 3.8644254207611084\n",
      "Epoch 448: Train Loss: 0.17570516020059584, Validation Loss: 3.940624237060547\n",
      "Epoch 449: Train Loss: 0.19112602472305298, Validation Loss: 3.788567543029785\n",
      "Epoch 450: Train Loss: 0.25109448432922366, Validation Loss: 3.820923328399658\n",
      "Epoch 451: Train Loss: 0.22175846099853516, Validation Loss: 3.8767991065979004\n",
      "Epoch 452: Train Loss: 0.2059052288532257, Validation Loss: 3.7260336875915527\n",
      "Epoch 453: Train Loss: 0.16515729129314421, Validation Loss: 3.8284614086151123\n",
      "Epoch 454: Train Loss: 0.17714539654552935, Validation Loss: 3.8758437633514404\n",
      "Epoch 455: Train Loss: 0.16605979725718498, Validation Loss: 3.797741413116455\n",
      "Epoch 456: Train Loss: 0.18353792130947114, Validation Loss: 3.9193756580352783\n",
      "Epoch 457: Train Loss: 0.24562330842018126, Validation Loss: 3.914933204650879\n",
      "Epoch 458: Train Loss: 0.1593490608036518, Validation Loss: 3.855351209640503\n",
      "Epoch 459: Train Loss: 0.2370810866355896, Validation Loss: 3.892096519470215\n",
      "Epoch 460: Train Loss: 0.2686206787824631, Validation Loss: 3.929553985595703\n",
      "Epoch 461: Train Loss: 0.4286207526922226, Validation Loss: 3.914602756500244\n",
      "Epoch 462: Train Loss: 0.29046006202697755, Validation Loss: 3.9304115772247314\n",
      "Epoch 463: Train Loss: 0.1978838175535202, Validation Loss: 3.9048638343811035\n",
      "Epoch 464: Train Loss: 0.281817352771759, Validation Loss: 3.855686902999878\n",
      "Epoch 465: Train Loss: 0.19343727976083755, Validation Loss: 3.874343156814575\n",
      "Epoch 466: Train Loss: 0.2760657697916031, Validation Loss: 3.7374014854431152\n",
      "Epoch 467: Train Loss: 0.2257436066865921, Validation Loss: 3.8238601684570312\n",
      "Epoch 468: Train Loss: 0.2644234925508499, Validation Loss: 3.8484747409820557\n",
      "Epoch 469: Train Loss: 0.20421799421310424, Validation Loss: 3.8038840293884277\n",
      "Epoch 470: Train Loss: 0.3099172204732895, Validation Loss: 3.9020094871520996\n",
      "Epoch 471: Train Loss: 0.20354598760604858, Validation Loss: 3.8519227504730225\n",
      "Epoch 472: Train Loss: 0.19241265952587128, Validation Loss: 3.8613264560699463\n",
      "Epoch 473: Train Loss: 0.18945492506027223, Validation Loss: 3.755385637283325\n",
      "Epoch 474: Train Loss: 0.1841476283967495, Validation Loss: 3.818063974380493\n",
      "Epoch 475: Train Loss: 0.205113685131073, Validation Loss: 3.9064295291900635\n",
      "Epoch 476: Train Loss: 0.14132853113114835, Validation Loss: 3.73154354095459\n",
      "Epoch 477: Train Loss: 0.23630175590515137, Validation Loss: 3.622880458831787\n",
      "Epoch 478: Train Loss: 0.18373190313577653, Validation Loss: 3.744699239730835\n",
      "Epoch 479: Train Loss: 0.19306391775608062, Validation Loss: 3.672635078430176\n",
      "Epoch 480: Train Loss: 0.21326533257961272, Validation Loss: 3.744302272796631\n",
      "Epoch 481: Train Loss: 0.1722608596086502, Validation Loss: 3.9622812271118164\n",
      "Epoch 482: Train Loss: 0.19563844203948974, Validation Loss: 4.089319229125977\n",
      "Epoch 483: Train Loss: 0.3473857015371323, Validation Loss: 4.100955963134766\n",
      "Epoch 484: Train Loss: 0.2226707637310028, Validation Loss: 4.096677303314209\n",
      "Epoch 485: Train Loss: 0.2117474764585495, Validation Loss: 3.950352668762207\n",
      "Epoch 486: Train Loss: 0.20706214010715485, Validation Loss: 3.955455780029297\n",
      "Epoch 487: Train Loss: 0.48552161157131196, Validation Loss: 3.9337942600250244\n",
      "Epoch 488: Train Loss: 0.20061495751142502, Validation Loss: 3.9126479625701904\n",
      "Epoch 489: Train Loss: 0.22977955639362335, Validation Loss: 3.913763999938965\n",
      "Epoch 490: Train Loss: 0.17751583456993103, Validation Loss: 3.8524510860443115\n",
      "Epoch 491: Train Loss: 0.3247629851102829, Validation Loss: 4.028181552886963\n",
      "Epoch 492: Train Loss: 0.2702781975269318, Validation Loss: 3.920665979385376\n",
      "Epoch 493: Train Loss: 0.1528883159160614, Validation Loss: 3.868011951446533\n",
      "Epoch 494: Train Loss: 0.15543021410703659, Validation Loss: 3.8815627098083496\n",
      "Epoch 495: Train Loss: 0.17453232854604722, Validation Loss: 3.852726697921753\n",
      "Epoch 496: Train Loss: 0.21459369063377381, Validation Loss: 3.896118402481079\n",
      "Epoch 497: Train Loss: 0.3523760557174683, Validation Loss: 3.859182119369507\n",
      "Epoch 498: Train Loss: 0.225686714053154, Validation Loss: 3.971320152282715\n",
      "Epoch 499: Train Loss: 0.17642824053764344, Validation Loss: 3.97811222076416\n",
      "Fold 9 Metrics:\n",
      "Accuracy: 0.2727272727272727, Precision: 0.25, Recall: 0.16666666666666666, F1-score: 0.2, AUC: 0.2833333333333334\n",
      "Confusion Matrix:\n",
      "[[2 3]\n",
      " [5 1]]\n",
      "Completed fold 9\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples from subject 5 to test set\n",
      "Adding 6 truth samples from subject 5 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 250)\n",
      "(132, 65, 250)\n",
      "Epoch 0: Train Loss: 0.6760414719581604, Validation Loss: 0.7071008086204529\n",
      "Epoch 1: Train Loss: 0.6569700121879578, Validation Loss: 0.718761146068573\n",
      "Epoch 2: Train Loss: 0.6685427188873291, Validation Loss: 0.7323789000511169\n",
      "Epoch 3: Train Loss: 0.720966923236847, Validation Loss: 0.7515426278114319\n",
      "Epoch 4: Train Loss: 0.6465935349464417, Validation Loss: 0.77277672290802\n",
      "Epoch 5: Train Loss: 0.6478322386741638, Validation Loss: 0.7931281924247742\n",
      "Epoch 6: Train Loss: 0.6800217032432556, Validation Loss: 0.8164794445037842\n",
      "Epoch 7: Train Loss: 0.6576424479484558, Validation Loss: 0.8366097807884216\n",
      "Epoch 8: Train Loss: 0.6179580211639404, Validation Loss: 0.8193517923355103\n",
      "Epoch 9: Train Loss: 0.6252375245094299, Validation Loss: 0.8265568017959595\n",
      "Epoch 10: Train Loss: 0.6407367825508118, Validation Loss: 0.8214896321296692\n",
      "Epoch 11: Train Loss: 0.6565712213516235, Validation Loss: 0.8252675533294678\n",
      "Epoch 12: Train Loss: 0.6534148097038269, Validation Loss: 0.8365122079849243\n",
      "Epoch 13: Train Loss: 0.6450787901878356, Validation Loss: 0.8396112322807312\n",
      "Epoch 14: Train Loss: 0.6354458093643188, Validation Loss: 0.8518829345703125\n",
      "Epoch 15: Train Loss: 0.626750910282135, Validation Loss: 0.8574803471565247\n",
      "Epoch 16: Train Loss: 0.6275325298309327, Validation Loss: 0.8568601608276367\n",
      "Epoch 17: Train Loss: 0.6026556432247162, Validation Loss: 0.8400378823280334\n",
      "Epoch 18: Train Loss: 0.6842783570289612, Validation Loss: 0.830439567565918\n",
      "Epoch 19: Train Loss: 0.642911171913147, Validation Loss: 0.8430749177932739\n",
      "Epoch 20: Train Loss: 0.6302476525306702, Validation Loss: 0.838248074054718\n",
      "Epoch 21: Train Loss: 0.5884581208229065, Validation Loss: 0.8547656536102295\n",
      "Epoch 22: Train Loss: 0.631619679927826, Validation Loss: 0.8780092597007751\n",
      "Epoch 23: Train Loss: 0.6169335126876831, Validation Loss: 0.8815436959266663\n",
      "Epoch 24: Train Loss: 0.662093186378479, Validation Loss: 0.8700897097587585\n",
      "Epoch 25: Train Loss: 0.5993663191795349, Validation Loss: 0.8816664814949036\n",
      "Epoch 26: Train Loss: 0.6176910758018493, Validation Loss: 0.8663299679756165\n",
      "Epoch 27: Train Loss: 0.6509045243263245, Validation Loss: 0.876521110534668\n",
      "Epoch 28: Train Loss: 0.6379523158073426, Validation Loss: 0.8968716859817505\n",
      "Epoch 29: Train Loss: 0.5987764596939087, Validation Loss: 0.896930456161499\n",
      "Epoch 30: Train Loss: 0.6546270608901977, Validation Loss: 0.9090600609779358\n",
      "Epoch 31: Train Loss: 0.5983448386192322, Validation Loss: 0.8986005783081055\n",
      "Epoch 32: Train Loss: 0.6479190945625305, Validation Loss: 0.9031730890274048\n",
      "Epoch 33: Train Loss: 0.6089708805084229, Validation Loss: 0.8871661424636841\n",
      "Epoch 34: Train Loss: 0.6315186858177185, Validation Loss: 0.8967727422714233\n",
      "Epoch 35: Train Loss: 0.6016450881958008, Validation Loss: 0.8762171864509583\n",
      "Epoch 36: Train Loss: 0.61055588722229, Validation Loss: 0.8695281744003296\n",
      "Epoch 37: Train Loss: 0.6391842007637024, Validation Loss: 0.8858193755149841\n",
      "Epoch 38: Train Loss: 0.6024651885032654, Validation Loss: 0.8918783068656921\n",
      "Epoch 39: Train Loss: 0.598863422870636, Validation Loss: 0.8953419923782349\n",
      "Epoch 40: Train Loss: 0.6168686032295227, Validation Loss: 0.899048924446106\n",
      "Epoch 41: Train Loss: 0.5910730481147766, Validation Loss: 0.8885443806648254\n",
      "Epoch 42: Train Loss: 0.5749049782752991, Validation Loss: 0.8949090838432312\n",
      "Epoch 43: Train Loss: 0.590585458278656, Validation Loss: 0.864362359046936\n",
      "Epoch 44: Train Loss: 0.5742318034172058, Validation Loss: 0.8780891299247742\n",
      "Epoch 45: Train Loss: 0.5723222613334655, Validation Loss: 0.8997947573661804\n",
      "Epoch 46: Train Loss: 0.6053885102272034, Validation Loss: 0.8985000252723694\n",
      "Epoch 47: Train Loss: 0.6153152227401734, Validation Loss: 0.890705406665802\n",
      "Epoch 48: Train Loss: 0.578904139995575, Validation Loss: 0.9102332592010498\n",
      "Epoch 49: Train Loss: 0.5686598658561707, Validation Loss: 0.9232802987098694\n",
      "Epoch 50: Train Loss: 0.618751335144043, Validation Loss: 0.9249889850616455\n",
      "Epoch 51: Train Loss: 0.6193350434303284, Validation Loss: 0.9293799996376038\n",
      "Epoch 52: Train Loss: 0.5760383129119873, Validation Loss: 0.9353177547454834\n",
      "Epoch 53: Train Loss: 0.5827405035495759, Validation Loss: 0.9310759902000427\n",
      "Epoch 54: Train Loss: 0.5622864246368409, Validation Loss: 0.9310771822929382\n",
      "Epoch 55: Train Loss: 0.6138252973556518, Validation Loss: 0.916525661945343\n",
      "Epoch 56: Train Loss: 0.6169862031936646, Validation Loss: 0.9165988564491272\n",
      "Epoch 57: Train Loss: 0.5633845329284668, Validation Loss: 0.9177170991897583\n",
      "Epoch 58: Train Loss: 0.5498881697654724, Validation Loss: 0.9432278871536255\n",
      "Epoch 59: Train Loss: 0.5687887072563171, Validation Loss: 0.9519229531288147\n",
      "Epoch 60: Train Loss: 0.5929206967353821, Validation Loss: 0.9489960074424744\n",
      "Epoch 61: Train Loss: 0.6231705546379089, Validation Loss: 0.9588246941566467\n",
      "Epoch 62: Train Loss: 0.5675676226615906, Validation Loss: 0.9475209712982178\n",
      "Epoch 63: Train Loss: 0.6433079957962036, Validation Loss: 0.9485660195350647\n",
      "Epoch 64: Train Loss: 0.5890666604042053, Validation Loss: 0.9308116436004639\n",
      "Epoch 65: Train Loss: 0.5318440020084381, Validation Loss: 0.9263970851898193\n",
      "Epoch 66: Train Loss: 0.5505212545394897, Validation Loss: 0.9325500726699829\n",
      "Epoch 67: Train Loss: 0.5601974844932556, Validation Loss: 0.9296344518661499\n",
      "Epoch 68: Train Loss: 0.5436440885066987, Validation Loss: 0.9452067613601685\n",
      "Epoch 69: Train Loss: 0.529856151342392, Validation Loss: 0.9472410082817078\n",
      "Epoch 70: Train Loss: 0.5333509147167206, Validation Loss: 0.9263168573379517\n",
      "Epoch 71: Train Loss: 0.5362785041332245, Validation Loss: 0.939713716506958\n",
      "Epoch 72: Train Loss: 0.5546681642532348, Validation Loss: 0.9506217241287231\n",
      "Epoch 73: Train Loss: 0.573762857913971, Validation Loss: 0.9545314908027649\n",
      "Epoch 74: Train Loss: 0.5712580919265747, Validation Loss: 0.955515444278717\n",
      "Epoch 75: Train Loss: 0.6135412335395813, Validation Loss: 0.9550517797470093\n",
      "Epoch 76: Train Loss: 0.5539140343666077, Validation Loss: 0.9771628975868225\n",
      "Epoch 77: Train Loss: 0.5305465459823608, Validation Loss: 0.9664061069488525\n",
      "Epoch 78: Train Loss: 0.5506328523159028, Validation Loss: 0.9566341042518616\n",
      "Epoch 79: Train Loss: 0.5154228210449219, Validation Loss: 0.9549829959869385\n",
      "Epoch 80: Train Loss: 0.5486294209957123, Validation Loss: 0.9463917016983032\n",
      "Epoch 81: Train Loss: 0.6337932169437408, Validation Loss: 0.9885408282279968\n",
      "Epoch 82: Train Loss: 0.545131528377533, Validation Loss: 0.9928770065307617\n",
      "Epoch 83: Train Loss: 0.5829898118972778, Validation Loss: 0.9745340943336487\n",
      "Epoch 84: Train Loss: 0.5369003057479859, Validation Loss: 0.978860080242157\n",
      "Epoch 85: Train Loss: 0.5724555015563965, Validation Loss: 0.9900095462799072\n",
      "Epoch 86: Train Loss: 0.5033955335617065, Validation Loss: 0.9939323663711548\n",
      "Epoch 87: Train Loss: 0.563446182012558, Validation Loss: 0.9876892566680908\n",
      "Epoch 88: Train Loss: 0.572126978635788, Validation Loss: 0.9875078201293945\n",
      "Epoch 89: Train Loss: 0.6013851284980773, Validation Loss: 1.0000172853469849\n",
      "Epoch 90: Train Loss: 0.5773779153823853, Validation Loss: 0.9981801509857178\n",
      "Epoch 91: Train Loss: 0.5124974548816681, Validation Loss: 0.9993990063667297\n",
      "Epoch 92: Train Loss: 0.5694446742534638, Validation Loss: 0.9734251499176025\n",
      "Epoch 93: Train Loss: 0.5617117762565613, Validation Loss: 1.0118181705474854\n",
      "Epoch 94: Train Loss: 0.5444644093513489, Validation Loss: 1.025248408317566\n",
      "Epoch 95: Train Loss: 0.5367148280143738, Validation Loss: 0.9996984601020813\n",
      "Epoch 96: Train Loss: 0.49449504613876344, Validation Loss: 0.9931092858314514\n",
      "Epoch 97: Train Loss: 0.5242433428764344, Validation Loss: 1.0006146430969238\n",
      "Epoch 98: Train Loss: 0.5388115167617797, Validation Loss: 0.9906372427940369\n",
      "Epoch 99: Train Loss: 0.5277552902698517, Validation Loss: 0.991436779499054\n",
      "Epoch 100: Train Loss: 0.5025805413722992, Validation Loss: 1.0026475191116333\n",
      "Epoch 101: Train Loss: 0.599562406539917, Validation Loss: 1.0185362100601196\n",
      "Epoch 102: Train Loss: 0.47715393304824827, Validation Loss: 1.0300496816635132\n",
      "Epoch 103: Train Loss: 0.5846670031547546, Validation Loss: 1.0506761074066162\n",
      "Epoch 104: Train Loss: 0.507310950756073, Validation Loss: 1.0281528234481812\n",
      "Epoch 105: Train Loss: 0.48391669392585757, Validation Loss: 1.0329735279083252\n",
      "Epoch 106: Train Loss: 0.49496096968650816, Validation Loss: 1.019181728363037\n",
      "Epoch 107: Train Loss: 0.5108711063861847, Validation Loss: 1.0184341669082642\n",
      "Epoch 108: Train Loss: 0.5266175746917725, Validation Loss: 1.0390450954437256\n",
      "Epoch 109: Train Loss: 0.5295674681663514, Validation Loss: 1.0352537631988525\n",
      "Epoch 110: Train Loss: 0.4796243071556091, Validation Loss: 1.0403894186019897\n",
      "Epoch 111: Train Loss: 0.5506023287773132, Validation Loss: 1.0416032075881958\n",
      "Epoch 112: Train Loss: 0.49200737476348877, Validation Loss: 1.0249265432357788\n",
      "Epoch 113: Train Loss: 0.4494540184736252, Validation Loss: 1.0513983964920044\n",
      "Epoch 114: Train Loss: 0.4786004424095154, Validation Loss: 1.0465577840805054\n",
      "Epoch 115: Train Loss: 0.4984784245491028, Validation Loss: 1.0357650518417358\n",
      "Epoch 116: Train Loss: 0.5509042501449585, Validation Loss: 1.0578676462173462\n",
      "Epoch 117: Train Loss: 0.45684850215911865, Validation Loss: 1.0634055137634277\n",
      "Epoch 118: Train Loss: 0.5299915611743927, Validation Loss: 1.0623486042022705\n",
      "Epoch 119: Train Loss: 0.5135558724403382, Validation Loss: 1.0649064779281616\n",
      "Epoch 120: Train Loss: 0.4944752514362335, Validation Loss: 1.077440857887268\n",
      "Epoch 121: Train Loss: 0.5526583433151245, Validation Loss: 1.0909535884857178\n",
      "Epoch 122: Train Loss: 0.536628782749176, Validation Loss: 1.1158620119094849\n",
      "Epoch 123: Train Loss: 0.45658873319625853, Validation Loss: 1.0669713020324707\n",
      "Epoch 124: Train Loss: 0.5086164355278016, Validation Loss: 1.086333990097046\n",
      "Epoch 125: Train Loss: 0.5376671135425568, Validation Loss: 1.0854134559631348\n",
      "Epoch 126: Train Loss: 0.5729482710361481, Validation Loss: 1.0787913799285889\n",
      "Epoch 127: Train Loss: 0.4695720970630646, Validation Loss: 1.0916922092437744\n",
      "Epoch 128: Train Loss: 0.5269766807556152, Validation Loss: 1.1081539392471313\n",
      "Epoch 129: Train Loss: 0.4890592396259308, Validation Loss: 1.1080589294433594\n",
      "Epoch 130: Train Loss: 0.4728542387485504, Validation Loss: 1.1022112369537354\n",
      "Epoch 131: Train Loss: 0.5150896549224854, Validation Loss: 1.0717419385910034\n",
      "Epoch 132: Train Loss: 0.4548851132392883, Validation Loss: 1.0851600170135498\n",
      "Epoch 133: Train Loss: 0.5575355291366577, Validation Loss: 1.0836654901504517\n",
      "Epoch 134: Train Loss: 0.47574967741966245, Validation Loss: 1.0352481603622437\n",
      "Epoch 135: Train Loss: 0.47706133127212524, Validation Loss: 1.0482497215270996\n",
      "Epoch 136: Train Loss: 0.5888795018196106, Validation Loss: 1.0830200910568237\n",
      "Epoch 137: Train Loss: 0.4369038224220276, Validation Loss: 1.108137845993042\n",
      "Epoch 138: Train Loss: 0.4930183470249176, Validation Loss: 1.0565775632858276\n",
      "Epoch 139: Train Loss: 0.45238240361213683, Validation Loss: 1.055859088897705\n",
      "Epoch 140: Train Loss: 0.46837865114212035, Validation Loss: 1.0840275287628174\n",
      "Epoch 141: Train Loss: 0.43317797780036926, Validation Loss: 1.084633231163025\n",
      "Epoch 142: Train Loss: 0.45024030208587645, Validation Loss: 1.1031265258789062\n",
      "Epoch 143: Train Loss: 0.4972917139530182, Validation Loss: 1.0906498432159424\n",
      "Epoch 144: Train Loss: 0.5016330599784851, Validation Loss: 1.0644562244415283\n",
      "Epoch 145: Train Loss: 0.4815963387489319, Validation Loss: 1.0696218013763428\n",
      "Epoch 146: Train Loss: 0.4506996750831604, Validation Loss: 1.0927045345306396\n",
      "Epoch 147: Train Loss: 0.44718695878982545, Validation Loss: 1.0937179327011108\n",
      "Epoch 148: Train Loss: 0.489297354221344, Validation Loss: 1.068213701248169\n",
      "Epoch 149: Train Loss: 0.4631265938282013, Validation Loss: 1.0635392665863037\n",
      "Epoch 150: Train Loss: 0.42363585233688356, Validation Loss: 1.0949068069458008\n",
      "Epoch 151: Train Loss: 0.48308247327804565, Validation Loss: 1.101662278175354\n",
      "Epoch 152: Train Loss: 0.4282334208488464, Validation Loss: 1.1095000505447388\n",
      "Epoch 153: Train Loss: 0.4736639797687531, Validation Loss: 1.1047368049621582\n",
      "Epoch 154: Train Loss: 0.4446155786514282, Validation Loss: 1.111894965171814\n",
      "Epoch 155: Train Loss: 0.499389237165451, Validation Loss: 1.1188817024230957\n",
      "Epoch 156: Train Loss: 0.4360282480716705, Validation Loss: 1.1464391946792603\n",
      "Epoch 157: Train Loss: 0.4458107888698578, Validation Loss: 1.1236361265182495\n",
      "Epoch 158: Train Loss: 0.4406779944896698, Validation Loss: 1.0803313255310059\n",
      "Epoch 159: Train Loss: 0.46210233569145204, Validation Loss: 1.1121842861175537\n",
      "Epoch 160: Train Loss: 0.4524582207202911, Validation Loss: 1.1365398168563843\n",
      "Epoch 161: Train Loss: 0.3974073797464371, Validation Loss: 1.162583827972412\n",
      "Epoch 162: Train Loss: 0.5092954456806182, Validation Loss: 1.1882315874099731\n",
      "Epoch 163: Train Loss: 0.38680248856544497, Validation Loss: 1.1764813661575317\n",
      "Epoch 164: Train Loss: 0.49213749170303345, Validation Loss: 1.1797912120819092\n",
      "Epoch 165: Train Loss: 0.5118745267391205, Validation Loss: 1.1785942316055298\n",
      "Epoch 166: Train Loss: 0.4934924840927124, Validation Loss: 1.1838465929031372\n",
      "Epoch 167: Train Loss: 0.46861355304718016, Validation Loss: 1.1734347343444824\n",
      "Epoch 168: Train Loss: 0.48598578572273254, Validation Loss: 1.1634395122528076\n",
      "Epoch 169: Train Loss: 0.46560491919517516, Validation Loss: 1.1640141010284424\n",
      "Epoch 170: Train Loss: 0.4318201243877411, Validation Loss: 1.16659414768219\n",
      "Epoch 171: Train Loss: 0.5033077299594879, Validation Loss: 1.1681263446807861\n",
      "Epoch 172: Train Loss: 0.4316081702709198, Validation Loss: 1.1402770280838013\n",
      "Epoch 173: Train Loss: 0.41045510172843935, Validation Loss: 1.1477315425872803\n",
      "Epoch 174: Train Loss: 0.5420897722244262, Validation Loss: 1.1636370420455933\n",
      "Epoch 175: Train Loss: 0.4037461161613464, Validation Loss: 1.1365764141082764\n",
      "Epoch 176: Train Loss: 0.47197315096855164, Validation Loss: 1.1340818405151367\n",
      "Epoch 177: Train Loss: 0.37749471962451936, Validation Loss: 1.1516053676605225\n",
      "Epoch 178: Train Loss: 0.46586690545082093, Validation Loss: 1.1810832023620605\n",
      "Epoch 179: Train Loss: 0.510913735628128, Validation Loss: 1.186285376548767\n",
      "Epoch 180: Train Loss: 0.4077239215373993, Validation Loss: 1.1893268823623657\n",
      "Epoch 181: Train Loss: 0.39925687909126284, Validation Loss: 1.2155917882919312\n",
      "Epoch 182: Train Loss: 0.39688422083854674, Validation Loss: 1.2231191396713257\n",
      "Epoch 183: Train Loss: 0.42015646696090697, Validation Loss: 1.1798980236053467\n",
      "Epoch 184: Train Loss: 0.36597007513046265, Validation Loss: 1.1830188035964966\n",
      "Epoch 185: Train Loss: 0.3942695468664169, Validation Loss: 1.1884586811065674\n",
      "Epoch 186: Train Loss: 0.44825153350830077, Validation Loss: 1.2140763998031616\n",
      "Epoch 187: Train Loss: 0.3904197633266449, Validation Loss: 1.2130194902420044\n",
      "Epoch 188: Train Loss: 0.4373791992664337, Validation Loss: 1.2291514873504639\n",
      "Epoch 189: Train Loss: 0.49625890254974364, Validation Loss: 1.2430527210235596\n",
      "Epoch 190: Train Loss: 0.4219700574874878, Validation Loss: 1.2197450399398804\n",
      "Epoch 191: Train Loss: 0.46305935382843016, Validation Loss: 1.2043426036834717\n",
      "Epoch 192: Train Loss: 0.41216862201690674, Validation Loss: 1.1466116905212402\n",
      "Epoch 193: Train Loss: 0.3826556891202927, Validation Loss: 1.1709411144256592\n",
      "Epoch 194: Train Loss: 0.5277218043804168, Validation Loss: 1.162685513496399\n",
      "Epoch 195: Train Loss: 0.39234288334846495, Validation Loss: 1.1479003429412842\n",
      "Epoch 196: Train Loss: 0.3947326898574829, Validation Loss: 1.165472388267517\n",
      "Epoch 197: Train Loss: 0.45682657361030576, Validation Loss: 1.1651387214660645\n",
      "Epoch 198: Train Loss: 0.5135290682315826, Validation Loss: 1.1571826934814453\n",
      "Epoch 199: Train Loss: 0.411222368478775, Validation Loss: 1.1652244329452515\n",
      "Epoch 200: Train Loss: 0.45823675990104673, Validation Loss: 1.178987979888916\n",
      "Epoch 201: Train Loss: 0.3589392274618149, Validation Loss: 1.1989933252334595\n",
      "Epoch 202: Train Loss: 0.4107590138912201, Validation Loss: 1.1913741827011108\n",
      "Epoch 203: Train Loss: 0.40633252263069153, Validation Loss: 1.214590072631836\n",
      "Epoch 204: Train Loss: 0.3862486660480499, Validation Loss: 1.1994813680648804\n",
      "Epoch 205: Train Loss: 0.426235294342041, Validation Loss: 1.211342215538025\n",
      "Epoch 206: Train Loss: 0.38322237730026243, Validation Loss: 1.2396585941314697\n",
      "Epoch 207: Train Loss: 0.47745378613471984, Validation Loss: 1.2287827730178833\n",
      "Epoch 208: Train Loss: 0.5263575315475464, Validation Loss: 1.2599387168884277\n",
      "Epoch 209: Train Loss: 0.43950414657592773, Validation Loss: 1.2315210103988647\n",
      "Epoch 210: Train Loss: 0.4522352874279022, Validation Loss: 1.2221115827560425\n",
      "Epoch 211: Train Loss: 0.4058165609836578, Validation Loss: 1.2353113889694214\n",
      "Epoch 212: Train Loss: 0.4058325469493866, Validation Loss: 1.1978280544281006\n",
      "Epoch 213: Train Loss: 0.37978833317756655, Validation Loss: 1.2342114448547363\n",
      "Epoch 214: Train Loss: 0.37317869663238523, Validation Loss: 1.2252492904663086\n",
      "Epoch 215: Train Loss: 0.42179195284843446, Validation Loss: 1.2439424991607666\n",
      "Epoch 216: Train Loss: 0.4089382171630859, Validation Loss: 1.2634345293045044\n",
      "Epoch 217: Train Loss: 0.37847915291786194, Validation Loss: 1.2447346448898315\n",
      "Epoch 218: Train Loss: 0.37829135060310365, Validation Loss: 1.25624418258667\n",
      "Epoch 219: Train Loss: 0.4584564924240112, Validation Loss: 1.2478965520858765\n",
      "Epoch 220: Train Loss: 0.42246112823486326, Validation Loss: 1.2562015056610107\n",
      "Epoch 221: Train Loss: 0.3919172823429108, Validation Loss: 1.2727382183074951\n",
      "Epoch 222: Train Loss: 0.40465214252471926, Validation Loss: 1.2682769298553467\n",
      "Epoch 223: Train Loss: 0.3981046736240387, Validation Loss: 1.2830458879470825\n",
      "Epoch 224: Train Loss: 0.41353986263275144, Validation Loss: 1.2913825511932373\n",
      "Epoch 225: Train Loss: 0.3586136281490326, Validation Loss: 1.319204568862915\n",
      "Epoch 226: Train Loss: 0.4105062007904053, Validation Loss: 1.2277494668960571\n",
      "Epoch 227: Train Loss: 0.4242610216140747, Validation Loss: 1.237891435623169\n",
      "Epoch 228: Train Loss: 0.38016473054885863, Validation Loss: 1.2612642049789429\n",
      "Epoch 229: Train Loss: 0.4151261210441589, Validation Loss: 1.279883623123169\n",
      "Epoch 230: Train Loss: 0.4781694233417511, Validation Loss: 1.2816518545150757\n",
      "Epoch 231: Train Loss: 0.37637072801589966, Validation Loss: 1.3160161972045898\n",
      "Epoch 232: Train Loss: 0.3619389981031418, Validation Loss: 1.3030604124069214\n",
      "Epoch 233: Train Loss: 0.40121614933013916, Validation Loss: 1.2904269695281982\n",
      "Epoch 234: Train Loss: 0.41395061612129214, Validation Loss: 1.2532352209091187\n",
      "Epoch 235: Train Loss: 0.38227052688598634, Validation Loss: 1.2392549514770508\n",
      "Epoch 236: Train Loss: 0.36080882549285886, Validation Loss: 1.2705527544021606\n",
      "Epoch 237: Train Loss: 0.457316517829895, Validation Loss: 1.2901828289031982\n",
      "Epoch 238: Train Loss: 0.3534135460853577, Validation Loss: 1.2882592678070068\n",
      "Epoch 239: Train Loss: 0.415283739566803, Validation Loss: 1.2967536449432373\n",
      "Epoch 240: Train Loss: 0.3619894504547119, Validation Loss: 1.2668542861938477\n",
      "Epoch 241: Train Loss: 0.4124968767166138, Validation Loss: 1.3289273977279663\n",
      "Epoch 242: Train Loss: 0.37112690806388854, Validation Loss: 1.3556185960769653\n",
      "Epoch 243: Train Loss: 0.3832388460636139, Validation Loss: 1.3597559928894043\n",
      "Epoch 244: Train Loss: 0.5100224196910859, Validation Loss: 1.3406039476394653\n",
      "Epoch 245: Train Loss: 0.3791075646877289, Validation Loss: 1.283016562461853\n",
      "Epoch 246: Train Loss: 0.4218760132789612, Validation Loss: 1.3260791301727295\n",
      "Epoch 247: Train Loss: 0.5165822386741639, Validation Loss: 1.3604843616485596\n",
      "Epoch 248: Train Loss: 0.363875812292099, Validation Loss: 1.366309404373169\n",
      "Epoch 249: Train Loss: 0.4357667565345764, Validation Loss: 1.3718010187149048\n",
      "Epoch 250: Train Loss: 0.4258281111717224, Validation Loss: 1.3580691814422607\n",
      "Epoch 251: Train Loss: 0.3413688063621521, Validation Loss: 1.315864086151123\n",
      "Epoch 252: Train Loss: 0.4058066844940186, Validation Loss: 1.3540574312210083\n",
      "Epoch 253: Train Loss: 0.42933780550956724, Validation Loss: 1.3668434619903564\n",
      "Epoch 254: Train Loss: 0.37819072008132937, Validation Loss: 1.3102508783340454\n",
      "Epoch 255: Train Loss: 0.3326577216386795, Validation Loss: 1.3237470388412476\n",
      "Epoch 256: Train Loss: 0.34227812886238096, Validation Loss: 1.3529161214828491\n",
      "Epoch 257: Train Loss: 0.4083680272102356, Validation Loss: 1.3615500926971436\n",
      "Epoch 258: Train Loss: 0.3073869362473488, Validation Loss: 1.3931639194488525\n",
      "Epoch 259: Train Loss: 0.45989585518836973, Validation Loss: 1.3988516330718994\n",
      "Epoch 260: Train Loss: 0.3506061345338821, Validation Loss: 1.4141104221343994\n",
      "Epoch 261: Train Loss: 0.3577132523059845, Validation Loss: 1.424530029296875\n",
      "Epoch 262: Train Loss: 0.37809057235717775, Validation Loss: 1.4421710968017578\n",
      "Epoch 263: Train Loss: 0.4204562962055206, Validation Loss: 1.4486303329467773\n",
      "Epoch 264: Train Loss: 0.3672280550003052, Validation Loss: 1.4461426734924316\n",
      "Epoch 265: Train Loss: 0.398519903421402, Validation Loss: 1.4766672849655151\n",
      "Epoch 266: Train Loss: 0.35659356117248536, Validation Loss: 1.4286537170410156\n",
      "Epoch 267: Train Loss: 0.464572811126709, Validation Loss: 1.4120852947235107\n",
      "Epoch 268: Train Loss: 0.3360184907913208, Validation Loss: 1.4157538414001465\n",
      "Epoch 269: Train Loss: 0.4054209470748901, Validation Loss: 1.4491077661514282\n",
      "Epoch 270: Train Loss: 0.3061433583498001, Validation Loss: 1.474866271018982\n",
      "Epoch 271: Train Loss: 0.3029029041528702, Validation Loss: 1.4978710412979126\n",
      "Epoch 272: Train Loss: 0.39861544966697693, Validation Loss: 1.4657751321792603\n",
      "Epoch 273: Train Loss: 0.33268452286720274, Validation Loss: 1.448426604270935\n",
      "Epoch 274: Train Loss: 0.33000285029411314, Validation Loss: 1.4660861492156982\n",
      "Epoch 275: Train Loss: 0.33216302990913393, Validation Loss: 1.4672472476959229\n",
      "Epoch 276: Train Loss: 0.4249990820884705, Validation Loss: 1.4181135892868042\n",
      "Epoch 277: Train Loss: 0.3554708659648895, Validation Loss: 1.373162031173706\n",
      "Epoch 278: Train Loss: 0.3812018930912018, Validation Loss: 1.4174224138259888\n",
      "Epoch 279: Train Loss: 0.42795554995536805, Validation Loss: 1.4085667133331299\n",
      "Epoch 280: Train Loss: 0.40294365882873534, Validation Loss: 1.4428496360778809\n",
      "Epoch 281: Train Loss: 0.3400205075740814, Validation Loss: 1.449610710144043\n",
      "Epoch 282: Train Loss: 0.3676710665225983, Validation Loss: 1.4117908477783203\n",
      "Epoch 283: Train Loss: 0.3259857833385468, Validation Loss: 1.4205381870269775\n",
      "Epoch 284: Train Loss: 0.45595474243164064, Validation Loss: 1.4362170696258545\n",
      "Epoch 285: Train Loss: 0.39956260323524473, Validation Loss: 1.4710211753845215\n",
      "Epoch 286: Train Loss: 0.31799531579017637, Validation Loss: 1.4797338247299194\n",
      "Epoch 287: Train Loss: 0.3636007845401764, Validation Loss: 1.4867236614227295\n",
      "Epoch 288: Train Loss: 0.4422134518623352, Validation Loss: 1.4749826192855835\n",
      "Epoch 289: Train Loss: 0.36882002353668214, Validation Loss: 1.3220348358154297\n",
      "Epoch 290: Train Loss: 0.3510601341724396, Validation Loss: 1.326643943786621\n",
      "Epoch 291: Train Loss: 0.381563276052475, Validation Loss: 1.3910883665084839\n",
      "Epoch 292: Train Loss: 0.3082509845495224, Validation Loss: 1.4443913698196411\n",
      "Epoch 293: Train Loss: 0.3179830819368362, Validation Loss: 1.3993208408355713\n",
      "Epoch 294: Train Loss: 0.3202564686536789, Validation Loss: 1.4049606323242188\n",
      "Epoch 295: Train Loss: 0.3676131904125214, Validation Loss: 1.43071448802948\n",
      "Epoch 296: Train Loss: 0.34037335515022277, Validation Loss: 1.4382308721542358\n",
      "Epoch 297: Train Loss: 0.34291494488716123, Validation Loss: 1.4857542514801025\n",
      "Epoch 298: Train Loss: 0.3630251348018646, Validation Loss: 1.510095477104187\n",
      "Epoch 299: Train Loss: 0.35931450724601743, Validation Loss: 1.5427134037017822\n",
      "Epoch 300: Train Loss: 0.3322217881679535, Validation Loss: 1.5029453039169312\n",
      "Epoch 301: Train Loss: 0.34797977805137636, Validation Loss: 1.480399250984192\n",
      "Epoch 302: Train Loss: 0.33120693564414977, Validation Loss: 1.3958481550216675\n",
      "Epoch 303: Train Loss: 0.3213475704193115, Validation Loss: 1.413900375366211\n",
      "Epoch 304: Train Loss: 0.32141866683959963, Validation Loss: 1.43793523311615\n",
      "Epoch 305: Train Loss: 0.33883862793445585, Validation Loss: 1.4994288682937622\n",
      "Epoch 306: Train Loss: 0.2882249429821968, Validation Loss: 1.513676643371582\n",
      "Epoch 307: Train Loss: 0.3256754010915756, Validation Loss: 1.528638482093811\n",
      "Epoch 308: Train Loss: 0.40706542134284973, Validation Loss: 1.5592010021209717\n",
      "Epoch 309: Train Loss: 0.2979289084672928, Validation Loss: 1.45551335811615\n",
      "Epoch 310: Train Loss: 0.3180191874504089, Validation Loss: 1.4995489120483398\n",
      "Epoch 311: Train Loss: 0.32828677296638487, Validation Loss: 1.5533891916275024\n",
      "Epoch 312: Train Loss: 0.3048398017883301, Validation Loss: 1.6023989915847778\n",
      "Epoch 313: Train Loss: 0.32129770517349243, Validation Loss: 1.5805842876434326\n",
      "Epoch 314: Train Loss: 0.3665432035923004, Validation Loss: 1.520417332649231\n",
      "Epoch 315: Train Loss: 0.3161653488874435, Validation Loss: 1.5906696319580078\n",
      "Epoch 316: Train Loss: 0.4011833071708679, Validation Loss: 1.528053879737854\n",
      "Epoch 317: Train Loss: 0.32044297754764556, Validation Loss: 1.5260554552078247\n",
      "Epoch 318: Train Loss: 0.3411578595638275, Validation Loss: 1.452939748764038\n",
      "Epoch 319: Train Loss: 0.32014913856983185, Validation Loss: 1.4949424266815186\n",
      "Epoch 320: Train Loss: 0.3112082153558731, Validation Loss: 1.5357245206832886\n",
      "Epoch 321: Train Loss: 0.32358994483947756, Validation Loss: 1.5801043510437012\n",
      "Epoch 322: Train Loss: 0.3082994967699051, Validation Loss: 1.637973666191101\n",
      "Epoch 323: Train Loss: 0.3356526970863342, Validation Loss: 1.6735725402832031\n",
      "Epoch 324: Train Loss: 0.29877910315990447, Validation Loss: 1.696320652961731\n",
      "Epoch 325: Train Loss: 0.35767897963523865, Validation Loss: 1.7053297758102417\n",
      "Epoch 326: Train Loss: 0.33469570279121397, Validation Loss: 1.6568233966827393\n",
      "Epoch 327: Train Loss: 0.2823956489562988, Validation Loss: 1.604201316833496\n",
      "Epoch 328: Train Loss: 0.34463473558425906, Validation Loss: 1.5723674297332764\n",
      "Epoch 329: Train Loss: 0.3120212495326996, Validation Loss: 1.6085628271102905\n",
      "Epoch 330: Train Loss: 0.3446295440196991, Validation Loss: 1.620752215385437\n",
      "Epoch 331: Train Loss: 0.30137215852737426, Validation Loss: 1.6518458127975464\n",
      "Epoch 332: Train Loss: 0.2687418520450592, Validation Loss: 1.6312071084976196\n",
      "Epoch 333: Train Loss: 0.3671527922153473, Validation Loss: 1.577418565750122\n",
      "Epoch 334: Train Loss: 0.2615156456828117, Validation Loss: 1.6014862060546875\n",
      "Epoch 335: Train Loss: 0.377034193277359, Validation Loss: 1.6649855375289917\n",
      "Epoch 336: Train Loss: 0.28268543630838394, Validation Loss: 1.6507450342178345\n",
      "Epoch 337: Train Loss: 0.34786377251148226, Validation Loss: 1.6574366092681885\n",
      "Epoch 338: Train Loss: 0.2782564997673035, Validation Loss: 1.6454155445098877\n",
      "Epoch 339: Train Loss: 0.2840317964553833, Validation Loss: 1.639327883720398\n",
      "Epoch 340: Train Loss: 0.3166593641042709, Validation Loss: 1.5904643535614014\n",
      "Epoch 341: Train Loss: 0.36727745532989503, Validation Loss: 1.6256606578826904\n",
      "Epoch 342: Train Loss: 0.3833218514919281, Validation Loss: 1.6609503030776978\n",
      "Epoch 343: Train Loss: 0.26821227073669435, Validation Loss: 1.6687430143356323\n",
      "Epoch 344: Train Loss: 0.30820644497871397, Validation Loss: 1.6097941398620605\n",
      "Epoch 345: Train Loss: 0.323461589217186, Validation Loss: 1.707686185836792\n",
      "Epoch 346: Train Loss: 0.29558939933776857, Validation Loss: 1.7101866006851196\n",
      "Epoch 347: Train Loss: 0.2528532400727272, Validation Loss: 1.6789429187774658\n",
      "Epoch 348: Train Loss: 0.31289889812469485, Validation Loss: 1.6931811571121216\n",
      "Epoch 349: Train Loss: 0.28505181074142455, Validation Loss: 1.5714107751846313\n",
      "Epoch 350: Train Loss: 0.3086314588785172, Validation Loss: 1.5445520877838135\n",
      "Epoch 351: Train Loss: 0.436712646484375, Validation Loss: 1.5859904289245605\n",
      "Epoch 352: Train Loss: 0.3023152083158493, Validation Loss: 1.568318486213684\n",
      "Epoch 353: Train Loss: 0.38217144310474394, Validation Loss: 1.6424273252487183\n",
      "Epoch 354: Train Loss: 0.31750741600990295, Validation Loss: 1.6276812553405762\n",
      "Epoch 355: Train Loss: 0.3153224468231201, Validation Loss: 1.6130489110946655\n",
      "Epoch 356: Train Loss: 0.25869956612586975, Validation Loss: 1.673336386680603\n",
      "Epoch 357: Train Loss: 0.302087140083313, Validation Loss: 1.675561547279358\n",
      "Epoch 358: Train Loss: 0.4197960555553436, Validation Loss: 1.716713309288025\n",
      "Epoch 359: Train Loss: 0.2975721836090088, Validation Loss: 1.7130590677261353\n",
      "Epoch 360: Train Loss: 0.31234024167060853, Validation Loss: 1.683265209197998\n",
      "Epoch 361: Train Loss: 0.2999241828918457, Validation Loss: 1.7918150424957275\n",
      "Epoch 362: Train Loss: 0.2995808064937592, Validation Loss: 1.758760690689087\n",
      "Epoch 363: Train Loss: 0.3148024559020996, Validation Loss: 1.723297357559204\n",
      "Epoch 364: Train Loss: 0.43455569744110106, Validation Loss: 1.8080408573150635\n",
      "Epoch 365: Train Loss: 0.34845698475837705, Validation Loss: 1.7184422016143799\n",
      "Epoch 366: Train Loss: 0.2835196554660797, Validation Loss: 1.7199591398239136\n",
      "Epoch 367: Train Loss: 0.3372171938419342, Validation Loss: 1.7654320001602173\n",
      "Epoch 368: Train Loss: 0.30864912271499634, Validation Loss: 1.7173975706100464\n",
      "Epoch 369: Train Loss: 0.37979312539100646, Validation Loss: 1.5679552555084229\n",
      "Epoch 370: Train Loss: 0.31321892738342283, Validation Loss: 1.6240001916885376\n",
      "Epoch 371: Train Loss: 0.261269611120224, Validation Loss: 1.6523029804229736\n",
      "Epoch 372: Train Loss: 0.34274382293224337, Validation Loss: 1.7274376153945923\n",
      "Epoch 373: Train Loss: 0.2850704550743103, Validation Loss: 1.7944399118423462\n",
      "Epoch 374: Train Loss: 0.25402727723121643, Validation Loss: 1.7315901517868042\n",
      "Epoch 375: Train Loss: 0.2652819722890854, Validation Loss: 1.7628414630889893\n",
      "Epoch 376: Train Loss: 0.28108492493629456, Validation Loss: 1.7888622283935547\n",
      "Epoch 377: Train Loss: 0.24265832006931304, Validation Loss: 1.8256809711456299\n",
      "Epoch 378: Train Loss: 0.26554986238479616, Validation Loss: 1.7901382446289062\n",
      "Epoch 379: Train Loss: 0.3149086803197861, Validation Loss: 1.8128972053527832\n",
      "Epoch 380: Train Loss: 0.3009868264198303, Validation Loss: 1.820400595664978\n",
      "Epoch 381: Train Loss: 0.3270475447177887, Validation Loss: 1.7790645360946655\n",
      "Epoch 382: Train Loss: 0.33365647196769715, Validation Loss: 1.846368670463562\n",
      "Epoch 383: Train Loss: 0.29648311734199523, Validation Loss: 1.9174578189849854\n",
      "Epoch 384: Train Loss: 0.4100697934627533, Validation Loss: 2.01945161819458\n",
      "Epoch 385: Train Loss: 0.38142403960227966, Validation Loss: 2.002394199371338\n",
      "Epoch 386: Train Loss: 0.30856473445892335, Validation Loss: 1.9449273347854614\n",
      "Epoch 387: Train Loss: 0.2295623876154423, Validation Loss: 1.9190928936004639\n",
      "Epoch 388: Train Loss: 0.24660293012857437, Validation Loss: 1.9277747869491577\n",
      "Epoch 389: Train Loss: 0.40847129225730894, Validation Loss: 1.9493361711502075\n",
      "Epoch 390: Train Loss: 0.36449311673641205, Validation Loss: 1.9323874711990356\n",
      "Epoch 391: Train Loss: 0.2583268523216248, Validation Loss: 1.9645781517028809\n",
      "Epoch 392: Train Loss: 0.2999378085136414, Validation Loss: 1.9506688117980957\n",
      "Epoch 393: Train Loss: 0.28732889890670776, Validation Loss: 1.9049930572509766\n",
      "Epoch 394: Train Loss: 0.2794773131608963, Validation Loss: 1.7757974863052368\n",
      "Epoch 395: Train Loss: 0.3172217309474945, Validation Loss: 1.8733891248703003\n",
      "Epoch 396: Train Loss: 0.25667394548654554, Validation Loss: 1.87751042842865\n",
      "Epoch 397: Train Loss: 0.3061977565288544, Validation Loss: 1.9357222318649292\n",
      "Epoch 398: Train Loss: 0.3892306715250015, Validation Loss: 1.7946628332138062\n",
      "Epoch 399: Train Loss: 0.2815965712070465, Validation Loss: 1.9190256595611572\n",
      "Epoch 400: Train Loss: 0.2884851098060608, Validation Loss: 1.9448761940002441\n",
      "Epoch 401: Train Loss: 0.2790578484535217, Validation Loss: 1.9403910636901855\n",
      "Epoch 402: Train Loss: 0.42493586242198944, Validation Loss: 1.9697250127792358\n",
      "Epoch 403: Train Loss: 0.266169410943985, Validation Loss: 1.9505184888839722\n",
      "Epoch 404: Train Loss: 0.22538180649280548, Validation Loss: 2.0224664211273193\n",
      "Epoch 405: Train Loss: 0.3276319533586502, Validation Loss: 1.8986846208572388\n",
      "Epoch 406: Train Loss: 0.2651120275259018, Validation Loss: 1.9237749576568604\n",
      "Epoch 407: Train Loss: 0.33261689841747283, Validation Loss: 1.8436508178710938\n",
      "Epoch 408: Train Loss: 0.2484441265463829, Validation Loss: 1.902018666267395\n",
      "Epoch 409: Train Loss: 0.3413379520177841, Validation Loss: 1.8778671026229858\n",
      "Epoch 410: Train Loss: 0.24240354895591737, Validation Loss: 1.9721572399139404\n",
      "Epoch 411: Train Loss: 0.249030464887619, Validation Loss: 2.0178394317626953\n",
      "Epoch 412: Train Loss: 0.2466190218925476, Validation Loss: 2.0457799434661865\n",
      "Epoch 413: Train Loss: 0.24365495443344115, Validation Loss: 2.084118604660034\n",
      "Epoch 414: Train Loss: 0.20732895508408547, Validation Loss: 2.1261651515960693\n",
      "Epoch 415: Train Loss: 0.4024224877357483, Validation Loss: 2.042545795440674\n",
      "Epoch 416: Train Loss: 0.29915741086006165, Validation Loss: 2.00335955619812\n",
      "Epoch 417: Train Loss: 0.23468294441699983, Validation Loss: 1.922563910484314\n",
      "Epoch 418: Train Loss: 0.2524478703737259, Validation Loss: 1.9655295610427856\n",
      "Epoch 419: Train Loss: 0.30617171823978423, Validation Loss: 1.9616246223449707\n",
      "Epoch 420: Train Loss: 0.3264447033405304, Validation Loss: 2.062866449356079\n",
      "Epoch 421: Train Loss: 0.2696335852146149, Validation Loss: 2.1152424812316895\n",
      "Epoch 422: Train Loss: 0.26794432401657103, Validation Loss: 2.029040813446045\n",
      "Epoch 423: Train Loss: 0.2684462070465088, Validation Loss: 2.0674397945404053\n",
      "Epoch 424: Train Loss: 0.29419725835323335, Validation Loss: 2.06362247467041\n",
      "Epoch 425: Train Loss: 0.23083129823207854, Validation Loss: 1.982934594154358\n",
      "Epoch 426: Train Loss: 0.25500700771808626, Validation Loss: 1.8966102600097656\n",
      "Epoch 427: Train Loss: 0.2527105987071991, Validation Loss: 1.9861873388290405\n",
      "Epoch 428: Train Loss: 0.23324232548475266, Validation Loss: 2.014155149459839\n",
      "Epoch 429: Train Loss: 0.2494337409734726, Validation Loss: 2.014150857925415\n",
      "Epoch 430: Train Loss: 0.2701870918273926, Validation Loss: 2.0025217533111572\n",
      "Epoch 431: Train Loss: 0.24965940415859222, Validation Loss: 2.046459674835205\n",
      "Epoch 432: Train Loss: 0.23814662396907807, Validation Loss: 2.0468132495880127\n",
      "Epoch 433: Train Loss: 0.2537939637899399, Validation Loss: 2.042705774307251\n",
      "Epoch 434: Train Loss: 0.23160505071282386, Validation Loss: 1.9716625213623047\n",
      "Epoch 435: Train Loss: 0.2677465170621872, Validation Loss: 2.090794563293457\n",
      "Epoch 436: Train Loss: 0.26439368426799775, Validation Loss: 2.1026787757873535\n",
      "Epoch 437: Train Loss: 0.27148939967155455, Validation Loss: 2.1301965713500977\n",
      "Epoch 438: Train Loss: 0.24084660708904265, Validation Loss: 1.9870356321334839\n",
      "Epoch 439: Train Loss: 0.43297710120677946, Validation Loss: 2.1104748249053955\n",
      "Epoch 440: Train Loss: 0.2357059523463249, Validation Loss: 2.105654239654541\n",
      "Epoch 441: Train Loss: 0.36651876866817473, Validation Loss: 1.920229434967041\n",
      "Epoch 442: Train Loss: 0.24342046976089476, Validation Loss: 1.9952647686004639\n",
      "Epoch 443: Train Loss: 0.29506569504737856, Validation Loss: 2.1462411880493164\n",
      "Epoch 444: Train Loss: 0.402081573009491, Validation Loss: 2.159968376159668\n",
      "Epoch 445: Train Loss: 0.2218582436442375, Validation Loss: 2.1923253536224365\n",
      "Epoch 446: Train Loss: 0.3477023184299469, Validation Loss: 2.223430633544922\n",
      "Epoch 447: Train Loss: 0.24724187552928925, Validation Loss: 2.157411813735962\n",
      "Epoch 448: Train Loss: 0.23087795078754425, Validation Loss: 2.107372522354126\n",
      "Epoch 449: Train Loss: 0.26461314857006074, Validation Loss: 2.169931411743164\n",
      "Epoch 450: Train Loss: 0.21269787102937698, Validation Loss: 2.1519055366516113\n",
      "Epoch 451: Train Loss: 0.23156377375125886, Validation Loss: 2.1579854488372803\n",
      "Epoch 452: Train Loss: 0.28360236883163453, Validation Loss: 2.1599767208099365\n",
      "Epoch 453: Train Loss: 0.2980649471282959, Validation Loss: 2.1887831687927246\n",
      "Epoch 454: Train Loss: 0.2833767294883728, Validation Loss: 2.2480993270874023\n",
      "Epoch 455: Train Loss: 0.2301335006952286, Validation Loss: 2.3202390670776367\n",
      "Epoch 456: Train Loss: 0.26596428751945494, Validation Loss: 2.326521158218384\n",
      "Epoch 457: Train Loss: 0.2758609354496002, Validation Loss: 2.323814630508423\n",
      "Epoch 458: Train Loss: 0.2240477666258812, Validation Loss: 2.2519826889038086\n",
      "Epoch 459: Train Loss: 0.26831836998462677, Validation Loss: 2.291919469833374\n",
      "Epoch 460: Train Loss: 0.21897203102707863, Validation Loss: 2.3397507667541504\n",
      "Epoch 461: Train Loss: 0.20882856249809265, Validation Loss: 2.329831600189209\n",
      "Epoch 462: Train Loss: 0.2901916325092316, Validation Loss: 2.336373805999756\n",
      "Epoch 463: Train Loss: 0.25279320776462555, Validation Loss: 2.2880918979644775\n",
      "Epoch 464: Train Loss: 0.24562955200672149, Validation Loss: 2.2854912281036377\n",
      "Epoch 465: Train Loss: 0.240406796336174, Validation Loss: 2.328908681869507\n",
      "Epoch 466: Train Loss: 0.21263985559344292, Validation Loss: 2.167978286743164\n",
      "Epoch 467: Train Loss: 0.37420453131198883, Validation Loss: 1.9948468208312988\n",
      "Epoch 468: Train Loss: 0.22406924962997438, Validation Loss: 2.0979387760162354\n",
      "Epoch 469: Train Loss: 0.299869966506958, Validation Loss: 2.202537775039673\n",
      "Epoch 470: Train Loss: 0.24989849627017974, Validation Loss: 2.103442907333374\n",
      "Epoch 471: Train Loss: 0.2863555759191513, Validation Loss: 2.0693233013153076\n",
      "Epoch 472: Train Loss: 0.2626940429210663, Validation Loss: 2.279860496520996\n",
      "Epoch 473: Train Loss: 0.2834361642599106, Validation Loss: 2.226011037826538\n",
      "Epoch 474: Train Loss: 0.2745545536279678, Validation Loss: 2.2731547355651855\n",
      "Epoch 475: Train Loss: 0.2581129729747772, Validation Loss: 2.1442925930023193\n",
      "Epoch 476: Train Loss: 0.2360580712556839, Validation Loss: 2.206742525100708\n",
      "Epoch 477: Train Loss: 0.23603378236293793, Validation Loss: 2.2378668785095215\n",
      "Epoch 478: Train Loss: 0.29401749968528745, Validation Loss: 2.2199084758758545\n",
      "Epoch 479: Train Loss: 0.23180138617753981, Validation Loss: 2.3245222568511963\n",
      "Epoch 480: Train Loss: 0.22766988575458527, Validation Loss: 2.252345561981201\n",
      "Epoch 481: Train Loss: 0.21111775636672975, Validation Loss: 2.200620174407959\n",
      "Epoch 482: Train Loss: 0.23591333329677583, Validation Loss: 2.1856093406677246\n",
      "Epoch 483: Train Loss: 0.24901846349239348, Validation Loss: 2.2250547409057617\n",
      "Epoch 484: Train Loss: 0.21893778294324875, Validation Loss: 2.3017659187316895\n",
      "Epoch 485: Train Loss: 0.31615170538425447, Validation Loss: 2.310816764831543\n",
      "Epoch 486: Train Loss: 0.2422921061515808, Validation Loss: 2.2778210639953613\n",
      "Epoch 487: Train Loss: 0.1985102653503418, Validation Loss: 2.4076590538024902\n",
      "Epoch 488: Train Loss: 0.22692283391952514, Validation Loss: 2.2870676517486572\n",
      "Epoch 489: Train Loss: 0.24560362696647645, Validation Loss: 2.306173086166382\n",
      "Epoch 490: Train Loss: 0.3004527658224106, Validation Loss: 2.3906729221343994\n",
      "Epoch 491: Train Loss: 0.22238715291023253, Validation Loss: 2.392176389694214\n",
      "Epoch 492: Train Loss: 0.2220953345298767, Validation Loss: 2.4213085174560547\n",
      "Epoch 493: Train Loss: 0.21988362967967987, Validation Loss: 2.468860387802124\n",
      "Epoch 494: Train Loss: 0.21686942577362062, Validation Loss: 2.573082208633423\n",
      "Epoch 495: Train Loss: 0.26860580742359164, Validation Loss: 2.508838176727295\n",
      "Epoch 496: Train Loss: 0.19278448671102524, Validation Loss: 2.2539684772491455\n",
      "Epoch 497: Train Loss: 0.28526902496814727, Validation Loss: 2.2836225032806396\n",
      "Epoch 498: Train Loss: 0.22977987825870513, Validation Loss: 2.2993247509002686\n",
      "Epoch 499: Train Loss: 0.25172384083271027, Validation Loss: 2.4119153022766113\n",
      "Fold 10 Metrics:\n",
      "Accuracy: 0.36363636363636365, Precision: 0.4, Recall: 0.3333333333333333, F1-score: 0.36363636363636365, AUC: 0.36666666666666664\n",
      "Confusion Matrix:\n",
      "[[2 3]\n",
      " [4 2]]\n",
      "Completed fold 10\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples from subject 1 to test set\n",
      "Adding 6 truth samples from subject 1 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 250)\n",
      "(132, 65, 250)\n",
      "Epoch 0: Train Loss: 0.8843867301940918, Validation Loss: 0.6966241002082825\n",
      "Epoch 1: Train Loss: 0.7551644563674926, Validation Loss: 0.6994969248771667\n",
      "Epoch 2: Train Loss: 0.7254512906074524, Validation Loss: 0.7033010125160217\n",
      "Epoch 3: Train Loss: 0.6862499475479126, Validation Loss: 0.7075037956237793\n",
      "Epoch 4: Train Loss: 0.7483788847923278, Validation Loss: 0.7137775421142578\n",
      "Epoch 5: Train Loss: 0.7852017045021057, Validation Loss: 0.7182559370994568\n",
      "Epoch 6: Train Loss: 0.7509068727493287, Validation Loss: 0.7200837135314941\n",
      "Epoch 7: Train Loss: 0.6650954842567444, Validation Loss: 0.7221026420593262\n",
      "Epoch 8: Train Loss: 0.7857877492904664, Validation Loss: 0.7243974804878235\n",
      "Epoch 9: Train Loss: 0.7772899627685547, Validation Loss: 0.7219932675361633\n",
      "Epoch 10: Train Loss: 0.7403238177299499, Validation Loss: 0.7233929634094238\n",
      "Epoch 11: Train Loss: 0.7493178009986877, Validation Loss: 0.7258052825927734\n",
      "Epoch 12: Train Loss: 0.7305176973342895, Validation Loss: 0.7235509753227234\n",
      "Epoch 13: Train Loss: 0.7186220645904541, Validation Loss: 0.7220801711082458\n",
      "Epoch 14: Train Loss: 0.7234297513961792, Validation Loss: 0.7218902707099915\n",
      "Epoch 15: Train Loss: 0.6954634308815002, Validation Loss: 0.7155178785324097\n",
      "Epoch 16: Train Loss: 0.6840461134910584, Validation Loss: 0.7190311551094055\n",
      "Epoch 17: Train Loss: 0.6419554471969604, Validation Loss: 0.7233832478523254\n",
      "Epoch 18: Train Loss: 0.6809033155441284, Validation Loss: 0.7261855602264404\n",
      "Epoch 19: Train Loss: 0.6964831471443176, Validation Loss: 0.7266597747802734\n",
      "Epoch 20: Train Loss: 0.7053170323371887, Validation Loss: 0.7277447581291199\n",
      "Epoch 21: Train Loss: 0.7196667551994324, Validation Loss: 0.7314673662185669\n",
      "Epoch 22: Train Loss: 0.7366285443305969, Validation Loss: 0.7362895607948303\n",
      "Epoch 23: Train Loss: 0.7052330613136292, Validation Loss: 0.7339375615119934\n",
      "Epoch 24: Train Loss: 0.7176672101020813, Validation Loss: 0.7303401231765747\n",
      "Epoch 25: Train Loss: 0.7521704792976379, Validation Loss: 0.7319455742835999\n",
      "Epoch 26: Train Loss: 0.6852963685989379, Validation Loss: 0.7319856286048889\n",
      "Epoch 27: Train Loss: 0.6512731313705444, Validation Loss: 0.7362252473831177\n",
      "Epoch 28: Train Loss: 0.6543406009674072, Validation Loss: 0.7388188242912292\n",
      "Epoch 29: Train Loss: 0.6557030856609345, Validation Loss: 0.7390668988227844\n",
      "Epoch 30: Train Loss: 0.7973470211029052, Validation Loss: 0.7368285655975342\n",
      "Epoch 31: Train Loss: 0.7004027485847473, Validation Loss: 0.7368483543395996\n",
      "Epoch 32: Train Loss: 0.6451843500137329, Validation Loss: 0.7358090877532959\n",
      "Epoch 33: Train Loss: 0.6969089388847352, Validation Loss: 0.7350587248802185\n",
      "Epoch 34: Train Loss: 0.7206260323524475, Validation Loss: 0.7350437045097351\n",
      "Epoch 35: Train Loss: 0.6802588701248169, Validation Loss: 0.7314692735671997\n",
      "Epoch 36: Train Loss: 0.6639274835586548, Validation Loss: 0.72809237241745\n",
      "Epoch 37: Train Loss: 0.6888900876045227, Validation Loss: 0.7292764782905579\n",
      "Epoch 38: Train Loss: 0.6606679439544678, Validation Loss: 0.7336075901985168\n",
      "Epoch 39: Train Loss: 0.6731960892677307, Validation Loss: 0.7332596778869629\n",
      "Epoch 40: Train Loss: 0.7235892057418823, Validation Loss: 0.7290360927581787\n",
      "Epoch 41: Train Loss: 0.6698690772056579, Validation Loss: 0.73276287317276\n",
      "Epoch 42: Train Loss: 0.7039390921592712, Validation Loss: 0.7374263405799866\n",
      "Epoch 43: Train Loss: 0.6388683915138245, Validation Loss: 0.7354714870452881\n",
      "Epoch 44: Train Loss: 0.6202620744705201, Validation Loss: 0.7369601130485535\n",
      "Epoch 45: Train Loss: 0.7014573454856873, Validation Loss: 0.7372424006462097\n",
      "Epoch 46: Train Loss: 0.6550979256629944, Validation Loss: 0.7364230751991272\n",
      "Epoch 47: Train Loss: 0.6318507790565491, Validation Loss: 0.7374732494354248\n",
      "Epoch 48: Train Loss: 0.6365745425224304, Validation Loss: 0.7359302639961243\n",
      "Epoch 49: Train Loss: 0.677988839149475, Validation Loss: 0.7418254017829895\n",
      "Epoch 50: Train Loss: 0.6197705268859863, Validation Loss: 0.7414384484291077\n",
      "Epoch 51: Train Loss: 0.5803791999816894, Validation Loss: 0.7389457821846008\n",
      "Epoch 52: Train Loss: 0.6495963573455811, Validation Loss: 0.7355267405509949\n",
      "Epoch 53: Train Loss: 0.6024693965911865, Validation Loss: 0.7363030910491943\n",
      "Epoch 54: Train Loss: 0.5760381817817688, Validation Loss: 0.7376216053962708\n",
      "Epoch 55: Train Loss: 0.678572130203247, Validation Loss: 0.7399436831474304\n",
      "Epoch 56: Train Loss: 0.6929358720779419, Validation Loss: 0.7429965138435364\n",
      "Epoch 57: Train Loss: 0.6506323099136353, Validation Loss: 0.7380797266960144\n",
      "Epoch 58: Train Loss: 0.6273890852928161, Validation Loss: 0.7377039790153503\n",
      "Epoch 59: Train Loss: 0.6227872848510743, Validation Loss: 0.7372506856918335\n",
      "Epoch 60: Train Loss: 0.6629513502120972, Validation Loss: 0.7390663027763367\n",
      "Epoch 61: Train Loss: 0.6041999220848083, Validation Loss: 0.7357946038246155\n",
      "Epoch 62: Train Loss: 0.599793303012848, Validation Loss: 0.733849287033081\n",
      "Epoch 63: Train Loss: 0.6061527252197265, Validation Loss: 0.7320517897605896\n",
      "Epoch 64: Train Loss: 0.5749919652938843, Validation Loss: 0.7368232011795044\n",
      "Epoch 65: Train Loss: 0.6214031338691711, Validation Loss: 0.7367684245109558\n",
      "Epoch 66: Train Loss: 0.5617437839508057, Validation Loss: 0.7392038106918335\n",
      "Epoch 67: Train Loss: 0.626091194152832, Validation Loss: 0.7387656569480896\n",
      "Epoch 68: Train Loss: 0.5958775401115417, Validation Loss: 0.7424901723861694\n",
      "Epoch 69: Train Loss: 0.6731059789657593, Validation Loss: 0.7393916249275208\n",
      "Epoch 70: Train Loss: 0.6282633423805237, Validation Loss: 0.7304988503456116\n",
      "Epoch 71: Train Loss: 0.6561333537101746, Validation Loss: 0.7333793044090271\n",
      "Epoch 72: Train Loss: 0.6184226393699646, Validation Loss: 0.737902045249939\n",
      "Epoch 73: Train Loss: 0.6234704375267028, Validation Loss: 0.7379499077796936\n",
      "Epoch 74: Train Loss: 0.5958993077278137, Validation Loss: 0.7372770309448242\n",
      "Epoch 75: Train Loss: 0.6536812663078309, Validation Loss: 0.7350112199783325\n",
      "Epoch 76: Train Loss: 0.6182675957679749, Validation Loss: 0.7377871870994568\n",
      "Epoch 77: Train Loss: 0.5666666626930237, Validation Loss: 0.7355983853340149\n",
      "Epoch 78: Train Loss: 0.6343871474266052, Validation Loss: 0.7352324724197388\n",
      "Epoch 79: Train Loss: 0.66583753824234, Validation Loss: 0.7362304329872131\n",
      "Epoch 80: Train Loss: 0.6588223457336426, Validation Loss: 0.7380123138427734\n",
      "Epoch 81: Train Loss: 0.5861394524574279, Validation Loss: 0.7382497191429138\n",
      "Epoch 82: Train Loss: 0.6533216595649719, Validation Loss: 0.7395095825195312\n",
      "Epoch 83: Train Loss: 0.5900344967842102, Validation Loss: 0.7456371188163757\n",
      "Epoch 84: Train Loss: 0.5659503221511841, Validation Loss: 0.7512723207473755\n",
      "Epoch 85: Train Loss: 0.5938913583755493, Validation Loss: 0.74777752161026\n",
      "Epoch 86: Train Loss: 0.5934458017349243, Validation Loss: 0.7489732503890991\n",
      "Epoch 87: Train Loss: 0.5608923137187958, Validation Loss: 0.7478501796722412\n",
      "Epoch 88: Train Loss: 0.5934725522994995, Validation Loss: 0.7462882399559021\n",
      "Epoch 89: Train Loss: 0.596916663646698, Validation Loss: 0.7450233101844788\n",
      "Epoch 90: Train Loss: 0.5789652109146118, Validation Loss: 0.7571492195129395\n",
      "Epoch 91: Train Loss: 0.5710650563240052, Validation Loss: 0.7537466883659363\n",
      "Epoch 92: Train Loss: 0.5369685471057892, Validation Loss: 0.756805419921875\n",
      "Epoch 93: Train Loss: 0.5834249913692474, Validation Loss: 0.7518693208694458\n",
      "Epoch 94: Train Loss: 0.5589068412780762, Validation Loss: 0.7571616768836975\n",
      "Epoch 95: Train Loss: 0.55613973736763, Validation Loss: 0.7605655789375305\n",
      "Epoch 96: Train Loss: 0.5657136023044587, Validation Loss: 0.7559753060340881\n",
      "Epoch 97: Train Loss: 0.5216359376907349, Validation Loss: 0.7534767985343933\n",
      "Epoch 98: Train Loss: 0.575360381603241, Validation Loss: 0.7546316981315613\n",
      "Epoch 99: Train Loss: 0.6055494546890259, Validation Loss: 0.7492713332176208\n",
      "Epoch 100: Train Loss: 0.5690140843391418, Validation Loss: 0.7510608434677124\n",
      "Epoch 101: Train Loss: 0.5224451839923858, Validation Loss: 0.756079375743866\n",
      "Epoch 102: Train Loss: 0.570193749666214, Validation Loss: 0.762275218963623\n",
      "Epoch 103: Train Loss: 0.5531034827232361, Validation Loss: 0.7659406065940857\n",
      "Epoch 104: Train Loss: 0.5199215888977051, Validation Loss: 0.7689492106437683\n",
      "Epoch 105: Train Loss: 0.6301966190338135, Validation Loss: 0.7716614603996277\n",
      "Epoch 106: Train Loss: 0.5269934415817261, Validation Loss: 0.76474928855896\n",
      "Epoch 107: Train Loss: 0.5569003343582153, Validation Loss: 0.7640109062194824\n",
      "Epoch 108: Train Loss: 0.592708170413971, Validation Loss: 0.7647137641906738\n",
      "Epoch 109: Train Loss: 0.5288215160369873, Validation Loss: 0.7650117874145508\n",
      "Epoch 110: Train Loss: 0.5675227344036102, Validation Loss: 0.7645890116691589\n",
      "Epoch 111: Train Loss: 0.5893323421478271, Validation Loss: 0.7613574862480164\n",
      "Epoch 112: Train Loss: 0.5657516121864319, Validation Loss: 0.7677716612815857\n",
      "Epoch 113: Train Loss: 0.5555322051048279, Validation Loss: 0.7654037475585938\n",
      "Epoch 114: Train Loss: 0.5943609595298767, Validation Loss: 0.7711818814277649\n",
      "Epoch 115: Train Loss: 0.5627885103225708, Validation Loss: 0.7713441252708435\n",
      "Epoch 116: Train Loss: 0.5672901630401611, Validation Loss: 0.7665347456932068\n",
      "Epoch 117: Train Loss: 0.6089818835258484, Validation Loss: 0.7676640748977661\n",
      "Epoch 118: Train Loss: 0.5263794600963593, Validation Loss: 0.7719876766204834\n",
      "Epoch 119: Train Loss: 0.5682644128799439, Validation Loss: 0.7811875939369202\n",
      "Epoch 120: Train Loss: 0.551689225435257, Validation Loss: 0.7775323390960693\n",
      "Epoch 121: Train Loss: 0.49035032987594607, Validation Loss: 0.7801403999328613\n",
      "Epoch 122: Train Loss: 0.5789970874786377, Validation Loss: 0.7816463708877563\n",
      "Epoch 123: Train Loss: 0.5723297476768494, Validation Loss: 0.7787168622016907\n",
      "Epoch 124: Train Loss: 0.5628701746463776, Validation Loss: 0.7736192345619202\n",
      "Epoch 125: Train Loss: 0.5093903720378876, Validation Loss: 0.7766558527946472\n",
      "Epoch 126: Train Loss: 0.5112716734409333, Validation Loss: 0.7818453907966614\n",
      "Epoch 127: Train Loss: 0.4975656092166901, Validation Loss: 0.7855657339096069\n",
      "Epoch 128: Train Loss: 0.5244912564754486, Validation Loss: 0.7798464894294739\n",
      "Epoch 129: Train Loss: 0.5381363153457641, Validation Loss: 0.7815046310424805\n",
      "Epoch 130: Train Loss: 0.47529296278953553, Validation Loss: 0.7897926568984985\n",
      "Epoch 131: Train Loss: 0.5585758745670318, Validation Loss: 0.7907698750495911\n",
      "Epoch 132: Train Loss: 0.48917866349220274, Validation Loss: 0.79815673828125\n",
      "Epoch 133: Train Loss: 0.5150969564914704, Validation Loss: 0.8014894127845764\n",
      "Epoch 134: Train Loss: 0.513579398393631, Validation Loss: 0.8040383458137512\n",
      "Epoch 135: Train Loss: 0.5086455523967743, Validation Loss: 0.8057684898376465\n",
      "Epoch 136: Train Loss: 0.5305202901363373, Validation Loss: 0.8027717471122742\n",
      "Epoch 137: Train Loss: 0.4994512915611267, Validation Loss: 0.7977348566055298\n",
      "Epoch 138: Train Loss: 0.5356777310371399, Validation Loss: 0.7950976490974426\n",
      "Epoch 139: Train Loss: 0.5102082848548889, Validation Loss: 0.8018571734428406\n",
      "Epoch 140: Train Loss: 0.5429861426353455, Validation Loss: 0.8040435910224915\n",
      "Epoch 141: Train Loss: 0.5559328079223633, Validation Loss: 0.8138696551322937\n",
      "Epoch 142: Train Loss: 0.5175313055515289, Validation Loss: 0.8138273358345032\n",
      "Epoch 143: Train Loss: 0.5191057562828064, Validation Loss: 0.8241786360740662\n",
      "Epoch 144: Train Loss: 0.5223715662956238, Validation Loss: 0.814976692199707\n",
      "Epoch 145: Train Loss: 0.4988416492938995, Validation Loss: 0.8240938186645508\n",
      "Epoch 146: Train Loss: 0.4681651473045349, Validation Loss: 0.8242962956428528\n",
      "Epoch 147: Train Loss: 0.46684796214103697, Validation Loss: 0.8176373839378357\n",
      "Epoch 148: Train Loss: 0.4459612607955933, Validation Loss: 0.8289182186126709\n",
      "Epoch 149: Train Loss: 0.4753857493400574, Validation Loss: 0.8244287371635437\n",
      "Epoch 150: Train Loss: 0.4686567008495331, Validation Loss: 0.8271129727363586\n",
      "Epoch 151: Train Loss: 0.5027113795280457, Validation Loss: 0.8270431756973267\n",
      "Epoch 152: Train Loss: 0.48617404103279116, Validation Loss: 0.8303713798522949\n",
      "Epoch 153: Train Loss: 0.47589545845985415, Validation Loss: 0.8191343545913696\n",
      "Epoch 154: Train Loss: 0.4594900131225586, Validation Loss: 0.835676372051239\n",
      "Epoch 155: Train Loss: 0.5097437381744385, Validation Loss: 0.8292950987815857\n",
      "Epoch 156: Train Loss: 0.5216254830360413, Validation Loss: 0.8268378973007202\n",
      "Epoch 157: Train Loss: 0.4696359932422638, Validation Loss: 0.829176664352417\n",
      "Epoch 158: Train Loss: 0.5008125007152557, Validation Loss: 0.8281493186950684\n",
      "Epoch 159: Train Loss: 0.5287480354309082, Validation Loss: 0.8304324746131897\n",
      "Epoch 160: Train Loss: 0.4523327171802521, Validation Loss: 0.8275023698806763\n",
      "Epoch 161: Train Loss: 0.5186959445476532, Validation Loss: 0.829520583152771\n",
      "Epoch 162: Train Loss: 0.5221897900104523, Validation Loss: 0.8342946171760559\n",
      "Epoch 163: Train Loss: 0.579390799999237, Validation Loss: 0.8432308435440063\n",
      "Epoch 164: Train Loss: 0.5555144131183625, Validation Loss: 0.8433449864387512\n",
      "Epoch 165: Train Loss: 0.47517290711402893, Validation Loss: 0.8487660884857178\n",
      "Epoch 166: Train Loss: 0.4418737769126892, Validation Loss: 0.8432923555374146\n",
      "Epoch 167: Train Loss: 0.4947419583797455, Validation Loss: 0.8501624464988708\n",
      "Epoch 168: Train Loss: 0.5473471879959106, Validation Loss: 0.8413779735565186\n",
      "Epoch 169: Train Loss: 0.4716655075550079, Validation Loss: 0.8382515907287598\n",
      "Epoch 170: Train Loss: 0.4398265898227692, Validation Loss: 0.850138247013092\n",
      "Epoch 171: Train Loss: 0.4361056923866272, Validation Loss: 0.860791027545929\n",
      "Epoch 172: Train Loss: 0.4966285526752472, Validation Loss: 0.8593048453330994\n",
      "Epoch 173: Train Loss: 0.5268016695976258, Validation Loss: 0.8615527749061584\n",
      "Epoch 174: Train Loss: 0.5122766971588135, Validation Loss: 0.86184161901474\n",
      "Epoch 175: Train Loss: 0.4763158082962036, Validation Loss: 0.8522594571113586\n",
      "Epoch 176: Train Loss: 0.4914069533348083, Validation Loss: 0.8516155481338501\n",
      "Epoch 177: Train Loss: 0.4907460629940033, Validation Loss: 0.856502890586853\n",
      "Epoch 178: Train Loss: 0.48979119658470155, Validation Loss: 0.8596465587615967\n",
      "Epoch 179: Train Loss: 0.5061283469200134, Validation Loss: 0.8696378469467163\n",
      "Epoch 180: Train Loss: 0.44791582226753235, Validation Loss: 0.8605955243110657\n",
      "Epoch 181: Train Loss: 0.44970073103904723, Validation Loss: 0.8703409433364868\n",
      "Epoch 182: Train Loss: 0.5062643349170685, Validation Loss: 0.8776924014091492\n",
      "Epoch 183: Train Loss: 0.45591840147972107, Validation Loss: 0.8792068362236023\n",
      "Epoch 184: Train Loss: 0.4630926251411438, Validation Loss: 0.8801526427268982\n",
      "Epoch 185: Train Loss: 0.5098565697669983, Validation Loss: 0.8950925469398499\n",
      "Epoch 186: Train Loss: 0.40306933522224425, Validation Loss: 0.8926891088485718\n",
      "Epoch 187: Train Loss: 0.4543077349662781, Validation Loss: 0.8949428796768188\n",
      "Epoch 188: Train Loss: 0.41704659163951874, Validation Loss: 0.8863065838813782\n",
      "Epoch 189: Train Loss: 0.47156731486320497, Validation Loss: 0.876761794090271\n",
      "Epoch 190: Train Loss: 0.4305129528045654, Validation Loss: 0.8646762371063232\n",
      "Epoch 191: Train Loss: 0.523432320356369, Validation Loss: 0.867527961730957\n",
      "Epoch 192: Train Loss: 0.4251593589782715, Validation Loss: 0.8720986843109131\n",
      "Epoch 193: Train Loss: 0.5035660028457641, Validation Loss: 0.8773688673973083\n",
      "Epoch 194: Train Loss: 0.44735188484191896, Validation Loss: 0.8762124180793762\n",
      "Epoch 195: Train Loss: 0.40865065455436705, Validation Loss: 0.8902369737625122\n",
      "Epoch 196: Train Loss: 0.4417851686477661, Validation Loss: 0.8961852788925171\n",
      "Epoch 197: Train Loss: 0.45684492588043213, Validation Loss: 0.89127117395401\n",
      "Epoch 198: Train Loss: 0.4335685849189758, Validation Loss: 0.8976702094078064\n",
      "Epoch 199: Train Loss: 0.44426936507225034, Validation Loss: 0.9044494032859802\n",
      "Epoch 200: Train Loss: 0.40939922332763673, Validation Loss: 0.9044292569160461\n",
      "Epoch 201: Train Loss: 0.3898922234773636, Validation Loss: 0.9026435017585754\n",
      "Epoch 202: Train Loss: 0.47834211587905884, Validation Loss: 0.9045966267585754\n",
      "Epoch 203: Train Loss: 0.5224909842014313, Validation Loss: 0.9008037447929382\n",
      "Epoch 204: Train Loss: 0.4546056568622589, Validation Loss: 0.9050424098968506\n",
      "Epoch 205: Train Loss: 0.40764530301094054, Validation Loss: 0.9054252505302429\n",
      "Epoch 206: Train Loss: 0.3941042959690094, Validation Loss: 0.916307270526886\n",
      "Epoch 207: Train Loss: 0.4922517597675323, Validation Loss: 0.9282163381576538\n",
      "Epoch 208: Train Loss: 0.49547394514083865, Validation Loss: 0.9083805084228516\n",
      "Epoch 209: Train Loss: 0.42329818606376646, Validation Loss: 0.9088600277900696\n",
      "Epoch 210: Train Loss: 0.40681637525558473, Validation Loss: 0.9043622612953186\n",
      "Epoch 211: Train Loss: 0.4569019079208374, Validation Loss: 0.905343234539032\n",
      "Epoch 212: Train Loss: 0.451709645986557, Validation Loss: 0.9227879047393799\n",
      "Epoch 213: Train Loss: 0.47615554332733157, Validation Loss: 0.9321550726890564\n",
      "Epoch 214: Train Loss: 0.4383123517036438, Validation Loss: 0.926609218120575\n",
      "Epoch 215: Train Loss: 0.45777345895767213, Validation Loss: 0.9201490879058838\n",
      "Epoch 216: Train Loss: 0.395073401927948, Validation Loss: 0.9354496002197266\n",
      "Epoch 217: Train Loss: 0.46292309165000917, Validation Loss: 0.9424700140953064\n",
      "Epoch 218: Train Loss: 0.3689228117465973, Validation Loss: 0.9301366806030273\n",
      "Epoch 219: Train Loss: 0.5243485629558563, Validation Loss: 0.9252700805664062\n",
      "Epoch 220: Train Loss: 0.37072067260742186, Validation Loss: 0.9395626187324524\n",
      "Epoch 221: Train Loss: 0.3869344413280487, Validation Loss: 0.9473164081573486\n",
      "Epoch 222: Train Loss: 0.4099141776561737, Validation Loss: 0.9810436367988586\n",
      "Epoch 223: Train Loss: 0.40485506057739257, Validation Loss: 0.9580991268157959\n",
      "Epoch 224: Train Loss: 0.4259259939193726, Validation Loss: 0.9540959596633911\n",
      "Epoch 225: Train Loss: 0.37224065363407133, Validation Loss: 0.9422582387924194\n",
      "Epoch 226: Train Loss: 0.3762142896652222, Validation Loss: 0.9554199576377869\n",
      "Epoch 227: Train Loss: 0.43121715188026427, Validation Loss: 0.9635193347930908\n",
      "Epoch 228: Train Loss: 0.36858364939689636, Validation Loss: 0.9471232891082764\n",
      "Epoch 229: Train Loss: 0.40420947670936586, Validation Loss: 0.9527194499969482\n",
      "Epoch 230: Train Loss: 0.376683109998703, Validation Loss: 0.9272137880325317\n",
      "Epoch 231: Train Loss: 0.43310898542404175, Validation Loss: 0.9416539669036865\n",
      "Epoch 232: Train Loss: 0.3783783853054047, Validation Loss: 0.9379994869232178\n",
      "Epoch 233: Train Loss: 0.4393129289150238, Validation Loss: 0.9233793020248413\n",
      "Epoch 234: Train Loss: 0.35015829205513, Validation Loss: 0.9276852011680603\n",
      "Epoch 235: Train Loss: 0.34607713520526884, Validation Loss: 0.9335254430770874\n",
      "Epoch 236: Train Loss: 0.44057891964912416, Validation Loss: 0.9485841989517212\n",
      "Epoch 237: Train Loss: 0.38076764941215513, Validation Loss: 0.9409657716751099\n",
      "Epoch 238: Train Loss: 0.43540858626365664, Validation Loss: 0.9410234093666077\n",
      "Epoch 239: Train Loss: 0.4533203363418579, Validation Loss: 0.9503694772720337\n",
      "Epoch 240: Train Loss: 0.3696030735969543, Validation Loss: 0.9482171535491943\n",
      "Epoch 241: Train Loss: 0.4037536561489105, Validation Loss: 0.9640530943870544\n",
      "Epoch 242: Train Loss: 0.440945440530777, Validation Loss: 0.9717530608177185\n",
      "Epoch 243: Train Loss: 0.4002087593078613, Validation Loss: 0.9548547863960266\n",
      "Epoch 244: Train Loss: 0.4214888870716095, Validation Loss: 0.9569339752197266\n",
      "Epoch 245: Train Loss: 0.4137664377689362, Validation Loss: 0.9586336016654968\n",
      "Epoch 246: Train Loss: 0.4120975732803345, Validation Loss: 0.9487964510917664\n",
      "Epoch 247: Train Loss: 0.4689824879169464, Validation Loss: 0.9748402237892151\n",
      "Epoch 248: Train Loss: 0.39107688069343566, Validation Loss: 0.9890583157539368\n",
      "Epoch 249: Train Loss: 0.36073368787765503, Validation Loss: 0.9864131808280945\n",
      "Epoch 250: Train Loss: 0.45075011253356934, Validation Loss: 0.9881489276885986\n",
      "Epoch 251: Train Loss: 0.40712699890136717, Validation Loss: 0.9885147213935852\n",
      "Epoch 252: Train Loss: 0.39499624967575075, Validation Loss: 0.983733594417572\n",
      "Epoch 253: Train Loss: 0.3376120328903198, Validation Loss: 0.9755680561065674\n",
      "Epoch 254: Train Loss: 0.40025519132614135, Validation Loss: 0.9553772211074829\n",
      "Epoch 255: Train Loss: 0.3777110517024994, Validation Loss: 0.9641833901405334\n",
      "Epoch 256: Train Loss: 0.43847825527191164, Validation Loss: 0.9695125222206116\n",
      "Epoch 257: Train Loss: 0.37000454068183897, Validation Loss: 0.9949060082435608\n",
      "Epoch 258: Train Loss: 0.3483264923095703, Validation Loss: 1.001665711402893\n",
      "Epoch 259: Train Loss: 0.38871824741363525, Validation Loss: 1.0044441223144531\n",
      "Epoch 260: Train Loss: 0.390012264251709, Validation Loss: 0.9776402115821838\n",
      "Epoch 261: Train Loss: 0.41502113938331603, Validation Loss: 0.9783468246459961\n",
      "Epoch 262: Train Loss: 0.35924755930900576, Validation Loss: 0.9946458339691162\n",
      "Epoch 263: Train Loss: 0.35890809893608094, Validation Loss: 1.0052648782730103\n",
      "Epoch 264: Train Loss: 0.4008135497570038, Validation Loss: 1.0056315660476685\n",
      "Epoch 265: Train Loss: 0.4047776281833649, Validation Loss: 0.9955629110336304\n",
      "Epoch 266: Train Loss: 0.4193077623844147, Validation Loss: 0.9804532527923584\n",
      "Epoch 267: Train Loss: 0.3436997950077057, Validation Loss: 0.969399094581604\n",
      "Epoch 268: Train Loss: 0.452429062128067, Validation Loss: 0.9711463451385498\n",
      "Epoch 269: Train Loss: 0.39835824966430666, Validation Loss: 0.9751890301704407\n",
      "Epoch 270: Train Loss: 0.37177327275276184, Validation Loss: 0.9901378154754639\n",
      "Epoch 271: Train Loss: 0.3454909086227417, Validation Loss: 0.99798983335495\n",
      "Epoch 272: Train Loss: 0.37329155802726743, Validation Loss: 0.9795524477958679\n",
      "Epoch 273: Train Loss: 0.38216364085674287, Validation Loss: 0.9515405297279358\n",
      "Epoch 274: Train Loss: 0.4583943009376526, Validation Loss: 0.9922395944595337\n",
      "Epoch 275: Train Loss: 0.5124888956546784, Validation Loss: 0.9895456433296204\n",
      "Epoch 276: Train Loss: 0.3387337923049927, Validation Loss: 1.0068610906600952\n",
      "Epoch 277: Train Loss: 0.37718465924263, Validation Loss: 0.9796474575996399\n",
      "Epoch 278: Train Loss: 0.3378383994102478, Validation Loss: 0.9905292391777039\n",
      "Epoch 279: Train Loss: 0.3471624761819839, Validation Loss: 0.9774269461631775\n",
      "Epoch 280: Train Loss: 0.32852266132831576, Validation Loss: 1.0111149549484253\n",
      "Epoch 281: Train Loss: 0.42090927958488467, Validation Loss: 0.9854593276977539\n",
      "Epoch 282: Train Loss: 0.3624159753322601, Validation Loss: 0.9822860956192017\n",
      "Epoch 283: Train Loss: 0.4542887330055237, Validation Loss: 1.0188014507293701\n",
      "Epoch 284: Train Loss: 0.4041991114616394, Validation Loss: 1.0147407054901123\n",
      "Epoch 285: Train Loss: 0.4272309482097626, Validation Loss: 1.0243558883666992\n",
      "Epoch 286: Train Loss: 0.315534383058548, Validation Loss: 1.003833293914795\n",
      "Epoch 287: Train Loss: 0.3963742136955261, Validation Loss: 0.9952841401100159\n",
      "Epoch 288: Train Loss: 0.303339010477066, Validation Loss: 1.008259892463684\n",
      "Epoch 289: Train Loss: 0.4102443277835846, Validation Loss: 1.0487357378005981\n",
      "Epoch 290: Train Loss: 0.36140863299369813, Validation Loss: 1.0538235902786255\n",
      "Epoch 291: Train Loss: 0.42348727583885193, Validation Loss: 1.0452243089675903\n",
      "Epoch 292: Train Loss: 0.34166025221347807, Validation Loss: 1.0423624515533447\n",
      "Epoch 293: Train Loss: 0.47645496129989623, Validation Loss: 1.0048565864562988\n",
      "Epoch 294: Train Loss: 0.34858801364898684, Validation Loss: 1.016053557395935\n",
      "Epoch 295: Train Loss: 0.34375244975090025, Validation Loss: 1.0347704887390137\n",
      "Epoch 296: Train Loss: 0.36724327206611634, Validation Loss: 1.025664210319519\n",
      "Epoch 297: Train Loss: 0.33262465596199037, Validation Loss: 1.0109779834747314\n",
      "Epoch 298: Train Loss: 0.3070062607526779, Validation Loss: 1.0193861722946167\n",
      "Epoch 299: Train Loss: 0.3323763608932495, Validation Loss: 0.9746957421302795\n",
      "Epoch 300: Train Loss: 0.45608438849449157, Validation Loss: 1.0117449760437012\n",
      "Epoch 301: Train Loss: 0.40848447680473327, Validation Loss: 1.0183348655700684\n",
      "Epoch 302: Train Loss: 0.2958153061568737, Validation Loss: 1.0071570873260498\n",
      "Epoch 303: Train Loss: 0.3100064754486084, Validation Loss: 0.9831005334854126\n",
      "Epoch 304: Train Loss: 0.3921366274356842, Validation Loss: 1.0391427278518677\n",
      "Epoch 305: Train Loss: 0.36224788427352905, Validation Loss: 1.008600115776062\n",
      "Epoch 306: Train Loss: 0.36054697036743166, Validation Loss: 1.0036637783050537\n",
      "Epoch 307: Train Loss: 0.30476532578468324, Validation Loss: 1.0167063474655151\n",
      "Epoch 308: Train Loss: 0.3217285305261612, Validation Loss: 1.0363138914108276\n",
      "Epoch 309: Train Loss: 0.33908427953720094, Validation Loss: 1.0321084260940552\n",
      "Epoch 310: Train Loss: 0.28077890276908873, Validation Loss: 1.0387283563613892\n",
      "Epoch 311: Train Loss: 0.32332536280155183, Validation Loss: 1.0608636140823364\n",
      "Epoch 312: Train Loss: 0.29643636047840116, Validation Loss: 1.0707027912139893\n",
      "Epoch 313: Train Loss: 0.34359102249145507, Validation Loss: 1.0990926027297974\n",
      "Epoch 314: Train Loss: 0.3128172755241394, Validation Loss: 1.0682199001312256\n",
      "Epoch 315: Train Loss: 0.36913491487503053, Validation Loss: 1.0837305784225464\n",
      "Epoch 316: Train Loss: 0.33323673009872434, Validation Loss: 1.1029845476150513\n",
      "Epoch 317: Train Loss: 0.32568886280059817, Validation Loss: 1.1265374422073364\n",
      "Epoch 318: Train Loss: 0.4601142108440399, Validation Loss: 1.095438003540039\n",
      "Epoch 319: Train Loss: 0.3005436986684799, Validation Loss: 1.1125425100326538\n",
      "Epoch 320: Train Loss: 0.35673330426216127, Validation Loss: 1.0668673515319824\n",
      "Epoch 321: Train Loss: 0.32231725454330445, Validation Loss: 1.06516695022583\n",
      "Epoch 322: Train Loss: 0.33859647512435914, Validation Loss: 1.0245966911315918\n",
      "Epoch 323: Train Loss: 0.29194359183311464, Validation Loss: 1.0448411703109741\n",
      "Epoch 324: Train Loss: 0.36314716935157776, Validation Loss: 1.036453366279602\n",
      "Epoch 325: Train Loss: 0.3761769115924835, Validation Loss: 1.0371829271316528\n",
      "Epoch 326: Train Loss: 0.3317856669425964, Validation Loss: 1.0357108116149902\n",
      "Epoch 327: Train Loss: 0.2914863586425781, Validation Loss: 1.0495011806488037\n",
      "Epoch 328: Train Loss: 0.30188883394002913, Validation Loss: 1.0982999801635742\n",
      "Epoch 329: Train Loss: 0.2787189334630966, Validation Loss: 1.0760303735733032\n",
      "Epoch 330: Train Loss: 0.3477460086345673, Validation Loss: 1.1135447025299072\n",
      "Epoch 331: Train Loss: 0.27488984614610673, Validation Loss: 1.0967057943344116\n",
      "Epoch 332: Train Loss: 0.32566018104553224, Validation Loss: 1.0916072130203247\n",
      "Epoch 333: Train Loss: 0.29377012848854067, Validation Loss: 1.0790410041809082\n",
      "Epoch 334: Train Loss: 0.42445108294487, Validation Loss: 1.0448033809661865\n",
      "Epoch 335: Train Loss: 0.3048692882061005, Validation Loss: 1.0819979906082153\n",
      "Epoch 336: Train Loss: 0.3264297008514404, Validation Loss: 1.0694347620010376\n",
      "Epoch 337: Train Loss: 0.35621715188026426, Validation Loss: 1.0888333320617676\n",
      "Epoch 338: Train Loss: 0.4231355309486389, Validation Loss: 1.120527982711792\n",
      "Epoch 339: Train Loss: 0.335511314868927, Validation Loss: 1.0857599973678589\n",
      "Epoch 340: Train Loss: 0.34865663647651673, Validation Loss: 1.0683388710021973\n",
      "Epoch 341: Train Loss: 0.35894104540348054, Validation Loss: 1.1035993099212646\n",
      "Epoch 342: Train Loss: 0.30027042627334594, Validation Loss: 1.0623632669448853\n",
      "Epoch 343: Train Loss: 0.34235873222351076, Validation Loss: 1.0536385774612427\n",
      "Epoch 344: Train Loss: 0.2925526678562164, Validation Loss: 1.0612781047821045\n",
      "Epoch 345: Train Loss: 0.3636413782835007, Validation Loss: 1.071648359298706\n",
      "Epoch 346: Train Loss: 0.3360698640346527, Validation Loss: 1.0553650856018066\n",
      "Epoch 347: Train Loss: 0.3603390693664551, Validation Loss: 1.1105656623840332\n",
      "Epoch 348: Train Loss: 0.27564877867698667, Validation Loss: 1.0760788917541504\n",
      "Epoch 349: Train Loss: 0.30542364716529846, Validation Loss: 1.0976383686065674\n",
      "Epoch 350: Train Loss: 0.4005053132772446, Validation Loss: 1.079311490058899\n",
      "Epoch 351: Train Loss: 0.33354562520980835, Validation Loss: 1.0850286483764648\n",
      "Epoch 352: Train Loss: 0.28772452771663665, Validation Loss: 1.0883069038391113\n",
      "Epoch 353: Train Loss: 0.30657132863998415, Validation Loss: 1.1189439296722412\n",
      "Epoch 354: Train Loss: 0.30067282915115356, Validation Loss: 1.1517055034637451\n",
      "Epoch 355: Train Loss: 0.2781565338373184, Validation Loss: 1.1414865255355835\n",
      "Epoch 356: Train Loss: 0.2962226331233978, Validation Loss: 1.1784337759017944\n",
      "Epoch 357: Train Loss: 0.27720385491847993, Validation Loss: 1.1280814409255981\n",
      "Epoch 358: Train Loss: 0.25468951761722564, Validation Loss: 1.1315017938613892\n",
      "Epoch 359: Train Loss: 0.30450436770915984, Validation Loss: 1.116593837738037\n",
      "Epoch 360: Train Loss: 0.34764389991760253, Validation Loss: 1.096580982208252\n",
      "Epoch 361: Train Loss: 0.36284451484680175, Validation Loss: 1.139130711555481\n",
      "Epoch 362: Train Loss: 0.3999849855899811, Validation Loss: 1.1577632427215576\n",
      "Epoch 363: Train Loss: 0.33508341312408446, Validation Loss: 1.1282668113708496\n",
      "Epoch 364: Train Loss: 0.307684725522995, Validation Loss: 1.091461181640625\n",
      "Epoch 365: Train Loss: 0.42298230826854705, Validation Loss: 1.0993504524230957\n",
      "Epoch 366: Train Loss: 0.29522267580032346, Validation Loss: 1.076962947845459\n",
      "Epoch 367: Train Loss: 0.25770496651530267, Validation Loss: 1.1177587509155273\n",
      "Epoch 368: Train Loss: 0.3524252772331238, Validation Loss: 1.1266189813613892\n",
      "Epoch 369: Train Loss: 0.25570663511753083, Validation Loss: 1.1591421365737915\n",
      "Epoch 370: Train Loss: 0.38361491560935973, Validation Loss: 1.1160449981689453\n",
      "Epoch 371: Train Loss: 0.38548892736434937, Validation Loss: 1.081839680671692\n",
      "Epoch 372: Train Loss: 0.2962856113910675, Validation Loss: 1.1250007152557373\n",
      "Epoch 373: Train Loss: 0.3324929177761078, Validation Loss: 1.1340858936309814\n",
      "Epoch 374: Train Loss: 0.26601496934890745, Validation Loss: 1.1067339181900024\n",
      "Epoch 375: Train Loss: 0.27371356785297396, Validation Loss: 1.1181480884552002\n",
      "Epoch 376: Train Loss: 0.30129600763320924, Validation Loss: 1.116368055343628\n",
      "Epoch 377: Train Loss: 0.3277450978755951, Validation Loss: 1.1105204820632935\n",
      "Epoch 378: Train Loss: 0.34219688177108765, Validation Loss: 1.067383885383606\n",
      "Epoch 379: Train Loss: 0.38716833889484403, Validation Loss: 1.099619746208191\n",
      "Epoch 380: Train Loss: 0.2563468456268311, Validation Loss: 1.104036808013916\n",
      "Epoch 381: Train Loss: 0.3142524302005768, Validation Loss: 1.097222924232483\n",
      "Epoch 382: Train Loss: 0.36198900640010834, Validation Loss: 1.1259055137634277\n",
      "Epoch 383: Train Loss: 0.3118725001811981, Validation Loss: 1.1060822010040283\n",
      "Epoch 384: Train Loss: 0.26755416989326475, Validation Loss: 1.07914137840271\n",
      "Epoch 385: Train Loss: 0.28882022500038146, Validation Loss: 1.0702686309814453\n",
      "Epoch 386: Train Loss: 0.22824286818504333, Validation Loss: 1.075242042541504\n",
      "Epoch 387: Train Loss: 0.2687010318040848, Validation Loss: 1.0918676853179932\n",
      "Epoch 388: Train Loss: 0.26296674013137816, Validation Loss: 1.0803121328353882\n",
      "Epoch 389: Train Loss: 0.2943825960159302, Validation Loss: 1.1223393678665161\n",
      "Epoch 390: Train Loss: 0.2758564352989197, Validation Loss: 1.1139824390411377\n",
      "Epoch 391: Train Loss: 0.363246813416481, Validation Loss: 1.1553806066513062\n",
      "Epoch 392: Train Loss: 0.2658334165811539, Validation Loss: 1.1583406925201416\n",
      "Epoch 393: Train Loss: 0.266209614276886, Validation Loss: 1.12539803981781\n",
      "Epoch 394: Train Loss: 0.30208160877227785, Validation Loss: 1.0972983837127686\n",
      "Epoch 395: Train Loss: 0.2626431554555893, Validation Loss: 1.0869262218475342\n",
      "Epoch 396: Train Loss: 0.2706539362668991, Validation Loss: 1.089656949043274\n",
      "Epoch 397: Train Loss: 0.2607220321893692, Validation Loss: 1.109527349472046\n",
      "Epoch 398: Train Loss: 0.3260356456041336, Validation Loss: 1.0949592590332031\n",
      "Epoch 399: Train Loss: 0.28810648024082186, Validation Loss: 1.1253801584243774\n",
      "Epoch 400: Train Loss: 0.2625644445419312, Validation Loss: 1.1627122163772583\n",
      "Epoch 401: Train Loss: 0.34784120321273804, Validation Loss: 1.1441890001296997\n",
      "Epoch 402: Train Loss: 0.3011417418718338, Validation Loss: 1.1462846994400024\n",
      "Epoch 403: Train Loss: 0.28918218314647676, Validation Loss: 1.1194568872451782\n",
      "Epoch 404: Train Loss: 0.3454988092184067, Validation Loss: 1.1346423625946045\n",
      "Epoch 405: Train Loss: 0.3061136811971664, Validation Loss: 1.100351095199585\n",
      "Epoch 406: Train Loss: 0.263424214720726, Validation Loss: 1.0778898000717163\n",
      "Epoch 407: Train Loss: 0.3254821538925171, Validation Loss: 1.0770283937454224\n",
      "Epoch 408: Train Loss: 0.24087798595428467, Validation Loss: 1.090796947479248\n",
      "Epoch 409: Train Loss: 0.2998517781496048, Validation Loss: 1.0632197856903076\n",
      "Epoch 410: Train Loss: 0.26280770599842074, Validation Loss: 1.110595703125\n",
      "Epoch 411: Train Loss: 0.3123780101537704, Validation Loss: 1.140949010848999\n",
      "Epoch 412: Train Loss: 0.28092930316925047, Validation Loss: 1.095489501953125\n",
      "Epoch 413: Train Loss: 0.2899701505899429, Validation Loss: 1.0882699489593506\n",
      "Epoch 414: Train Loss: 0.2457945466041565, Validation Loss: 1.0548696517944336\n",
      "Epoch 415: Train Loss: 0.4127674579620361, Validation Loss: 1.090659737586975\n",
      "Epoch 416: Train Loss: 0.2503239393234253, Validation Loss: 1.1229674816131592\n",
      "Epoch 417: Train Loss: 0.2798593819141388, Validation Loss: 1.1200473308563232\n",
      "Epoch 418: Train Loss: 0.23368034660816192, Validation Loss: 1.1141539812088013\n",
      "Epoch 419: Train Loss: 0.27606495320796964, Validation Loss: 1.0959473848342896\n",
      "Epoch 420: Train Loss: 0.47413686811923983, Validation Loss: 1.128317952156067\n",
      "Epoch 421: Train Loss: 0.2582448422908783, Validation Loss: 1.1320476531982422\n",
      "Epoch 422: Train Loss: 0.26998170614242556, Validation Loss: 1.0992556810379028\n",
      "Epoch 423: Train Loss: 0.2407132238149643, Validation Loss: 1.091697096824646\n",
      "Epoch 424: Train Loss: 0.2335386961698532, Validation Loss: 1.1467204093933105\n",
      "Epoch 425: Train Loss: 0.3375158578157425, Validation Loss: 1.1167935132980347\n",
      "Epoch 426: Train Loss: 0.25406881272792814, Validation Loss: 1.131020188331604\n",
      "Epoch 427: Train Loss: 0.318667808175087, Validation Loss: 1.1174376010894775\n",
      "Epoch 428: Train Loss: 0.2415640503168106, Validation Loss: 1.120212435722351\n",
      "Epoch 429: Train Loss: 0.3170622378587723, Validation Loss: 1.1063302755355835\n",
      "Epoch 430: Train Loss: 0.36407025158405304, Validation Loss: 1.0804499387741089\n",
      "Epoch 431: Train Loss: 0.3272015661001205, Validation Loss: 1.0543973445892334\n",
      "Epoch 432: Train Loss: 0.22830214202404023, Validation Loss: 1.0518369674682617\n",
      "Epoch 433: Train Loss: 0.2548195242881775, Validation Loss: 1.0492372512817383\n",
      "Epoch 434: Train Loss: 0.2521740198135376, Validation Loss: 1.0539145469665527\n",
      "Epoch 435: Train Loss: 0.28748359382152555, Validation Loss: 1.0976340770721436\n",
      "Epoch 436: Train Loss: 0.2505369782447815, Validation Loss: 1.0899083614349365\n",
      "Epoch 437: Train Loss: 0.22879369854927062, Validation Loss: 1.0602110624313354\n",
      "Epoch 438: Train Loss: 0.22398588359355925, Validation Loss: 1.0591009855270386\n",
      "Epoch 439: Train Loss: 0.30763945877552035, Validation Loss: 1.0608566999435425\n",
      "Epoch 440: Train Loss: 0.28017068207263945, Validation Loss: 1.0693860054016113\n",
      "Epoch 441: Train Loss: 0.23359102308750151, Validation Loss: 1.0985701084136963\n",
      "Epoch 442: Train Loss: 0.2586816966533661, Validation Loss: 1.1270793676376343\n",
      "Epoch 443: Train Loss: 0.20982823371887208, Validation Loss: 1.1346244812011719\n",
      "Epoch 444: Train Loss: 0.23376279771327974, Validation Loss: 1.141121745109558\n",
      "Epoch 445: Train Loss: 0.2856138855218887, Validation Loss: 1.0887240171432495\n",
      "Epoch 446: Train Loss: 0.23730006217956542, Validation Loss: 1.1436752080917358\n",
      "Epoch 447: Train Loss: 0.21343092918395995, Validation Loss: 1.1052056550979614\n",
      "Epoch 448: Train Loss: 0.3231249928474426, Validation Loss: 1.1473193168640137\n",
      "Epoch 449: Train Loss: 0.2701557368040085, Validation Loss: 1.1773866415023804\n",
      "Epoch 450: Train Loss: 0.28470870554447175, Validation Loss: 1.205178141593933\n",
      "Epoch 451: Train Loss: 0.2309465929865837, Validation Loss: 1.2066816091537476\n",
      "Epoch 452: Train Loss: 0.40374426543712616, Validation Loss: 1.172195553779602\n",
      "Epoch 453: Train Loss: 0.22591754347085952, Validation Loss: 1.1581133604049683\n",
      "Epoch 454: Train Loss: 0.3150604456663132, Validation Loss: 1.1884392499923706\n",
      "Epoch 455: Train Loss: 0.2546482264995575, Validation Loss: 1.1681182384490967\n",
      "Epoch 456: Train Loss: 0.31627259254455564, Validation Loss: 1.175827145576477\n",
      "Epoch 457: Train Loss: 0.22240239530801773, Validation Loss: 1.1636395454406738\n",
      "Epoch 458: Train Loss: 0.18520864248275756, Validation Loss: 1.1626389026641846\n",
      "Epoch 459: Train Loss: 0.3076369047164917, Validation Loss: 1.1423629522323608\n",
      "Epoch 460: Train Loss: 0.3276397317647934, Validation Loss: 1.1259468793869019\n",
      "Epoch 461: Train Loss: 0.24956386685371398, Validation Loss: 1.1639528274536133\n",
      "Epoch 462: Train Loss: 0.2341267704963684, Validation Loss: 1.1253743171691895\n",
      "Epoch 463: Train Loss: 0.29362462759017943, Validation Loss: 1.13145112991333\n",
      "Epoch 464: Train Loss: 0.19204249754548072, Validation Loss: 1.1466190814971924\n",
      "Epoch 465: Train Loss: 0.1994563564658165, Validation Loss: 1.0914192199707031\n",
      "Epoch 466: Train Loss: 0.2531318932771683, Validation Loss: 1.1307551860809326\n",
      "Epoch 467: Train Loss: 0.19084407761693, Validation Loss: 1.1865836381912231\n",
      "Epoch 468: Train Loss: 0.27048503160476683, Validation Loss: 1.1791669130325317\n",
      "Epoch 469: Train Loss: 0.24038407802581788, Validation Loss: 1.1454083919525146\n",
      "Epoch 470: Train Loss: 0.36544889509677886, Validation Loss: 1.1435061693191528\n",
      "Epoch 471: Train Loss: 0.2313621461391449, Validation Loss: 1.1725534200668335\n",
      "Epoch 472: Train Loss: 0.4234588533639908, Validation Loss: 1.1760112047195435\n",
      "Epoch 473: Train Loss: 0.25655435025691986, Validation Loss: 1.1450002193450928\n",
      "Epoch 474: Train Loss: 0.22537328600883483, Validation Loss: 1.1552493572235107\n",
      "Epoch 475: Train Loss: 0.22699531614780427, Validation Loss: 1.164466381072998\n",
      "Epoch 476: Train Loss: 0.2901786148548126, Validation Loss: 1.1234380006790161\n",
      "Epoch 477: Train Loss: 0.25498590618371964, Validation Loss: 1.1570178270339966\n",
      "Epoch 478: Train Loss: 0.3087930053472519, Validation Loss: 1.1543290615081787\n",
      "Epoch 479: Train Loss: 0.21022546291351318, Validation Loss: 1.1746057271957397\n",
      "Epoch 480: Train Loss: 0.2696486055850983, Validation Loss: 1.162640929222107\n",
      "Epoch 481: Train Loss: 0.28891006112098694, Validation Loss: 1.1639313697814941\n",
      "Epoch 482: Train Loss: 0.18474534749984742, Validation Loss: 1.2138748168945312\n",
      "Epoch 483: Train Loss: 0.2197022870182991, Validation Loss: 1.185137391090393\n",
      "Epoch 484: Train Loss: 0.26390582919120786, Validation Loss: 1.204504132270813\n",
      "Epoch 485: Train Loss: 0.22836332023143768, Validation Loss: 1.213165044784546\n",
      "Epoch 486: Train Loss: 0.20813868790864945, Validation Loss: 1.210449457168579\n",
      "Epoch 487: Train Loss: 0.2154344692826271, Validation Loss: 1.220970869064331\n",
      "Epoch 488: Train Loss: 0.23203099370002747, Validation Loss: 1.195576548576355\n",
      "Epoch 489: Train Loss: 0.21203851997852324, Validation Loss: 1.1987394094467163\n",
      "Epoch 490: Train Loss: 0.2155677616596222, Validation Loss: 1.196569561958313\n",
      "Epoch 491: Train Loss: 0.3535305827856064, Validation Loss: 1.2578192949295044\n",
      "Epoch 492: Train Loss: 0.26016163229942324, Validation Loss: 1.2175713777542114\n",
      "Epoch 493: Train Loss: 0.21963013112545013, Validation Loss: 1.1757142543792725\n",
      "Epoch 494: Train Loss: 0.2087090142071247, Validation Loss: 1.182059407234192\n",
      "Epoch 495: Train Loss: 0.2924739569425583, Validation Loss: 1.2200506925582886\n",
      "Epoch 496: Train Loss: 0.26290651857852937, Validation Loss: 1.215977430343628\n",
      "Epoch 497: Train Loss: 0.22263616025447847, Validation Loss: 1.1956899166107178\n",
      "Epoch 498: Train Loss: 0.24380957186222077, Validation Loss: 1.1453465223312378\n",
      "Epoch 499: Train Loss: 0.2693999171257019, Validation Loss: 1.1753108501434326\n",
      "Fold 11 Metrics:\n",
      "Accuracy: 0.45454545454545453, Precision: 0.0, Recall: 0.0, F1-score: 0.0, AUC: 0.5\n",
      "Confusion Matrix:\n",
      "[[5 0]\n",
      " [6 0]]\n",
      "Completed fold 11\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples from subject 2 to test set\n",
      "Adding 6 truth samples from subject 2 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 250)\n",
      "(132, 65, 250)\n",
      "Epoch 0: Train Loss: 0.7347498178482056, Validation Loss: 0.6888301372528076\n",
      "Epoch 1: Train Loss: 0.7629546642303466, Validation Loss: 0.6852108836174011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\detectlie\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.6580873727798462, Validation Loss: 0.6793584227561951\n",
      "Epoch 3: Train Loss: 0.7726146936416626, Validation Loss: 0.6698598265647888\n",
      "Epoch 4: Train Loss: 0.6907626390457153, Validation Loss: 0.656006395816803\n",
      "Epoch 5: Train Loss: 0.6440469443798065, Validation Loss: 0.6431587934494019\n",
      "Epoch 6: Train Loss: 0.634396779537201, Validation Loss: 0.6385018229484558\n",
      "Epoch 7: Train Loss: 0.7010484099388122, Validation Loss: 0.6331694722175598\n",
      "Epoch 8: Train Loss: 0.6388112902641296, Validation Loss: 0.6291465163230896\n",
      "Epoch 9: Train Loss: 0.6433927536010742, Validation Loss: 0.6245177984237671\n",
      "Epoch 10: Train Loss: 0.6549885869026184, Validation Loss: 0.6212558746337891\n",
      "Epoch 11: Train Loss: 0.659087085723877, Validation Loss: 0.6235234141349792\n",
      "Epoch 12: Train Loss: 0.6617623090744018, Validation Loss: 0.627937376499176\n",
      "Epoch 13: Train Loss: 0.6745916962623596, Validation Loss: 0.6292227506637573\n",
      "Epoch 14: Train Loss: 0.6327063560485839, Validation Loss: 0.6344506144523621\n",
      "Epoch 15: Train Loss: 0.6318436443805695, Validation Loss: 0.6334341764450073\n",
      "Epoch 16: Train Loss: 0.6641552686691284, Validation Loss: 0.644386887550354\n",
      "Epoch 17: Train Loss: 0.6019914865493774, Validation Loss: 0.6401885151863098\n",
      "Epoch 18: Train Loss: 0.6209004998207093, Validation Loss: 0.6457107663154602\n",
      "Epoch 19: Train Loss: 0.6901578903198242, Validation Loss: 0.6440801024436951\n",
      "Epoch 20: Train Loss: 0.6831674218177796, Validation Loss: 0.6389598250389099\n",
      "Epoch 21: Train Loss: 0.6410948157310485, Validation Loss: 0.636545717716217\n",
      "Epoch 22: Train Loss: 0.6885507345199585, Validation Loss: 0.6409173607826233\n",
      "Epoch 23: Train Loss: 0.6413449764251709, Validation Loss: 0.6429916620254517\n",
      "Epoch 24: Train Loss: 0.6121647596359253, Validation Loss: 0.6395807862281799\n",
      "Epoch 25: Train Loss: 0.700253677368164, Validation Loss: 0.638340175151825\n",
      "Epoch 26: Train Loss: 0.5730864524841308, Validation Loss: 0.6444849371910095\n",
      "Epoch 27: Train Loss: 0.6741696715354919, Validation Loss: 0.641680896282196\n",
      "Epoch 28: Train Loss: 0.6497294306755066, Validation Loss: 0.645177960395813\n",
      "Epoch 29: Train Loss: 0.5863203763961792, Validation Loss: 0.6424874663352966\n",
      "Epoch 30: Train Loss: 0.6186954379081726, Validation Loss: 0.6480530500411987\n",
      "Epoch 31: Train Loss: 0.6823888421058655, Validation Loss: 0.6468906998634338\n",
      "Epoch 32: Train Loss: 0.5947587490081787, Validation Loss: 0.6472864747047424\n",
      "Epoch 33: Train Loss: 0.6406591534614563, Validation Loss: 0.6465491056442261\n",
      "Epoch 34: Train Loss: 0.6604343533515931, Validation Loss: 0.6505316495895386\n",
      "Epoch 35: Train Loss: 0.605094850063324, Validation Loss: 0.6508090496063232\n",
      "Epoch 36: Train Loss: 0.6043152213096619, Validation Loss: 0.6553968787193298\n",
      "Epoch 37: Train Loss: 0.600004243850708, Validation Loss: 0.6490322947502136\n",
      "Epoch 38: Train Loss: 0.573101794719696, Validation Loss: 0.6574552655220032\n",
      "Epoch 39: Train Loss: 0.6455843567848205, Validation Loss: 0.6577571630477905\n",
      "Epoch 40: Train Loss: 0.7029938340187073, Validation Loss: 0.6557435393333435\n",
      "Epoch 41: Train Loss: 0.5921766877174377, Validation Loss: 0.6610749959945679\n",
      "Epoch 42: Train Loss: 0.590508097410202, Validation Loss: 0.6607047915458679\n",
      "Epoch 43: Train Loss: 0.6592186093330383, Validation Loss: 0.6679458618164062\n",
      "Epoch 44: Train Loss: 0.6128069400787354, Validation Loss: 0.6662622690200806\n",
      "Epoch 45: Train Loss: 0.5590663969516754, Validation Loss: 0.6768500208854675\n",
      "Epoch 46: Train Loss: 0.6534457683563233, Validation Loss: 0.6702988743782043\n",
      "Epoch 47: Train Loss: 0.5475339651107788, Validation Loss: 0.6733707189559937\n",
      "Epoch 48: Train Loss: 0.6012295126914978, Validation Loss: 0.6756851077079773\n",
      "Epoch 49: Train Loss: 0.5684500157833099, Validation Loss: 0.6772152781486511\n",
      "Epoch 50: Train Loss: 0.6592622041702271, Validation Loss: 0.6713075637817383\n",
      "Epoch 51: Train Loss: 0.603365683555603, Validation Loss: 0.6819634437561035\n",
      "Epoch 52: Train Loss: 0.5985361576080322, Validation Loss: 0.6784347295761108\n",
      "Epoch 53: Train Loss: 0.5730568528175354, Validation Loss: 0.6771668791770935\n",
      "Epoch 54: Train Loss: 0.6440796256065369, Validation Loss: 0.6777133941650391\n",
      "Epoch 55: Train Loss: 0.6061901092529297, Validation Loss: 0.6816039085388184\n",
      "Epoch 56: Train Loss: 0.6419726014137268, Validation Loss: 0.6857847571372986\n",
      "Epoch 57: Train Loss: 0.5623413324356079, Validation Loss: 0.6857287287712097\n",
      "Epoch 58: Train Loss: 0.5163262844085693, Validation Loss: 0.6845701336860657\n",
      "Epoch 59: Train Loss: 0.5742473304271698, Validation Loss: 0.6765443682670593\n",
      "Epoch 60: Train Loss: 0.5400847315788269, Validation Loss: 0.6717454195022583\n",
      "Epoch 61: Train Loss: 0.5880287766456604, Validation Loss: 0.6815755367279053\n",
      "Epoch 62: Train Loss: 0.5806097388267517, Validation Loss: 0.6772751212120056\n",
      "Epoch 63: Train Loss: 0.5296519100666046, Validation Loss: 0.6844343543052673\n",
      "Epoch 64: Train Loss: 0.5239228844642639, Validation Loss: 0.6889057755470276\n",
      "Epoch 65: Train Loss: 0.5550199806690216, Validation Loss: 0.6869402527809143\n",
      "Epoch 66: Train Loss: 0.5667507708072662, Validation Loss: 0.7020136713981628\n",
      "Epoch 67: Train Loss: 0.5878306150436401, Validation Loss: 0.6993840336799622\n",
      "Epoch 68: Train Loss: 0.5392696619033813, Validation Loss: 0.699521005153656\n",
      "Epoch 69: Train Loss: 0.5662081599235534, Validation Loss: 0.6932737231254578\n",
      "Epoch 70: Train Loss: 0.5828999400138855, Validation Loss: 0.6866263747215271\n",
      "Epoch 71: Train Loss: 0.5266438305377961, Validation Loss: 0.6946099400520325\n",
      "Epoch 72: Train Loss: 0.5570590257644653, Validation Loss: 0.6982876658439636\n",
      "Epoch 73: Train Loss: 0.5235125005245209, Validation Loss: 0.7069923281669617\n",
      "Epoch 74: Train Loss: 0.5422972977161408, Validation Loss: 0.7031164169311523\n",
      "Epoch 75: Train Loss: 0.5529864728450775, Validation Loss: 0.7018519043922424\n",
      "Epoch 76: Train Loss: 0.5599451541900635, Validation Loss: 0.6987563967704773\n",
      "Epoch 77: Train Loss: 0.611154842376709, Validation Loss: 0.6927180290222168\n",
      "Epoch 78: Train Loss: 0.5397903800010682, Validation Loss: 0.7000688910484314\n",
      "Epoch 79: Train Loss: 0.5214968085289001, Validation Loss: 0.6926751136779785\n",
      "Epoch 80: Train Loss: 0.5681009531021118, Validation Loss: 0.7102331519126892\n",
      "Epoch 81: Train Loss: 0.5070075035095215, Validation Loss: 0.7066256403923035\n",
      "Epoch 82: Train Loss: 0.5581330955028534, Validation Loss: 0.7010785937309265\n",
      "Epoch 83: Train Loss: 0.6035816669464111, Validation Loss: 0.7119290828704834\n",
      "Epoch 84: Train Loss: 0.5683989524841309, Validation Loss: 0.7139008045196533\n",
      "Epoch 85: Train Loss: 0.5472904205322265, Validation Loss: 0.7033949494361877\n",
      "Epoch 86: Train Loss: 0.5980690956115723, Validation Loss: 0.7150362133979797\n",
      "Epoch 87: Train Loss: 0.5255916953086853, Validation Loss: 0.7067162394523621\n",
      "Epoch 88: Train Loss: 0.56772181391716, Validation Loss: 0.7057475447654724\n",
      "Epoch 89: Train Loss: 0.5252834260463715, Validation Loss: 0.7042808532714844\n",
      "Epoch 90: Train Loss: 0.5663669764995575, Validation Loss: 0.7212848663330078\n",
      "Epoch 91: Train Loss: 0.5261259555816651, Validation Loss: 0.7203037738800049\n",
      "Epoch 92: Train Loss: 0.6123193681240082, Validation Loss: 0.7197617292404175\n",
      "Epoch 93: Train Loss: 0.4831556141376495, Validation Loss: 0.7084546089172363\n",
      "Epoch 94: Train Loss: 0.47621572613716123, Validation Loss: 0.7158381938934326\n",
      "Epoch 95: Train Loss: 0.5800453245639801, Validation Loss: 0.7374770641326904\n",
      "Epoch 96: Train Loss: 0.5041392385959625, Validation Loss: 0.7287492156028748\n",
      "Epoch 97: Train Loss: 0.5131632208824157, Validation Loss: 0.7292512655258179\n",
      "Epoch 98: Train Loss: 0.5286597788333893, Validation Loss: 0.7138409614562988\n",
      "Epoch 99: Train Loss: 0.5008780837059021, Validation Loss: 0.7294057607650757\n",
      "Epoch 100: Train Loss: 0.47047157883644103, Validation Loss: 0.7376357913017273\n",
      "Epoch 101: Train Loss: 0.5420433104038238, Validation Loss: 0.7415339946746826\n",
      "Epoch 102: Train Loss: 0.6183564305305481, Validation Loss: 0.7571431398391724\n",
      "Epoch 103: Train Loss: 0.5247309565544128, Validation Loss: 0.7633185982704163\n",
      "Epoch 104: Train Loss: 0.4830774784088135, Validation Loss: 0.7659322619438171\n",
      "Epoch 105: Train Loss: 0.5098807871341705, Validation Loss: 0.767553985118866\n",
      "Epoch 106: Train Loss: 0.4791078925132751, Validation Loss: 0.761282742023468\n",
      "Epoch 107: Train Loss: 0.5105989694595336, Validation Loss: 0.755094051361084\n",
      "Epoch 108: Train Loss: 0.4949122488498688, Validation Loss: 0.7529342174530029\n",
      "Epoch 109: Train Loss: 0.539439570903778, Validation Loss: 0.7572011947631836\n",
      "Epoch 110: Train Loss: 0.5130357086658478, Validation Loss: 0.7661603093147278\n",
      "Epoch 111: Train Loss: 0.4865548014640808, Validation Loss: 0.7680678963661194\n",
      "Epoch 112: Train Loss: 0.47108293175697324, Validation Loss: 0.7669718861579895\n",
      "Epoch 113: Train Loss: 0.4737443089485168, Validation Loss: 0.7738788723945618\n",
      "Epoch 114: Train Loss: 0.5264167487621307, Validation Loss: 0.7743470072746277\n",
      "Epoch 115: Train Loss: 0.5519158363342285, Validation Loss: 0.7807723879814148\n",
      "Epoch 116: Train Loss: 0.4795994281768799, Validation Loss: 0.788836658000946\n",
      "Epoch 117: Train Loss: 0.4912075698375702, Validation Loss: 0.7895808219909668\n",
      "Epoch 118: Train Loss: 0.4195124298334122, Validation Loss: 0.7818418145179749\n",
      "Epoch 119: Train Loss: 0.5712787508964539, Validation Loss: 0.7939993143081665\n",
      "Epoch 120: Train Loss: 0.48448951840400695, Validation Loss: 0.7933782339096069\n",
      "Epoch 121: Train Loss: 0.46077858209609984, Validation Loss: 0.7959056496620178\n",
      "Epoch 122: Train Loss: 0.5151102244853973, Validation Loss: 0.8061249852180481\n",
      "Epoch 123: Train Loss: 0.5787435293197631, Validation Loss: 0.7997667193412781\n",
      "Epoch 124: Train Loss: 0.44825342297554016, Validation Loss: 0.8040378093719482\n",
      "Epoch 125: Train Loss: 0.4858164370059967, Validation Loss: 0.8115564584732056\n",
      "Epoch 126: Train Loss: 0.4362116277217865, Validation Loss: 0.8065875172615051\n",
      "Epoch 127: Train Loss: 0.501668655872345, Validation Loss: 0.8029987215995789\n",
      "Epoch 128: Train Loss: 0.48608962297439573, Validation Loss: 0.8137162327766418\n",
      "Epoch 129: Train Loss: 0.5106887221336365, Validation Loss: 0.8260238170623779\n",
      "Epoch 130: Train Loss: 0.41293211579322814, Validation Loss: 0.8371644020080566\n",
      "Epoch 131: Train Loss: 0.4836310803890228, Validation Loss: 0.8346812725067139\n",
      "Epoch 132: Train Loss: 0.4885175287723541, Validation Loss: 0.8173856139183044\n",
      "Epoch 133: Train Loss: 0.5071171343326568, Validation Loss: 0.8197518587112427\n",
      "Epoch 134: Train Loss: 0.5096477329730987, Validation Loss: 0.8370664715766907\n",
      "Epoch 135: Train Loss: 0.44774295687675475, Validation Loss: 0.8282384872436523\n",
      "Epoch 136: Train Loss: 0.5024273574352265, Validation Loss: 0.8303517699241638\n",
      "Epoch 137: Train Loss: 0.4233249962329865, Validation Loss: 0.8210170865058899\n",
      "Epoch 138: Train Loss: 0.4362554013729095, Validation Loss: 0.8207313418388367\n",
      "Epoch 139: Train Loss: 0.5937366306781768, Validation Loss: 0.8166629672050476\n",
      "Epoch 140: Train Loss: 0.388170400261879, Validation Loss: 0.8429310917854309\n",
      "Epoch 141: Train Loss: 0.45182924866676333, Validation Loss: 0.8517772555351257\n",
      "Epoch 142: Train Loss: 0.5358757138252258, Validation Loss: 0.8585188984870911\n",
      "Epoch 143: Train Loss: 0.4526441693305969, Validation Loss: 0.8562677502632141\n",
      "Epoch 144: Train Loss: 0.4736911356449127, Validation Loss: 0.8410153985023499\n",
      "Epoch 145: Train Loss: 0.4836297631263733, Validation Loss: 0.8478347063064575\n",
      "Epoch 146: Train Loss: 0.5042382955551148, Validation Loss: 0.8618800640106201\n",
      "Epoch 147: Train Loss: 0.48001413345336913, Validation Loss: 0.8546215295791626\n",
      "Epoch 148: Train Loss: 0.4648850202560425, Validation Loss: 0.8593078255653381\n",
      "Epoch 149: Train Loss: 0.45370015501976013, Validation Loss: 0.8546315431594849\n",
      "Epoch 150: Train Loss: 0.42848448157310487, Validation Loss: 0.8584123253822327\n",
      "Epoch 151: Train Loss: 0.4565439701080322, Validation Loss: 0.858479917049408\n",
      "Epoch 152: Train Loss: 0.5291045129299163, Validation Loss: 0.8689540028572083\n",
      "Epoch 153: Train Loss: 0.41420676112174987, Validation Loss: 0.8779060244560242\n",
      "Epoch 154: Train Loss: 0.38915395736694336, Validation Loss: 0.8884421586990356\n",
      "Epoch 155: Train Loss: 0.4929579198360443, Validation Loss: 0.89399254322052\n",
      "Epoch 156: Train Loss: 0.4704282879829407, Validation Loss: 0.8991767764091492\n",
      "Epoch 157: Train Loss: 0.46360906958580017, Validation Loss: 0.9063023924827576\n",
      "Epoch 158: Train Loss: 0.5238038301467896, Validation Loss: 0.9084815382957458\n",
      "Epoch 159: Train Loss: 0.41555667519569395, Validation Loss: 0.8913238644599915\n",
      "Epoch 160: Train Loss: 0.4846811056137085, Validation Loss: 0.8812549710273743\n",
      "Epoch 161: Train Loss: 0.42197840213775634, Validation Loss: 0.8989444971084595\n",
      "Epoch 162: Train Loss: 0.5289961218833923, Validation Loss: 0.9088542461395264\n",
      "Epoch 163: Train Loss: 0.47763574719429014, Validation Loss: 0.9128354787826538\n",
      "Epoch 164: Train Loss: 0.3968679189682007, Validation Loss: 0.9162008166313171\n",
      "Epoch 165: Train Loss: 0.45765233039855957, Validation Loss: 0.9215368628501892\n",
      "Epoch 166: Train Loss: 0.43668092489242555, Validation Loss: 0.9242094159126282\n",
      "Epoch 167: Train Loss: 0.45481197237968446, Validation Loss: 0.908545732498169\n",
      "Epoch 168: Train Loss: 0.4820371210575104, Validation Loss: 0.8974912762641907\n",
      "Epoch 169: Train Loss: 0.4142427027225494, Validation Loss: 0.8883213400840759\n",
      "Epoch 170: Train Loss: 0.41310672760009765, Validation Loss: 0.8944277167320251\n",
      "Epoch 171: Train Loss: 0.3904817312955856, Validation Loss: 0.8909803628921509\n",
      "Epoch 172: Train Loss: 0.4744878947734833, Validation Loss: 0.9189416766166687\n",
      "Epoch 173: Train Loss: 0.4897441029548645, Validation Loss: 0.9338289499282837\n",
      "Epoch 174: Train Loss: 0.46396031975746155, Validation Loss: 0.9449735283851624\n",
      "Epoch 175: Train Loss: 0.4573879837989807, Validation Loss: 0.9165546298027039\n",
      "Epoch 176: Train Loss: 0.5193639099597931, Validation Loss: 0.9257299900054932\n",
      "Epoch 177: Train Loss: 0.43064218759536743, Validation Loss: 0.930711030960083\n",
      "Epoch 178: Train Loss: 0.4613435208797455, Validation Loss: 0.9125625491142273\n",
      "Epoch 179: Train Loss: 0.4205504834651947, Validation Loss: 0.9037141799926758\n",
      "Epoch 180: Train Loss: 0.47015870809555055, Validation Loss: 0.9293273687362671\n",
      "Epoch 181: Train Loss: 0.38324992060661317, Validation Loss: 0.9375209212303162\n",
      "Epoch 182: Train Loss: 0.39065392017364503, Validation Loss: 0.9250404834747314\n",
      "Epoch 183: Train Loss: 0.37920817732810974, Validation Loss: 0.9303825497627258\n",
      "Epoch 184: Train Loss: 0.3993461668491364, Validation Loss: 0.9220682382583618\n",
      "Epoch 185: Train Loss: 0.3948705613613129, Validation Loss: 0.9313225150108337\n",
      "Epoch 186: Train Loss: 0.3817147374153137, Validation Loss: 0.9274110794067383\n",
      "Epoch 187: Train Loss: 0.46022123098373413, Validation Loss: 0.9261037707328796\n",
      "Epoch 188: Train Loss: 0.452208411693573, Validation Loss: 0.9542310237884521\n",
      "Epoch 189: Train Loss: 0.3967702388763428, Validation Loss: 0.9443323612213135\n",
      "Epoch 190: Train Loss: 0.4288277983665466, Validation Loss: 0.9615437984466553\n",
      "Epoch 191: Train Loss: 0.5156115651130676, Validation Loss: 0.9439486861228943\n",
      "Epoch 192: Train Loss: 0.3645353764295578, Validation Loss: 0.9282476305961609\n",
      "Epoch 193: Train Loss: 0.3908817768096924, Validation Loss: 0.9379287362098694\n",
      "Epoch 194: Train Loss: 0.4179183840751648, Validation Loss: 0.9330688118934631\n",
      "Epoch 195: Train Loss: 0.4574321389198303, Validation Loss: 0.9442942142486572\n",
      "Epoch 196: Train Loss: 0.43307808637619016, Validation Loss: 0.9500246047973633\n",
      "Epoch 197: Train Loss: 0.4733452260494232, Validation Loss: 0.9317631125450134\n",
      "Epoch 198: Train Loss: 0.34284719824790955, Validation Loss: 0.9382394552230835\n",
      "Epoch 199: Train Loss: 0.404483425617218, Validation Loss: 0.9495621919631958\n",
      "Epoch 200: Train Loss: 0.4112042963504791, Validation Loss: 0.9420528411865234\n",
      "Epoch 201: Train Loss: 0.3612579435110092, Validation Loss: 0.969430685043335\n",
      "Epoch 202: Train Loss: 0.5089769899845124, Validation Loss: 0.978027880191803\n",
      "Epoch 203: Train Loss: 0.40455089807510375, Validation Loss: 0.9760140776634216\n",
      "Epoch 204: Train Loss: 0.39083925187587737, Validation Loss: 0.9849833846092224\n",
      "Epoch 205: Train Loss: 0.39210609197616575, Validation Loss: 0.9812701940536499\n",
      "Epoch 206: Train Loss: 0.39556105732917785, Validation Loss: 0.9834312200546265\n",
      "Epoch 207: Train Loss: 0.3874282896518707, Validation Loss: 0.9814443588256836\n",
      "Epoch 208: Train Loss: 0.43268164396286013, Validation Loss: 0.9896907806396484\n",
      "Epoch 209: Train Loss: 0.4279903471469879, Validation Loss: 0.971159040927887\n",
      "Epoch 210: Train Loss: 0.3658077299594879, Validation Loss: 0.9810550808906555\n",
      "Epoch 211: Train Loss: 0.39666180610656737, Validation Loss: 1.003873348236084\n",
      "Epoch 212: Train Loss: 0.35665506720542905, Validation Loss: 1.0079941749572754\n",
      "Epoch 213: Train Loss: 0.38196465373039246, Validation Loss: 1.0150055885314941\n",
      "Epoch 214: Train Loss: 0.4190592050552368, Validation Loss: 1.0523322820663452\n",
      "Epoch 215: Train Loss: 0.38442872166633607, Validation Loss: 1.0552312135696411\n",
      "Epoch 216: Train Loss: 0.40153477191925047, Validation Loss: 1.0638275146484375\n",
      "Epoch 217: Train Loss: 0.357992497086525, Validation Loss: 1.0492857694625854\n",
      "Epoch 218: Train Loss: 0.46266716718673706, Validation Loss: 1.0095620155334473\n",
      "Epoch 219: Train Loss: 0.3489228129386902, Validation Loss: 1.010101318359375\n",
      "Epoch 220: Train Loss: 0.38950669169425967, Validation Loss: 1.0255080461502075\n",
      "Epoch 221: Train Loss: 0.4184566020965576, Validation Loss: 1.007857084274292\n",
      "Epoch 222: Train Loss: 0.38501036167144775, Validation Loss: 1.0108788013458252\n",
      "Epoch 223: Train Loss: 0.36915296912193296, Validation Loss: 1.0315734148025513\n",
      "Epoch 224: Train Loss: 0.3544969022274017, Validation Loss: 1.037096381187439\n",
      "Epoch 225: Train Loss: 0.33852612376213076, Validation Loss: 1.0418463945388794\n",
      "Epoch 226: Train Loss: 0.3550939977169037, Validation Loss: 1.0374016761779785\n",
      "Epoch 227: Train Loss: 0.35162103176116943, Validation Loss: 1.0247913599014282\n",
      "Epoch 228: Train Loss: 0.4199510276317596, Validation Loss: 1.036286473274231\n",
      "Epoch 229: Train Loss: 0.42050362229347227, Validation Loss: 1.0526987314224243\n",
      "Epoch 230: Train Loss: 0.34036622047424314, Validation Loss: 1.0643656253814697\n",
      "Epoch 231: Train Loss: 0.4816692531108856, Validation Loss: 1.093176245689392\n",
      "Epoch 232: Train Loss: 0.39345537424087523, Validation Loss: 1.087185025215149\n",
      "Epoch 233: Train Loss: 0.379929906129837, Validation Loss: 1.0820879936218262\n",
      "Epoch 234: Train Loss: 0.3733008861541748, Validation Loss: 1.1000837087631226\n",
      "Epoch 235: Train Loss: 0.37327657341957093, Validation Loss: 1.119401454925537\n",
      "Epoch 236: Train Loss: 0.3394600361585617, Validation Loss: 1.110500454902649\n",
      "Epoch 237: Train Loss: 0.3599965512752533, Validation Loss: 1.0980702638626099\n",
      "Epoch 238: Train Loss: 0.4113797128200531, Validation Loss: 1.1139373779296875\n",
      "Epoch 239: Train Loss: 0.4030081212520599, Validation Loss: 1.1145108938217163\n",
      "Epoch 240: Train Loss: 0.47692887783050536, Validation Loss: 1.1196675300598145\n",
      "Epoch 241: Train Loss: 0.34666174054145815, Validation Loss: 1.0569777488708496\n",
      "Epoch 242: Train Loss: 0.3460857093334198, Validation Loss: 1.0752441883087158\n",
      "Epoch 243: Train Loss: 0.3709800958633423, Validation Loss: 1.052912950515747\n",
      "Epoch 244: Train Loss: 0.44291322231292723, Validation Loss: 1.0395994186401367\n",
      "Epoch 245: Train Loss: 0.38647710978984834, Validation Loss: 1.0488554239273071\n",
      "Epoch 246: Train Loss: 0.3420929670333862, Validation Loss: 1.0525895357131958\n",
      "Epoch 247: Train Loss: 0.43059864044189455, Validation Loss: 1.0745267868041992\n",
      "Epoch 248: Train Loss: 0.3156338706612587, Validation Loss: 1.0654717683792114\n",
      "Epoch 249: Train Loss: 0.37344287633895873, Validation Loss: 1.0785987377166748\n",
      "Epoch 250: Train Loss: 0.3827381432056427, Validation Loss: 1.0541530847549438\n",
      "Epoch 251: Train Loss: 0.37348266541957853, Validation Loss: 1.0479215383529663\n",
      "Epoch 252: Train Loss: 0.40350953340530393, Validation Loss: 1.0733284950256348\n",
      "Epoch 253: Train Loss: 0.37338014245033263, Validation Loss: 1.1031914949417114\n",
      "Epoch 254: Train Loss: 0.3505601823329926, Validation Loss: 1.087802529335022\n",
      "Epoch 255: Train Loss: 0.32717678546905515, Validation Loss: 1.1033916473388672\n",
      "Epoch 256: Train Loss: 0.306709223985672, Validation Loss: 1.0994473695755005\n",
      "Epoch 257: Train Loss: 0.32384378612041476, Validation Loss: 1.1014806032180786\n",
      "Epoch 258: Train Loss: 0.3480041742324829, Validation Loss: 1.0906999111175537\n",
      "Epoch 259: Train Loss: 0.3275018215179443, Validation Loss: 1.1135519742965698\n",
      "Epoch 260: Train Loss: 0.3378994971513748, Validation Loss: 1.1299455165863037\n",
      "Epoch 261: Train Loss: 0.3174679845571518, Validation Loss: 1.1190053224563599\n",
      "Epoch 262: Train Loss: 0.3343601554632187, Validation Loss: 1.138333797454834\n",
      "Epoch 263: Train Loss: 0.2959419757127762, Validation Loss: 1.1240607500076294\n",
      "Epoch 264: Train Loss: 0.3659259796142578, Validation Loss: 1.1147987842559814\n",
      "Epoch 265: Train Loss: 0.34042943120002744, Validation Loss: 1.1449925899505615\n",
      "Epoch 266: Train Loss: 0.34961307644844053, Validation Loss: 1.1372809410095215\n",
      "Epoch 267: Train Loss: 0.4621445059776306, Validation Loss: 1.167535424232483\n",
      "Epoch 268: Train Loss: 0.3495337009429932, Validation Loss: 1.15580153465271\n",
      "Epoch 269: Train Loss: 0.37460173964500426, Validation Loss: 1.1855612993240356\n",
      "Epoch 270: Train Loss: 0.4406842827796936, Validation Loss: 1.1834689378738403\n",
      "Epoch 271: Train Loss: 0.40981090664863584, Validation Loss: 1.2039977312088013\n",
      "Epoch 272: Train Loss: 0.45774476528167723, Validation Loss: 1.1695475578308105\n",
      "Epoch 273: Train Loss: 0.3888669550418854, Validation Loss: 1.1445857286453247\n",
      "Epoch 274: Train Loss: 0.34863420128822326, Validation Loss: 1.1501818895339966\n",
      "Epoch 275: Train Loss: 0.3157460629940033, Validation Loss: 1.128980040550232\n",
      "Epoch 276: Train Loss: 0.3363380193710327, Validation Loss: 1.1063921451568604\n",
      "Epoch 277: Train Loss: 0.36439662575721743, Validation Loss: 1.126628041267395\n",
      "Epoch 278: Train Loss: 0.3540196716785431, Validation Loss: 1.1051243543624878\n",
      "Epoch 279: Train Loss: 0.30857546627521515, Validation Loss: 1.1192424297332764\n",
      "Epoch 280: Train Loss: 0.31654095351696016, Validation Loss: 1.1250611543655396\n",
      "Epoch 281: Train Loss: 0.28569947332143786, Validation Loss: 1.127989649772644\n",
      "Epoch 282: Train Loss: 0.32372820377349854, Validation Loss: 1.1290861368179321\n",
      "Epoch 283: Train Loss: 0.2724385216832161, Validation Loss: 1.176645040512085\n",
      "Epoch 284: Train Loss: 0.298051455616951, Validation Loss: 1.167803168296814\n",
      "Epoch 285: Train Loss: 0.28397509455680847, Validation Loss: 1.1648221015930176\n",
      "Epoch 286: Train Loss: 0.32550905346870423, Validation Loss: 1.1733332872390747\n",
      "Epoch 287: Train Loss: 0.32027918100357056, Validation Loss: 1.2193503379821777\n",
      "Epoch 288: Train Loss: 0.31485429108142854, Validation Loss: 1.2187819480895996\n",
      "Epoch 289: Train Loss: 0.4165619254112244, Validation Loss: 1.2122701406478882\n",
      "Epoch 290: Train Loss: 0.2823470547795296, Validation Loss: 1.194149136543274\n",
      "Epoch 291: Train Loss: 0.4012352526187897, Validation Loss: 1.227518081665039\n",
      "Epoch 292: Train Loss: 0.358314049243927, Validation Loss: 1.2370738983154297\n",
      "Epoch 293: Train Loss: 0.33522081971168516, Validation Loss: 1.2182306051254272\n",
      "Epoch 294: Train Loss: 0.3469303250312805, Validation Loss: 1.1829715967178345\n",
      "Epoch 295: Train Loss: 0.5202809035778045, Validation Loss: 1.2441339492797852\n",
      "Epoch 296: Train Loss: 0.3791989892721176, Validation Loss: 1.2798484563827515\n",
      "Epoch 297: Train Loss: 0.3365336984395981, Validation Loss: 1.2589954137802124\n",
      "Epoch 298: Train Loss: 0.2923796892166138, Validation Loss: 1.248125433921814\n",
      "Epoch 299: Train Loss: 0.32117273807525637, Validation Loss: 1.1886471509933472\n",
      "Epoch 300: Train Loss: 0.2834858804941177, Validation Loss: 1.2080155611038208\n",
      "Epoch 301: Train Loss: 0.3493643462657928, Validation Loss: 1.222832441329956\n",
      "Epoch 302: Train Loss: 0.3473671853542328, Validation Loss: 1.209035038948059\n",
      "Epoch 303: Train Loss: 0.2787255123257637, Validation Loss: 1.2115378379821777\n",
      "Epoch 304: Train Loss: 0.28076517283916474, Validation Loss: 1.1986364126205444\n",
      "Epoch 305: Train Loss: 0.3914986431598663, Validation Loss: 1.2043359279632568\n",
      "Epoch 306: Train Loss: 0.295899310708046, Validation Loss: 1.2025277614593506\n",
      "Epoch 307: Train Loss: 0.39970970451831817, Validation Loss: 1.2412275075912476\n",
      "Epoch 308: Train Loss: 0.4305489122867584, Validation Loss: 1.2200771570205688\n",
      "Epoch 309: Train Loss: 0.3079437494277954, Validation Loss: 1.2479219436645508\n",
      "Epoch 310: Train Loss: 0.3081618189811707, Validation Loss: 1.225050687789917\n",
      "Epoch 311: Train Loss: 0.29109793305397036, Validation Loss: 1.2145987749099731\n",
      "Epoch 312: Train Loss: 0.35473528504371643, Validation Loss: 1.1829720735549927\n",
      "Epoch 313: Train Loss: 0.4333754897117615, Validation Loss: 1.2357028722763062\n",
      "Epoch 314: Train Loss: 0.2622700959444046, Validation Loss: 1.2319295406341553\n",
      "Epoch 315: Train Loss: 0.3432583391666412, Validation Loss: 1.206837773323059\n",
      "Epoch 316: Train Loss: 0.3268316775560379, Validation Loss: 1.2263604402542114\n",
      "Epoch 317: Train Loss: 0.335531610250473, Validation Loss: 1.2099567651748657\n",
      "Epoch 318: Train Loss: 0.3298283338546753, Validation Loss: 1.2325917482376099\n",
      "Epoch 319: Train Loss: 0.3582976937294006, Validation Loss: 1.2213224172592163\n",
      "Epoch 320: Train Loss: 0.29556450843811033, Validation Loss: 1.2537215948104858\n",
      "Epoch 321: Train Loss: 0.3619433641433716, Validation Loss: 1.2324802875518799\n",
      "Epoch 322: Train Loss: 0.2660433888435364, Validation Loss: 1.267659068107605\n",
      "Epoch 323: Train Loss: 0.28173570036888124, Validation Loss: 1.2933427095413208\n",
      "Epoch 324: Train Loss: 0.292855304479599, Validation Loss: 1.2982150316238403\n",
      "Epoch 325: Train Loss: 0.28612465858459474, Validation Loss: 1.307266354560852\n",
      "Epoch 326: Train Loss: 0.27418385446071625, Validation Loss: 1.290840744972229\n",
      "Epoch 327: Train Loss: 0.2548710763454437, Validation Loss: 1.3111158609390259\n",
      "Epoch 328: Train Loss: 0.3175926446914673, Validation Loss: 1.2663249969482422\n",
      "Epoch 329: Train Loss: 0.4142330139875412, Validation Loss: 1.3471037149429321\n",
      "Epoch 330: Train Loss: 0.29507943987846375, Validation Loss: 1.411368489265442\n",
      "Epoch 331: Train Loss: 0.30647924542427063, Validation Loss: 1.3430403470993042\n",
      "Epoch 332: Train Loss: 0.3295865058898926, Validation Loss: 1.2988228797912598\n",
      "Epoch 333: Train Loss: 0.25904969573020936, Validation Loss: 1.3047878742218018\n",
      "Epoch 334: Train Loss: 0.2975045323371887, Validation Loss: 1.3206713199615479\n",
      "Epoch 335: Train Loss: 0.37812474370002747, Validation Loss: 1.3807377815246582\n",
      "Epoch 336: Train Loss: 0.2915203243494034, Validation Loss: 1.355939269065857\n",
      "Epoch 337: Train Loss: 0.2303176924586296, Validation Loss: 1.3503471612930298\n",
      "Epoch 338: Train Loss: 0.3159387528896332, Validation Loss: 1.3261290788650513\n",
      "Epoch 339: Train Loss: 0.3028115540742874, Validation Loss: 1.3485804796218872\n",
      "Epoch 340: Train Loss: 0.33304397463798524, Validation Loss: 1.3782086372375488\n",
      "Epoch 341: Train Loss: 0.28228655457496643, Validation Loss: 1.3472856283187866\n",
      "Epoch 342: Train Loss: 0.28579509556293486, Validation Loss: 1.3452426195144653\n",
      "Epoch 343: Train Loss: 0.3627756953239441, Validation Loss: 1.417773723602295\n",
      "Epoch 344: Train Loss: 0.3490569531917572, Validation Loss: 1.3463633060455322\n",
      "Epoch 345: Train Loss: 0.3790120303630829, Validation Loss: 1.4088718891143799\n",
      "Epoch 346: Train Loss: 0.2969286382198334, Validation Loss: 1.3531997203826904\n",
      "Epoch 347: Train Loss: 0.29701376855373385, Validation Loss: 1.328917145729065\n",
      "Epoch 348: Train Loss: 0.4502222537994385, Validation Loss: 1.3667066097259521\n",
      "Epoch 349: Train Loss: 0.2847531259059906, Validation Loss: 1.4178820848464966\n",
      "Epoch 350: Train Loss: 0.27367365062236787, Validation Loss: 1.374541997909546\n",
      "Epoch 351: Train Loss: 0.2584606409072876, Validation Loss: 1.4284268617630005\n",
      "Epoch 352: Train Loss: 0.337454554438591, Validation Loss: 1.3720842599868774\n",
      "Epoch 353: Train Loss: 0.37350126802921296, Validation Loss: 1.3897130489349365\n",
      "Epoch 354: Train Loss: 0.32871837019920347, Validation Loss: 1.4103753566741943\n",
      "Epoch 355: Train Loss: 0.39651098251342776, Validation Loss: 1.3251632452011108\n",
      "Epoch 356: Train Loss: 0.2706967979669571, Validation Loss: 1.3637197017669678\n",
      "Epoch 357: Train Loss: 0.22851793915033342, Validation Loss: 1.3183526992797852\n",
      "Epoch 358: Train Loss: 0.3532800585031509, Validation Loss: 1.301973581314087\n",
      "Epoch 359: Train Loss: 0.32945052087306975, Validation Loss: 1.371029257774353\n",
      "Epoch 360: Train Loss: 0.26265874207019807, Validation Loss: 1.3868869543075562\n",
      "Epoch 361: Train Loss: 0.34864728450775145, Validation Loss: 1.3721258640289307\n",
      "Epoch 362: Train Loss: 0.33250888288021085, Validation Loss: 1.3559209108352661\n",
      "Epoch 363: Train Loss: 0.3252327024936676, Validation Loss: 1.3995494842529297\n",
      "Epoch 364: Train Loss: 0.3277359127998352, Validation Loss: 1.3607314825057983\n",
      "Epoch 365: Train Loss: 0.26844858229160307, Validation Loss: 1.4299322366714478\n",
      "Epoch 366: Train Loss: 0.23380813449621202, Validation Loss: 1.484554648399353\n",
      "Epoch 367: Train Loss: 0.28047102987766265, Validation Loss: 1.4147899150848389\n",
      "Epoch 368: Train Loss: 0.25369359701871874, Validation Loss: 1.4075576066970825\n",
      "Epoch 369: Train Loss: 0.2919460624456406, Validation Loss: 1.3816595077514648\n",
      "Epoch 370: Train Loss: 0.3121475487947464, Validation Loss: 1.4101670980453491\n",
      "Epoch 371: Train Loss: 0.2642304182052612, Validation Loss: 1.399553894996643\n",
      "Epoch 372: Train Loss: 0.25073401927947997, Validation Loss: 1.4229564666748047\n",
      "Epoch 373: Train Loss: 0.22237312644720078, Validation Loss: 1.469373345375061\n",
      "Epoch 374: Train Loss: 0.277646341919899, Validation Loss: 1.4744625091552734\n",
      "Epoch 375: Train Loss: 0.2701143443584442, Validation Loss: 1.5172996520996094\n",
      "Epoch 376: Train Loss: 0.37311090230941774, Validation Loss: 1.46242356300354\n",
      "Epoch 377: Train Loss: 0.3158467173576355, Validation Loss: 1.4826135635375977\n",
      "Epoch 378: Train Loss: 0.278137069940567, Validation Loss: 1.4258522987365723\n",
      "Epoch 379: Train Loss: 0.23955999836325645, Validation Loss: 1.4243998527526855\n",
      "Epoch 380: Train Loss: 0.39308062195777893, Validation Loss: 1.4848737716674805\n",
      "Epoch 381: Train Loss: 0.25189020335674284, Validation Loss: 1.4742943048477173\n",
      "Epoch 382: Train Loss: 0.24167394936084746, Validation Loss: 1.4503573179244995\n",
      "Epoch 383: Train Loss: 0.26592067778110506, Validation Loss: 1.4331446886062622\n",
      "Epoch 384: Train Loss: 0.2379072666168213, Validation Loss: 1.4477211236953735\n",
      "Epoch 385: Train Loss: 0.3422747790813446, Validation Loss: 1.3972121477127075\n",
      "Epoch 386: Train Loss: 0.2232412114739418, Validation Loss: 1.4452495574951172\n",
      "Epoch 387: Train Loss: 0.31726292371749876, Validation Loss: 1.455972671508789\n",
      "Epoch 388: Train Loss: 0.29755678176879885, Validation Loss: 1.3995742797851562\n",
      "Epoch 389: Train Loss: 0.2521430402994156, Validation Loss: 1.4118773937225342\n",
      "Epoch 390: Train Loss: 0.4074830174446106, Validation Loss: 1.3954206705093384\n",
      "Epoch 391: Train Loss: 0.24868325293064117, Validation Loss: 1.4294936656951904\n",
      "Epoch 392: Train Loss: 0.31318103075027465, Validation Loss: 1.4834661483764648\n",
      "Epoch 393: Train Loss: 0.31278038322925567, Validation Loss: 1.5186556577682495\n",
      "Epoch 394: Train Loss: 0.32739539444446564, Validation Loss: 1.5637409687042236\n",
      "Epoch 395: Train Loss: 0.36765027046203613, Validation Loss: 1.5027320384979248\n",
      "Epoch 396: Train Loss: 0.25167007744312286, Validation Loss: 1.4640551805496216\n",
      "Epoch 397: Train Loss: 0.22102047204971315, Validation Loss: 1.463230848312378\n",
      "Epoch 398: Train Loss: 0.3895091712474823, Validation Loss: 1.5147898197174072\n",
      "Epoch 399: Train Loss: 0.2941034734249115, Validation Loss: 1.4821192026138306\n",
      "Epoch 400: Train Loss: 0.252865269780159, Validation Loss: 1.5300575494766235\n",
      "Epoch 401: Train Loss: 0.37432368099689484, Validation Loss: 1.5512927770614624\n",
      "Epoch 402: Train Loss: 0.24398873150348663, Validation Loss: 1.4787921905517578\n",
      "Epoch 403: Train Loss: 0.2586432725191116, Validation Loss: 1.486006736755371\n",
      "Epoch 404: Train Loss: 0.2376854881644249, Validation Loss: 1.4723902940750122\n",
      "Epoch 405: Train Loss: 0.2704763948917389, Validation Loss: 1.4879213571548462\n",
      "Epoch 406: Train Loss: 0.23905435353517532, Validation Loss: 1.5019710063934326\n",
      "Epoch 407: Train Loss: 0.34289529025554655, Validation Loss: 1.4505341053009033\n",
      "Epoch 408: Train Loss: 0.2550988018512726, Validation Loss: 1.4619520902633667\n",
      "Epoch 409: Train Loss: 0.2640979826450348, Validation Loss: 1.493655800819397\n",
      "Epoch 410: Train Loss: 0.2339652433991432, Validation Loss: 1.5084365606307983\n",
      "Epoch 411: Train Loss: 0.2509256720542908, Validation Loss: 1.4928938150405884\n",
      "Epoch 412: Train Loss: 0.3396489590406418, Validation Loss: 1.479454755783081\n",
      "Epoch 413: Train Loss: 0.3095484137535095, Validation Loss: 1.4770994186401367\n",
      "Epoch 414: Train Loss: 0.35823369324207305, Validation Loss: 1.4754327535629272\n",
      "Epoch 415: Train Loss: 0.2332157999277115, Validation Loss: 1.4956599473953247\n",
      "Epoch 416: Train Loss: 0.23301212191581727, Validation Loss: 1.566476821899414\n",
      "Epoch 417: Train Loss: 0.2640221953392029, Validation Loss: 1.5548820495605469\n",
      "Epoch 418: Train Loss: 0.30362176299095156, Validation Loss: 1.5525412559509277\n",
      "Epoch 419: Train Loss: 0.3763511449098587, Validation Loss: 1.5767344236373901\n",
      "Epoch 420: Train Loss: 0.28537358343601227, Validation Loss: 1.5386344194412231\n",
      "Epoch 421: Train Loss: 0.2550918906927109, Validation Loss: 1.4790072441101074\n",
      "Epoch 422: Train Loss: 0.2538586765527725, Validation Loss: 1.5394439697265625\n",
      "Epoch 423: Train Loss: 0.2920592904090881, Validation Loss: 1.4753984212875366\n",
      "Epoch 424: Train Loss: 0.21440183073282243, Validation Loss: 1.483420968055725\n",
      "Epoch 425: Train Loss: 0.22808180451393129, Validation Loss: 1.5024088621139526\n",
      "Epoch 426: Train Loss: 0.23290814459323883, Validation Loss: 1.5375895500183105\n",
      "Epoch 427: Train Loss: 0.2533728390932083, Validation Loss: 1.5275951623916626\n",
      "Epoch 428: Train Loss: 0.3023032695055008, Validation Loss: 1.5689092874526978\n",
      "Epoch 429: Train Loss: 0.29930164515972135, Validation Loss: 1.5297995805740356\n",
      "Epoch 430: Train Loss: 0.2325112849473953, Validation Loss: 1.5659294128417969\n",
      "Epoch 431: Train Loss: 0.2439290851354599, Validation Loss: 1.5670726299285889\n",
      "Epoch 432: Train Loss: 0.2890830457210541, Validation Loss: 1.5714092254638672\n",
      "Epoch 433: Train Loss: 0.21194702088832856, Validation Loss: 1.5435115098953247\n",
      "Epoch 434: Train Loss: 0.30085856318473814, Validation Loss: 1.5597999095916748\n",
      "Epoch 435: Train Loss: 0.23376373648643495, Validation Loss: 1.5077346563339233\n",
      "Epoch 436: Train Loss: 0.20002208054065704, Validation Loss: 1.4985036849975586\n",
      "Epoch 437: Train Loss: 0.21195525228977202, Validation Loss: 1.5266311168670654\n",
      "Epoch 438: Train Loss: 0.29544776380062104, Validation Loss: 1.4532102346420288\n",
      "Epoch 439: Train Loss: 0.24757543504238128, Validation Loss: 1.502576470375061\n",
      "Epoch 440: Train Loss: 0.24041248857975006, Validation Loss: 1.5239505767822266\n",
      "Epoch 441: Train Loss: 0.2079584002494812, Validation Loss: 1.5104122161865234\n",
      "Epoch 442: Train Loss: 0.2781814277172089, Validation Loss: 1.5424232482910156\n",
      "Epoch 443: Train Loss: 0.22240439802408218, Validation Loss: 1.5545177459716797\n",
      "Epoch 444: Train Loss: 0.27029791474342346, Validation Loss: 1.613729476928711\n",
      "Epoch 445: Train Loss: 0.33522447347640993, Validation Loss: 1.6128380298614502\n",
      "Epoch 446: Train Loss: 0.24705721139907838, Validation Loss: 1.5793545246124268\n",
      "Epoch 447: Train Loss: 0.20900124311447144, Validation Loss: 1.5723865032196045\n",
      "Epoch 448: Train Loss: 0.2187744289636612, Validation Loss: 1.577305793762207\n",
      "Epoch 449: Train Loss: 0.22432006895542145, Validation Loss: 1.5822105407714844\n",
      "Epoch 450: Train Loss: 0.21445959210395812, Validation Loss: 1.565248727798462\n",
      "Epoch 451: Train Loss: 0.23437518775463104, Validation Loss: 1.5402415990829468\n",
      "Epoch 452: Train Loss: 0.22886012494564056, Validation Loss: 1.5989923477172852\n",
      "Epoch 453: Train Loss: 0.2552256524562836, Validation Loss: 1.5641868114471436\n",
      "Epoch 454: Train Loss: 0.38184626698493956, Validation Loss: 1.5592749118804932\n",
      "Epoch 455: Train Loss: 0.3765925645828247, Validation Loss: 1.6028021574020386\n",
      "Epoch 456: Train Loss: 0.2513653188943863, Validation Loss: 1.5483332872390747\n",
      "Epoch 457: Train Loss: 0.22215543687343597, Validation Loss: 1.525819182395935\n",
      "Epoch 458: Train Loss: 0.39736045598983766, Validation Loss: 1.5300081968307495\n",
      "Epoch 459: Train Loss: 0.2483580380678177, Validation Loss: 1.6089235544204712\n",
      "Epoch 460: Train Loss: 0.2524411350488663, Validation Loss: 1.6515933275222778\n",
      "Epoch 461: Train Loss: 0.2866351634263992, Validation Loss: 1.6378647089004517\n",
      "Epoch 462: Train Loss: 0.25694787204265596, Validation Loss: 1.5730003118515015\n",
      "Epoch 463: Train Loss: 0.26314457654953005, Validation Loss: 1.530776023864746\n",
      "Epoch 464: Train Loss: 0.2916933327913284, Validation Loss: 1.5390342473983765\n",
      "Epoch 465: Train Loss: 0.2187192916870117, Validation Loss: 1.559787631034851\n",
      "Epoch 466: Train Loss: 0.33141844272613524, Validation Loss: 1.5315684080123901\n",
      "Epoch 467: Train Loss: 0.21825431883335114, Validation Loss: 1.5654425621032715\n",
      "Epoch 468: Train Loss: 0.3238194763660431, Validation Loss: 1.5426065921783447\n",
      "Epoch 469: Train Loss: 0.22752492725849152, Validation Loss: 1.6042238473892212\n",
      "Epoch 470: Train Loss: 0.21458976566791535, Validation Loss: 1.5740264654159546\n",
      "Epoch 471: Train Loss: 0.25000028908252714, Validation Loss: 1.5743951797485352\n",
      "Epoch 472: Train Loss: 0.22833628654479982, Validation Loss: 1.5941202640533447\n",
      "Epoch 473: Train Loss: 0.21078612431883811, Validation Loss: 1.5916457176208496\n",
      "Epoch 474: Train Loss: 0.21637898683547974, Validation Loss: 1.6603323221206665\n",
      "Epoch 475: Train Loss: 0.24051790237426757, Validation Loss: 1.6961708068847656\n",
      "Epoch 476: Train Loss: 0.22367675006389617, Validation Loss: 1.6524863243103027\n",
      "Epoch 477: Train Loss: 0.2287726730108261, Validation Loss: 1.6855815649032593\n",
      "Epoch 478: Train Loss: 0.2779557406902313, Validation Loss: 1.6423957347869873\n",
      "Epoch 479: Train Loss: 0.19507195949554443, Validation Loss: 1.6213833093643188\n",
      "Epoch 480: Train Loss: 0.3488760232925415, Validation Loss: 1.6134700775146484\n",
      "Epoch 481: Train Loss: 0.268117293715477, Validation Loss: 1.6990817785263062\n",
      "Epoch 482: Train Loss: 0.30503795146942136, Validation Loss: 1.73402738571167\n",
      "Epoch 483: Train Loss: 0.20715858936309814, Validation Loss: 1.7270303964614868\n",
      "Epoch 484: Train Loss: 0.21549930199980735, Validation Loss: 1.6835730075836182\n",
      "Epoch 485: Train Loss: 0.23013700693845748, Validation Loss: 1.6864123344421387\n",
      "Epoch 486: Train Loss: 0.20320734083652497, Validation Loss: 1.7287933826446533\n",
      "Epoch 487: Train Loss: 0.2826430946588516, Validation Loss: 1.7124165296554565\n",
      "Epoch 488: Train Loss: 0.2541885733604431, Validation Loss: 1.6497938632965088\n",
      "Epoch 489: Train Loss: 0.3896497756242752, Validation Loss: 1.6940574645996094\n",
      "Epoch 490: Train Loss: 0.21330372989177704, Validation Loss: 1.7130000591278076\n",
      "Epoch 491: Train Loss: 0.23607204854488373, Validation Loss: 1.665156364440918\n",
      "Epoch 492: Train Loss: 0.21996114850044252, Validation Loss: 1.6499240398406982\n",
      "Epoch 493: Train Loss: 0.26020580530166626, Validation Loss: 1.6130499839782715\n",
      "Epoch 494: Train Loss: 0.2900126546621323, Validation Loss: 1.666576623916626\n",
      "Epoch 495: Train Loss: 0.21370536386966704, Validation Loss: 1.6352072954177856\n",
      "Epoch 496: Train Loss: 0.24269404709339143, Validation Loss: 1.6195628643035889\n",
      "Epoch 497: Train Loss: 0.1837205708026886, Validation Loss: 1.6150091886520386\n",
      "Epoch 498: Train Loss: 0.20955797135829926, Validation Loss: 1.6535221338272095\n",
      "Epoch 499: Train Loss: 0.21112748086452485, Validation Loss: 1.66542387008667\n",
      "Fold 12 Metrics:\n",
      "Accuracy: 0.2727272727272727, Precision: 0.375, Recall: 0.5, F1-score: 0.42857142857142855, AUC: 0.25\n",
      "Confusion Matrix:\n",
      "[[0 5]\n",
      " [3 3]]\n",
      "Completed fold 12\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples from subject 11 to test set\n",
      "Adding 6 truth samples from subject 11 to test set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 250)\n",
      "(132, 65, 250)\n",
      "Epoch 0: Train Loss: 0.6962176203727722, Validation Loss: 0.7009924650192261\n",
      "Epoch 1: Train Loss: 0.7823820948600769, Validation Loss: 0.7050812244415283\n",
      "Epoch 2: Train Loss: 0.8171318411827088, Validation Loss: 0.7076879143714905\n",
      "Epoch 3: Train Loss: 0.7898962736129761, Validation Loss: 0.707019031047821\n",
      "Epoch 4: Train Loss: 0.6970553278923035, Validation Loss: 0.7174113988876343\n",
      "Epoch 5: Train Loss: 0.7230895161628723, Validation Loss: 0.7226249575614929\n",
      "Epoch 6: Train Loss: 0.7929680824279786, Validation Loss: 0.7198773622512817\n",
      "Epoch 7: Train Loss: 0.6998276233673095, Validation Loss: 0.7294549942016602\n",
      "Epoch 8: Train Loss: 0.717675530910492, Validation Loss: 0.735257089138031\n",
      "Epoch 9: Train Loss: 0.7002777218818664, Validation Loss: 0.7302087545394897\n",
      "Epoch 10: Train Loss: 0.6726867437362671, Validation Loss: 0.7291505932807922\n",
      "Epoch 11: Train Loss: 0.7424487113952637, Validation Loss: 0.7336850762367249\n",
      "Epoch 12: Train Loss: 0.7171980857849121, Validation Loss: 0.7330169677734375\n",
      "Epoch 13: Train Loss: 0.6677377820014954, Validation Loss: 0.7496865391731262\n",
      "Epoch 14: Train Loss: 0.6566278159618377, Validation Loss: 0.7446575164794922\n",
      "Epoch 15: Train Loss: 0.7424240827560424, Validation Loss: 0.7328041195869446\n",
      "Epoch 16: Train Loss: 0.655852997303009, Validation Loss: 0.737206757068634\n",
      "Epoch 17: Train Loss: 0.6928886651992798, Validation Loss: 0.7376012206077576\n",
      "Epoch 18: Train Loss: 0.6575129747390747, Validation Loss: 0.7412579655647278\n",
      "Epoch 19: Train Loss: 0.6877164602279663, Validation Loss: 0.7323977947235107\n",
      "Epoch 20: Train Loss: 0.6734800100326538, Validation Loss: 0.7286226153373718\n",
      "Epoch 21: Train Loss: 0.6591170310974122, Validation Loss: 0.729794979095459\n",
      "Epoch 22: Train Loss: 0.685903012752533, Validation Loss: 0.7338714003562927\n",
      "Epoch 23: Train Loss: 0.7755167841911316, Validation Loss: 0.7391586303710938\n",
      "Epoch 24: Train Loss: 0.713862681388855, Validation Loss: 0.7493975758552551\n",
      "Epoch 25: Train Loss: 0.6728558778762818, Validation Loss: 0.7506146430969238\n",
      "Epoch 26: Train Loss: 0.698799479007721, Validation Loss: 0.749666154384613\n",
      "Epoch 27: Train Loss: 0.6693824052810669, Validation Loss: 0.7551984190940857\n",
      "Epoch 28: Train Loss: 0.706107234954834, Validation Loss: 0.7567269206047058\n",
      "Epoch 29: Train Loss: 0.6959346652030944, Validation Loss: 0.7551414370536804\n",
      "Epoch 30: Train Loss: 0.7107937812805176, Validation Loss: 0.7746570110321045\n",
      "Epoch 31: Train Loss: 0.6807035446166992, Validation Loss: 0.7731428146362305\n",
      "Epoch 32: Train Loss: 0.6372831106185913, Validation Loss: 0.7597860097885132\n",
      "Epoch 33: Train Loss: 0.6722324967384339, Validation Loss: 0.758651852607727\n",
      "Epoch 34: Train Loss: 0.6968249797821044, Validation Loss: 0.7496312260627747\n",
      "Epoch 35: Train Loss: 0.6647838115692138, Validation Loss: 0.7520694732666016\n",
      "Epoch 36: Train Loss: 0.6679592967033386, Validation Loss: 0.7454096674919128\n",
      "Epoch 37: Train Loss: 0.6877055764198303, Validation Loss: 0.7458944320678711\n",
      "Epoch 38: Train Loss: 0.6346793532371521, Validation Loss: 0.7537051439285278\n",
      "Epoch 39: Train Loss: 0.6104312539100647, Validation Loss: 0.7588029503822327\n",
      "Epoch 40: Train Loss: 0.6740654468536377, Validation Loss: 0.7523763179779053\n",
      "Epoch 41: Train Loss: 0.6518847107887268, Validation Loss: 0.7465447783470154\n",
      "Epoch 42: Train Loss: 0.7008228659629822, Validation Loss: 0.7534463405609131\n",
      "Epoch 43: Train Loss: 0.6417886257171631, Validation Loss: 0.7523622512817383\n",
      "Epoch 44: Train Loss: 0.6758256435394288, Validation Loss: 0.7581258416175842\n",
      "Epoch 45: Train Loss: 0.6629006505012512, Validation Loss: 0.7645637392997742\n",
      "Epoch 46: Train Loss: 0.6463168382644653, Validation Loss: 0.7731172442436218\n",
      "Epoch 47: Train Loss: 0.6654348731040954, Validation Loss: 0.7808290123939514\n",
      "Epoch 48: Train Loss: 0.6724564790725708, Validation Loss: 0.7683101296424866\n",
      "Epoch 49: Train Loss: 0.6450850367546082, Validation Loss: 0.767112672328949\n",
      "Epoch 50: Train Loss: 0.6462149620056152, Validation Loss: 0.756395697593689\n",
      "Epoch 51: Train Loss: 0.6102344393730164, Validation Loss: 0.7572925686836243\n",
      "Epoch 52: Train Loss: 0.6431479215621948, Validation Loss: 0.7552325129508972\n",
      "Epoch 53: Train Loss: 0.6618477940559387, Validation Loss: 0.7516359090805054\n",
      "Epoch 54: Train Loss: 0.6523527264595032, Validation Loss: 0.7437238097190857\n",
      "Epoch 55: Train Loss: 0.6569542646408081, Validation Loss: 0.7476010322570801\n",
      "Epoch 56: Train Loss: 0.6497090458869934, Validation Loss: 0.7476534843444824\n",
      "Epoch 57: Train Loss: 0.6139641046524048, Validation Loss: 0.7541062235832214\n",
      "Epoch 58: Train Loss: 0.6825913548469543, Validation Loss: 0.7509694695472717\n",
      "Epoch 59: Train Loss: 0.6300884127616883, Validation Loss: 0.7456707954406738\n",
      "Epoch 60: Train Loss: 0.6085131525993347, Validation Loss: 0.7472951412200928\n",
      "Epoch 61: Train Loss: 0.7115872025489807, Validation Loss: 0.748663604259491\n",
      "Epoch 62: Train Loss: 0.630057978630066, Validation Loss: 0.7569055557250977\n",
      "Epoch 63: Train Loss: 0.6992394804954529, Validation Loss: 0.7817140221595764\n",
      "Epoch 64: Train Loss: 0.6608241438865662, Validation Loss: 0.7832350730895996\n",
      "Epoch 65: Train Loss: 0.6055216312408447, Validation Loss: 0.772822380065918\n",
      "Epoch 66: Train Loss: 0.6640828251838684, Validation Loss: 0.7746047377586365\n",
      "Epoch 67: Train Loss: 0.5922183871269227, Validation Loss: 0.7731946110725403\n",
      "Epoch 68: Train Loss: 0.6461625933647156, Validation Loss: 0.7774238586425781\n",
      "Epoch 69: Train Loss: 0.6375930905342102, Validation Loss: 0.7757441401481628\n",
      "Epoch 70: Train Loss: 0.6571801543235779, Validation Loss: 0.7761725783348083\n",
      "Epoch 71: Train Loss: 0.6483226299285889, Validation Loss: 0.7676255702972412\n",
      "Epoch 72: Train Loss: 0.6693691730499267, Validation Loss: 0.764894425868988\n",
      "Epoch 73: Train Loss: 0.6283095955848694, Validation Loss: 0.7627156376838684\n",
      "Epoch 74: Train Loss: 0.645249319076538, Validation Loss: 0.7855408787727356\n",
      "Epoch 75: Train Loss: 0.6321911096572876, Validation Loss: 0.7875276207923889\n",
      "Epoch 76: Train Loss: 0.5624357759952545, Validation Loss: 0.7876315712928772\n",
      "Epoch 77: Train Loss: 0.6178206443786621, Validation Loss: 0.7882312536239624\n",
      "Epoch 78: Train Loss: 0.5801559925079346, Validation Loss: 0.7866016030311584\n",
      "Epoch 79: Train Loss: 0.6118848562240601, Validation Loss: 0.7905296087265015\n",
      "Epoch 80: Train Loss: 0.5896766602993011, Validation Loss: 0.7860615849494934\n",
      "Epoch 81: Train Loss: 0.6129467606544494, Validation Loss: 0.7898364663124084\n",
      "Epoch 82: Train Loss: 0.5806481897830963, Validation Loss: 0.8031749725341797\n",
      "Epoch 83: Train Loss: 0.6017027378082276, Validation Loss: 0.8015528917312622\n",
      "Epoch 84: Train Loss: 0.6098694205284119, Validation Loss: 0.7923967242240906\n",
      "Epoch 85: Train Loss: 0.5657603442668915, Validation Loss: 0.7803441882133484\n",
      "Epoch 86: Train Loss: 0.5586374521255493, Validation Loss: 0.7862382531166077\n",
      "Epoch 87: Train Loss: 0.5901755571365357, Validation Loss: 0.7826375961303711\n",
      "Epoch 88: Train Loss: 0.6143504500389099, Validation Loss: 0.7769942879676819\n",
      "Epoch 89: Train Loss: 0.6611699223518371, Validation Loss: 0.775946319103241\n",
      "Epoch 90: Train Loss: 0.5752420127391815, Validation Loss: 0.7829902768135071\n",
      "Epoch 91: Train Loss: 0.5792967498302459, Validation Loss: 0.7906181216239929\n",
      "Epoch 92: Train Loss: 0.5969008326530456, Validation Loss: 0.7846145033836365\n",
      "Epoch 93: Train Loss: 0.5616254508495331, Validation Loss: 0.7761028409004211\n",
      "Epoch 94: Train Loss: 0.5572829127311707, Validation Loss: 0.7713019251823425\n",
      "Epoch 95: Train Loss: 0.6052556037902832, Validation Loss: 0.7701492309570312\n",
      "Epoch 96: Train Loss: 0.5910103797912598, Validation Loss: 0.7698258757591248\n",
      "Epoch 97: Train Loss: 0.7068328022956848, Validation Loss: 0.7718253135681152\n",
      "Epoch 98: Train Loss: 0.598084318637848, Validation Loss: 0.7749889492988586\n",
      "Epoch 99: Train Loss: 0.5363837659358979, Validation Loss: 0.7693288922309875\n",
      "Epoch 100: Train Loss: 0.5352252960205078, Validation Loss: 0.7807798385620117\n",
      "Epoch 101: Train Loss: 0.6277740836143494, Validation Loss: 0.7893911004066467\n",
      "Epoch 102: Train Loss: 0.6166067123413086, Validation Loss: 0.7922864556312561\n",
      "Epoch 103: Train Loss: 0.6131769061088562, Validation Loss: 0.8191688060760498\n",
      "Epoch 104: Train Loss: 0.5764963865280152, Validation Loss: 0.8122549057006836\n",
      "Epoch 105: Train Loss: 0.6072810411453247, Validation Loss: 0.8122607469558716\n",
      "Epoch 106: Train Loss: 0.6239231824874878, Validation Loss: 0.8107408285140991\n",
      "Epoch 107: Train Loss: 0.5831380724906922, Validation Loss: 0.8123227953910828\n",
      "Epoch 108: Train Loss: 0.6147213816642761, Validation Loss: 0.7935682535171509\n",
      "Epoch 109: Train Loss: 0.5559913516044617, Validation Loss: 0.7978376150131226\n",
      "Epoch 110: Train Loss: 0.6129808187484741, Validation Loss: 0.7980806231498718\n",
      "Epoch 111: Train Loss: 0.576849365234375, Validation Loss: 0.7986084222793579\n",
      "Epoch 112: Train Loss: 0.5526097178459167, Validation Loss: 0.8090379238128662\n",
      "Epoch 113: Train Loss: 0.551528400182724, Validation Loss: 0.8045030832290649\n",
      "Epoch 114: Train Loss: 0.5858804523944855, Validation Loss: 0.821185290813446\n",
      "Epoch 115: Train Loss: 0.5695913314819336, Validation Loss: 0.8324235081672668\n",
      "Epoch 116: Train Loss: 0.66204833984375, Validation Loss: 0.8289352655410767\n",
      "Epoch 117: Train Loss: 0.5216848015785217, Validation Loss: 0.8290098309516907\n",
      "Epoch 118: Train Loss: 0.5775375843048096, Validation Loss: 0.8242069482803345\n",
      "Epoch 119: Train Loss: 0.5349085330963135, Validation Loss: 0.8201373815536499\n",
      "Epoch 120: Train Loss: 0.6261988639831543, Validation Loss: 0.8184690475463867\n",
      "Epoch 121: Train Loss: 0.5962906837463379, Validation Loss: 0.8085634112358093\n",
      "Epoch 122: Train Loss: 0.5823826432228089, Validation Loss: 0.7891700863838196\n",
      "Epoch 123: Train Loss: 0.5266098976135254, Validation Loss: 0.7953595519065857\n",
      "Epoch 124: Train Loss: 0.570632666349411, Validation Loss: 0.7812436819076538\n",
      "Epoch 125: Train Loss: 0.5551445364952088, Validation Loss: 0.7783017754554749\n",
      "Epoch 126: Train Loss: 0.5713182687759399, Validation Loss: 0.7755071520805359\n",
      "Epoch 127: Train Loss: 0.591176700592041, Validation Loss: 0.7716484069824219\n",
      "Epoch 128: Train Loss: 0.5593278229236602, Validation Loss: 0.7728676795959473\n",
      "Epoch 129: Train Loss: 0.6143112063407898, Validation Loss: 0.7832076549530029\n",
      "Epoch 130: Train Loss: 0.578418242931366, Validation Loss: 0.7743407487869263\n",
      "Epoch 131: Train Loss: 0.5588878393173218, Validation Loss: 0.7941247820854187\n",
      "Epoch 132: Train Loss: 0.563417625427246, Validation Loss: 0.7999122142791748\n",
      "Epoch 133: Train Loss: 0.5668108105659485, Validation Loss: 0.7895873785018921\n",
      "Epoch 134: Train Loss: 0.5584954142570495, Validation Loss: 0.795941948890686\n",
      "Epoch 135: Train Loss: 0.5486073791980743, Validation Loss: 0.7907716035842896\n",
      "Epoch 136: Train Loss: 0.5798533797264099, Validation Loss: 0.7867897152900696\n",
      "Epoch 137: Train Loss: 0.5752959251403809, Validation Loss: 0.7902228832244873\n",
      "Epoch 138: Train Loss: 0.6133765459060669, Validation Loss: 0.7884095311164856\n",
      "Epoch 139: Train Loss: 0.5362280786037446, Validation Loss: 0.7897591590881348\n",
      "Epoch 140: Train Loss: 0.56603102684021, Validation Loss: 0.8224214315414429\n",
      "Epoch 141: Train Loss: 0.5937568426132203, Validation Loss: 0.8274478316307068\n",
      "Epoch 142: Train Loss: 0.6215956330299377, Validation Loss: 0.8340754508972168\n",
      "Epoch 143: Train Loss: 0.5338365137577057, Validation Loss: 0.8100231289863586\n",
      "Epoch 144: Train Loss: 0.5581091165542602, Validation Loss: 0.7994565367698669\n",
      "Epoch 145: Train Loss: 0.5066796064376831, Validation Loss: 0.7814880609512329\n",
      "Epoch 146: Train Loss: 0.5183339357376099, Validation Loss: 0.767909824848175\n",
      "Epoch 147: Train Loss: 0.5606228590011597, Validation Loss: 0.781292736530304\n",
      "Epoch 148: Train Loss: 0.6001084923744202, Validation Loss: 0.7870211601257324\n",
      "Epoch 149: Train Loss: 0.5299143552780151, Validation Loss: 0.7845607399940491\n",
      "Epoch 150: Train Loss: 0.5564508974552155, Validation Loss: 0.8107884526252747\n",
      "Epoch 151: Train Loss: 0.5523241043090821, Validation Loss: 0.8488972783088684\n",
      "Epoch 152: Train Loss: 0.5960467457771301, Validation Loss: 0.8632133603096008\n",
      "Epoch 153: Train Loss: 0.5460033595561982, Validation Loss: 0.8658494353294373\n",
      "Epoch 154: Train Loss: 0.5485005736351013, Validation Loss: 0.8548721075057983\n",
      "Epoch 155: Train Loss: 0.5550962388515472, Validation Loss: 0.8312910199165344\n",
      "Epoch 156: Train Loss: 0.515286248922348, Validation Loss: 0.8077676892280579\n",
      "Epoch 157: Train Loss: 0.5029646217823028, Validation Loss: 0.8042644262313843\n",
      "Epoch 158: Train Loss: 0.5265891194343567, Validation Loss: 0.8067322969436646\n",
      "Epoch 159: Train Loss: 0.52454092502594, Validation Loss: 0.8083688020706177\n",
      "Epoch 160: Train Loss: 0.5217179656028748, Validation Loss: 0.8175597190856934\n",
      "Epoch 161: Train Loss: 0.511517059803009, Validation Loss: 0.8274829387664795\n",
      "Epoch 162: Train Loss: 0.5549522876739502, Validation Loss: 0.8286597728729248\n",
      "Epoch 163: Train Loss: 0.5553058981895447, Validation Loss: 0.8526187539100647\n",
      "Epoch 164: Train Loss: 0.5439644098281861, Validation Loss: 0.862444281578064\n",
      "Epoch 165: Train Loss: 0.5080884516239166, Validation Loss: 0.8657256960868835\n",
      "Epoch 166: Train Loss: 0.47170028686523435, Validation Loss: 0.8694285750389099\n",
      "Epoch 167: Train Loss: 0.4915998876094818, Validation Loss: 0.8786594271659851\n",
      "Epoch 168: Train Loss: 0.5670335948467254, Validation Loss: 0.9013751745223999\n",
      "Epoch 169: Train Loss: 0.539094227552414, Validation Loss: 0.902957558631897\n",
      "Epoch 170: Train Loss: 0.548811811208725, Validation Loss: 0.8460628390312195\n",
      "Epoch 171: Train Loss: 0.5684147119522095, Validation Loss: 0.825627326965332\n",
      "Epoch 172: Train Loss: 0.579440838098526, Validation Loss: 0.8246812224388123\n",
      "Epoch 173: Train Loss: 0.48927422761917116, Validation Loss: 0.8089890480041504\n",
      "Epoch 174: Train Loss: 0.518208509683609, Validation Loss: 0.7968086004257202\n",
      "Epoch 175: Train Loss: 0.48902924060821534, Validation Loss: 0.7915351986885071\n",
      "Epoch 176: Train Loss: 0.531479012966156, Validation Loss: 0.7906354665756226\n",
      "Epoch 177: Train Loss: 0.5264797806739807, Validation Loss: 0.7885578274726868\n",
      "Epoch 178: Train Loss: 0.529917985200882, Validation Loss: 0.7940927743911743\n",
      "Epoch 179: Train Loss: 0.5539246380329133, Validation Loss: 0.7941689491271973\n",
      "Epoch 180: Train Loss: 0.5178665578365326, Validation Loss: 0.8146611452102661\n",
      "Epoch 181: Train Loss: 0.5843933641910553, Validation Loss: 0.8068772554397583\n",
      "Epoch 182: Train Loss: 0.5427306890487671, Validation Loss: 0.7994069457054138\n",
      "Epoch 183: Train Loss: 0.5512094736099243, Validation Loss: 0.8097808361053467\n",
      "Epoch 184: Train Loss: 0.5466125786304474, Validation Loss: 0.8051764965057373\n",
      "Epoch 185: Train Loss: 0.5297691583633423, Validation Loss: 0.8076415061950684\n",
      "Epoch 186: Train Loss: 0.6016871571540833, Validation Loss: 0.8190382122993469\n",
      "Epoch 187: Train Loss: 0.5073462665081024, Validation Loss: 0.8370002508163452\n",
      "Epoch 188: Train Loss: 0.5018880307674408, Validation Loss: 0.8310613036155701\n",
      "Epoch 189: Train Loss: 0.514110016822815, Validation Loss: 0.8206761479377747\n",
      "Epoch 190: Train Loss: 0.4550955772399902, Validation Loss: 0.8084099888801575\n",
      "Epoch 191: Train Loss: 0.47442891597747805, Validation Loss: 0.7986189126968384\n",
      "Epoch 192: Train Loss: 0.4905829966068268, Validation Loss: 0.7701185345649719\n",
      "Epoch 193: Train Loss: 0.5236162483692169, Validation Loss: 0.7732744216918945\n",
      "Epoch 194: Train Loss: 0.5285390973091125, Validation Loss: 0.7542120814323425\n",
      "Epoch 195: Train Loss: 0.45071412324905397, Validation Loss: 0.7618851661682129\n",
      "Epoch 196: Train Loss: 0.5934691488742828, Validation Loss: 0.7580668926239014\n",
      "Epoch 197: Train Loss: 0.5223798096179962, Validation Loss: 0.7699973583221436\n",
      "Epoch 198: Train Loss: 0.5264336884021759, Validation Loss: 0.7567095756530762\n",
      "Epoch 199: Train Loss: 0.5009148001670838, Validation Loss: 0.7672966122627258\n",
      "Epoch 200: Train Loss: 0.5058803498744965, Validation Loss: 0.74103844165802\n",
      "Epoch 201: Train Loss: 0.5282278180122375, Validation Loss: 0.7471988201141357\n",
      "Epoch 202: Train Loss: 0.49408723711967467, Validation Loss: 0.7411099672317505\n",
      "Epoch 203: Train Loss: 0.5257265448570252, Validation Loss: 0.748703122138977\n",
      "Epoch 204: Train Loss: 0.5081072866916656, Validation Loss: 0.7368539571762085\n",
      "Epoch 205: Train Loss: 0.5958847880363465, Validation Loss: 0.752032458782196\n",
      "Epoch 206: Train Loss: 0.481875878572464, Validation Loss: 0.7546157836914062\n",
      "Epoch 207: Train Loss: 0.49092166423797606, Validation Loss: 0.7497827410697937\n",
      "Epoch 208: Train Loss: 0.5047120332717896, Validation Loss: 0.7477849721908569\n",
      "Epoch 209: Train Loss: 0.48103995323181153, Validation Loss: 0.7496705055236816\n",
      "Epoch 210: Train Loss: 0.4743088066577911, Validation Loss: 0.7590274810791016\n",
      "Epoch 211: Train Loss: 0.5644815504550934, Validation Loss: 0.7735159993171692\n",
      "Epoch 212: Train Loss: 0.4544552803039551, Validation Loss: 0.8052799105644226\n",
      "Epoch 213: Train Loss: 0.5014910399913788, Validation Loss: 0.8203347325325012\n",
      "Epoch 214: Train Loss: 0.5624536633491516, Validation Loss: 0.8142513036727905\n",
      "Epoch 215: Train Loss: 0.5542194783687592, Validation Loss: 0.8063588738441467\n",
      "Epoch 216: Train Loss: 0.49246559143066404, Validation Loss: 0.7957764863967896\n",
      "Epoch 217: Train Loss: 0.5049027085304261, Validation Loss: 0.800812304019928\n",
      "Epoch 218: Train Loss: 0.4828576147556305, Validation Loss: 0.8007796406745911\n",
      "Epoch 219: Train Loss: 0.4835296869277954, Validation Loss: 0.7969763875007629\n",
      "Epoch 220: Train Loss: 0.5172058522701264, Validation Loss: 0.8071710467338562\n",
      "Epoch 221: Train Loss: 0.5601132094860077, Validation Loss: 0.7939048409461975\n",
      "Epoch 222: Train Loss: 0.43757652044296264, Validation Loss: 0.7907391786575317\n",
      "Epoch 223: Train Loss: 0.49842131733894346, Validation Loss: 0.777153730392456\n",
      "Epoch 224: Train Loss: 0.6012000620365143, Validation Loss: 0.7611120343208313\n",
      "Epoch 225: Train Loss: 0.5208830416202546, Validation Loss: 0.746482789516449\n",
      "Epoch 226: Train Loss: 0.46142866015434264, Validation Loss: 0.7436909675598145\n",
      "Epoch 227: Train Loss: 0.44444839656352997, Validation Loss: 0.7261309027671814\n",
      "Epoch 228: Train Loss: 0.453282368183136, Validation Loss: 0.7245418429374695\n",
      "Epoch 229: Train Loss: 0.46028117537498475, Validation Loss: 0.7416894435882568\n",
      "Epoch 230: Train Loss: 0.4447726786136627, Validation Loss: 0.7826871275901794\n",
      "Epoch 231: Train Loss: 0.5622211754322052, Validation Loss: 0.8121951818466187\n",
      "Epoch 232: Train Loss: 0.45946707129478453, Validation Loss: 0.8041406869888306\n",
      "Epoch 233: Train Loss: 0.5155650794506073, Validation Loss: 0.8062441945075989\n",
      "Epoch 234: Train Loss: 0.4483631610870361, Validation Loss: 0.8403726816177368\n",
      "Epoch 235: Train Loss: 0.46617494225502015, Validation Loss: 0.8291991353034973\n",
      "Epoch 236: Train Loss: 0.4484608113765717, Validation Loss: 0.8304330110549927\n",
      "Epoch 237: Train Loss: 0.5080654442310333, Validation Loss: 0.8407756090164185\n",
      "Epoch 238: Train Loss: 0.49946627020835876, Validation Loss: 0.8385412096977234\n",
      "Epoch 239: Train Loss: 0.5221471965312958, Validation Loss: 0.8301677703857422\n",
      "Epoch 240: Train Loss: 0.5360918402671814, Validation Loss: 0.8249140977859497\n",
      "Epoch 241: Train Loss: 0.48728789687156676, Validation Loss: 0.8289771676063538\n",
      "Epoch 242: Train Loss: 0.5523743510246277, Validation Loss: 0.7900172472000122\n",
      "Epoch 243: Train Loss: 0.5276292145252228, Validation Loss: 0.7925243377685547\n",
      "Epoch 244: Train Loss: 0.45767199993133545, Validation Loss: 0.8429046869277954\n",
      "Epoch 245: Train Loss: 0.5231761515140534, Validation Loss: 0.8737221956253052\n",
      "Epoch 246: Train Loss: 0.4911745607852936, Validation Loss: 0.8632466793060303\n",
      "Epoch 247: Train Loss: 0.5462597131729126, Validation Loss: 0.8876228332519531\n",
      "Epoch 248: Train Loss: 0.4772369503974915, Validation Loss: 0.8703092932701111\n",
      "Epoch 249: Train Loss: 0.47325754165649414, Validation Loss: 0.8652499318122864\n",
      "Epoch 250: Train Loss: 0.5040307700634002, Validation Loss: 0.8334704637527466\n",
      "Epoch 251: Train Loss: 0.4447940349578857, Validation Loss: 0.8477101922035217\n",
      "Epoch 252: Train Loss: 0.46201335787773135, Validation Loss: 0.8383085131645203\n",
      "Epoch 253: Train Loss: 0.5006193459033966, Validation Loss: 0.8200644850730896\n",
      "Epoch 254: Train Loss: 0.45688048005104065, Validation Loss: 0.8093724846839905\n",
      "Epoch 255: Train Loss: 0.4367647349834442, Validation Loss: 0.795483410358429\n",
      "Epoch 256: Train Loss: 0.4927809238433838, Validation Loss: 0.8070284128189087\n",
      "Epoch 257: Train Loss: 0.43804554343223573, Validation Loss: 0.8174842000007629\n",
      "Epoch 258: Train Loss: 0.5274104356765748, Validation Loss: 0.8011825680732727\n",
      "Epoch 259: Train Loss: 0.4261105239391327, Validation Loss: 0.7962262630462646\n",
      "Epoch 260: Train Loss: 0.49686760902404786, Validation Loss: 0.7967740893363953\n",
      "Epoch 261: Train Loss: 0.46742974519729613, Validation Loss: 0.8175235986709595\n",
      "Epoch 262: Train Loss: 0.43487749099731443, Validation Loss: 0.8050749897956848\n",
      "Epoch 263: Train Loss: 0.49485543370246887, Validation Loss: 0.7819617390632629\n",
      "Epoch 264: Train Loss: 0.45590218901634216, Validation Loss: 0.7721227407455444\n",
      "Epoch 265: Train Loss: 0.42923136949539187, Validation Loss: 0.7753175497055054\n",
      "Epoch 266: Train Loss: 0.48109519481658936, Validation Loss: 0.7898210287094116\n",
      "Epoch 267: Train Loss: 0.4928197622299194, Validation Loss: 0.7682998776435852\n",
      "Epoch 268: Train Loss: 0.41470217108726504, Validation Loss: 0.7573879361152649\n",
      "Epoch 269: Train Loss: 0.45268273949623106, Validation Loss: 0.7762690782546997\n",
      "Epoch 270: Train Loss: 0.4102425634860992, Validation Loss: 0.7635669112205505\n",
      "Epoch 271: Train Loss: 0.45279263854026797, Validation Loss: 0.7702018022537231\n",
      "Epoch 272: Train Loss: 0.4662349998950958, Validation Loss: 0.7451499700546265\n",
      "Epoch 273: Train Loss: 0.517388916015625, Validation Loss: 0.7279106974601746\n",
      "Epoch 274: Train Loss: 0.42743096947669984, Validation Loss: 0.7259032130241394\n",
      "Epoch 275: Train Loss: 0.4190548062324524, Validation Loss: 0.730566143989563\n",
      "Epoch 276: Train Loss: 0.4181795358657837, Validation Loss: 0.7239213585853577\n",
      "Epoch 277: Train Loss: 0.47768871784210204, Validation Loss: 0.743365466594696\n",
      "Epoch 278: Train Loss: 0.45592367053031924, Validation Loss: 0.7406947612762451\n",
      "Epoch 279: Train Loss: 0.4417023420333862, Validation Loss: 0.7528550624847412\n",
      "Epoch 280: Train Loss: 0.47364630103111266, Validation Loss: 0.7830066084861755\n",
      "Epoch 281: Train Loss: 0.4097771406173706, Validation Loss: 0.7671996355056763\n",
      "Epoch 282: Train Loss: 0.44642251133918764, Validation Loss: 0.7601330876350403\n",
      "Epoch 283: Train Loss: 0.4942489147186279, Validation Loss: 0.7392681241035461\n",
      "Epoch 284: Train Loss: 0.3959647178649902, Validation Loss: 0.7161319851875305\n",
      "Epoch 285: Train Loss: 0.4919127881526947, Validation Loss: 0.7070863842964172\n",
      "Epoch 286: Train Loss: 0.43468503952026366, Validation Loss: 0.71145099401474\n",
      "Epoch 287: Train Loss: 0.3996747314929962, Validation Loss: 0.7239897847175598\n",
      "Epoch 288: Train Loss: 0.4619275748729706, Validation Loss: 0.7220088839530945\n",
      "Epoch 289: Train Loss: 0.4082782924175262, Validation Loss: 0.7288156151771545\n",
      "Epoch 290: Train Loss: 0.562571907043457, Validation Loss: 0.7046194672584534\n",
      "Epoch 291: Train Loss: 0.3781647503376007, Validation Loss: 0.7176715731620789\n",
      "Epoch 292: Train Loss: 0.39351496696472166, Validation Loss: 0.737048864364624\n",
      "Epoch 293: Train Loss: 0.42308364510536195, Validation Loss: 0.7205026149749756\n",
      "Epoch 294: Train Loss: 0.3983290731906891, Validation Loss: 0.723934531211853\n",
      "Epoch 295: Train Loss: 0.4005011558532715, Validation Loss: 0.7263129353523254\n",
      "Epoch 296: Train Loss: 0.46556265354156495, Validation Loss: 0.7200960516929626\n",
      "Epoch 297: Train Loss: 0.45371667146682737, Validation Loss: 0.7148874998092651\n",
      "Epoch 298: Train Loss: 0.5017616271972656, Validation Loss: 0.6929528117179871\n",
      "Epoch 299: Train Loss: 0.41484296321868896, Validation Loss: 0.7019216418266296\n",
      "Epoch 300: Train Loss: 0.4211811423301697, Validation Loss: 0.6637886762619019\n",
      "Epoch 301: Train Loss: 0.41908087134361266, Validation Loss: 0.6471627950668335\n",
      "Epoch 302: Train Loss: 0.40221627354621886, Validation Loss: 0.6770938634872437\n",
      "Epoch 303: Train Loss: 0.4366895377635956, Validation Loss: 0.6802011728286743\n",
      "Epoch 304: Train Loss: 0.4157260060310364, Validation Loss: 0.6663863062858582\n",
      "Epoch 305: Train Loss: 0.36735087633132935, Validation Loss: 0.6813966035842896\n",
      "Epoch 306: Train Loss: 0.47674013376235963, Validation Loss: 0.7188401222229004\n",
      "Epoch 307: Train Loss: 0.4102965533733368, Validation Loss: 0.7052229642868042\n",
      "Epoch 308: Train Loss: 0.3912592113018036, Validation Loss: 0.7093952894210815\n",
      "Epoch 309: Train Loss: 0.4707767188549042, Validation Loss: 0.7086235880851746\n",
      "Epoch 310: Train Loss: 0.4884606719017029, Validation Loss: 0.7283597588539124\n",
      "Epoch 311: Train Loss: 0.4397478818893433, Validation Loss: 0.7623389363288879\n",
      "Epoch 312: Train Loss: 0.4371758222579956, Validation Loss: 0.7618613839149475\n",
      "Epoch 313: Train Loss: 0.4086823105812073, Validation Loss: 0.7574701309204102\n",
      "Epoch 314: Train Loss: 0.46064333319664, Validation Loss: 0.7763326168060303\n",
      "Epoch 315: Train Loss: 0.412898051738739, Validation Loss: 0.8107014894485474\n",
      "Epoch 316: Train Loss: 0.4155689775943756, Validation Loss: 0.7878148555755615\n",
      "Epoch 317: Train Loss: 0.43550989031791687, Validation Loss: 0.7787003517150879\n",
      "Epoch 318: Train Loss: 0.4352246344089508, Validation Loss: 0.7748467922210693\n",
      "Epoch 319: Train Loss: 0.37811005115509033, Validation Loss: 0.7826135754585266\n",
      "Epoch 320: Train Loss: 0.4327952146530151, Validation Loss: 0.7843266725540161\n",
      "Epoch 321: Train Loss: 0.4110670924186707, Validation Loss: 0.7859542965888977\n",
      "Epoch 322: Train Loss: 0.4318761587142944, Validation Loss: 0.7885475158691406\n",
      "Epoch 323: Train Loss: 0.4353380024433136, Validation Loss: 0.7809243202209473\n",
      "Epoch 324: Train Loss: 0.4030045747756958, Validation Loss: 0.7889317870140076\n",
      "Epoch 325: Train Loss: 0.3682046592235565, Validation Loss: 0.8111798763275146\n",
      "Epoch 326: Train Loss: 0.39210968613624575, Validation Loss: 0.8107638359069824\n",
      "Epoch 327: Train Loss: 0.3997944474220276, Validation Loss: 0.8107473254203796\n",
      "Epoch 328: Train Loss: 0.39890095591545105, Validation Loss: 0.8065423965454102\n",
      "Epoch 329: Train Loss: 0.39523378014564514, Validation Loss: 0.796953022480011\n",
      "Epoch 330: Train Loss: 0.4576747536659241, Validation Loss: 0.7919740676879883\n",
      "Epoch 331: Train Loss: 0.428592324256897, Validation Loss: 0.7810830473899841\n",
      "Epoch 332: Train Loss: 0.41484416127204893, Validation Loss: 0.8060455322265625\n",
      "Epoch 333: Train Loss: 0.46065916419029235, Validation Loss: 0.8389685750007629\n",
      "Epoch 334: Train Loss: 0.3661620542407036, Validation Loss: 0.7898737192153931\n",
      "Epoch 335: Train Loss: 0.3419849783182144, Validation Loss: 0.7777635455131531\n",
      "Epoch 336: Train Loss: 0.42160449028015134, Validation Loss: 0.7851570248603821\n",
      "Epoch 337: Train Loss: 0.4241447925567627, Validation Loss: 0.7758008241653442\n",
      "Epoch 338: Train Loss: 0.3787221312522888, Validation Loss: 0.7530797123908997\n",
      "Epoch 339: Train Loss: 0.460951042175293, Validation Loss: 0.7552587389945984\n",
      "Epoch 340: Train Loss: 0.3957299947738647, Validation Loss: 0.7838897705078125\n",
      "Epoch 341: Train Loss: 0.4257121026515961, Validation Loss: 0.7824950218200684\n",
      "Epoch 342: Train Loss: 0.39355050325393676, Validation Loss: 0.8102250695228577\n",
      "Epoch 343: Train Loss: 0.3809939235448837, Validation Loss: 0.7774219512939453\n",
      "Epoch 344: Train Loss: 0.42609394192695615, Validation Loss: 0.7625358700752258\n",
      "Epoch 345: Train Loss: 0.4732002913951874, Validation Loss: 0.7214337587356567\n",
      "Epoch 346: Train Loss: 0.3843907356262207, Validation Loss: 0.7014589309692383\n",
      "Epoch 347: Train Loss: 0.4245976686477661, Validation Loss: 0.7178425788879395\n",
      "Epoch 348: Train Loss: 0.41923803091049194, Validation Loss: 0.7409095168113708\n",
      "Epoch 349: Train Loss: 0.38029147386550904, Validation Loss: 0.729458212852478\n",
      "Epoch 350: Train Loss: 0.3996885597705841, Validation Loss: 0.7595498561859131\n",
      "Epoch 351: Train Loss: 0.40176944732666015, Validation Loss: 0.8001203536987305\n",
      "Epoch 352: Train Loss: 0.421306174993515, Validation Loss: 0.8030765652656555\n",
      "Epoch 353: Train Loss: 0.3784738600254059, Validation Loss: 0.7843985557556152\n",
      "Epoch 354: Train Loss: 0.4093526482582092, Validation Loss: 0.7895151376724243\n",
      "Epoch 355: Train Loss: 0.4182111084461212, Validation Loss: 0.7944316864013672\n",
      "Epoch 356: Train Loss: 0.3714638531208038, Validation Loss: 0.7701640725135803\n",
      "Epoch 357: Train Loss: 0.4479659259319305, Validation Loss: 0.7788276672363281\n",
      "Epoch 358: Train Loss: 0.5252024471759796, Validation Loss: 0.7898845076560974\n",
      "Epoch 359: Train Loss: 0.5099961698055268, Validation Loss: 0.7941434979438782\n",
      "Epoch 360: Train Loss: 0.4310500383377075, Validation Loss: 0.7562267184257507\n",
      "Epoch 361: Train Loss: 0.35510565638542174, Validation Loss: 0.7221516966819763\n",
      "Epoch 362: Train Loss: 0.398939710855484, Validation Loss: 0.7128496170043945\n",
      "Epoch 363: Train Loss: 0.35451331436634065, Validation Loss: 0.6821085810661316\n",
      "Epoch 364: Train Loss: 0.3806824445724487, Validation Loss: 0.6961653828620911\n",
      "Epoch 365: Train Loss: 0.3868980884552002, Validation Loss: 0.7178001999855042\n",
      "Epoch 366: Train Loss: 0.38185128569602966, Validation Loss: 0.7198459506034851\n",
      "Epoch 367: Train Loss: 0.37494840025901793, Validation Loss: 0.7103903293609619\n",
      "Epoch 368: Train Loss: 0.35216560065746305, Validation Loss: 0.7236700057983398\n",
      "Epoch 369: Train Loss: 0.35337552428245544, Validation Loss: 0.6754663586616516\n",
      "Epoch 370: Train Loss: 0.39211764335632326, Validation Loss: 0.7275593876838684\n",
      "Epoch 371: Train Loss: 0.34085610806941985, Validation Loss: 0.7345382571220398\n",
      "Epoch 372: Train Loss: 0.355718269944191, Validation Loss: 0.7451748847961426\n",
      "Epoch 373: Train Loss: 0.33556097745895386, Validation Loss: 0.7720671892166138\n",
      "Epoch 374: Train Loss: 0.39690475463867186, Validation Loss: 0.7465111613273621\n",
      "Epoch 375: Train Loss: 0.36303306818008424, Validation Loss: 0.7461423873901367\n",
      "Epoch 376: Train Loss: 0.3528729349374771, Validation Loss: 0.7251896262168884\n",
      "Epoch 377: Train Loss: 0.3483941286802292, Validation Loss: 0.7043022513389587\n",
      "Epoch 378: Train Loss: 0.314197239279747, Validation Loss: 0.6970377564430237\n",
      "Epoch 379: Train Loss: 0.35509116053581236, Validation Loss: 0.7231624126434326\n",
      "Epoch 380: Train Loss: 0.3208316475152969, Validation Loss: 0.7420964241027832\n",
      "Epoch 381: Train Loss: 0.42463658452034, Validation Loss: 0.7555761337280273\n",
      "Epoch 382: Train Loss: 0.44076913595199585, Validation Loss: 0.7758890986442566\n",
      "Epoch 383: Train Loss: 0.34128355979919434, Validation Loss: 0.7882205247879028\n",
      "Epoch 384: Train Loss: 0.3615014910697937, Validation Loss: 0.7979898452758789\n",
      "Epoch 385: Train Loss: 0.37894766330718993, Validation Loss: 0.8006229400634766\n",
      "Epoch 386: Train Loss: 0.36993741393089297, Validation Loss: 0.8043820858001709\n",
      "Epoch 387: Train Loss: 0.6027064859867096, Validation Loss: 0.7785585522651672\n",
      "Epoch 388: Train Loss: 0.32350875735282897, Validation Loss: 0.7614505887031555\n",
      "Epoch 389: Train Loss: 0.33675479590892793, Validation Loss: 0.7684088945388794\n",
      "Epoch 390: Train Loss: 0.3393630877137184, Validation Loss: 0.6922106742858887\n",
      "Epoch 391: Train Loss: 0.42919180989265443, Validation Loss: 0.7193137407302856\n",
      "Epoch 392: Train Loss: 0.37043204307556155, Validation Loss: 0.7127147316932678\n",
      "Epoch 393: Train Loss: 0.4463835656642914, Validation Loss: 0.7355856895446777\n",
      "Epoch 394: Train Loss: 0.30944634079933164, Validation Loss: 0.7478622198104858\n",
      "Epoch 395: Train Loss: 0.4295438230037689, Validation Loss: 0.7538143992424011\n",
      "Epoch 396: Train Loss: 0.37616044878959654, Validation Loss: 0.7668954133987427\n",
      "Epoch 397: Train Loss: 0.3331430286169052, Validation Loss: 0.7541251182556152\n",
      "Epoch 398: Train Loss: 0.3591295540332794, Validation Loss: 0.7335883975028992\n",
      "Epoch 399: Train Loss: 0.41529313921928407, Validation Loss: 0.7320917248725891\n",
      "Epoch 400: Train Loss: 0.3899501323699951, Validation Loss: 0.7664481997489929\n",
      "Epoch 401: Train Loss: 0.35530564188957214, Validation Loss: 0.7237542271614075\n",
      "Epoch 402: Train Loss: 0.3271134540438652, Validation Loss: 0.7417243123054504\n",
      "Epoch 403: Train Loss: 0.36496399641036986, Validation Loss: 0.7562670707702637\n",
      "Epoch 404: Train Loss: 0.4052114486694336, Validation Loss: 0.7988553643226624\n",
      "Epoch 405: Train Loss: 0.3366255462169647, Validation Loss: 0.7783721685409546\n",
      "Epoch 406: Train Loss: 0.4306961536407471, Validation Loss: 0.7919895052909851\n",
      "Epoch 407: Train Loss: 0.35462459921836853, Validation Loss: 0.7556935548782349\n",
      "Epoch 408: Train Loss: 0.35756943225860593, Validation Loss: 0.780798077583313\n",
      "Epoch 409: Train Loss: 0.32003739178180696, Validation Loss: 0.7716495394706726\n",
      "Epoch 410: Train Loss: 0.3375847011804581, Validation Loss: 0.7621086239814758\n",
      "Epoch 411: Train Loss: 0.3909483253955841, Validation Loss: 0.7987342476844788\n",
      "Epoch 412: Train Loss: 0.3548306584358215, Validation Loss: 0.8196864128112793\n",
      "Epoch 413: Train Loss: 0.33108772337436676, Validation Loss: 0.8255767822265625\n",
      "Epoch 414: Train Loss: 0.345033860206604, Validation Loss: 0.8128118515014648\n",
      "Epoch 415: Train Loss: 0.3355670034885406, Validation Loss: 0.8050016164779663\n",
      "Epoch 416: Train Loss: 0.4041721910238266, Validation Loss: 0.787084698677063\n",
      "Epoch 417: Train Loss: 0.3908317148685455, Validation Loss: 0.7886936068534851\n",
      "Epoch 418: Train Loss: 0.3754846751689911, Validation Loss: 0.8067641258239746\n",
      "Epoch 419: Train Loss: 0.36362571716308595, Validation Loss: 0.7726268172264099\n",
      "Epoch 420: Train Loss: 0.3288993835449219, Validation Loss: 0.7626670598983765\n",
      "Epoch 421: Train Loss: 0.2930471509695053, Validation Loss: 0.7502650618553162\n",
      "Epoch 422: Train Loss: 0.3365225404500961, Validation Loss: 0.7414628863334656\n",
      "Epoch 423: Train Loss: 0.3848341703414917, Validation Loss: 0.7683088183403015\n",
      "Epoch 424: Train Loss: 0.28971087485551833, Validation Loss: 0.7793785929679871\n",
      "Epoch 425: Train Loss: 0.34466797709465025, Validation Loss: 0.7567338943481445\n",
      "Epoch 426: Train Loss: 0.3316637068986893, Validation Loss: 0.7274085879325867\n",
      "Epoch 427: Train Loss: 0.45423394441604614, Validation Loss: 0.7101696729660034\n",
      "Epoch 428: Train Loss: 0.30104937255382536, Validation Loss: 0.7114687561988831\n",
      "Epoch 429: Train Loss: 0.3561791777610779, Validation Loss: 0.7092032432556152\n",
      "Epoch 430: Train Loss: 0.38280529975891114, Validation Loss: 0.6826977133750916\n",
      "Epoch 431: Train Loss: 0.4273135721683502, Validation Loss: 0.6959519386291504\n",
      "Epoch 432: Train Loss: 0.4530372381210327, Validation Loss: 0.6896462440490723\n",
      "Epoch 433: Train Loss: 0.3582970201969147, Validation Loss: 0.7041594386100769\n",
      "Epoch 434: Train Loss: 0.30450248122215273, Validation Loss: 0.7172669172286987\n",
      "Epoch 435: Train Loss: 0.3741208702325821, Validation Loss: 0.7363313436508179\n",
      "Epoch 436: Train Loss: 0.44361635446548464, Validation Loss: 0.746068000793457\n",
      "Epoch 437: Train Loss: 0.3492594540119171, Validation Loss: 0.7421174645423889\n",
      "Epoch 438: Train Loss: 0.3061574697494507, Validation Loss: 0.7746440172195435\n",
      "Epoch 439: Train Loss: 0.2810597434639931, Validation Loss: 0.7796292901039124\n",
      "Epoch 440: Train Loss: 0.3715109348297119, Validation Loss: 0.7586247324943542\n",
      "Epoch 441: Train Loss: 0.3312482863664627, Validation Loss: 0.793779730796814\n",
      "Epoch 442: Train Loss: 0.36956722736358644, Validation Loss: 0.8156679272651672\n",
      "Epoch 443: Train Loss: 0.3074082314968109, Validation Loss: 0.8304134607315063\n",
      "Epoch 444: Train Loss: 0.40387240052223206, Validation Loss: 0.7989816069602966\n",
      "Epoch 445: Train Loss: 0.33403719663619996, Validation Loss: 0.8119201064109802\n",
      "Epoch 446: Train Loss: 0.34665109515190123, Validation Loss: 0.8329424858093262\n",
      "Epoch 447: Train Loss: 0.31287374496459963, Validation Loss: 0.8313033580780029\n",
      "Epoch 448: Train Loss: 0.29351497888565065, Validation Loss: 0.8005149364471436\n",
      "Epoch 449: Train Loss: 0.3591684639453888, Validation Loss: 0.8068733215332031\n",
      "Epoch 450: Train Loss: 0.2699320912361145, Validation Loss: 0.7665342092514038\n",
      "Epoch 451: Train Loss: 0.33835385739803314, Validation Loss: 0.7602083086967468\n",
      "Epoch 452: Train Loss: 0.3858941614627838, Validation Loss: 0.7445414662361145\n",
      "Epoch 453: Train Loss: 0.3743220567703247, Validation Loss: 0.7696133852005005\n",
      "Epoch 454: Train Loss: 0.29086715579032896, Validation Loss: 0.7635616064071655\n",
      "Epoch 455: Train Loss: 0.3436716556549072, Validation Loss: 0.8068059682846069\n",
      "Epoch 456: Train Loss: 0.3964155614376068, Validation Loss: 0.823604941368103\n",
      "Epoch 457: Train Loss: 0.32219829261302946, Validation Loss: 0.7950081825256348\n",
      "Epoch 458: Train Loss: 0.41055362224578856, Validation Loss: 0.8162779808044434\n",
      "Epoch 459: Train Loss: 0.3402899205684662, Validation Loss: 0.7985641360282898\n",
      "Epoch 460: Train Loss: 0.3064686477184296, Validation Loss: 0.7777045369148254\n",
      "Epoch 461: Train Loss: 0.3029474377632141, Validation Loss: 0.7983897924423218\n",
      "Epoch 462: Train Loss: 0.33849655389785765, Validation Loss: 0.7644605040550232\n",
      "Epoch 463: Train Loss: 0.2880625486373901, Validation Loss: 0.7776151299476624\n",
      "Epoch 464: Train Loss: 0.33774334192276, Validation Loss: 0.739294707775116\n",
      "Epoch 465: Train Loss: 0.3029677003622055, Validation Loss: 0.7243658304214478\n",
      "Epoch 466: Train Loss: 0.31244581937789917, Validation Loss: 0.7119086980819702\n",
      "Epoch 467: Train Loss: 0.3111090838909149, Validation Loss: 0.7055263519287109\n",
      "Epoch 468: Train Loss: 0.41966974437236787, Validation Loss: 0.7123443484306335\n",
      "Epoch 469: Train Loss: 0.29193754196166993, Validation Loss: 0.6957477331161499\n",
      "Epoch 470: Train Loss: 0.35226042866706847, Validation Loss: 0.7290952801704407\n",
      "Epoch 471: Train Loss: 0.33864589035511017, Validation Loss: 0.7253977060317993\n",
      "Epoch 472: Train Loss: 0.3293888926506042, Validation Loss: 0.7273496389389038\n",
      "Epoch 473: Train Loss: 0.2963409274816513, Validation Loss: 0.7264366149902344\n",
      "Epoch 474: Train Loss: 0.31756465435028075, Validation Loss: 0.7552301287651062\n",
      "Epoch 475: Train Loss: 0.2642371103167534, Validation Loss: 0.7483375072479248\n",
      "Epoch 476: Train Loss: 0.346550190448761, Validation Loss: 0.769060492515564\n",
      "Epoch 477: Train Loss: 0.3738949477672577, Validation Loss: 0.7499076128005981\n",
      "Epoch 478: Train Loss: 0.4017199158668518, Validation Loss: 0.7708327770233154\n",
      "Epoch 479: Train Loss: 0.31610681414604186, Validation Loss: 0.772571325302124\n",
      "Epoch 480: Train Loss: 0.3065251588821411, Validation Loss: 0.7949197292327881\n",
      "Epoch 481: Train Loss: 0.28231915533542634, Validation Loss: 0.7891082763671875\n",
      "Epoch 482: Train Loss: 0.29890399873256684, Validation Loss: 0.7975322604179382\n",
      "Epoch 483: Train Loss: 0.37003540992736816, Validation Loss: 0.8088787198066711\n",
      "Epoch 484: Train Loss: 0.26405429244041445, Validation Loss: 0.7519176602363586\n",
      "Epoch 485: Train Loss: 0.26673224866390227, Validation Loss: 0.7502091526985168\n",
      "Epoch 486: Train Loss: 0.32827929854393006, Validation Loss: 0.7687721252441406\n",
      "Epoch 487: Train Loss: 0.39887412190437316, Validation Loss: 0.7607915997505188\n",
      "Epoch 488: Train Loss: 0.2912147879600525, Validation Loss: 0.7550120949745178\n",
      "Epoch 489: Train Loss: 0.29342124462127683, Validation Loss: 0.7458136677742004\n",
      "Epoch 490: Train Loss: 0.3601663261651993, Validation Loss: 0.7547040581703186\n",
      "Epoch 491: Train Loss: 0.2855933904647827, Validation Loss: 0.7577055096626282\n",
      "Epoch 492: Train Loss: 0.42239442467689514, Validation Loss: 0.7729791402816772\n",
      "Epoch 493: Train Loss: 0.38843345642089844, Validation Loss: 0.7605358958244324\n",
      "Epoch 494: Train Loss: 0.2721038118004799, Validation Loss: 0.8152252435684204\n",
      "Epoch 495: Train Loss: 0.34309597611427306, Validation Loss: 0.8287782669067383\n",
      "Epoch 496: Train Loss: 0.3283134549856186, Validation Loss: 0.8011762499809265\n",
      "Epoch 497: Train Loss: 0.37083660066127777, Validation Loss: 0.7808539271354675\n",
      "Epoch 498: Train Loss: 0.2814986139535904, Validation Loss: 0.8257609605789185\n",
      "Epoch 499: Train Loss: 0.3733208209276199, Validation Loss: 0.8007283210754395\n",
      "Fold 13 Metrics:\n",
      "Accuracy: 0.6363636363636364, Precision: 0.625, Recall: 0.8333333333333334, F1-score: 0.7142857142857143, AUC: 0.6166666666666667\n",
      "Confusion Matrix:\n",
      "[[2 3]\n",
      " [1 5]]\n",
      "Completed fold 13\n",
      "--------------------------------------------------\n",
      "Overall Metrics:\n",
      "Accuracy: 0.34265734265734266, Precision: 0.38571428571428573, Recall: 0.34615384615384615, F1-score: 0.36486486486486486, AUC: 0.3423076923076923\n",
      "Overall Confusion Matrix:\n",
      "[[22 43]\n",
      " [51 27]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "model_save_dir = r'C:\\Users\\User\\Documents\\Lie detect data\\Model'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion Matrix'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='viridis', xticklabels=['lie', 'truth'], yticklabels=['lie', 'truth'])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.show()\n",
    "\n",
    "# Define a function to load and pad data\n",
    "def load_data(data_dir, max_length):\n",
    "    subject_data = {'lie': {}, 'truth': {}}\n",
    "    \n",
    "    file_list = os.listdir(data_dir)\n",
    "    \n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Determine if the file is 'truth' or 'lie'\n",
    "        label_type = 'truth' if 'truth' in file else 'lie'\n",
    "        subj_id = int(file.split('_')[-1].split('.')[0])\n",
    "        \n",
    "        # Grouping logic\n",
    "        if label_type == 'lie':\n",
    "            # Mapping each 5 lie samples to one subject\n",
    "            subject_key = (subj_id - 1) // 5 + 1\n",
    "        else:  # 'truth'\n",
    "            # Mapping each 6 truth samples to one subject\n",
    "            subject_key = (subj_id - 1) // 6 + 1\n",
    "            \n",
    "        # Initialize the subject's list if it doesn't exist\n",
    "        if subject_key not in subject_data[label_type]:\n",
    "            subject_data[label_type][subject_key] = []\n",
    "            \n",
    "        # Extract only the portion of the data from time length 3000 to 3750\n",
    "        data_subset = data[:, 0:250]\n",
    "\n",
    "        # Pad or truncate the data to match max_length\n",
    "        if data_subset.shape[1] > max_length:\n",
    "            processed_data = data_subset[:, :max_length]  # Truncate if it exceeds max_length\n",
    "        else:\n",
    "            processed_data = np.zeros((data_subset.shape[0], max_length))\n",
    "            processed_data[:, :data_subset.shape[1]] = data_subset  # Pad if it is shorter than max_length\n",
    "        \n",
    "        # Add the processed data to the appropriate list\n",
    "        subject_data[label_type][subject_key].append(processed_data)\n",
    "    \n",
    "    return subject_data\n",
    "\n",
    "# Load dataset and pad/truncate the data\n",
    "data_dir = \"C:\\\\Users\\\\User\\\\Documents\\\\Lie detect data\\\\7M_EEGData\"\n",
    "max_length = 250 # Define maximum length for padding\n",
    "subject_data = load_data(data_dir, max_length)\n",
    "\n",
    "# Count the total number of samples\n",
    "num_lie_samples = sum(len(subject_data['lie'][subject_key]) for subject_key in subject_data['lie'])\n",
    "num_truth_samples = sum(len(subject_data['truth'][subject_key]) for subject_key in subject_data['truth'])\n",
    "\n",
    "print(f\"Number of 'lie' samples: {num_lie_samples}\")\n",
    "print(f\"Number of 'truth' samples: {num_truth_samples}\")\n",
    "print(f\"Total number of samples: {num_lie_samples + num_truth_samples}\")\n",
    "\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure the data is reshaped to [1, Chans, Samples] for Conv2d input\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32).unsqueeze(0), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "nb_classes = 2\n",
    "Chans = 65\n",
    "Samples = 250\n",
    "dropoutRate = 0.5 \n",
    "kernLength = 125\n",
    "F1 = 16\n",
    "D = 2\n",
    "F2 = 32\n",
    "\n",
    "# New EEGNet Model Definition in PyTorch\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, nb_classes, Chans=65, Samples=3750,\n",
    "                 dropoutRate=0.6, kernLength=125, F1=8, \n",
    "                 D=2, F2=16, norm_rate=0.25, dropoutType='Dropout'):\n",
    "        super(EEGNet, self).__init__()\n",
    "\n",
    "        if dropoutType == 'SpatialDropout2D':\n",
    "            dropoutLayer = nn.Dropout2d\n",
    "        elif dropoutType == 'Dropout':\n",
    "            dropoutLayer = nn.Dropout\n",
    "        else:\n",
    "            raise ValueError('dropoutType must be one of \"SpatialDropout2D\" or \"Dropout\".')\n",
    "\n",
    "        # Block 1\n",
    "        self.conv1 = nn.Conv2d(1, F1, (1, kernLength), padding='same', bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "        \n",
    "        self.depthwiseConv = nn.Conv2d(F1, F1 * D, (Chans, 1), \n",
    "                                       groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(F1 * D)\n",
    "        self.elu = nn.ELU()\n",
    "        self.avgpool1 = nn.AvgPool2d((1, 4))\n",
    "        self.dropout1 = dropoutLayer(dropoutRate)\n",
    "\n",
    "        # Block 2\n",
    "        self.separableConv = nn.Sequential(\n",
    "            nn.Conv2d(F1 * D, F1 * D, (1, 16), padding='same', groups=F1 * D, bias=False),\n",
    "            nn.Conv2d(F1 * D, F2, (1, 1), bias=False)\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(F2)\n",
    "        self.avgpool2 = nn.AvgPool2d((1, 8))\n",
    "        self.dropout2 = dropoutLayer(dropoutRate)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(F2 * (Samples // (4 * 8)), nb_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input block\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.depthwiseConv(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.avgpool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Block 2\n",
    "        x = self.separableConv(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.avgpool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Flatten before the fully connected layer\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        # Fully connected output\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_and_evaluate(train_loader, val_loader, y_train):\n",
    "    # Initialize the updated EEGNet model\n",
    "    model = EEGNet(nb_classes=nb_classes, Chans=Chans, Samples=Samples,\n",
    "                   dropoutRate=dropoutRate, kernLength=kernLength, F1=F1, D=D, F2=F2, norm_rate=0.25, dropoutType='Dropout').to(device)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "    num_epochs = 500\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 500\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "            fold_model_path = os.path.join(model_save_dir, f'revise_model_fold_{fold_idx}.pth')\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': best_val_loss,\n",
    "            }, fold_model_path)\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize the cross-validation\n",
    "subject_ids = list(range(1, 14))  # Since there are 13 subjects for lie and truth\n",
    "random.shuffle(subject_ids)  # Shuffle to ensure random selection\n",
    "\n",
    "# Initialize arrays to store all labels, predictions, and metrics\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "fold_accuracies = []\n",
    "fold_precisions = []\n",
    "fold_recalls = []\n",
    "fold_f1_scores = []\n",
    "fold_aucs = []\n",
    "fold_conf_matrices = []\n",
    "\n",
    "for fold_idx in range(13):\n",
    "    # Split subjects into test and training sets\n",
    "    test_subject = subject_ids[fold_idx]\n",
    "\n",
    "    # Prepare training and test data\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for subject_id in subject_ids:\n",
    "        lie_samples = subject_data['lie'].get(subject_id, [])\n",
    "        truth_samples = subject_data['truth'].get(subject_id, [])\n",
    "\n",
    "        if subject_id == test_subject:\n",
    "            X_test.extend(lie_samples)\n",
    "            y_test.extend([0] * len(lie_samples))\n",
    "            print(f\"Adding {len(lie_samples)} lie samples from subject {subject_id} to test set\")\n",
    "            X_test.extend(truth_samples)\n",
    "            y_test.extend([1] * len(truth_samples))\n",
    "            print(f\"Adding {len(truth_samples)} truth samples from subject {subject_id} to test set\")\n",
    "        else:\n",
    "            X_train.extend(lie_samples)\n",
    "            y_train.extend([0] * len(lie_samples))\n",
    "            X_train.extend(truth_samples)\n",
    "            y_train.extend([1] * len(truth_samples))\n",
    "            print(f\"Adding {len(lie_samples)} lie samples and {len(truth_samples)} truth samples from subject {subject_id} to train set\")\n",
    "\n",
    "    print(f\"Total training samples: {len(X_train)}, Total test samples: {len(X_test)}\")\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    # Normalize data using scaler fitted on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_test = scaler.transform(X_test.reshape(X_test.shape[0], -1))\n",
    "    X_train = X_train.reshape(-1, 65, max_length)\n",
    "    X_test = X_test.reshape(-1, 65, max_length)\n",
    "\n",
    "    print(X_test.shape)\n",
    "    print(X_train.shape)\n",
    "\n",
    "    # Save the scaler to a file\n",
    "    with open(r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\RevisedEEGNet_scaler_{}.pkl'.format(fold_idx), 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = EEGDataset(X_train, y_train)\n",
    "    test_dataset = EEGDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    model = train_and_evaluate(train_loader, test_loader, y_train)\n",
    "\n",
    "    # Evaluate on the test set and calculate metrics\n",
    "    model.eval()\n",
    "    fold_labels = []\n",
    "    fold_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            fold_labels.extend(y_batch.cpu().numpy())\n",
    "            fold_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics for the current fold\n",
    "    accuracy = accuracy_score(fold_labels, fold_predictions)\n",
    "    precision = precision_score(fold_labels, fold_predictions)\n",
    "    recall = recall_score(fold_labels, fold_predictions)\n",
    "    f1 = f1_score(fold_labels, fold_predictions)\n",
    "    auc = roc_auc_score(fold_labels, fold_predictions)\n",
    "    conf_matrix = confusion_matrix(fold_labels, fold_predictions)\n",
    "\n",
    "    # Store fold metrics\n",
    "    fold_accuracies.append(accuracy)\n",
    "    fold_precisions.append(precision)\n",
    "    fold_recalls.append(recall)\n",
    "    fold_f1_scores.append(f1)\n",
    "    fold_aucs.append(auc)\n",
    "    fold_conf_matrices.append(conf_matrix)\n",
    "\n",
    "    # Print metrics for the current fold\n",
    "    print(f'Fold {fold_idx + 1} Metrics:')\n",
    "    print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Aggregate all labels and predictions for final evaluation across all folds\n",
    "    all_labels.extend(fold_labels)\n",
    "    all_predictions.extend(fold_predictions)\n",
    "\n",
    "    print(f'Completed fold {fold_idx + 1}')\n",
    "    print('-' * 50)\n",
    "\n",
    "# Calculate overall metrics across all folds\n",
    "overall_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "overall_precision = precision_score(all_labels, all_predictions)\n",
    "overall_recall = recall_score(all_labels, all_predictions)\n",
    "overall_f1 = f1_score(all_labels, all_predictions)\n",
    "overall_auc = roc_auc_score(all_labels, all_predictions)\n",
    "overall_conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(f'Overall Metrics:')\n",
    "print(f'Accuracy: {overall_accuracy}, Precision: {overall_precision}, Recall: {overall_recall}, F1-score: {overall_f1}, AUC: {overall_auc}')\n",
    "print('Overall Confusion Matrix:')\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "plot_confusion_matrix(overall_conf_matrix, title='Overall Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae639325-1800-4763-ad37-b5931e4391c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
