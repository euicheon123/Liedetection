{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29d5ee57-cf0b-4330-985e-ada385863b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'lie' samples: 65\n",
      "Number of 'truth' samples: 78\n",
      "Total number of samples: 143\n",
      "Adding 5 lie samples from subject 8 to test set\n",
      "Adding 6 truth samples from subject 8 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.6704879879951477, Validation Loss: 0.6923238039016724\n",
      "Epoch 1: Train Loss: 0.6988991379737854, Validation Loss: 0.6908635497093201\n",
      "Epoch 2: Train Loss: 0.7063122391700745, Validation Loss: 0.6916767358779907\n",
      "Epoch 3: Train Loss: 0.7336490392684937, Validation Loss: 0.6932494640350342\n",
      "Epoch 4: Train Loss: 0.6635202169418335, Validation Loss: 0.6954030394554138\n",
      "Epoch 5: Train Loss: 0.7113707184791564, Validation Loss: 0.6970950365066528\n",
      "Epoch 6: Train Loss: 0.6426581501960754, Validation Loss: 0.6971617341041565\n",
      "Epoch 7: Train Loss: 0.7009898781776428, Validation Loss: 0.6970782279968262\n",
      "Epoch 8: Train Loss: 0.7059745550155639, Validation Loss: 0.6977638006210327\n",
      "Epoch 9: Train Loss: 0.6885253429412842, Validation Loss: 0.6979414224624634\n",
      "Epoch 10: Train Loss: 0.6738563656806946, Validation Loss: 0.6995817422866821\n",
      "Epoch 11: Train Loss: 0.6363312721252441, Validation Loss: 0.6985934972763062\n",
      "Epoch 12: Train Loss: 0.7175579786300659, Validation Loss: 0.6995148062705994\n",
      "Epoch 13: Train Loss: 0.6530625104904175, Validation Loss: 0.7023682594299316\n",
      "Epoch 14: Train Loss: 0.6311399579048157, Validation Loss: 0.7042457461357117\n",
      "Epoch 15: Train Loss: 0.6780844569206238, Validation Loss: 0.7051503658294678\n",
      "Epoch 16: Train Loss: 0.638473916053772, Validation Loss: 0.7056182622909546\n",
      "Epoch 17: Train Loss: 0.6693852305412292, Validation Loss: 0.7055346369743347\n",
      "Epoch 18: Train Loss: 0.6192681789398193, Validation Loss: 0.705418586730957\n",
      "Epoch 19: Train Loss: 0.6352730512619018, Validation Loss: 0.7056918144226074\n",
      "Epoch 20: Train Loss: 0.6326326727867126, Validation Loss: 0.7071915864944458\n",
      "Epoch 21: Train Loss: 0.6605752944946289, Validation Loss: 0.7074099779129028\n",
      "Epoch 22: Train Loss: 0.61324462890625, Validation Loss: 0.7044441103935242\n",
      "Epoch 23: Train Loss: 0.6250476121902466, Validation Loss: 0.7003691792488098\n",
      "Epoch 24: Train Loss: 0.6011161386966706, Validation Loss: 0.6975676417350769\n",
      "Epoch 25: Train Loss: 0.6246988415718079, Validation Loss: 0.6949123740196228\n",
      "Epoch 26: Train Loss: 0.5957360625267029, Validation Loss: 0.6923984289169312\n",
      "Epoch 27: Train Loss: 0.6680705428123475, Validation Loss: 0.6909403800964355\n",
      "Epoch 28: Train Loss: 0.650447428226471, Validation Loss: 0.6902109980583191\n",
      "Epoch 29: Train Loss: 0.633719801902771, Validation Loss: 0.690019965171814\n",
      "Epoch 30: Train Loss: 0.6515488505363465, Validation Loss: 0.6877131462097168\n",
      "Epoch 31: Train Loss: 0.6205071210861206, Validation Loss: 0.6889560222625732\n",
      "Epoch 32: Train Loss: 0.6253091216087341, Validation Loss: 0.6900679469108582\n",
      "Epoch 33: Train Loss: 0.6133757710456849, Validation Loss: 0.6919213533401489\n",
      "Epoch 34: Train Loss: 0.6117785692214965, Validation Loss: 0.6932328939437866\n",
      "Epoch 35: Train Loss: 0.5921796679496765, Validation Loss: 0.6950836181640625\n",
      "Epoch 36: Train Loss: 0.6341766238212585, Validation Loss: 0.6953138113021851\n",
      "Epoch 37: Train Loss: 0.6955584406852722, Validation Loss: 0.6958248019218445\n",
      "Epoch 38: Train Loss: 0.6218363642692566, Validation Loss: 0.6957117915153503\n",
      "Epoch 39: Train Loss: 0.6501330494880676, Validation Loss: 0.695534884929657\n",
      "Epoch 40: Train Loss: 0.6502130508422852, Validation Loss: 0.6899704337120056\n",
      "Epoch 41: Train Loss: 0.6257939457893371, Validation Loss: 0.6867415308952332\n",
      "Epoch 42: Train Loss: 0.6490246057510376, Validation Loss: 0.6870896220207214\n",
      "Epoch 43: Train Loss: 0.6181761980056762, Validation Loss: 0.6855548024177551\n",
      "Epoch 44: Train Loss: 0.5822421669960022, Validation Loss: 0.684454619884491\n",
      "Epoch 45: Train Loss: 0.6131060242652893, Validation Loss: 0.6831560134887695\n",
      "Epoch 46: Train Loss: 0.593682062625885, Validation Loss: 0.6830177307128906\n",
      "Epoch 47: Train Loss: 0.5843465566635132, Validation Loss: 0.6827787160873413\n",
      "Epoch 48: Train Loss: 0.5806589961051941, Validation Loss: 0.682816207408905\n",
      "Epoch 49: Train Loss: 0.559168916940689, Validation Loss: 0.6824177503585815\n",
      "Epoch 50: Train Loss: 0.6267826080322265, Validation Loss: 0.6832218170166016\n",
      "Epoch 51: Train Loss: 0.5933914065361023, Validation Loss: 0.6846613883972168\n",
      "Epoch 52: Train Loss: 0.5883516788482666, Validation Loss: 0.6849790215492249\n",
      "Epoch 53: Train Loss: 0.5603399276733398, Validation Loss: 0.6842703223228455\n",
      "Epoch 54: Train Loss: 0.597772216796875, Validation Loss: 0.6844032406806946\n",
      "Epoch 55: Train Loss: 0.5684839725494385, Validation Loss: 0.6847916841506958\n",
      "Epoch 56: Train Loss: 0.5567181229591369, Validation Loss: 0.6858171224594116\n",
      "Epoch 57: Train Loss: 0.5429048418998719, Validation Loss: 0.6867310404777527\n",
      "Epoch 58: Train Loss: 0.5748928904533386, Validation Loss: 0.6878020763397217\n",
      "Epoch 59: Train Loss: 0.5999958157539368, Validation Loss: 0.6882901191711426\n",
      "Epoch 60: Train Loss: 0.5830070734024048, Validation Loss: 0.6894293427467346\n",
      "Epoch 61: Train Loss: 0.5239444136619568, Validation Loss: 0.686464786529541\n",
      "Epoch 62: Train Loss: 0.5554072737693787, Validation Loss: 0.6852215528488159\n",
      "Epoch 63: Train Loss: 0.5750094056129456, Validation Loss: 0.6877486109733582\n",
      "Epoch 64: Train Loss: 0.5621267557144165, Validation Loss: 0.6871341466903687\n",
      "Epoch 65: Train Loss: 0.56717569231987, Validation Loss: 0.6875676512718201\n",
      "Epoch 66: Train Loss: 0.5107032001018524, Validation Loss: 0.6868305206298828\n",
      "Epoch 67: Train Loss: 0.5697728991508484, Validation Loss: 0.6859719157218933\n",
      "Epoch 68: Train Loss: 0.5917176961898803, Validation Loss: 0.6863574981689453\n",
      "Epoch 69: Train Loss: 0.5752318620681762, Validation Loss: 0.6858114004135132\n",
      "Epoch 70: Train Loss: 0.5533569931983948, Validation Loss: 0.684954047203064\n",
      "Epoch 71: Train Loss: 0.526805305480957, Validation Loss: 0.6848723888397217\n",
      "Epoch 72: Train Loss: 0.4995602309703827, Validation Loss: 0.6845452189445496\n",
      "Epoch 73: Train Loss: 0.537626999616623, Validation Loss: 0.684065043926239\n",
      "Epoch 74: Train Loss: 0.550828367471695, Validation Loss: 0.6860122680664062\n",
      "Epoch 75: Train Loss: 0.4834989011287689, Validation Loss: 0.6912022829055786\n",
      "Epoch 76: Train Loss: 0.5529056549072265, Validation Loss: 0.6935761570930481\n",
      "Epoch 77: Train Loss: 0.537949937582016, Validation Loss: 0.6944942474365234\n",
      "Epoch 78: Train Loss: 0.5154374957084655, Validation Loss: 0.6951568722724915\n",
      "Epoch 79: Train Loss: 0.5409953176975251, Validation Loss: 0.6952206492424011\n",
      "Epoch 80: Train Loss: 0.5129099607467651, Validation Loss: 0.697283148765564\n",
      "Epoch 81: Train Loss: 0.5068276584148407, Validation Loss: 0.7017305493354797\n",
      "Epoch 82: Train Loss: 0.48956777453422545, Validation Loss: 0.7031748294830322\n",
      "Epoch 83: Train Loss: 0.5033652722835541, Validation Loss: 0.7017456293106079\n",
      "Epoch 84: Train Loss: 0.4933119833469391, Validation Loss: 0.7008522748947144\n",
      "Epoch 85: Train Loss: 0.5204908609390259, Validation Loss: 0.7006996273994446\n",
      "Epoch 86: Train Loss: 0.49913437366485597, Validation Loss: 0.701043963432312\n",
      "Epoch 87: Train Loss: 0.526131808757782, Validation Loss: 0.7015497088432312\n",
      "Epoch 88: Train Loss: 0.5111696600914002, Validation Loss: 0.7016915678977966\n",
      "Epoch 89: Train Loss: 0.49907190799713136, Validation Loss: 0.7007002830505371\n",
      "Epoch 90: Train Loss: 0.5231903195381165, Validation Loss: 0.69384765625\n",
      "Epoch 91: Train Loss: 0.5039252638816833, Validation Loss: 0.6913708448410034\n",
      "Epoch 92: Train Loss: 0.5924558639526367, Validation Loss: 0.6895648837089539\n",
      "Epoch 93: Train Loss: 0.48843584060668943, Validation Loss: 0.6858111023902893\n",
      "Epoch 94: Train Loss: 0.5006847560405732, Validation Loss: 0.6889712810516357\n",
      "Epoch 95: Train Loss: 0.48230551481246947, Validation Loss: 0.6947243213653564\n",
      "Epoch 96: Train Loss: 0.47362774014472964, Validation Loss: 0.6971105337142944\n",
      "Epoch 97: Train Loss: 0.4505283832550049, Validation Loss: 0.6976058483123779\n",
      "Epoch 98: Train Loss: 0.48350260853767396, Validation Loss: 0.6982921361923218\n",
      "Epoch 99: Train Loss: 0.48644608855247495, Validation Loss: 0.6975045204162598\n",
      "Epoch 100: Train Loss: 0.47164124846458433, Validation Loss: 0.7000597715377808\n",
      "Epoch 101: Train Loss: 0.47055466175079347, Validation Loss: 0.6931891441345215\n",
      "Epoch 102: Train Loss: 0.5271964013576508, Validation Loss: 0.6889691352844238\n",
      "Epoch 103: Train Loss: 0.46159854531288147, Validation Loss: 0.6881592273712158\n",
      "Epoch 104: Train Loss: 0.5107925117015839, Validation Loss: 0.6917077898979187\n",
      "Epoch 105: Train Loss: 0.46979894042015075, Validation Loss: 0.6988048553466797\n",
      "Epoch 106: Train Loss: 0.5357172966003418, Validation Loss: 0.7033868432044983\n",
      "Epoch 107: Train Loss: 0.46823859214782715, Validation Loss: 0.7071196436882019\n",
      "Epoch 108: Train Loss: 0.4763938248157501, Validation Loss: 0.7087966203689575\n",
      "Epoch 109: Train Loss: 0.5415441155433655, Validation Loss: 0.7091732025146484\n",
      "Epoch 110: Train Loss: 0.44585877656936646, Validation Loss: 0.7229647636413574\n",
      "Epoch 111: Train Loss: 0.4409309148788452, Validation Loss: 0.7253079414367676\n",
      "Epoch 112: Train Loss: 0.49408780932426455, Validation Loss: 0.7160953879356384\n",
      "Epoch 113: Train Loss: 0.4691899538040161, Validation Loss: 0.712264895439148\n",
      "Epoch 114: Train Loss: 0.45088682174682615, Validation Loss: 0.7041105628013611\n",
      "Epoch 115: Train Loss: 0.41809635162353515, Validation Loss: 0.6977444887161255\n",
      "Epoch 116: Train Loss: 0.45305031538009644, Validation Loss: 0.6937085390090942\n",
      "Epoch 117: Train Loss: 0.5123659610748291, Validation Loss: 0.6925579905509949\n",
      "Epoch 118: Train Loss: 0.4316452622413635, Validation Loss: 0.6924678683280945\n",
      "Epoch 119: Train Loss: 0.4474714040756226, Validation Loss: 0.6919474601745605\n",
      "Epoch 120: Train Loss: 0.42069955468177794, Validation Loss: 0.7029581069946289\n",
      "Epoch 121: Train Loss: 0.4497590482234955, Validation Loss: 0.7152048945426941\n",
      "Epoch 122: Train Loss: 0.3983603537082672, Validation Loss: 0.7261685729026794\n",
      "Epoch 123: Train Loss: 0.43259336352348327, Validation Loss: 0.7281096577644348\n",
      "Epoch 124: Train Loss: 0.4267335534095764, Validation Loss: 0.7262051701545715\n",
      "Epoch 125: Train Loss: 0.43104919195175173, Validation Loss: 0.7266677021980286\n",
      "Epoch 126: Train Loss: 0.43301552534103394, Validation Loss: 0.727008044719696\n",
      "Epoch 127: Train Loss: 0.4155368983745575, Validation Loss: 0.7262385487556458\n",
      "Epoch 128: Train Loss: 0.396675306558609, Validation Loss: 0.7267524600028992\n",
      "Epoch 129: Train Loss: 0.4019283473491669, Validation Loss: 0.7272056937217712\n",
      "Epoch 130: Train Loss: 0.45700701475143435, Validation Loss: 0.7234554290771484\n",
      "Epoch 131: Train Loss: 0.4051993012428284, Validation Loss: 0.717722475528717\n",
      "Epoch 132: Train Loss: 0.39433143138885496, Validation Loss: 0.7147228717803955\n",
      "Epoch 133: Train Loss: 0.3988233804702759, Validation Loss: 0.7268876433372498\n",
      "Epoch 134: Train Loss: 0.39078163504600527, Validation Loss: 0.7354328036308289\n",
      "Epoch 135: Train Loss: 0.42684701085090637, Validation Loss: 0.7359765768051147\n",
      "Epoch 136: Train Loss: 0.36480114459991453, Validation Loss: 0.7350196242332458\n",
      "Epoch 137: Train Loss: 0.3837288379669189, Validation Loss: 0.7357398867607117\n",
      "Epoch 138: Train Loss: 0.41144168972969053, Validation Loss: 0.7394654154777527\n",
      "Epoch 139: Train Loss: 0.3452130973339081, Validation Loss: 0.7403292655944824\n",
      "Epoch 140: Train Loss: 0.40011698603630064, Validation Loss: 0.746160089969635\n",
      "Epoch 141: Train Loss: 0.381392115354538, Validation Loss: 0.7471538186073303\n",
      "Epoch 142: Train Loss: 0.332613542675972, Validation Loss: 0.7526852488517761\n",
      "Epoch 143: Train Loss: 0.37268235683441164, Validation Loss: 0.7469446063041687\n",
      "Epoch 144: Train Loss: 0.36028826236724854, Validation Loss: 0.7453952431678772\n",
      "Epoch 145: Train Loss: 0.3815709352493286, Validation Loss: 0.7476039528846741\n",
      "Epoch 146: Train Loss: 0.3455562174320221, Validation Loss: 0.7454895973205566\n",
      "Epoch 147: Train Loss: 0.3864104926586151, Validation Loss: 0.7471656799316406\n",
      "Epoch 148: Train Loss: 0.3682256877422333, Validation Loss: 0.7453652620315552\n",
      "Epoch 149: Train Loss: 0.32554093599319456, Validation Loss: 0.7454600930213928\n",
      "Epoch 150: Train Loss: 0.33801870346069335, Validation Loss: 0.7309873104095459\n",
      "Epoch 151: Train Loss: 0.35862157344818113, Validation Loss: 0.721145749092102\n",
      "Epoch 152: Train Loss: 0.3433480322360992, Validation Loss: 0.701208770275116\n",
      "Epoch 153: Train Loss: 0.37672561407089233, Validation Loss: 0.6832805871963501\n",
      "Epoch 154: Train Loss: 0.3078491657972336, Validation Loss: 0.6753875613212585\n",
      "Epoch 155: Train Loss: 0.31041695475578307, Validation Loss: 0.6660363078117371\n",
      "Epoch 156: Train Loss: 0.28213498890399935, Validation Loss: 0.6582290530204773\n",
      "Epoch 157: Train Loss: 0.30107463896274567, Validation Loss: 0.6560785174369812\n",
      "Epoch 158: Train Loss: 0.364366614818573, Validation Loss: 0.6544525027275085\n",
      "Epoch 159: Train Loss: 0.3550221860408783, Validation Loss: 0.6515726447105408\n",
      "Epoch 160: Train Loss: 0.3381133019924164, Validation Loss: 0.6531318426132202\n",
      "Epoch 161: Train Loss: 0.304198169708252, Validation Loss: 0.6364567875862122\n",
      "Epoch 162: Train Loss: 0.2981050729751587, Validation Loss: 0.6128925085067749\n",
      "Epoch 163: Train Loss: 0.3203171670436859, Validation Loss: 0.6016115546226501\n",
      "Epoch 164: Train Loss: 0.39790067076683044, Validation Loss: 0.5816291570663452\n",
      "Epoch 165: Train Loss: 0.2821721464395523, Validation Loss: 0.5890207886695862\n",
      "Epoch 166: Train Loss: 0.2980504721403122, Validation Loss: 0.5792403221130371\n",
      "Epoch 167: Train Loss: 0.2788391500711441, Validation Loss: 0.5845374464988708\n",
      "Epoch 168: Train Loss: 0.35421229600906373, Validation Loss: 0.5889705419540405\n",
      "Epoch 169: Train Loss: 0.2924387574195862, Validation Loss: 0.5928936004638672\n",
      "Epoch 170: Train Loss: 0.32734320163726804, Validation Loss: 0.5616245865821838\n",
      "Epoch 171: Train Loss: 0.3043725937604904, Validation Loss: 0.5479596853256226\n",
      "Epoch 172: Train Loss: 0.2986817300319672, Validation Loss: 0.54433673620224\n",
      "Epoch 173: Train Loss: 0.27098634243011477, Validation Loss: 0.5440593957901001\n",
      "Epoch 174: Train Loss: 0.24075301885604858, Validation Loss: 0.5270577669143677\n",
      "Epoch 175: Train Loss: 0.2675905406475067, Validation Loss: 0.5212824940681458\n",
      "Epoch 176: Train Loss: 0.23505191504955292, Validation Loss: 0.5174129009246826\n",
      "Epoch 177: Train Loss: 0.33510938584804534, Validation Loss: 0.5302395820617676\n",
      "Epoch 178: Train Loss: 0.2924353778362274, Validation Loss: 0.5209608674049377\n",
      "Epoch 179: Train Loss: 0.25131724774837494, Validation Loss: 0.5185972452163696\n",
      "Epoch 180: Train Loss: 0.24439761340618132, Validation Loss: 0.535816490650177\n",
      "Epoch 181: Train Loss: 0.2725474119186401, Validation Loss: 0.5111076831817627\n",
      "Epoch 182: Train Loss: 0.33850400149822235, Validation Loss: 0.5157648921012878\n",
      "Epoch 183: Train Loss: 0.255943301320076, Validation Loss: 0.529349148273468\n",
      "Epoch 184: Train Loss: 0.2775456875562668, Validation Loss: 0.5072376728057861\n",
      "Epoch 185: Train Loss: 0.22127465307712554, Validation Loss: 0.5135155320167542\n",
      "Epoch 186: Train Loss: 0.2150120809674263, Validation Loss: 0.5237869620323181\n",
      "Epoch 187: Train Loss: 0.26120794713497164, Validation Loss: 0.5269906520843506\n",
      "Epoch 188: Train Loss: 0.23212502002716065, Validation Loss: 0.5421274304389954\n",
      "Epoch 189: Train Loss: 0.2086589366197586, Validation Loss: 0.5323529243469238\n",
      "Epoch 190: Train Loss: 0.23272871673107148, Validation Loss: 0.5394057035446167\n",
      "Epoch 191: Train Loss: 0.24738073945045472, Validation Loss: 0.5453003644943237\n",
      "Epoch 192: Train Loss: 0.3026571810245514, Validation Loss: 0.5246807932853699\n",
      "Epoch 193: Train Loss: 0.2759792059659958, Validation Loss: 0.49555256962776184\n",
      "Epoch 194: Train Loss: 0.25122388601303103, Validation Loss: 0.5028321743011475\n",
      "Epoch 195: Train Loss: 0.23764659762382506, Validation Loss: 0.531504213809967\n",
      "Epoch 196: Train Loss: 0.27185567617416384, Validation Loss: 0.5379737019538879\n",
      "Epoch 197: Train Loss: 0.24160733819007874, Validation Loss: 0.5237438678741455\n",
      "Epoch 198: Train Loss: 0.24227689802646638, Validation Loss: 0.5147565007209778\n",
      "Epoch 199: Train Loss: 0.17835230231285096, Validation Loss: 0.5259386897087097\n",
      "Fold 1 Metrics:\n",
      "Accuracy: 0.7272727272727273, Precision: 0.6666666666666666, Recall: 1.0, F1-score: 0.8, AUC: 0.7\n",
      "Confusion Matrix:\n",
      "[[2 3]\n",
      " [0 6]]\n",
      "Completed fold 1\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples from subject 7 to test set\n",
      "Adding 6 truth samples from subject 7 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.7094646215438842, Validation Loss: 0.7003235220909119\n",
      "Epoch 1: Train Loss: 0.666320514678955, Validation Loss: 0.7017221450805664\n",
      "Epoch 2: Train Loss: 0.7444764733314514, Validation Loss: 0.7021481990814209\n",
      "Epoch 3: Train Loss: 0.722546648979187, Validation Loss: 0.7010866403579712\n",
      "Epoch 4: Train Loss: 0.7019105195999146, Validation Loss: 0.6992189288139343\n",
      "Epoch 5: Train Loss: 0.6797573804855347, Validation Loss: 0.69759601354599\n",
      "Epoch 6: Train Loss: 0.6747408032417297, Validation Loss: 0.6958695650100708\n",
      "Epoch 7: Train Loss: 0.6582346320152282, Validation Loss: 0.6952396035194397\n",
      "Epoch 8: Train Loss: 0.7003551125526428, Validation Loss: 0.6948750019073486\n",
      "Epoch 9: Train Loss: 0.6862075567245484, Validation Loss: 0.6946781277656555\n",
      "Epoch 10: Train Loss: 0.7123326539993287, Validation Loss: 0.7000114321708679\n",
      "Epoch 11: Train Loss: 0.7324452638626099, Validation Loss: 0.7031603455543518\n",
      "Epoch 12: Train Loss: 0.6489988446235657, Validation Loss: 0.7028716206550598\n",
      "Epoch 13: Train Loss: 0.6734745144844055, Validation Loss: 0.7026612758636475\n",
      "Epoch 14: Train Loss: 0.6493127465248107, Validation Loss: 0.7005395293235779\n",
      "Epoch 15: Train Loss: 0.6480257153511048, Validation Loss: 0.6992974281311035\n",
      "Epoch 16: Train Loss: 0.6606072783470154, Validation Loss: 0.6989697217941284\n",
      "Epoch 17: Train Loss: 0.6850876927375793, Validation Loss: 0.6992095112800598\n",
      "Epoch 18: Train Loss: 0.6875380992889404, Validation Loss: 0.6990383863449097\n",
      "Epoch 19: Train Loss: 0.6424762845039368, Validation Loss: 0.6986157894134521\n",
      "Epoch 20: Train Loss: 0.6505755305290222, Validation Loss: 0.699470043182373\n",
      "Epoch 21: Train Loss: 0.6525393962860108, Validation Loss: 0.6979273557662964\n",
      "Epoch 22: Train Loss: 0.6582880973815918, Validation Loss: 0.6952921748161316\n",
      "Epoch 23: Train Loss: 0.6572362899780273, Validation Loss: 0.6925386190414429\n",
      "Epoch 24: Train Loss: 0.6935425996780396, Validation Loss: 0.6920943856239319\n",
      "Epoch 25: Train Loss: 0.6712936043739319, Validation Loss: 0.6921931505203247\n",
      "Epoch 26: Train Loss: 0.6363323926925659, Validation Loss: 0.6932892799377441\n",
      "Epoch 27: Train Loss: 0.6542914867401123, Validation Loss: 0.6939712166786194\n",
      "Epoch 28: Train Loss: 0.6508321046829224, Validation Loss: 0.6944720149040222\n",
      "Epoch 29: Train Loss: 0.6525695323944092, Validation Loss: 0.694065272808075\n",
      "Epoch 30: Train Loss: 0.6337726473808288, Validation Loss: 0.6975237727165222\n",
      "Epoch 31: Train Loss: 0.6056350350379944, Validation Loss: 0.6974735260009766\n",
      "Epoch 32: Train Loss: 0.6196168303489685, Validation Loss: 0.6965602040290833\n",
      "Epoch 33: Train Loss: 0.6368800044059754, Validation Loss: 0.6955947875976562\n",
      "Epoch 34: Train Loss: 0.6725593328475952, Validation Loss: 0.6971122026443481\n",
      "Epoch 35: Train Loss: 0.6287164688110352, Validation Loss: 0.6986902952194214\n",
      "Epoch 36: Train Loss: 0.6586546421051025, Validation Loss: 0.7004991173744202\n",
      "Epoch 37: Train Loss: 0.6442513585090637, Validation Loss: 0.7011770009994507\n",
      "Epoch 38: Train Loss: 0.6202751994132996, Validation Loss: 0.7011959552764893\n",
      "Epoch 39: Train Loss: 0.6424511671066284, Validation Loss: 0.7012880444526672\n",
      "Epoch 40: Train Loss: 0.6008592963218689, Validation Loss: 0.6990789175033569\n",
      "Epoch 41: Train Loss: 0.6306073904037476, Validation Loss: 0.6987396478652954\n",
      "Epoch 42: Train Loss: 0.6297309875488282, Validation Loss: 0.6995648145675659\n",
      "Epoch 43: Train Loss: 0.6077311277389527, Validation Loss: 0.6992723345756531\n",
      "Epoch 44: Train Loss: 0.6036165356636047, Validation Loss: 0.6986408233642578\n",
      "Epoch 45: Train Loss: 0.650635302066803, Validation Loss: 0.6995233297348022\n",
      "Epoch 46: Train Loss: 0.6306416273117066, Validation Loss: 0.6992930173873901\n",
      "Epoch 47: Train Loss: 0.6111742734909058, Validation Loss: 0.7008847594261169\n",
      "Epoch 48: Train Loss: 0.5959273219108582, Validation Loss: 0.7019577622413635\n",
      "Epoch 49: Train Loss: 0.6308293104171753, Validation Loss: 0.7020421028137207\n",
      "Epoch 50: Train Loss: 0.679236900806427, Validation Loss: 0.7051650285720825\n",
      "Epoch 51: Train Loss: 0.6153221607208252, Validation Loss: 0.7057980895042419\n",
      "Epoch 52: Train Loss: 0.6481837630271912, Validation Loss: 0.7002716660499573\n",
      "Epoch 53: Train Loss: 0.6021247267723083, Validation Loss: 0.6965453028678894\n",
      "Epoch 54: Train Loss: 0.6069362044334412, Validation Loss: 0.6971369981765747\n",
      "Epoch 55: Train Loss: 0.600394320487976, Validation Loss: 0.7004820108413696\n",
      "Epoch 56: Train Loss: 0.6399141907691955, Validation Loss: 0.7045009732246399\n",
      "Epoch 57: Train Loss: 0.6246508121490478, Validation Loss: 0.7056578397750854\n",
      "Epoch 58: Train Loss: 0.6280913472175598, Validation Loss: 0.7063030004501343\n",
      "Epoch 59: Train Loss: 0.5922280311584472, Validation Loss: 0.707097053527832\n",
      "Epoch 60: Train Loss: 0.6098355770111084, Validation Loss: 0.7093417644500732\n",
      "Epoch 61: Train Loss: 0.5611053347587586, Validation Loss: 0.712469220161438\n",
      "Epoch 62: Train Loss: 0.5568657755851746, Validation Loss: 0.7135393619537354\n",
      "Epoch 63: Train Loss: 0.5672488749027252, Validation Loss: 0.7129858732223511\n",
      "Epoch 64: Train Loss: 0.5462071418762207, Validation Loss: 0.7148353457450867\n",
      "Epoch 65: Train Loss: 0.5875808596611023, Validation Loss: 0.7131364941596985\n",
      "Epoch 66: Train Loss: 0.5361393749713897, Validation Loss: 0.712462306022644\n",
      "Epoch 67: Train Loss: 0.5826751470565796, Validation Loss: 0.7136696577072144\n",
      "Epoch 68: Train Loss: 0.5621213614940643, Validation Loss: 0.7139498591423035\n",
      "Epoch 69: Train Loss: 0.58199063539505, Validation Loss: 0.7152735590934753\n",
      "Epoch 70: Train Loss: 0.5986065030097961, Validation Loss: 0.7111070156097412\n",
      "Epoch 71: Train Loss: 0.5872049689292907, Validation Loss: 0.7103814482688904\n",
      "Epoch 72: Train Loss: 0.5844250917434692, Validation Loss: 0.7102307677268982\n",
      "Epoch 73: Train Loss: 0.5685616612434388, Validation Loss: 0.7071959972381592\n",
      "Epoch 74: Train Loss: 0.5354578018188476, Validation Loss: 0.7145249247550964\n",
      "Epoch 75: Train Loss: 0.5388012945652008, Validation Loss: 0.720848560333252\n",
      "Epoch 76: Train Loss: 0.4981186032295227, Validation Loss: 0.7248286604881287\n",
      "Epoch 77: Train Loss: 0.5409730076789856, Validation Loss: 0.726317286491394\n",
      "Epoch 78: Train Loss: 0.5171206295490265, Validation Loss: 0.7257778644561768\n",
      "Epoch 79: Train Loss: 0.5469607651233673, Validation Loss: 0.7271235585212708\n",
      "Epoch 80: Train Loss: 0.5424161553382874, Validation Loss: 0.7205562591552734\n",
      "Epoch 81: Train Loss: 0.5144444525241851, Validation Loss: 0.7181929349899292\n",
      "Epoch 82: Train Loss: 0.5042743623256684, Validation Loss: 0.7241222262382507\n",
      "Epoch 83: Train Loss: 0.498015946149826, Validation Loss: 0.7312585711479187\n",
      "Epoch 84: Train Loss: 0.4715664446353912, Validation Loss: 0.7344732284545898\n",
      "Epoch 85: Train Loss: 0.5735634148120881, Validation Loss: 0.7358260750770569\n",
      "Epoch 86: Train Loss: 0.5326479315757752, Validation Loss: 0.7406628727912903\n",
      "Epoch 87: Train Loss: 0.4777309954166412, Validation Loss: 0.7417605519294739\n",
      "Epoch 88: Train Loss: 0.5252488613128662, Validation Loss: 0.7405390739440918\n",
      "Epoch 89: Train Loss: 0.5053115904331207, Validation Loss: 0.7406682968139648\n",
      "Epoch 90: Train Loss: 0.5028720200061798, Validation Loss: 0.7052096128463745\n",
      "Epoch 91: Train Loss: 0.5053920269012451, Validation Loss: 0.7098064422607422\n",
      "Epoch 92: Train Loss: 0.46289061903953554, Validation Loss: 0.7255805134773254\n",
      "Epoch 93: Train Loss: 0.4650991797447205, Validation Loss: 0.7431546449661255\n",
      "Epoch 94: Train Loss: 0.4545537829399109, Validation Loss: 0.7600606083869934\n",
      "Epoch 95: Train Loss: 0.5178033530712127, Validation Loss: 0.7624688744544983\n",
      "Epoch 96: Train Loss: 0.4360850930213928, Validation Loss: 0.762636661529541\n",
      "Epoch 97: Train Loss: 0.4007739722728729, Validation Loss: 0.760469377040863\n",
      "Epoch 98: Train Loss: 0.4282565534114838, Validation Loss: 0.7541894316673279\n",
      "Epoch 99: Train Loss: 0.46780369281768797, Validation Loss: 0.7502570152282715\n",
      "Epoch 100: Train Loss: 0.4060029864311218, Validation Loss: 0.7494105696678162\n",
      "Epoch 101: Train Loss: 0.4099863708019257, Validation Loss: 0.7658791542053223\n",
      "Epoch 102: Train Loss: 0.4438146770000458, Validation Loss: 0.7749802470207214\n",
      "Epoch 103: Train Loss: 0.40555618405342103, Validation Loss: 0.7984885573387146\n",
      "Epoch 104: Train Loss: 0.41293309926986693, Validation Loss: 0.8155467510223389\n",
      "Epoch 105: Train Loss: 0.38983837366104124, Validation Loss: 0.8203098177909851\n",
      "Epoch 106: Train Loss: 0.3843835562467575, Validation Loss: 0.8237212300300598\n",
      "Epoch 107: Train Loss: 0.4196794331073761, Validation Loss: 0.820248007774353\n",
      "Epoch 108: Train Loss: 0.3911505460739136, Validation Loss: 0.8303212523460388\n",
      "Epoch 109: Train Loss: 0.3937170147895813, Validation Loss: 0.8363850712776184\n",
      "Epoch 110: Train Loss: 0.39982993006706236, Validation Loss: 0.815004289150238\n",
      "Epoch 111: Train Loss: 0.45277194380760194, Validation Loss: 0.8174859881401062\n",
      "Epoch 112: Train Loss: 0.3651604026556015, Validation Loss: 0.8560121655464172\n",
      "Epoch 113: Train Loss: 0.41425299644470215, Validation Loss: 0.8742390871047974\n",
      "Epoch 114: Train Loss: 0.40685338973999025, Validation Loss: 0.8664535880088806\n",
      "Epoch 115: Train Loss: 0.42049494981765745, Validation Loss: 0.8744798302650452\n",
      "Epoch 116: Train Loss: 0.34451245069503783, Validation Loss: 0.8781207799911499\n",
      "Epoch 117: Train Loss: 0.38537517189979553, Validation Loss: 0.8938243389129639\n",
      "Epoch 118: Train Loss: 0.36680943965911866, Validation Loss: 0.8939681649208069\n",
      "Epoch 119: Train Loss: 0.39316084384918215, Validation Loss: 0.9009815454483032\n",
      "Epoch 120: Train Loss: 0.4265767574310303, Validation Loss: 0.8997363448143005\n",
      "Epoch 121: Train Loss: 0.44068207740783694, Validation Loss: 0.9259796142578125\n",
      "Epoch 122: Train Loss: 0.34547847509384155, Validation Loss: 0.9056276082992554\n",
      "Epoch 123: Train Loss: 0.4677176296710968, Validation Loss: 0.9038570523262024\n",
      "Epoch 124: Train Loss: 0.3834374904632568, Validation Loss: 0.9066759347915649\n",
      "Epoch 125: Train Loss: 0.3500263214111328, Validation Loss: 0.9448180794715881\n",
      "Epoch 126: Train Loss: 0.3600910484790802, Validation Loss: 0.9575634002685547\n",
      "Epoch 127: Train Loss: 0.39988758563995364, Validation Loss: 0.9754517078399658\n",
      "Epoch 128: Train Loss: 0.32792348265647886, Validation Loss: 1.0103590488433838\n",
      "Epoch 129: Train Loss: 0.3697995662689209, Validation Loss: 0.9904142618179321\n",
      "Epoch 130: Train Loss: 0.33927907347679137, Validation Loss: 0.9129219055175781\n",
      "Epoch 131: Train Loss: 0.34970980882644653, Validation Loss: 0.9070844650268555\n",
      "Epoch 132: Train Loss: 0.35195719003677367, Validation Loss: 0.9053747057914734\n",
      "Epoch 133: Train Loss: 0.37435216307640073, Validation Loss: 0.9262674450874329\n",
      "Epoch 134: Train Loss: 0.28671998977661134, Validation Loss: 0.9499790668487549\n",
      "Epoch 135: Train Loss: 0.3145887732505798, Validation Loss: 0.9495895504951477\n",
      "Epoch 136: Train Loss: 0.2974422037601471, Validation Loss: 0.9387505054473877\n",
      "Epoch 137: Train Loss: 0.32044997215271, Validation Loss: 0.9257649779319763\n",
      "Epoch 138: Train Loss: 0.3675510585308075, Validation Loss: 0.9423972964286804\n",
      "Epoch 139: Train Loss: 0.3539706826210022, Validation Loss: 0.9375993013381958\n",
      "Epoch 140: Train Loss: 0.2810810476541519, Validation Loss: 0.9572336673736572\n",
      "Epoch 141: Train Loss: 0.30392338037490846, Validation Loss: 0.9312648177146912\n",
      "Epoch 142: Train Loss: 0.3053952544927597, Validation Loss: 0.9635288715362549\n",
      "Epoch 143: Train Loss: 0.28940437734127045, Validation Loss: 0.964447021484375\n",
      "Epoch 144: Train Loss: 0.2688790500164032, Validation Loss: 0.9579311013221741\n",
      "Epoch 145: Train Loss: 0.2850387066602707, Validation Loss: 0.9417726397514343\n",
      "Epoch 146: Train Loss: 0.3806059420108795, Validation Loss: 0.9717147946357727\n",
      "Epoch 147: Train Loss: 0.3076803207397461, Validation Loss: 0.9793500304222107\n",
      "Epoch 148: Train Loss: 0.2821280241012573, Validation Loss: 0.9956937432289124\n",
      "Epoch 149: Train Loss: 0.2524815261363983, Validation Loss: 0.9863884449005127\n",
      "Epoch 150: Train Loss: 0.2852608859539032, Validation Loss: 0.9801458120346069\n",
      "Epoch 151: Train Loss: 0.31320376992225646, Validation Loss: 0.999192476272583\n",
      "Epoch 152: Train Loss: 0.27210578322410583, Validation Loss: 1.0374270677566528\n",
      "Epoch 153: Train Loss: 0.33767217099666597, Validation Loss: 1.0658961534500122\n",
      "Epoch 154: Train Loss: 0.2476263791322708, Validation Loss: 1.0451892614364624\n",
      "Epoch 155: Train Loss: 0.22284343838691711, Validation Loss: 1.0675221681594849\n",
      "Epoch 156: Train Loss: 0.23478268682956696, Validation Loss: 1.0324331521987915\n",
      "Epoch 157: Train Loss: 0.24977368116378784, Validation Loss: 1.0297256708145142\n",
      "Epoch 158: Train Loss: 0.2259805142879486, Validation Loss: 1.047727108001709\n",
      "Epoch 159: Train Loss: 0.293663415312767, Validation Loss: 1.049756646156311\n",
      "Epoch 160: Train Loss: 0.259766486287117, Validation Loss: 1.080077886581421\n",
      "Epoch 161: Train Loss: 0.25540604889392854, Validation Loss: 1.11257803440094\n",
      "Epoch 162: Train Loss: 0.22801611423492432, Validation Loss: 1.1294270753860474\n",
      "Epoch 163: Train Loss: 0.2238341510295868, Validation Loss: 1.1090277433395386\n",
      "Epoch 164: Train Loss: 0.21843464374542237, Validation Loss: 1.0946190357208252\n",
      "Epoch 165: Train Loss: 0.19956290423870088, Validation Loss: 1.0978655815124512\n",
      "Epoch 166: Train Loss: 0.1990705281496048, Validation Loss: 1.1077369451522827\n",
      "Epoch 167: Train Loss: 0.194820436835289, Validation Loss: 1.1112021207809448\n",
      "Epoch 168: Train Loss: 0.24411314427852632, Validation Loss: 1.1697611808776855\n",
      "Epoch 169: Train Loss: 0.1881856292486191, Validation Loss: 1.1670970916748047\n",
      "Epoch 170: Train Loss: 0.2517514437437057, Validation Loss: 1.126471996307373\n",
      "Epoch 171: Train Loss: 0.2501176714897156, Validation Loss: 1.1347756385803223\n",
      "Epoch 172: Train Loss: 0.24535862803459169, Validation Loss: 1.146146535873413\n",
      "Epoch 173: Train Loss: 0.21683332324028015, Validation Loss: 1.115001916885376\n",
      "Epoch 174: Train Loss: 0.24412164986133575, Validation Loss: 1.0896912813186646\n",
      "Epoch 175: Train Loss: 0.2002801239490509, Validation Loss: 1.106182336807251\n",
      "Epoch 176: Train Loss: 0.17630651146173476, Validation Loss: 1.1254427433013916\n",
      "Epoch 177: Train Loss: 0.2054218053817749, Validation Loss: 1.1457116603851318\n",
      "Epoch 178: Train Loss: 0.18359703421592713, Validation Loss: 1.1471400260925293\n",
      "Epoch 179: Train Loss: 0.18618232309818267, Validation Loss: 1.1554696559906006\n",
      "Epoch 180: Train Loss: 0.20956864953041077, Validation Loss: 1.1526620388031006\n",
      "Epoch 181: Train Loss: 0.15866418033838273, Validation Loss: 1.199709415435791\n",
      "Epoch 182: Train Loss: 0.16949140429496765, Validation Loss: 1.209570050239563\n",
      "Epoch 183: Train Loss: 0.18292687088251114, Validation Loss: 1.2271970510482788\n",
      "Epoch 184: Train Loss: 0.1762519210577011, Validation Loss: 1.2320630550384521\n",
      "Epoch 185: Train Loss: 0.17795831859111785, Validation Loss: 1.252765417098999\n",
      "Epoch 186: Train Loss: 0.17838585376739502, Validation Loss: 1.2528997659683228\n",
      "Epoch 187: Train Loss: 0.17796069532632827, Validation Loss: 1.249864101409912\n",
      "Epoch 188: Train Loss: 0.15781939625740052, Validation Loss: 1.2592631578445435\n",
      "Epoch 189: Train Loss: 0.1869108960032463, Validation Loss: 1.245703101158142\n",
      "Epoch 190: Train Loss: 0.16442493945360184, Validation Loss: 1.2217334508895874\n",
      "Epoch 191: Train Loss: 0.14072639420628547, Validation Loss: 1.2223219871520996\n",
      "Epoch 192: Train Loss: 0.16777626276016236, Validation Loss: 1.2466650009155273\n",
      "Epoch 193: Train Loss: 0.1408519223332405, Validation Loss: 1.2543596029281616\n",
      "Epoch 194: Train Loss: 0.13905994594097137, Validation Loss: 1.285866141319275\n",
      "Epoch 195: Train Loss: 0.1586594060063362, Validation Loss: 1.2878490686416626\n",
      "Epoch 196: Train Loss: 0.246783284842968, Validation Loss: 1.2920737266540527\n",
      "Epoch 197: Train Loss: 0.14445917457342147, Validation Loss: 1.3863471746444702\n",
      "Epoch 198: Train Loss: 0.11526454240083694, Validation Loss: 1.3891677856445312\n",
      "Epoch 199: Train Loss: 0.16162413954734803, Validation Loss: 1.363651156425476\n",
      "Fold 2 Metrics:\n",
      "Accuracy: 0.36363636363636365, Precision: 0.0, Recall: 0.0, F1-score: 0.0, AUC: 0.4\n",
      "Confusion Matrix:\n",
      "[[4 1]\n",
      " [6 0]]\n",
      "Completed fold 2\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples from subject 3 to test set\n",
      "Adding 6 truth samples from subject 3 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.775508451461792, Validation Loss: 0.7022311091423035\n",
      "Epoch 1: Train Loss: 0.7028320789337158, Validation Loss: 0.7028165459632874\n",
      "Epoch 2: Train Loss: 0.7308810114860534, Validation Loss: 0.7023305296897888\n",
      "Epoch 3: Train Loss: 0.6951285243034363, Validation Loss: 0.7010401487350464\n",
      "Epoch 4: Train Loss: 0.7163289666175843, Validation Loss: 0.7010646462440491\n",
      "Epoch 5: Train Loss: 0.7112883925437927, Validation Loss: 0.7025928497314453\n",
      "Epoch 6: Train Loss: 0.6654011845588684, Validation Loss: 0.7043111324310303\n",
      "Epoch 7: Train Loss: 0.6793783068656921, Validation Loss: 0.7051277756690979\n",
      "Epoch 8: Train Loss: 0.6602721810340881, Validation Loss: 0.7054852843284607\n",
      "Epoch 9: Train Loss: 0.7831206679344177, Validation Loss: 0.7055842280387878\n",
      "Epoch 10: Train Loss: 0.6849459767341614, Validation Loss: 0.7068657875061035\n",
      "Epoch 11: Train Loss: 0.6770844459533691, Validation Loss: 0.7051486372947693\n",
      "Epoch 12: Train Loss: 0.6063776552677155, Validation Loss: 0.6972813010215759\n",
      "Epoch 13: Train Loss: 0.6495488464832306, Validation Loss: 0.691981315612793\n",
      "Epoch 14: Train Loss: 0.6278362989425659, Validation Loss: 0.6898390650749207\n",
      "Epoch 15: Train Loss: 0.6102306723594666, Validation Loss: 0.6867548823356628\n",
      "Epoch 16: Train Loss: 0.6333122611045837, Validation Loss: 0.6856344938278198\n",
      "Epoch 17: Train Loss: 0.643496286869049, Validation Loss: 0.6859604716300964\n",
      "Epoch 18: Train Loss: 0.6489835143089294, Validation Loss: 0.6857571601867676\n",
      "Epoch 19: Train Loss: 0.6488458514213562, Validation Loss: 0.6858394145965576\n",
      "Epoch 20: Train Loss: 0.6139134049415589, Validation Loss: 0.6835446357727051\n",
      "Epoch 21: Train Loss: 0.6557100057601929, Validation Loss: 0.6805795431137085\n",
      "Epoch 22: Train Loss: 0.6056230545043946, Validation Loss: 0.6781297922134399\n",
      "Epoch 23: Train Loss: 0.6114514470100403, Validation Loss: 0.6799246072769165\n",
      "Epoch 24: Train Loss: 0.633440637588501, Validation Loss: 0.6827764511108398\n",
      "Epoch 25: Train Loss: 0.6268723130226135, Validation Loss: 0.6819496154785156\n",
      "Epoch 26: Train Loss: 0.6434560894966126, Validation Loss: 0.6837997436523438\n",
      "Epoch 27: Train Loss: 0.6321294784545899, Validation Loss: 0.684985339641571\n",
      "Epoch 28: Train Loss: 0.6124042272567749, Validation Loss: 0.6839809417724609\n",
      "Epoch 29: Train Loss: 0.6588987946510315, Validation Loss: 0.6841866970062256\n",
      "Epoch 30: Train Loss: 0.6525189876556396, Validation Loss: 0.6782213449478149\n",
      "Epoch 31: Train Loss: 0.6072252869606019, Validation Loss: 0.679449737071991\n",
      "Epoch 32: Train Loss: 0.611224365234375, Validation Loss: 0.6777505874633789\n",
      "Epoch 33: Train Loss: 0.6168071746826171, Validation Loss: 0.6751124262809753\n",
      "Epoch 34: Train Loss: 0.6319736123085022, Validation Loss: 0.671367883682251\n",
      "Epoch 35: Train Loss: 0.6266518592834472, Validation Loss: 0.6674210429191589\n",
      "Epoch 36: Train Loss: 0.5897715449333191, Validation Loss: 0.6627022624015808\n",
      "Epoch 37: Train Loss: 0.6017566800117493, Validation Loss: 0.6614352464675903\n",
      "Epoch 38: Train Loss: 0.6739612817764282, Validation Loss: 0.6607427597045898\n",
      "Epoch 39: Train Loss: 0.5938899040222168, Validation Loss: 0.6614630222320557\n",
      "Epoch 40: Train Loss: 0.6252764463424683, Validation Loss: 0.6622864007949829\n",
      "Epoch 41: Train Loss: 0.6568176031112671, Validation Loss: 0.6658012270927429\n",
      "Epoch 42: Train Loss: 0.5952147126197815, Validation Loss: 0.6708015203475952\n",
      "Epoch 43: Train Loss: 0.6089795351028442, Validation Loss: 0.6745430827140808\n",
      "Epoch 44: Train Loss: 0.5754758000373841, Validation Loss: 0.6745566725730896\n",
      "Epoch 45: Train Loss: 0.5844206213951111, Validation Loss: 0.6759494543075562\n",
      "Epoch 46: Train Loss: 0.6015265583992004, Validation Loss: 0.6750251054763794\n",
      "Epoch 47: Train Loss: 0.5666104912757873, Validation Loss: 0.6749172210693359\n",
      "Epoch 48: Train Loss: 0.5741935968399048, Validation Loss: 0.6749404668807983\n",
      "Epoch 49: Train Loss: 0.5989726424217224, Validation Loss: 0.6747144460678101\n",
      "Epoch 50: Train Loss: 0.5700937390327454, Validation Loss: 0.6737367510795593\n",
      "Epoch 51: Train Loss: 0.5556018710136413, Validation Loss: 0.668496310710907\n",
      "Epoch 52: Train Loss: 0.5587702751159668, Validation Loss: 0.6593307256698608\n",
      "Epoch 53: Train Loss: 0.5652535080909729, Validation Loss: 0.6505651473999023\n",
      "Epoch 54: Train Loss: 0.5716791152954102, Validation Loss: 0.6467522382736206\n",
      "Epoch 55: Train Loss: 0.5977580547332764, Validation Loss: 0.6437979340553284\n",
      "Epoch 56: Train Loss: 0.6429916501045227, Validation Loss: 0.6411645412445068\n",
      "Epoch 57: Train Loss: 0.531620192527771, Validation Loss: 0.6390295624732971\n",
      "Epoch 58: Train Loss: 0.5603929758071899, Validation Loss: 0.6376188397407532\n",
      "Epoch 59: Train Loss: 0.5532432913780212, Validation Loss: 0.6394557952880859\n",
      "Epoch 60: Train Loss: 0.5553133487701416, Validation Loss: 0.6410558819770813\n",
      "Epoch 61: Train Loss: 0.6249473214149475, Validation Loss: 0.6394493579864502\n",
      "Epoch 62: Train Loss: 0.5461215138435364, Validation Loss: 0.6346923112869263\n",
      "Epoch 63: Train Loss: 0.5285572409629822, Validation Loss: 0.6369474530220032\n",
      "Epoch 64: Train Loss: 0.5591981053352356, Validation Loss: 0.6385564804077148\n",
      "Epoch 65: Train Loss: 0.5478733062744141, Validation Loss: 0.6444987058639526\n",
      "Epoch 66: Train Loss: 0.5035550534725189, Validation Loss: 0.6487272381782532\n",
      "Epoch 67: Train Loss: 0.5045183897018433, Validation Loss: 0.6503118276596069\n",
      "Epoch 68: Train Loss: 0.5402014017105102, Validation Loss: 0.6511265635490417\n",
      "Epoch 69: Train Loss: 0.5266959130764007, Validation Loss: 0.6510357856750488\n",
      "Epoch 70: Train Loss: 0.5079494059085846, Validation Loss: 0.6591963171958923\n",
      "Epoch 71: Train Loss: 0.5291571199893952, Validation Loss: 0.6717051863670349\n",
      "Epoch 72: Train Loss: 0.5323188066482544, Validation Loss: 0.6627882719039917\n",
      "Epoch 73: Train Loss: 0.49927574396133423, Validation Loss: 0.6487740874290466\n",
      "Epoch 74: Train Loss: 0.5313591241836548, Validation Loss: 0.6401288509368896\n",
      "Epoch 75: Train Loss: 0.5228033781051635, Validation Loss: 0.6423638463020325\n",
      "Epoch 76: Train Loss: 0.5036888062953949, Validation Loss: 0.6421336531639099\n",
      "Epoch 77: Train Loss: 0.5208237409591675, Validation Loss: 0.6396761536598206\n",
      "Epoch 78: Train Loss: 0.47455894947052, Validation Loss: 0.6400989294052124\n",
      "Epoch 79: Train Loss: 0.47093707919120786, Validation Loss: 0.6396607756614685\n",
      "Epoch 80: Train Loss: 0.4975917339324951, Validation Loss: 0.6315446496009827\n",
      "Epoch 81: Train Loss: 0.45331326127052307, Validation Loss: 0.630219042301178\n",
      "Epoch 82: Train Loss: 0.44731185436248777, Validation Loss: 0.6201900243759155\n",
      "Epoch 83: Train Loss: 0.49372920393943787, Validation Loss: 0.6282938718795776\n",
      "Epoch 84: Train Loss: 0.44116668701171874, Validation Loss: 0.6395919322967529\n",
      "Epoch 85: Train Loss: 0.4484727084636688, Validation Loss: 0.6432550549507141\n",
      "Epoch 86: Train Loss: 0.4882031738758087, Validation Loss: 0.6403665542602539\n",
      "Epoch 87: Train Loss: 0.4411732017993927, Validation Loss: 0.6384077072143555\n",
      "Epoch 88: Train Loss: 0.48140265345573424, Validation Loss: 0.6347556710243225\n",
      "Epoch 89: Train Loss: 0.4834111213684082, Validation Loss: 0.6340274810791016\n",
      "Epoch 90: Train Loss: 0.5692542612552642, Validation Loss: 0.6421551704406738\n",
      "Epoch 91: Train Loss: 0.42731472849845886, Validation Loss: 0.6551563143730164\n",
      "Epoch 92: Train Loss: 0.4046589136123657, Validation Loss: 0.6185554265975952\n",
      "Epoch 93: Train Loss: 0.4605581879615784, Validation Loss: 0.6282517910003662\n",
      "Epoch 94: Train Loss: 0.44073292016983034, Validation Loss: 0.6727592349052429\n",
      "Epoch 95: Train Loss: 0.40643513202667236, Validation Loss: 0.6846537590026855\n",
      "Epoch 96: Train Loss: 0.4024941086769104, Validation Loss: 0.6622008085250854\n",
      "Epoch 97: Train Loss: 0.4131273329257965, Validation Loss: 0.6486827731132507\n",
      "Epoch 98: Train Loss: 0.3642145425081253, Validation Loss: 0.648907482624054\n",
      "Epoch 99: Train Loss: 0.410791015625, Validation Loss: 0.650626003742218\n",
      "Epoch 100: Train Loss: 0.39159042239189146, Validation Loss: 0.6602438688278198\n",
      "Epoch 101: Train Loss: 0.424872899055481, Validation Loss: 0.6231673359870911\n",
      "Epoch 102: Train Loss: 0.34782190024852755, Validation Loss: 0.6181370615959167\n",
      "Epoch 103: Train Loss: 0.3728214919567108, Validation Loss: 0.6547763347625732\n",
      "Epoch 104: Train Loss: 0.4444798409938812, Validation Loss: 0.6696567535400391\n",
      "Epoch 105: Train Loss: 0.49347070455551145, Validation Loss: 0.669375479221344\n",
      "Epoch 106: Train Loss: 0.4120933413505554, Validation Loss: 0.6644861698150635\n",
      "Epoch 107: Train Loss: 0.3202369958162308, Validation Loss: 0.6657136082649231\n",
      "Epoch 108: Train Loss: 0.3611599922180176, Validation Loss: 0.6683578491210938\n",
      "Epoch 109: Train Loss: 0.3347567319869995, Validation Loss: 0.6750900149345398\n",
      "Epoch 110: Train Loss: 0.36930749416351316, Validation Loss: 0.7023894190788269\n",
      "Epoch 111: Train Loss: 0.3037032693624496, Validation Loss: 0.7634217739105225\n",
      "Epoch 112: Train Loss: 0.3625415861606598, Validation Loss: 0.7508543133735657\n",
      "Epoch 113: Train Loss: 0.3439614295959473, Validation Loss: 0.7551667094230652\n",
      "Epoch 114: Train Loss: 0.37744041681289675, Validation Loss: 0.7522565126419067\n",
      "Epoch 115: Train Loss: 0.2813462376594543, Validation Loss: 0.7690213918685913\n",
      "Epoch 116: Train Loss: 0.3652171313762665, Validation Loss: 0.7566817998886108\n",
      "Epoch 117: Train Loss: 0.32788864374160764, Validation Loss: 0.7473323941230774\n",
      "Epoch 118: Train Loss: 0.3723651945590973, Validation Loss: 0.8033105134963989\n",
      "Epoch 119: Train Loss: 0.41764295697212217, Validation Loss: 0.7961596846580505\n",
      "Epoch 120: Train Loss: 0.2667397528886795, Validation Loss: 0.7524645924568176\n",
      "Epoch 121: Train Loss: 0.3081171751022339, Validation Loss: 0.7233946919441223\n",
      "Epoch 122: Train Loss: 0.35175043940544126, Validation Loss: 0.7444453835487366\n",
      "Epoch 123: Train Loss: 0.3476471483707428, Validation Loss: 0.7428929805755615\n",
      "Epoch 124: Train Loss: 0.299907261133194, Validation Loss: 0.7043275833129883\n",
      "Epoch 125: Train Loss: 0.41009644865989686, Validation Loss: 0.707402765750885\n",
      "Epoch 126: Train Loss: 0.3062955975532532, Validation Loss: 0.6871320009231567\n",
      "Epoch 127: Train Loss: 0.2574510931968689, Validation Loss: 0.6740733981132507\n",
      "Epoch 128: Train Loss: 0.3254151850938797, Validation Loss: 0.6729546785354614\n",
      "Epoch 129: Train Loss: 0.29616339802742003, Validation Loss: 0.6957507133483887\n",
      "Epoch 130: Train Loss: 0.28511144816875456, Validation Loss: 0.8216147422790527\n",
      "Epoch 131: Train Loss: 0.25294302105903627, Validation Loss: 0.7917903661727905\n",
      "Epoch 132: Train Loss: 0.3633158981800079, Validation Loss: 0.7234594821929932\n",
      "Epoch 133: Train Loss: 0.2404119163751602, Validation Loss: 0.7587102055549622\n",
      "Epoch 134: Train Loss: 0.3303965091705322, Validation Loss: 0.7827787399291992\n",
      "Epoch 135: Train Loss: 0.33363452553749084, Validation Loss: 0.8089286088943481\n",
      "Epoch 136: Train Loss: 0.28646973967552186, Validation Loss: 0.8092275261878967\n",
      "Epoch 137: Train Loss: 0.3010099083185196, Validation Loss: 0.7957008481025696\n",
      "Epoch 138: Train Loss: 0.2480921596288681, Validation Loss: 0.7984038591384888\n",
      "Epoch 139: Train Loss: 0.2930569887161255, Validation Loss: 0.7800469994544983\n",
      "Epoch 140: Train Loss: 0.2845120370388031, Validation Loss: 0.7800012230873108\n",
      "Epoch 141: Train Loss: 0.24887243211269378, Validation Loss: 0.8053054809570312\n",
      "Epoch 142: Train Loss: 0.2932123005390167, Validation Loss: 0.794792890548706\n",
      "Epoch 143: Train Loss: 0.20839302465319634, Validation Loss: 0.8452165722846985\n",
      "Epoch 144: Train Loss: 0.24545948803424836, Validation Loss: 0.91727614402771\n",
      "Epoch 145: Train Loss: 0.2647545635700226, Validation Loss: 0.8984165191650391\n",
      "Epoch 146: Train Loss: 0.2467470347881317, Validation Loss: 0.9042207598686218\n",
      "Epoch 147: Train Loss: 0.22502377033233642, Validation Loss: 0.8849214911460876\n",
      "Epoch 148: Train Loss: 0.24366754591464995, Validation Loss: 0.8753750920295715\n",
      "Epoch 149: Train Loss: 0.20966358184814454, Validation Loss: 0.8697794079780579\n",
      "Epoch 150: Train Loss: 0.27493482232093813, Validation Loss: 0.8915212154388428\n",
      "Epoch 151: Train Loss: 0.3319601327180862, Validation Loss: 0.9642765522003174\n",
      "Epoch 152: Train Loss: 0.2946148604154587, Validation Loss: 0.8538841605186462\n",
      "Epoch 153: Train Loss: 0.22264940440654754, Validation Loss: 0.8063594102859497\n",
      "Epoch 154: Train Loss: 0.21534212827682495, Validation Loss: 0.8433321118354797\n",
      "Epoch 155: Train Loss: 0.25733844339847567, Validation Loss: 0.8941013216972351\n",
      "Epoch 156: Train Loss: 0.2323931485414505, Validation Loss: 0.9334375262260437\n",
      "Epoch 157: Train Loss: 0.2169800281524658, Validation Loss: 0.9121474623680115\n",
      "Epoch 158: Train Loss: 0.24447464644908906, Validation Loss: 0.9111246466636658\n",
      "Epoch 159: Train Loss: 0.20815992951393128, Validation Loss: 0.9000748991966248\n",
      "Epoch 160: Train Loss: 0.2411477893590927, Validation Loss: 0.8213147521018982\n",
      "Epoch 161: Train Loss: 0.23290157914161683, Validation Loss: 0.8510030508041382\n",
      "Epoch 162: Train Loss: 0.23346173167228698, Validation Loss: 0.9866635799407959\n",
      "Epoch 163: Train Loss: 0.20178622007369995, Validation Loss: 0.998839795589447\n",
      "Epoch 164: Train Loss: 0.23897034227848052, Validation Loss: 0.9364394545555115\n",
      "Epoch 165: Train Loss: 0.2188693568110466, Validation Loss: 0.8812897801399231\n",
      "Epoch 166: Train Loss: 0.22247980535030365, Validation Loss: 0.898754894733429\n",
      "Epoch 167: Train Loss: 0.1770547553896904, Validation Loss: 0.9923946261405945\n",
      "Epoch 168: Train Loss: 0.193591171503067, Validation Loss: 0.9862353801727295\n",
      "Epoch 169: Train Loss: 0.18746743202209473, Validation Loss: 0.9943937063217163\n",
      "Epoch 170: Train Loss: 0.19055033922195436, Validation Loss: 1.0239572525024414\n",
      "Epoch 171: Train Loss: 0.23320552706718445, Validation Loss: 1.0001376867294312\n",
      "Epoch 172: Train Loss: 0.18422746956348418, Validation Loss: 0.9987913370132446\n",
      "Epoch 173: Train Loss: 0.26120401322841647, Validation Loss: 0.9505332708358765\n",
      "Epoch 174: Train Loss: 0.17885424345731735, Validation Loss: 1.0519499778747559\n",
      "Epoch 175: Train Loss: 0.1572296366095543, Validation Loss: 1.049389362335205\n",
      "Epoch 176: Train Loss: 0.20495069324970244, Validation Loss: 1.0017098188400269\n",
      "Epoch 177: Train Loss: 0.1701977699995041, Validation Loss: 1.0128625631332397\n",
      "Epoch 178: Train Loss: 0.17995185554027557, Validation Loss: 1.03927481174469\n",
      "Epoch 179: Train Loss: 0.2863210588693619, Validation Loss: 1.1069046258926392\n",
      "Epoch 180: Train Loss: 0.2795918107032776, Validation Loss: 1.1143345832824707\n",
      "Epoch 181: Train Loss: 0.2519890874624252, Validation Loss: 0.9871357083320618\n",
      "Epoch 182: Train Loss: 0.23020448088645934, Validation Loss: 0.9414952993392944\n",
      "Epoch 183: Train Loss: 0.2847939610481262, Validation Loss: 0.9656506776809692\n",
      "Epoch 184: Train Loss: 0.207737934589386, Validation Loss: 1.0005606412887573\n",
      "Epoch 185: Train Loss: 0.16941587775945663, Validation Loss: 0.9696639776229858\n",
      "Epoch 186: Train Loss: 0.27545935809612276, Validation Loss: 0.95528644323349\n",
      "Epoch 187: Train Loss: 0.2113228514790535, Validation Loss: 0.9753323197364807\n",
      "Epoch 188: Train Loss: 0.1990038961172104, Validation Loss: 0.9722185134887695\n",
      "Epoch 189: Train Loss: 0.2214477151632309, Validation Loss: 0.9798350930213928\n",
      "Epoch 190: Train Loss: 0.1875326953828335, Validation Loss: 0.9953340291976929\n",
      "Epoch 191: Train Loss: 0.1959904372692108, Validation Loss: 1.0662645101547241\n",
      "Epoch 192: Train Loss: 0.2150995135307312, Validation Loss: 1.0259604454040527\n",
      "Epoch 193: Train Loss: 0.1429457485675812, Validation Loss: 0.9630625247955322\n",
      "Epoch 194: Train Loss: 0.21061254143714905, Validation Loss: 1.0632777214050293\n",
      "Epoch 195: Train Loss: 0.3231925517320633, Validation Loss: 1.1192055940628052\n",
      "Epoch 196: Train Loss: 0.20612920075654984, Validation Loss: 1.151429295539856\n",
      "Epoch 197: Train Loss: 0.20425045490264893, Validation Loss: 1.1371738910675049\n",
      "Epoch 198: Train Loss: 0.17532030045986174, Validation Loss: 1.1117888689041138\n",
      "Epoch 199: Train Loss: 0.15535831153392793, Validation Loss: 1.126981258392334\n",
      "Fold 3 Metrics:\n",
      "Accuracy: 0.36363636363636365, Precision: 0.0, Recall: 0.0, F1-score: 0.0, AUC: 0.4\n",
      "Confusion Matrix:\n",
      "[[4 1]\n",
      " [6 0]]\n",
      "Completed fold 3\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples from subject 10 to test set\n",
      "Adding 6 truth samples from subject 10 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.7448834657669068, Validation Loss: 0.6894874572753906\n",
      "Epoch 1: Train Loss: 0.7063323140144349, Validation Loss: 0.6898770332336426\n",
      "Epoch 2: Train Loss: 0.7245331883430481, Validation Loss: 0.6949232220649719\n",
      "Epoch 3: Train Loss: 0.675691282749176, Validation Loss: 0.7024182081222534\n",
      "Epoch 4: Train Loss: 0.6714298367500305, Validation Loss: 0.7100719213485718\n",
      "Epoch 5: Train Loss: 0.6627753913402558, Validation Loss: 0.7155261635780334\n",
      "Epoch 6: Train Loss: 0.7153630018234253, Validation Loss: 0.7203620672225952\n",
      "Epoch 7: Train Loss: 0.6434621453285218, Validation Loss: 0.7227267622947693\n",
      "Epoch 8: Train Loss: 0.6727823138236999, Validation Loss: 0.7245174050331116\n",
      "Epoch 9: Train Loss: 0.6685723423957824, Validation Loss: 0.7245678305625916\n",
      "Epoch 10: Train Loss: 0.6743222594261169, Validation Loss: 0.7320041060447693\n",
      "Epoch 11: Train Loss: 0.6866321563720703, Validation Loss: 0.7378464937210083\n",
      "Epoch 12: Train Loss: 0.7152551054954529, Validation Loss: 0.734205961227417\n",
      "Epoch 13: Train Loss: 0.6271027088165283, Validation Loss: 0.7277581095695496\n",
      "Epoch 14: Train Loss: 0.644354784488678, Validation Loss: 0.7257580161094666\n",
      "Epoch 15: Train Loss: 0.6351998209953308, Validation Loss: 0.7240470051765442\n",
      "Epoch 16: Train Loss: 0.6528763175010681, Validation Loss: 0.7240001559257507\n",
      "Epoch 17: Train Loss: 0.6400468707084656, Validation Loss: 0.7241376042366028\n",
      "Epoch 18: Train Loss: 0.6409584045410156, Validation Loss: 0.7246548533439636\n",
      "Epoch 19: Train Loss: 0.6446366906166077, Validation Loss: 0.7248695492744446\n",
      "Epoch 20: Train Loss: 0.6064981341361999, Validation Loss: 0.728382408618927\n",
      "Epoch 21: Train Loss: 0.6711471676826477, Validation Loss: 0.7307723760604858\n",
      "Epoch 22: Train Loss: 0.6091155290603638, Validation Loss: 0.7330363392829895\n",
      "Epoch 23: Train Loss: 0.6186377525329589, Validation Loss: 0.7340230941772461\n",
      "Epoch 24: Train Loss: 0.6096057176589966, Validation Loss: 0.7330811619758606\n",
      "Epoch 25: Train Loss: 0.6429648995399475, Validation Loss: 0.7326453924179077\n",
      "Epoch 26: Train Loss: 0.697248899936676, Validation Loss: 0.7325961589813232\n",
      "Epoch 27: Train Loss: 0.6238781452178955, Validation Loss: 0.7324689626693726\n",
      "Epoch 28: Train Loss: 0.5824393153190612, Validation Loss: 0.7325202226638794\n",
      "Epoch 29: Train Loss: 0.6364649891853332, Validation Loss: 0.7324336767196655\n",
      "Epoch 30: Train Loss: 0.5936992287635803, Validation Loss: 0.7302356362342834\n",
      "Epoch 31: Train Loss: 0.6440248727798462, Validation Loss: 0.7345410585403442\n",
      "Epoch 32: Train Loss: 0.6279385805130004, Validation Loss: 0.7377879619598389\n",
      "Epoch 33: Train Loss: 0.5888901591300965, Validation Loss: 0.7385408282279968\n",
      "Epoch 34: Train Loss: 0.6343427062034607, Validation Loss: 0.7418121099472046\n",
      "Epoch 35: Train Loss: 0.6047014594078064, Validation Loss: 0.7430972456932068\n",
      "Epoch 36: Train Loss: 0.6156404137611389, Validation Loss: 0.7429234981536865\n",
      "Epoch 37: Train Loss: 0.5742796897888184, Validation Loss: 0.7415788173675537\n",
      "Epoch 38: Train Loss: 0.610489821434021, Validation Loss: 0.7414875030517578\n",
      "Epoch 39: Train Loss: 0.5743718922138215, Validation Loss: 0.7416952252388\n",
      "Epoch 40: Train Loss: 0.5816525280475616, Validation Loss: 0.7388693690299988\n",
      "Epoch 41: Train Loss: 0.5775655388832093, Validation Loss: 0.7374064922332764\n",
      "Epoch 42: Train Loss: 0.5963003993034363, Validation Loss: 0.7402787208557129\n",
      "Epoch 43: Train Loss: 0.5811796069145203, Validation Loss: 0.7363160252571106\n",
      "Epoch 44: Train Loss: 0.589638888835907, Validation Loss: 0.7301432490348816\n",
      "Epoch 45: Train Loss: 0.5932327151298523, Validation Loss: 0.7245203852653503\n",
      "Epoch 46: Train Loss: 0.5850051522254944, Validation Loss: 0.7260934710502625\n",
      "Epoch 47: Train Loss: 0.5718832850456238, Validation Loss: 0.729671061038971\n",
      "Epoch 48: Train Loss: 0.5800769448280334, Validation Loss: 0.7306418418884277\n",
      "Epoch 49: Train Loss: 0.5477748692035675, Validation Loss: 0.7312238812446594\n",
      "Epoch 50: Train Loss: 0.5829755067825317, Validation Loss: 0.7365137934684753\n",
      "Epoch 51: Train Loss: 0.5360533058643341, Validation Loss: 0.7452895045280457\n",
      "Epoch 52: Train Loss: 0.5560428261756897, Validation Loss: 0.7442981004714966\n",
      "Epoch 53: Train Loss: 0.5615927338600158, Validation Loss: 0.7382756471633911\n",
      "Epoch 54: Train Loss: 0.5298000752925873, Validation Loss: 0.7335497736930847\n",
      "Epoch 55: Train Loss: 0.5287296414375305, Validation Loss: 0.7342778444290161\n",
      "Epoch 56: Train Loss: 0.5147784292697907, Validation Loss: 0.7351300716400146\n",
      "Epoch 57: Train Loss: 0.552675849199295, Validation Loss: 0.7344024181365967\n",
      "Epoch 58: Train Loss: 0.5123398959636688, Validation Loss: 0.7339532971382141\n",
      "Epoch 59: Train Loss: 0.5625270128250122, Validation Loss: 0.7343675494194031\n",
      "Epoch 60: Train Loss: 0.5579380214214325, Validation Loss: 0.7578662633895874\n",
      "Epoch 61: Train Loss: 0.548372995853424, Validation Loss: 0.7588130235671997\n",
      "Epoch 62: Train Loss: 0.5235034346580505, Validation Loss: 0.7579526305198669\n",
      "Epoch 63: Train Loss: 0.5124920666217804, Validation Loss: 0.7604657411575317\n",
      "Epoch 64: Train Loss: 0.49171037077903745, Validation Loss: 0.7661240696907043\n",
      "Epoch 65: Train Loss: 0.4893123507499695, Validation Loss: 0.7697905898094177\n",
      "Epoch 66: Train Loss: 0.5240322232246399, Validation Loss: 0.7745570540428162\n",
      "Epoch 67: Train Loss: 0.48871209025382994, Validation Loss: 0.77459716796875\n",
      "Epoch 68: Train Loss: 0.5092392802238465, Validation Loss: 0.7755669951438904\n",
      "Epoch 69: Train Loss: 0.5159385979175568, Validation Loss: 0.7741231918334961\n",
      "Epoch 70: Train Loss: 0.4725111722946167, Validation Loss: 0.7714511752128601\n",
      "Epoch 71: Train Loss: 0.5608604431152344, Validation Loss: 0.7876523733139038\n",
      "Epoch 72: Train Loss: 0.5136290967464447, Validation Loss: 0.8213031888008118\n",
      "Epoch 73: Train Loss: 0.4379637956619263, Validation Loss: 0.8327990770339966\n",
      "Epoch 74: Train Loss: 0.5168397903442383, Validation Loss: 0.8269546627998352\n",
      "Epoch 75: Train Loss: 0.5510725855827332, Validation Loss: 0.8186778426170349\n",
      "Epoch 76: Train Loss: 0.5334741115570069, Validation Loss: 0.8149327635765076\n",
      "Epoch 77: Train Loss: 0.4697289288043976, Validation Loss: 0.8148436546325684\n",
      "Epoch 78: Train Loss: 0.47442718744277956, Validation Loss: 0.8166463375091553\n",
      "Epoch 79: Train Loss: 0.5223476946353912, Validation Loss: 0.8183606266975403\n",
      "Epoch 80: Train Loss: 0.49901230335235597, Validation Loss: 0.7900224328041077\n",
      "Epoch 81: Train Loss: 0.4640470504760742, Validation Loss: 0.7740429043769836\n",
      "Epoch 82: Train Loss: 0.4628694176673889, Validation Loss: 0.7783042788505554\n",
      "Epoch 83: Train Loss: 0.49063716530799867, Validation Loss: 0.7891899347305298\n",
      "Epoch 84: Train Loss: 0.47158971428871155, Validation Loss: 0.7919567227363586\n",
      "Epoch 85: Train Loss: 0.5040777862071991, Validation Loss: 0.7903885841369629\n",
      "Epoch 86: Train Loss: 0.44878892302513124, Validation Loss: 0.7870242595672607\n",
      "Epoch 87: Train Loss: 0.5113166332244873, Validation Loss: 0.7879501581192017\n",
      "Epoch 88: Train Loss: 0.43115903735160827, Validation Loss: 0.7898589372634888\n",
      "Epoch 89: Train Loss: 0.47615193724632265, Validation Loss: 0.7913886308670044\n",
      "Epoch 90: Train Loss: 0.4618993282318115, Validation Loss: 0.8063338398933411\n",
      "Epoch 91: Train Loss: 0.46373545527458193, Validation Loss: 0.8015422821044922\n",
      "Epoch 92: Train Loss: 0.4822004854679108, Validation Loss: 0.8079700469970703\n",
      "Epoch 93: Train Loss: 0.4253783285617828, Validation Loss: 0.8053901791572571\n",
      "Epoch 94: Train Loss: 0.45309445858001707, Validation Loss: 0.8176704049110413\n",
      "Epoch 95: Train Loss: 0.459524804353714, Validation Loss: 0.8374148607254028\n",
      "Epoch 96: Train Loss: 0.4667030215263367, Validation Loss: 0.8495962023735046\n",
      "Epoch 97: Train Loss: 0.41053720116615294, Validation Loss: 0.8460524678230286\n",
      "Epoch 98: Train Loss: 0.4483749449253082, Validation Loss: 0.8427520394325256\n",
      "Epoch 99: Train Loss: 0.46401846408843994, Validation Loss: 0.8417357802391052\n",
      "Epoch 100: Train Loss: 0.4703377723693848, Validation Loss: 0.8092660307884216\n",
      "Epoch 101: Train Loss: 0.4480826497077942, Validation Loss: 0.8033460378646851\n",
      "Epoch 102: Train Loss: 0.4202534556388855, Validation Loss: 0.7984377145767212\n",
      "Epoch 103: Train Loss: 0.3952313423156738, Validation Loss: 0.7895601391792297\n",
      "Epoch 104: Train Loss: 0.3914964318275452, Validation Loss: 0.7864730358123779\n",
      "Epoch 105: Train Loss: 0.3720047354698181, Validation Loss: 0.7831354141235352\n",
      "Epoch 106: Train Loss: 0.3810016751289368, Validation Loss: 0.7825520634651184\n",
      "Epoch 107: Train Loss: 0.40207740664482117, Validation Loss: 0.7796875834465027\n",
      "Epoch 108: Train Loss: 0.39863739609718324, Validation Loss: 0.7735505104064941\n",
      "Epoch 109: Train Loss: 0.3907193303108215, Validation Loss: 0.7715120315551758\n",
      "Epoch 110: Train Loss: 0.3548759937286377, Validation Loss: 0.7372075915336609\n",
      "Epoch 111: Train Loss: 0.39333794116973875, Validation Loss: 0.7310302257537842\n",
      "Epoch 112: Train Loss: 0.41288561224937437, Validation Loss: 0.7444642186164856\n",
      "Epoch 113: Train Loss: 0.4333805561065674, Validation Loss: 0.7441312074661255\n",
      "Epoch 114: Train Loss: 0.38741242289543154, Validation Loss: 0.7438051104545593\n",
      "Epoch 115: Train Loss: 0.40068977475166323, Validation Loss: 0.7491568922996521\n",
      "Epoch 116: Train Loss: 0.35589527487754824, Validation Loss: 0.771503746509552\n",
      "Epoch 117: Train Loss: 0.34731443524360656, Validation Loss: 0.7786834836006165\n",
      "Epoch 118: Train Loss: 0.3593641877174377, Validation Loss: 0.7790210843086243\n",
      "Epoch 119: Train Loss: 0.3180886387825012, Validation Loss: 0.7759571075439453\n",
      "Epoch 120: Train Loss: 0.37535219788551333, Validation Loss: 0.7947770953178406\n",
      "Epoch 121: Train Loss: 0.34617549180984497, Validation Loss: 0.7801187634468079\n",
      "Epoch 122: Train Loss: 0.3645443618297577, Validation Loss: 0.7735040783882141\n",
      "Epoch 123: Train Loss: 0.3118598073720932, Validation Loss: 0.7589505314826965\n",
      "Epoch 124: Train Loss: 0.3293971955776215, Validation Loss: 0.7557074427604675\n",
      "Epoch 125: Train Loss: 0.3246333420276642, Validation Loss: 0.7297236919403076\n",
      "Epoch 126: Train Loss: 0.3902204692363739, Validation Loss: 0.7111377120018005\n",
      "Epoch 127: Train Loss: 0.3002708613872528, Validation Loss: 0.7125252485275269\n",
      "Epoch 128: Train Loss: 0.2720026642084122, Validation Loss: 0.7271456718444824\n",
      "Epoch 129: Train Loss: 0.3492461919784546, Validation Loss: 0.7340217232704163\n",
      "Epoch 130: Train Loss: 0.30055775940418245, Validation Loss: 0.7402148842811584\n",
      "Epoch 131: Train Loss: 0.2864313870668411, Validation Loss: 0.7514364123344421\n",
      "Epoch 132: Train Loss: 0.2997690588235855, Validation Loss: 0.7499785423278809\n",
      "Epoch 133: Train Loss: 0.2857166200876236, Validation Loss: 0.7349084615707397\n",
      "Epoch 134: Train Loss: 0.2726499676704407, Validation Loss: 0.7596492171287537\n",
      "Epoch 135: Train Loss: 0.3266867458820343, Validation Loss: 0.7851260900497437\n",
      "Epoch 136: Train Loss: 0.28873659372329713, Validation Loss: 0.77058345079422\n",
      "Epoch 137: Train Loss: 0.26964721381664275, Validation Loss: 0.774936854839325\n",
      "Epoch 138: Train Loss: 0.29331088364124297, Validation Loss: 0.7645768523216248\n",
      "Epoch 139: Train Loss: 0.28727705776691437, Validation Loss: 0.761200487613678\n",
      "Epoch 140: Train Loss: 0.27833264172077177, Validation Loss: 0.732997715473175\n",
      "Epoch 141: Train Loss: 0.26639779210090636, Validation Loss: 0.7104196548461914\n",
      "Epoch 142: Train Loss: 0.251448318362236, Validation Loss: 0.7194750905036926\n",
      "Epoch 143: Train Loss: 0.22111401110887527, Validation Loss: 0.729076087474823\n",
      "Epoch 144: Train Loss: 0.246897354722023, Validation Loss: 0.7515354156494141\n",
      "Epoch 145: Train Loss: 0.2260326236486435, Validation Loss: 0.7742123603820801\n",
      "Epoch 146: Train Loss: 0.2545089066028595, Validation Loss: 0.7837907075881958\n",
      "Epoch 147: Train Loss: 0.2827242434024811, Validation Loss: 0.8177871108055115\n",
      "Epoch 148: Train Loss: 0.2502856761217117, Validation Loss: 0.8335424661636353\n",
      "Epoch 149: Train Loss: 0.2400166392326355, Validation Loss: 0.8616889119148254\n",
      "Epoch 150: Train Loss: 0.27404193580150604, Validation Loss: 0.8698596358299255\n",
      "Epoch 151: Train Loss: 0.2182140439748764, Validation Loss: 0.8480185866355896\n",
      "Epoch 152: Train Loss: 0.23238700926303862, Validation Loss: 0.846966564655304\n",
      "Epoch 153: Train Loss: 0.2271993041038513, Validation Loss: 0.8257972598075867\n",
      "Epoch 154: Train Loss: 0.2149819999933243, Validation Loss: 0.8212119936943054\n",
      "Epoch 155: Train Loss: 0.21273896098136902, Validation Loss: 0.8256248235702515\n",
      "Epoch 156: Train Loss: 0.2340581238269806, Validation Loss: 0.830240786075592\n",
      "Epoch 157: Train Loss: 0.19962147772312164, Validation Loss: 0.8219960927963257\n",
      "Epoch 158: Train Loss: 0.1878459334373474, Validation Loss: 0.8072575926780701\n",
      "Epoch 159: Train Loss: 0.20851123929023743, Validation Loss: 0.8123567700386047\n",
      "Epoch 160: Train Loss: 0.15979224145412446, Validation Loss: 0.8181068301200867\n",
      "Epoch 161: Train Loss: 0.1904624179005623, Validation Loss: 0.8420728445053101\n",
      "Epoch 162: Train Loss: 0.20105608701705932, Validation Loss: 0.8928568959236145\n",
      "Epoch 163: Train Loss: 0.1769968330860138, Validation Loss: 0.8752663731575012\n",
      "Epoch 164: Train Loss: 0.1627999871969223, Validation Loss: 0.9214715361595154\n",
      "Epoch 165: Train Loss: 0.19710587859153747, Validation Loss: 0.8979948163032532\n",
      "Epoch 166: Train Loss: 0.25695283710956573, Validation Loss: 0.848933219909668\n",
      "Epoch 167: Train Loss: 0.1677802622318268, Validation Loss: 0.9183990955352783\n",
      "Epoch 168: Train Loss: 0.18377403616905214, Validation Loss: 0.9493188858032227\n",
      "Epoch 169: Train Loss: 0.22644723057746888, Validation Loss: 0.9105907678604126\n",
      "Epoch 170: Train Loss: 0.15031218379735947, Validation Loss: 0.9905462265014648\n",
      "Epoch 171: Train Loss: 0.2402627170085907, Validation Loss: 0.9265390634536743\n",
      "Epoch 172: Train Loss: 0.19153003096580506, Validation Loss: 0.9328586459159851\n",
      "Epoch 173: Train Loss: 0.1765490248799324, Validation Loss: 0.9809455871582031\n",
      "Epoch 174: Train Loss: 0.1678381770849228, Validation Loss: 1.0036306381225586\n",
      "Epoch 175: Train Loss: 0.15798689872026445, Validation Loss: 1.0377593040466309\n",
      "Epoch 176: Train Loss: 0.19265370965003967, Validation Loss: 1.0667235851287842\n",
      "Epoch 177: Train Loss: 0.15632664263248444, Validation Loss: 0.9888179898262024\n",
      "Epoch 178: Train Loss: 0.14453965723514556, Validation Loss: 1.0124021768569946\n",
      "Epoch 179: Train Loss: 0.1412376955151558, Validation Loss: 1.0684521198272705\n",
      "Epoch 180: Train Loss: 0.16154299080371856, Validation Loss: 1.0494712591171265\n",
      "Epoch 181: Train Loss: 0.16889008581638337, Validation Loss: 1.0013797283172607\n",
      "Epoch 182: Train Loss: 0.16452467292547227, Validation Loss: 1.0239331722259521\n",
      "Epoch 183: Train Loss: 0.19933392852544785, Validation Loss: 1.075791835784912\n",
      "Epoch 184: Train Loss: 0.15313166975975037, Validation Loss: 1.1002869606018066\n",
      "Epoch 185: Train Loss: 0.13586425483226777, Validation Loss: 1.0983664989471436\n",
      "Epoch 186: Train Loss: 0.17174666821956636, Validation Loss: 1.1369562149047852\n",
      "Epoch 187: Train Loss: 0.1374477431178093, Validation Loss: 1.1957695484161377\n",
      "Epoch 188: Train Loss: 0.14472166895866395, Validation Loss: 1.1895699501037598\n",
      "Epoch 189: Train Loss: 0.1286047399044037, Validation Loss: 1.1770811080932617\n",
      "Epoch 190: Train Loss: 0.15427169501781463, Validation Loss: 1.2969890832901\n",
      "Epoch 191: Train Loss: 0.22438104301691056, Validation Loss: 1.1339869499206543\n",
      "Epoch 192: Train Loss: 0.14041481763124466, Validation Loss: 1.0635851621627808\n",
      "Epoch 193: Train Loss: 0.11713961660861968, Validation Loss: 0.9633716344833374\n",
      "Epoch 194: Train Loss: 0.12518396973609924, Validation Loss: 0.9301438927650452\n",
      "Epoch 195: Train Loss: 0.1284391015768051, Validation Loss: 0.9559122323989868\n",
      "Epoch 196: Train Loss: 0.10335504710674286, Validation Loss: 0.9915952682495117\n",
      "Epoch 197: Train Loss: 0.12266580015420914, Validation Loss: 1.0160305500030518\n",
      "Epoch 198: Train Loss: 0.14299842566251755, Validation Loss: 1.0414960384368896\n",
      "Epoch 199: Train Loss: 0.12964628934860228, Validation Loss: 1.0568801164627075\n",
      "Fold 4 Metrics:\n",
      "Accuracy: 0.5454545454545454, Precision: 0.5454545454545454, Recall: 1.0, F1-score: 0.7058823529411765, AUC: 0.5\n",
      "Confusion Matrix:\n",
      "[[0 5]\n",
      " [0 6]]\n",
      "Completed fold 4\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples from subject 13 to test set\n",
      "Adding 6 truth samples from subject 13 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.7641764879226685, Validation Loss: 0.6777487397193909\n",
      "Epoch 1: Train Loss: 0.6846178054809571, Validation Loss: 0.6802857518196106\n",
      "Epoch 2: Train Loss: 0.6748594522476197, Validation Loss: 0.6799755692481995\n",
      "Epoch 3: Train Loss: 0.7568965792655945, Validation Loss: 0.6774451732635498\n",
      "Epoch 4: Train Loss: 0.6808670163154602, Validation Loss: 0.6773412823677063\n",
      "Epoch 5: Train Loss: 0.663466727733612, Validation Loss: 0.6799001693725586\n",
      "Epoch 6: Train Loss: 0.7090915322303772, Validation Loss: 0.6832631826400757\n",
      "Epoch 7: Train Loss: 0.6826388001441955, Validation Loss: 0.6849765777587891\n",
      "Epoch 8: Train Loss: 0.6994718790054322, Validation Loss: 0.685897707939148\n",
      "Epoch 9: Train Loss: 0.6664919137954712, Validation Loss: 0.6865605115890503\n",
      "Epoch 10: Train Loss: 0.7171756744384765, Validation Loss: 0.6760702729225159\n",
      "Epoch 11: Train Loss: 0.6437085270881653, Validation Loss: 0.6706944704055786\n",
      "Epoch 12: Train Loss: 0.6706542491912841, Validation Loss: 0.6721858382225037\n",
      "Epoch 13: Train Loss: 0.6646142363548279, Validation Loss: 0.6688229441642761\n",
      "Epoch 14: Train Loss: 0.6826547980308533, Validation Loss: 0.6656201481819153\n",
      "Epoch 15: Train Loss: 0.6603024125099182, Validation Loss: 0.6647337675094604\n",
      "Epoch 16: Train Loss: 0.6341089248657227, Validation Loss: 0.6653594970703125\n",
      "Epoch 17: Train Loss: 0.7381956100463867, Validation Loss: 0.6664824485778809\n",
      "Epoch 18: Train Loss: 0.6298608422279358, Validation Loss: 0.6666669845581055\n",
      "Epoch 19: Train Loss: 0.6073672652244568, Validation Loss: 0.6667283773422241\n",
      "Epoch 20: Train Loss: 0.6615617036819458, Validation Loss: 0.6669224500656128\n",
      "Epoch 21: Train Loss: 0.6957716345787048, Validation Loss: 0.6686045527458191\n",
      "Epoch 22: Train Loss: 0.6046999633312226, Validation Loss: 0.6712018847465515\n",
      "Epoch 23: Train Loss: 0.668367576599121, Validation Loss: 0.6698102355003357\n",
      "Epoch 24: Train Loss: 0.6148658275604248, Validation Loss: 0.6729651689529419\n",
      "Epoch 25: Train Loss: 0.6535740613937377, Validation Loss: 0.6779839992523193\n",
      "Epoch 26: Train Loss: 0.667198371887207, Validation Loss: 0.6821817755699158\n",
      "Epoch 27: Train Loss: 0.5970592439174652, Validation Loss: 0.6825926303863525\n",
      "Epoch 28: Train Loss: 0.6790811061859131, Validation Loss: 0.6830309629440308\n",
      "Epoch 29: Train Loss: 0.6553979277610779, Validation Loss: 0.6826769113540649\n",
      "Epoch 30: Train Loss: 0.6525007486343384, Validation Loss: 0.6775293350219727\n",
      "Epoch 31: Train Loss: 0.607611608505249, Validation Loss: 0.664787232875824\n",
      "Epoch 32: Train Loss: 0.669105613231659, Validation Loss: 0.6516905426979065\n",
      "Epoch 33: Train Loss: 0.601730489730835, Validation Loss: 0.6359366178512573\n",
      "Epoch 34: Train Loss: 0.628572428226471, Validation Loss: 0.6271874308586121\n",
      "Epoch 35: Train Loss: 0.663969898223877, Validation Loss: 0.6270765066146851\n",
      "Epoch 36: Train Loss: 0.6630230665206909, Validation Loss: 0.628733217716217\n",
      "Epoch 37: Train Loss: 0.6632550477981567, Validation Loss: 0.6300747394561768\n",
      "Epoch 38: Train Loss: 0.6074734568595886, Validation Loss: 0.6306027173995972\n",
      "Epoch 39: Train Loss: 0.617858374118805, Validation Loss: 0.6307742595672607\n",
      "Epoch 40: Train Loss: 0.6568730235099792, Validation Loss: 0.6279670596122742\n",
      "Epoch 41: Train Loss: 0.6321413516998291, Validation Loss: 0.626230776309967\n",
      "Epoch 42: Train Loss: 0.5960482478141784, Validation Loss: 0.6251628398895264\n",
      "Epoch 43: Train Loss: 0.5824526667594909, Validation Loss: 0.6275221705436707\n",
      "Epoch 44: Train Loss: 0.6034307241439819, Validation Loss: 0.6323710083961487\n",
      "Epoch 45: Train Loss: 0.5766112625598907, Validation Loss: 0.6344333291053772\n",
      "Epoch 46: Train Loss: 0.6367882251739502, Validation Loss: 0.6291386485099792\n",
      "Epoch 47: Train Loss: 0.5765626072883606, Validation Loss: 0.6271825432777405\n",
      "Epoch 48: Train Loss: 0.5895906448364258, Validation Loss: 0.6268271207809448\n",
      "Epoch 49: Train Loss: 0.5923990964889526, Validation Loss: 0.627197265625\n",
      "Epoch 50: Train Loss: 0.5632305085659027, Validation Loss: 0.6276869177818298\n",
      "Epoch 51: Train Loss: 0.6208502292633057, Validation Loss: 0.6376619338989258\n",
      "Epoch 52: Train Loss: 0.5926911115646363, Validation Loss: 0.6424977779388428\n",
      "Epoch 53: Train Loss: 0.5738833785057068, Validation Loss: 0.6416577100753784\n",
      "Epoch 54: Train Loss: 0.5796932339668274, Validation Loss: 0.6416340470314026\n",
      "Epoch 55: Train Loss: 0.5210455119609833, Validation Loss: 0.6425443291664124\n",
      "Epoch 56: Train Loss: 0.6322262287139893, Validation Loss: 0.6438186764717102\n",
      "Epoch 57: Train Loss: 0.5528834700584412, Validation Loss: 0.6451517939567566\n",
      "Epoch 58: Train Loss: 0.5497011303901672, Validation Loss: 0.6441803574562073\n",
      "Epoch 59: Train Loss: 0.6087318658828735, Validation Loss: 0.6439274549484253\n",
      "Epoch 60: Train Loss: 0.5779425144195557, Validation Loss: 0.640741229057312\n",
      "Epoch 61: Train Loss: 0.5722279131412507, Validation Loss: 0.6474125981330872\n",
      "Epoch 62: Train Loss: 0.5721381545066834, Validation Loss: 0.6675328016281128\n",
      "Epoch 63: Train Loss: 0.5730007529258728, Validation Loss: 0.6694658994674683\n",
      "Epoch 64: Train Loss: 0.5576585710048676, Validation Loss: 0.6656659245491028\n",
      "Epoch 65: Train Loss: 0.5385307788848877, Validation Loss: 0.6574053764343262\n",
      "Epoch 66: Train Loss: 0.5660149693489075, Validation Loss: 0.6529303789138794\n",
      "Epoch 67: Train Loss: 0.5724758505821228, Validation Loss: 0.6496486663818359\n",
      "Epoch 68: Train Loss: 0.5746736705303193, Validation Loss: 0.6467100381851196\n",
      "Epoch 69: Train Loss: 0.5362450301647186, Validation Loss: 0.6462305784225464\n",
      "Epoch 70: Train Loss: 0.5750003695487976, Validation Loss: 0.619417130947113\n",
      "Epoch 71: Train Loss: 0.5186073958873749, Validation Loss: 0.6018542647361755\n",
      "Epoch 72: Train Loss: 0.5011492371559143, Validation Loss: 0.5951064229011536\n",
      "Epoch 73: Train Loss: 0.5778242826461792, Validation Loss: 0.5883416533470154\n",
      "Epoch 74: Train Loss: 0.5921820342540741, Validation Loss: 0.5873681306838989\n",
      "Epoch 75: Train Loss: 0.5584326982498169, Validation Loss: 0.5849200487136841\n",
      "Epoch 76: Train Loss: 0.5695300698280334, Validation Loss: 0.5849736332893372\n",
      "Epoch 77: Train Loss: 0.5620894193649292, Validation Loss: 0.5864437818527222\n",
      "Epoch 78: Train Loss: 0.4766544342041016, Validation Loss: 0.586079478263855\n",
      "Epoch 79: Train Loss: 0.5463086843490601, Validation Loss: 0.5860923528671265\n",
      "Epoch 80: Train Loss: 0.5844200134277344, Validation Loss: 0.6022299528121948\n",
      "Epoch 81: Train Loss: 0.5361712634563446, Validation Loss: 0.6324307918548584\n",
      "Epoch 82: Train Loss: 0.5656662583351135, Validation Loss: 0.642717719078064\n",
      "Epoch 83: Train Loss: 0.49060524702072145, Validation Loss: 0.6459828615188599\n",
      "Epoch 84: Train Loss: 0.524744063615799, Validation Loss: 0.6416274309158325\n",
      "Epoch 85: Train Loss: 0.5151002109050751, Validation Loss: 0.6377938985824585\n",
      "Epoch 86: Train Loss: 0.5838329792022705, Validation Loss: 0.6342175006866455\n",
      "Epoch 87: Train Loss: 0.544633436203003, Validation Loss: 0.6293781995773315\n",
      "Epoch 88: Train Loss: 0.4692145109176636, Validation Loss: 0.6269684433937073\n",
      "Epoch 89: Train Loss: 0.5690245151519775, Validation Loss: 0.626047670841217\n",
      "Epoch 90: Train Loss: 0.5837892889976501, Validation Loss: 0.6047024130821228\n",
      "Epoch 91: Train Loss: 0.5221653878688812, Validation Loss: 0.5875609517097473\n",
      "Epoch 92: Train Loss: 0.4864319980144501, Validation Loss: 0.5837207436561584\n",
      "Epoch 93: Train Loss: 0.5026250779628754, Validation Loss: 0.591715395450592\n",
      "Epoch 94: Train Loss: 0.48543366193771365, Validation Loss: 0.5903843641281128\n",
      "Epoch 95: Train Loss: 0.46091551780700685, Validation Loss: 0.5899466276168823\n",
      "Epoch 96: Train Loss: 0.5080917060375214, Validation Loss: 0.583865225315094\n",
      "Epoch 97: Train Loss: 0.5326961994171142, Validation Loss: 0.5809970498085022\n",
      "Epoch 98: Train Loss: 0.4578173756599426, Validation Loss: 0.5795676708221436\n",
      "Epoch 99: Train Loss: 0.4750483274459839, Validation Loss: 0.5794262886047363\n",
      "Epoch 100: Train Loss: 0.481448096036911, Validation Loss: 0.5705482959747314\n",
      "Epoch 101: Train Loss: 0.4862492322921753, Validation Loss: 0.5645226240158081\n",
      "Epoch 102: Train Loss: 0.47048299908638, Validation Loss: 0.561349630355835\n",
      "Epoch 103: Train Loss: 0.4524261176586151, Validation Loss: 0.5571925044059753\n",
      "Epoch 104: Train Loss: 0.5020423173904419, Validation Loss: 0.546232283115387\n",
      "Epoch 105: Train Loss: 0.5275706470012664, Validation Loss: 0.5390390157699585\n",
      "Epoch 106: Train Loss: 0.4747830808162689, Validation Loss: 0.5336560606956482\n",
      "Epoch 107: Train Loss: 0.41617891788482664, Validation Loss: 0.5354254245758057\n",
      "Epoch 108: Train Loss: 0.5248349726200103, Validation Loss: 0.5355640649795532\n",
      "Epoch 109: Train Loss: 0.5363189876079559, Validation Loss: 0.53472501039505\n",
      "Epoch 110: Train Loss: 0.44553006291389463, Validation Loss: 0.5478289723396301\n",
      "Epoch 111: Train Loss: 0.44027097821235656, Validation Loss: 0.5266785025596619\n",
      "Epoch 112: Train Loss: 0.4865545928478241, Validation Loss: 0.5124354958534241\n",
      "Epoch 113: Train Loss: 0.481440132856369, Validation Loss: 0.5136588215827942\n",
      "Epoch 114: Train Loss: 0.40145097076892855, Validation Loss: 0.5202376246452332\n",
      "Epoch 115: Train Loss: 0.4717141389846802, Validation Loss: 0.521384596824646\n",
      "Epoch 116: Train Loss: 0.42161719799041747, Validation Loss: 0.5202128291130066\n",
      "Epoch 117: Train Loss: 0.41797760128974915, Validation Loss: 0.5165244340896606\n",
      "Epoch 118: Train Loss: 0.49665518999099734, Validation Loss: 0.5145561695098877\n",
      "Epoch 119: Train Loss: 0.4146748185157776, Validation Loss: 0.5131915211677551\n",
      "Epoch 120: Train Loss: 0.4716120302677155, Validation Loss: 0.49356552958488464\n",
      "Epoch 121: Train Loss: 0.4402915835380554, Validation Loss: 0.48883524537086487\n",
      "Epoch 122: Train Loss: 0.42166593074798586, Validation Loss: 0.4783664643764496\n",
      "Epoch 123: Train Loss: 0.47623270750045776, Validation Loss: 0.4590696096420288\n",
      "Epoch 124: Train Loss: 0.3902741551399231, Validation Loss: 0.4410703778266907\n",
      "Epoch 125: Train Loss: 0.4530686676502228, Validation Loss: 0.42648524045944214\n",
      "Epoch 126: Train Loss: 0.4318283021450043, Validation Loss: 0.4214136004447937\n",
      "Epoch 127: Train Loss: 0.4556289553642273, Validation Loss: 0.41973695158958435\n",
      "Epoch 128: Train Loss: 0.37556969225406645, Validation Loss: 0.4253050982952118\n",
      "Epoch 129: Train Loss: 0.44865269064903257, Validation Loss: 0.4274519979953766\n",
      "Epoch 130: Train Loss: 0.36286500096321106, Validation Loss: 0.4385594129562378\n",
      "Epoch 131: Train Loss: 0.3795230150222778, Validation Loss: 0.4398830533027649\n",
      "Epoch 132: Train Loss: 0.41210936903953554, Validation Loss: 0.4393937885761261\n",
      "Epoch 133: Train Loss: 0.38897026181221006, Validation Loss: 0.4259006381034851\n",
      "Epoch 134: Train Loss: 0.4194420278072357, Validation Loss: 0.41624751687049866\n",
      "Epoch 135: Train Loss: 0.3722262144088745, Validation Loss: 0.4069521427154541\n",
      "Epoch 136: Train Loss: 0.3922888159751892, Validation Loss: 0.3983834385871887\n",
      "Epoch 137: Train Loss: 0.44166704416275027, Validation Loss: 0.39500418305397034\n",
      "Epoch 138: Train Loss: 0.3421563744544983, Validation Loss: 0.39144638180732727\n",
      "Epoch 139: Train Loss: 0.38467827439308167, Validation Loss: 0.3875249922275543\n",
      "Epoch 140: Train Loss: 0.38618133068084715, Validation Loss: 0.3688056468963623\n",
      "Epoch 141: Train Loss: 0.3375774472951889, Validation Loss: 0.3502746522426605\n",
      "Epoch 142: Train Loss: 0.4291709363460541, Validation Loss: 0.3346085548400879\n",
      "Epoch 143: Train Loss: 0.43724107146263125, Validation Loss: 0.33251476287841797\n",
      "Epoch 144: Train Loss: 0.3582656145095825, Validation Loss: 0.33007580041885376\n",
      "Epoch 145: Train Loss: 0.41114182472229005, Validation Loss: 0.33475226163864136\n",
      "Epoch 146: Train Loss: 0.35500422716140745, Validation Loss: 0.34578046202659607\n",
      "Epoch 147: Train Loss: 0.40708657503128054, Validation Loss: 0.3372970521450043\n",
      "Epoch 148: Train Loss: 0.34275307059288024, Validation Loss: 0.3273586630821228\n",
      "Epoch 149: Train Loss: 0.35244475603103637, Validation Loss: 0.32470953464508057\n",
      "Epoch 150: Train Loss: 0.425870281457901, Validation Loss: 0.28608033061027527\n",
      "Epoch 151: Train Loss: 0.32527132630348204, Validation Loss: 0.3014964461326599\n",
      "Epoch 152: Train Loss: 0.32105604112148284, Validation Loss: 0.31268012523651123\n",
      "Epoch 153: Train Loss: 0.3538962483406067, Validation Loss: 0.3078024685382843\n",
      "Epoch 154: Train Loss: 0.4006261944770813, Validation Loss: 0.2906706929206848\n",
      "Epoch 155: Train Loss: 0.336270272731781, Validation Loss: 0.2919403910636902\n",
      "Epoch 156: Train Loss: 0.3453278750181198, Validation Loss: 0.28218376636505127\n",
      "Epoch 157: Train Loss: 0.3163942456245422, Validation Loss: 0.2797531485557556\n",
      "Epoch 158: Train Loss: 0.3564861059188843, Validation Loss: 0.27823060750961304\n",
      "Epoch 159: Train Loss: 0.2909828543663025, Validation Loss: 0.27557510137557983\n",
      "Epoch 160: Train Loss: 0.3238279163837433, Validation Loss: 0.2549957036972046\n",
      "Epoch 161: Train Loss: 0.2782989054918289, Validation Loss: 0.23591700196266174\n",
      "Epoch 162: Train Loss: 0.3334172487258911, Validation Loss: 0.22953540086746216\n",
      "Epoch 163: Train Loss: 0.2867334604263306, Validation Loss: 0.23502808809280396\n",
      "Epoch 164: Train Loss: 0.30230375528335574, Validation Loss: 0.23306876420974731\n",
      "Epoch 165: Train Loss: 0.37174572944641116, Validation Loss: 0.2278892993927002\n",
      "Epoch 166: Train Loss: 0.3404342532157898, Validation Loss: 0.21941934525966644\n",
      "Epoch 167: Train Loss: 0.30616568922996523, Validation Loss: 0.20706132054328918\n",
      "Epoch 168: Train Loss: 0.3017700850963593, Validation Loss: 0.20218002796173096\n",
      "Epoch 169: Train Loss: 0.27627145051956176, Validation Loss: 0.20047086477279663\n",
      "Epoch 170: Train Loss: 0.24522293359041214, Validation Loss: 0.180258110165596\n",
      "Epoch 171: Train Loss: 0.3137601435184479, Validation Loss: 0.20755113661289215\n",
      "Epoch 172: Train Loss: 0.2636011987924576, Validation Loss: 0.2470465749502182\n",
      "Epoch 173: Train Loss: 0.2707826018333435, Validation Loss: 0.24482423067092896\n",
      "Epoch 174: Train Loss: 0.2815472483634949, Validation Loss: 0.2151487171649933\n",
      "Epoch 175: Train Loss: 0.24366163909435273, Validation Loss: 0.21336880326271057\n",
      "Epoch 176: Train Loss: 0.2330117791891098, Validation Loss: 0.20125222206115723\n",
      "Epoch 177: Train Loss: 0.2506467431783676, Validation Loss: 0.20155809819698334\n",
      "Epoch 178: Train Loss: 0.2887186884880066, Validation Loss: 0.2012067288160324\n",
      "Epoch 179: Train Loss: 0.2695533871650696, Validation Loss: 0.20008957386016846\n",
      "Epoch 180: Train Loss: 0.2633745580911636, Validation Loss: 0.17549248039722443\n",
      "Epoch 181: Train Loss: 0.27998802363872527, Validation Loss: 0.1580583155155182\n",
      "Epoch 182: Train Loss: 0.2588684558868408, Validation Loss: 0.14942008256912231\n",
      "Epoch 183: Train Loss: 0.2679677367210388, Validation Loss: 0.15834282338619232\n",
      "Epoch 184: Train Loss: 0.23009029328823088, Validation Loss: 0.17001329362392426\n",
      "Epoch 185: Train Loss: 0.26974742114543915, Validation Loss: 0.18442583084106445\n",
      "Epoch 186: Train Loss: 0.21650887727737428, Validation Loss: 0.18179884552955627\n",
      "Epoch 187: Train Loss: 0.25517000555992125, Validation Loss: 0.17512787878513336\n",
      "Epoch 188: Train Loss: 0.23906806409358977, Validation Loss: 0.1652718186378479\n",
      "Epoch 189: Train Loss: 0.1907282993197441, Validation Loss: 0.17454102635383606\n",
      "Epoch 190: Train Loss: 0.2326067566871643, Validation Loss: 0.1682223081588745\n",
      "Epoch 191: Train Loss: 0.22951345443725585, Validation Loss: 0.16774775087833405\n",
      "Epoch 192: Train Loss: 0.24777203798294067, Validation Loss: 0.14009977877140045\n",
      "Epoch 193: Train Loss: 0.27520769238471987, Validation Loss: 0.15885023772716522\n",
      "Epoch 194: Train Loss: 0.2735422819852829, Validation Loss: 0.13464786112308502\n",
      "Epoch 195: Train Loss: 0.23667542338371278, Validation Loss: 0.13677917420864105\n",
      "Epoch 196: Train Loss: 0.24488359391689302, Validation Loss: 0.13134320080280304\n",
      "Epoch 197: Train Loss: 0.24887555539608003, Validation Loss: 0.14093464612960815\n",
      "Epoch 198: Train Loss: 0.2400273025035858, Validation Loss: 0.15238089859485626\n",
      "Epoch 199: Train Loss: 0.2201532930135727, Validation Loss: 0.15861178934574127\n",
      "Fold 5 Metrics:\n",
      "Accuracy: 1.0, Precision: 1.0, Recall: 1.0, F1-score: 1.0, AUC: 1.0\n",
      "Confusion Matrix:\n",
      "[[5 0]\n",
      " [0 6]]\n",
      "Completed fold 5\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples from subject 6 to test set\n",
      "Adding 6 truth samples from subject 6 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.7371538758277894, Validation Loss: 0.6951629519462585\n",
      "Epoch 1: Train Loss: 0.6952368378639221, Validation Loss: 0.6978796124458313\n",
      "Epoch 2: Train Loss: 0.6844115138053894, Validation Loss: 0.7016110420227051\n",
      "Epoch 3: Train Loss: 0.6768669605255127, Validation Loss: 0.7049424648284912\n",
      "Epoch 4: Train Loss: 0.7264991521835327, Validation Loss: 0.7064130306243896\n",
      "Epoch 5: Train Loss: 0.7040706872940063, Validation Loss: 0.7066054940223694\n",
      "Epoch 6: Train Loss: 0.6875060796737671, Validation Loss: 0.7075608372688293\n",
      "Epoch 7: Train Loss: 0.7173825860023498, Validation Loss: 0.7080281972885132\n",
      "Epoch 8: Train Loss: 0.6967720985412598, Validation Loss: 0.7087399363517761\n",
      "Epoch 9: Train Loss: 0.709999942779541, Validation Loss: 0.7092832326889038\n",
      "Epoch 10: Train Loss: 0.7244934797286987, Validation Loss: 0.7098278999328613\n",
      "Epoch 11: Train Loss: 0.6961362600326538, Validation Loss: 0.7113454937934875\n",
      "Epoch 12: Train Loss: 0.6983556985855103, Validation Loss: 0.7105746269226074\n",
      "Epoch 13: Train Loss: 0.6739631295204163, Validation Loss: 0.7098103165626526\n",
      "Epoch 14: Train Loss: 0.6651755094528198, Validation Loss: 0.7095938920974731\n",
      "Epoch 15: Train Loss: 0.6681691408157349, Validation Loss: 0.7094857692718506\n",
      "Epoch 16: Train Loss: 0.6398563027381897, Validation Loss: 0.709701418876648\n",
      "Epoch 17: Train Loss: 0.6962283372879028, Validation Loss: 0.7088925838470459\n",
      "Epoch 18: Train Loss: 0.6836055040359497, Validation Loss: 0.7082885503768921\n",
      "Epoch 19: Train Loss: 0.7334502816200257, Validation Loss: 0.7081533670425415\n",
      "Epoch 20: Train Loss: 0.6856569886207581, Validation Loss: 0.7071641683578491\n",
      "Epoch 21: Train Loss: 0.6618252396583557, Validation Loss: 0.7057641744613647\n",
      "Epoch 22: Train Loss: 0.6979162454605102, Validation Loss: 0.7041483521461487\n",
      "Epoch 23: Train Loss: 0.7290905952453614, Validation Loss: 0.7017976641654968\n",
      "Epoch 24: Train Loss: 0.7675156354904175, Validation Loss: 0.7005784511566162\n",
      "Epoch 25: Train Loss: 0.6773884654045105, Validation Loss: 0.7000839114189148\n",
      "Epoch 26: Train Loss: 0.63380868434906, Validation Loss: 0.6995541453361511\n",
      "Epoch 27: Train Loss: 0.6349864959716797, Validation Loss: 0.6993616223335266\n",
      "Epoch 28: Train Loss: 0.6662110924720764, Validation Loss: 0.6988570690155029\n",
      "Epoch 29: Train Loss: 0.6807578802108765, Validation Loss: 0.6990842819213867\n",
      "Epoch 30: Train Loss: 0.6975323557853699, Validation Loss: 0.6978968381881714\n",
      "Epoch 31: Train Loss: 0.6506697416305542, Validation Loss: 0.6962652206420898\n",
      "Epoch 32: Train Loss: 0.6926795840263367, Validation Loss: 0.6959821581840515\n",
      "Epoch 33: Train Loss: 0.6592315554618835, Validation Loss: 0.695565938949585\n",
      "Epoch 34: Train Loss: 0.6114132881164551, Validation Loss: 0.6940701007843018\n",
      "Epoch 35: Train Loss: 0.6262633919715881, Validation Loss: 0.6938389539718628\n",
      "Epoch 36: Train Loss: 0.6484809517860413, Validation Loss: 0.6937763690948486\n",
      "Epoch 37: Train Loss: 0.6361639857292175, Validation Loss: 0.6934049129486084\n",
      "Epoch 38: Train Loss: 0.6330076694488526, Validation Loss: 0.6932453513145447\n",
      "Epoch 39: Train Loss: 0.6643427014350891, Validation Loss: 0.6931309103965759\n",
      "Epoch 40: Train Loss: 0.6310929536819458, Validation Loss: 0.6927525997161865\n",
      "Epoch 41: Train Loss: 0.6358577251434326, Validation Loss: 0.6916598677635193\n",
      "Epoch 42: Train Loss: 0.6350544929504395, Validation Loss: 0.6906167268753052\n",
      "Epoch 43: Train Loss: 0.6350408434867859, Validation Loss: 0.6893739104270935\n",
      "Epoch 44: Train Loss: 0.6381368994712829, Validation Loss: 0.6883212924003601\n",
      "Epoch 45: Train Loss: 0.6591644048690796, Validation Loss: 0.6889269948005676\n",
      "Epoch 46: Train Loss: 0.6535758733749389, Validation Loss: 0.6883281469345093\n",
      "Epoch 47: Train Loss: 0.6340025782585144, Validation Loss: 0.6880933046340942\n",
      "Epoch 48: Train Loss: 0.6202027797698975, Validation Loss: 0.6881522536277771\n",
      "Epoch 49: Train Loss: 0.5902692973613739, Validation Loss: 0.6881869435310364\n",
      "Epoch 50: Train Loss: 0.6165209770202636, Validation Loss: 0.6854037046432495\n",
      "Epoch 51: Train Loss: 0.6353088021278381, Validation Loss: 0.6825647950172424\n",
      "Epoch 52: Train Loss: 0.7020802021026611, Validation Loss: 0.6807699799537659\n",
      "Epoch 53: Train Loss: 0.6368748903274536, Validation Loss: 0.6775578856468201\n",
      "Epoch 54: Train Loss: 0.6199707508087158, Validation Loss: 0.6722424626350403\n",
      "Epoch 55: Train Loss: 0.605526614189148, Validation Loss: 0.6698923707008362\n",
      "Epoch 56: Train Loss: 0.642802608013153, Validation Loss: 0.668213963508606\n",
      "Epoch 57: Train Loss: 0.6065137267112732, Validation Loss: 0.6680057048797607\n",
      "Epoch 58: Train Loss: 0.6813024997711181, Validation Loss: 0.6674931645393372\n",
      "Epoch 59: Train Loss: 0.6060664534568787, Validation Loss: 0.6675139665603638\n",
      "Epoch 60: Train Loss: 0.5978012681007385, Validation Loss: 0.6660338640213013\n",
      "Epoch 61: Train Loss: 0.5991492152214051, Validation Loss: 0.6641861796379089\n",
      "Epoch 62: Train Loss: 0.5644598126411438, Validation Loss: 0.6626349091529846\n",
      "Epoch 63: Train Loss: 0.6280253052711486, Validation Loss: 0.6612261533737183\n",
      "Epoch 64: Train Loss: 0.6203581929206848, Validation Loss: 0.6597299575805664\n",
      "Epoch 65: Train Loss: 0.6163675427436829, Validation Loss: 0.6597443222999573\n",
      "Epoch 66: Train Loss: 0.5939729452133179, Validation Loss: 0.6594730615615845\n",
      "Epoch 67: Train Loss: 0.6399760842323303, Validation Loss: 0.6588445901870728\n",
      "Epoch 68: Train Loss: 0.5989981055259704, Validation Loss: 0.6587648391723633\n",
      "Epoch 69: Train Loss: 0.5766673147678375, Validation Loss: 0.6588302850723267\n",
      "Epoch 70: Train Loss: 0.5967514276504516, Validation Loss: 0.658658504486084\n",
      "Epoch 71: Train Loss: 0.5847458720207215, Validation Loss: 0.6569766998291016\n",
      "Epoch 72: Train Loss: 0.5649107396602631, Validation Loss: 0.6558833122253418\n",
      "Epoch 73: Train Loss: 0.5579371452331543, Validation Loss: 0.6556840538978577\n",
      "Epoch 74: Train Loss: 0.5616336166858673, Validation Loss: 0.6553829312324524\n",
      "Epoch 75: Train Loss: 0.5505308866500854, Validation Loss: 0.6600946187973022\n",
      "Epoch 76: Train Loss: 0.5570803821086884, Validation Loss: 0.6611816883087158\n",
      "Epoch 77: Train Loss: 0.55963613986969, Validation Loss: 0.6604743003845215\n",
      "Epoch 78: Train Loss: 0.5635658979415894, Validation Loss: 0.6598795652389526\n",
      "Epoch 79: Train Loss: 0.5976919412612915, Validation Loss: 0.6599979400634766\n",
      "Epoch 80: Train Loss: 0.5709206223487854, Validation Loss: 0.653486967086792\n",
      "Epoch 81: Train Loss: 0.5778608441352844, Validation Loss: 0.649627685546875\n",
      "Epoch 82: Train Loss: 0.5685784816741943, Validation Loss: 0.6414675116539001\n",
      "Epoch 83: Train Loss: 0.5606332421302795, Validation Loss: 0.6423138380050659\n",
      "Epoch 84: Train Loss: 0.5559704065322876, Validation Loss: 0.6440274715423584\n",
      "Epoch 85: Train Loss: 0.5608529448509216, Validation Loss: 0.6415436267852783\n",
      "Epoch 86: Train Loss: 0.5515529990196228, Validation Loss: 0.6407269239425659\n",
      "Epoch 87: Train Loss: 0.5314245462417603, Validation Loss: 0.6390844583511353\n",
      "Epoch 88: Train Loss: 0.5461530923843384, Validation Loss: 0.6376482844352722\n",
      "Epoch 89: Train Loss: 0.6138225078582764, Validation Loss: 0.637084424495697\n",
      "Epoch 90: Train Loss: 0.547973358631134, Validation Loss: 0.635786235332489\n",
      "Epoch 91: Train Loss: 0.6025784254074097, Validation Loss: 0.6350384950637817\n",
      "Epoch 92: Train Loss: 0.5169337213039398, Validation Loss: 0.62919020652771\n",
      "Epoch 93: Train Loss: 0.5532667517662049, Validation Loss: 0.6167330145835876\n",
      "Epoch 94: Train Loss: 0.5478394031524658, Validation Loss: 0.6189060807228088\n",
      "Epoch 95: Train Loss: 0.5475639283657074, Validation Loss: 0.6185612678527832\n",
      "Epoch 96: Train Loss: 0.4831915497779846, Validation Loss: 0.6190953254699707\n",
      "Epoch 97: Train Loss: 0.5338332831859589, Validation Loss: 0.621548056602478\n",
      "Epoch 98: Train Loss: 0.513609254360199, Validation Loss: 0.6211735606193542\n",
      "Epoch 99: Train Loss: 0.5219525933265686, Validation Loss: 0.6213322281837463\n",
      "Epoch 100: Train Loss: 0.5493136048316956, Validation Loss: 0.6251605749130249\n",
      "Epoch 101: Train Loss: 0.5423533082008362, Validation Loss: 0.6248714327812195\n",
      "Epoch 102: Train Loss: 0.5361044108867645, Validation Loss: 0.6290735006332397\n",
      "Epoch 103: Train Loss: 0.5269452095031738, Validation Loss: 0.6334241032600403\n",
      "Epoch 104: Train Loss: 0.5561567544937134, Validation Loss: 0.6312721967697144\n",
      "Epoch 105: Train Loss: 0.5232642829418183, Validation Loss: 0.6251653432846069\n",
      "Epoch 106: Train Loss: 0.5458353638648987, Validation Loss: 0.6230313777923584\n",
      "Epoch 107: Train Loss: 0.5238629221916199, Validation Loss: 0.6219691038131714\n",
      "Epoch 108: Train Loss: 0.4694015920162201, Validation Loss: 0.6241132020950317\n",
      "Epoch 109: Train Loss: 0.5278560042381286, Validation Loss: 0.623275637626648\n",
      "Epoch 110: Train Loss: 0.5006271183490754, Validation Loss: 0.6267244815826416\n",
      "Epoch 111: Train Loss: 0.4732900202274323, Validation Loss: 0.6283047795295715\n",
      "Epoch 112: Train Loss: 0.4776187539100647, Validation Loss: 0.6255735754966736\n",
      "Epoch 113: Train Loss: 0.5311583399772644, Validation Loss: 0.624688446521759\n",
      "Epoch 114: Train Loss: 0.491121506690979, Validation Loss: 0.6219300031661987\n",
      "Epoch 115: Train Loss: 0.49291825890541074, Validation Loss: 0.6173878908157349\n",
      "Epoch 116: Train Loss: 0.4547495663166046, Validation Loss: 0.6136779189109802\n",
      "Epoch 117: Train Loss: 0.4992267549037933, Validation Loss: 0.6123513579368591\n",
      "Epoch 118: Train Loss: 0.4771392703056335, Validation Loss: 0.6118329763412476\n",
      "Epoch 119: Train Loss: 0.47038660049438474, Validation Loss: 0.6120277047157288\n",
      "Epoch 120: Train Loss: 0.45754502415657045, Validation Loss: 0.6155784726142883\n",
      "Epoch 121: Train Loss: 0.44756304621696474, Validation Loss: 0.6199094653129578\n",
      "Epoch 122: Train Loss: 0.5236774623394013, Validation Loss: 0.6157488226890564\n",
      "Epoch 123: Train Loss: 0.4451367735862732, Validation Loss: 0.6042196154594421\n",
      "Epoch 124: Train Loss: 0.4215056002140045, Validation Loss: 0.6015002727508545\n",
      "Epoch 125: Train Loss: 0.4641367495059967, Validation Loss: 0.6044822931289673\n",
      "Epoch 126: Train Loss: 0.4786311686038971, Validation Loss: 0.6068929433822632\n",
      "Epoch 127: Train Loss: 0.42079888582229613, Validation Loss: 0.6080189347267151\n",
      "Epoch 128: Train Loss: 0.4743223249912262, Validation Loss: 0.6078883409500122\n",
      "Epoch 129: Train Loss: 0.42509132623672485, Validation Loss: 0.607952356338501\n",
      "Epoch 130: Train Loss: 0.4511586964130402, Validation Loss: 0.6108775734901428\n",
      "Epoch 131: Train Loss: 0.45016431212425234, Validation Loss: 0.600688636302948\n",
      "Epoch 132: Train Loss: 0.43376149535179137, Validation Loss: 0.5938344597816467\n",
      "Epoch 133: Train Loss: 0.44233388900756837, Validation Loss: 0.603451669216156\n",
      "Epoch 134: Train Loss: 0.4899131953716278, Validation Loss: 0.6064515709877014\n",
      "Epoch 135: Train Loss: 0.4286059319972992, Validation Loss: 0.6113166213035583\n",
      "Epoch 136: Train Loss: 0.43834097385406495, Validation Loss: 0.6084175705909729\n",
      "Epoch 137: Train Loss: 0.4365441739559174, Validation Loss: 0.6017963290214539\n",
      "Epoch 138: Train Loss: 0.4274823725223541, Validation Loss: 0.5970378518104553\n",
      "Epoch 139: Train Loss: 0.4304098725318909, Validation Loss: 0.5985171794891357\n",
      "Epoch 140: Train Loss: 0.42606104016304014, Validation Loss: 0.5988560318946838\n",
      "Epoch 141: Train Loss: 0.42116445302963257, Validation Loss: 0.6082550883293152\n",
      "Epoch 142: Train Loss: 0.4646802067756653, Validation Loss: 0.6171863675117493\n",
      "Epoch 143: Train Loss: 0.4297190546989441, Validation Loss: 0.6120191216468811\n",
      "Epoch 144: Train Loss: 0.4146707713603973, Validation Loss: 0.5953234434127808\n",
      "Epoch 145: Train Loss: 0.4213242769241333, Validation Loss: 0.5844727158546448\n",
      "Epoch 146: Train Loss: 0.45477430820465087, Validation Loss: 0.5806179642677307\n",
      "Epoch 147: Train Loss: 0.42467968463897704, Validation Loss: 0.5814153552055359\n",
      "Epoch 148: Train Loss: 0.3629359513521194, Validation Loss: 0.5839933156967163\n",
      "Epoch 149: Train Loss: 0.38886902332305906, Validation Loss: 0.5831995010375977\n",
      "Epoch 150: Train Loss: 0.42055994272232056, Validation Loss: 0.6285019516944885\n",
      "Epoch 151: Train Loss: 0.41401962041854856, Validation Loss: 0.6140491366386414\n",
      "Epoch 152: Train Loss: 0.4410619854927063, Validation Loss: 0.5949699878692627\n",
      "Epoch 153: Train Loss: 0.41515403985977173, Validation Loss: 0.5975722074508667\n",
      "Epoch 154: Train Loss: 0.3958927929401398, Validation Loss: 0.6060401201248169\n",
      "Epoch 155: Train Loss: 0.3654869258403778, Validation Loss: 0.6181792616844177\n",
      "Epoch 156: Train Loss: 0.42421711683273317, Validation Loss: 0.6194533705711365\n",
      "Epoch 157: Train Loss: 0.3790658891201019, Validation Loss: 0.6083403825759888\n",
      "Epoch 158: Train Loss: 0.36336510181427, Validation Loss: 0.6062430739402771\n",
      "Epoch 159: Train Loss: 0.3578337371349335, Validation Loss: 0.6122423410415649\n",
      "Epoch 160: Train Loss: 0.4088921546936035, Validation Loss: 0.5868949890136719\n",
      "Epoch 161: Train Loss: 0.35498915016651156, Validation Loss: 0.592100977897644\n",
      "Epoch 162: Train Loss: 0.3981878817081451, Validation Loss: 0.6053532361984253\n",
      "Epoch 163: Train Loss: 0.35598676800727846, Validation Loss: 0.6191776990890503\n",
      "Epoch 164: Train Loss: 0.45403275489807127, Validation Loss: 0.6135881543159485\n",
      "Epoch 165: Train Loss: 0.3496783018112183, Validation Loss: 0.6147471070289612\n",
      "Epoch 166: Train Loss: 0.3635454475879669, Validation Loss: 0.5985875725746155\n",
      "Epoch 167: Train Loss: 0.393148148059845, Validation Loss: 0.5890015363693237\n",
      "Epoch 168: Train Loss: 0.36718756556510923, Validation Loss: 0.5898542404174805\n",
      "Epoch 169: Train Loss: 0.341239857673645, Validation Loss: 0.5901092290878296\n",
      "Epoch 170: Train Loss: 0.335204541683197, Validation Loss: 0.6038445234298706\n",
      "Epoch 171: Train Loss: 0.36779929995536803, Validation Loss: 0.5987173318862915\n",
      "Epoch 172: Train Loss: 0.4169283270835876, Validation Loss: 0.5849155187606812\n",
      "Epoch 173: Train Loss: 0.3818384945392609, Validation Loss: 0.5825450420379639\n",
      "Epoch 174: Train Loss: 0.34347966611385344, Validation Loss: 0.5788993835449219\n",
      "Epoch 175: Train Loss: 0.3475839376449585, Validation Loss: 0.5798894762992859\n",
      "Epoch 176: Train Loss: 0.37282646894454957, Validation Loss: 0.5944663882255554\n",
      "Epoch 177: Train Loss: 0.36561359763145446, Validation Loss: 0.597626805305481\n",
      "Epoch 178: Train Loss: 0.30753369331359864, Validation Loss: 0.5952264666557312\n",
      "Epoch 179: Train Loss: 0.38709431886672974, Validation Loss: 0.5925450325012207\n",
      "Epoch 180: Train Loss: 0.38498268127441404, Validation Loss: 0.5819878578186035\n",
      "Epoch 181: Train Loss: 0.36081539988517763, Validation Loss: 0.6092890501022339\n",
      "Epoch 182: Train Loss: 0.39999821186065676, Validation Loss: 0.619264543056488\n",
      "Epoch 183: Train Loss: 0.31768601536750796, Validation Loss: 0.6449983716011047\n",
      "Epoch 184: Train Loss: 0.3386802613735199, Validation Loss: 0.6587974429130554\n",
      "Epoch 185: Train Loss: 0.3496487498283386, Validation Loss: 0.6345800757408142\n",
      "Epoch 186: Train Loss: 0.33167092204093934, Validation Loss: 0.6078036427497864\n",
      "Epoch 187: Train Loss: 0.2765764266252518, Validation Loss: 0.600021243095398\n",
      "Epoch 188: Train Loss: 0.3188459187746048, Validation Loss: 0.5992456674575806\n",
      "Epoch 189: Train Loss: 0.3012459218502045, Validation Loss: 0.5982489585876465\n",
      "Epoch 190: Train Loss: 0.32412819266319276, Validation Loss: 0.5821378827095032\n",
      "Epoch 191: Train Loss: 0.29347331523895265, Validation Loss: 0.5843036770820618\n",
      "Epoch 192: Train Loss: 0.3014171004295349, Validation Loss: 0.6036818623542786\n",
      "Epoch 193: Train Loss: 0.2861234605312347, Validation Loss: 0.6037257313728333\n",
      "Epoch 194: Train Loss: 0.3369940876960754, Validation Loss: 0.593238353729248\n",
      "Epoch 195: Train Loss: 0.29155481457710264, Validation Loss: 0.5965893268585205\n",
      "Epoch 196: Train Loss: 0.26964261531829836, Validation Loss: 0.6025363802909851\n",
      "Epoch 197: Train Loss: 0.283629897236824, Validation Loss: 0.6125597953796387\n",
      "Epoch 198: Train Loss: 0.26245070099830625, Validation Loss: 0.6153844594955444\n",
      "Epoch 199: Train Loss: 0.27722408473491666, Validation Loss: 0.614989697933197\n",
      "Fold 6 Metrics:\n",
      "Accuracy: 0.6363636363636364, Precision: 0.6, Recall: 1.0, F1-score: 0.75, AUC: 0.6\n",
      "Confusion Matrix:\n",
      "[[1 4]\n",
      " [0 6]]\n",
      "Completed fold 6\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples from subject 12 to test set\n",
      "Adding 6 truth samples from subject 12 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.7600199341773987, Validation Loss: 0.7037760019302368\n",
      "Epoch 1: Train Loss: 0.7133842825889587, Validation Loss: 0.7006646990776062\n",
      "Epoch 2: Train Loss: 0.686567199230194, Validation Loss: 0.6976318955421448\n",
      "Epoch 3: Train Loss: 0.69581218957901, Validation Loss: 0.6999306678771973\n",
      "Epoch 4: Train Loss: 0.6361341953277588, Validation Loss: 0.7014654874801636\n",
      "Epoch 5: Train Loss: 0.6781413674354553, Validation Loss: 0.7001599073410034\n",
      "Epoch 6: Train Loss: 0.645218575000763, Validation Loss: 0.6998039484024048\n",
      "Epoch 7: Train Loss: 0.6604647397994995, Validation Loss: 0.6991558074951172\n",
      "Epoch 8: Train Loss: 0.6652941584587098, Validation Loss: 0.6986664533615112\n",
      "Epoch 9: Train Loss: 0.6410436153411865, Validation Loss: 0.6985520720481873\n",
      "Epoch 10: Train Loss: 0.6664508223533631, Validation Loss: 0.6898141503334045\n",
      "Epoch 11: Train Loss: 0.6812899589538575, Validation Loss: 0.6840932369232178\n",
      "Epoch 12: Train Loss: 0.6382301807403564, Validation Loss: 0.6813858151435852\n",
      "Epoch 13: Train Loss: 0.694692325592041, Validation Loss: 0.6794022917747498\n",
      "Epoch 14: Train Loss: 0.6617636322975159, Validation Loss: 0.6773238182067871\n",
      "Epoch 15: Train Loss: 0.6429564118385315, Validation Loss: 0.6764875650405884\n",
      "Epoch 16: Train Loss: 0.6490820407867431, Validation Loss: 0.6757397651672363\n",
      "Epoch 17: Train Loss: 0.6101347208023071, Validation Loss: 0.6758050918579102\n",
      "Epoch 18: Train Loss: 0.6973624706268311, Validation Loss: 0.6757399439811707\n",
      "Epoch 19: Train Loss: 0.6732909440994262, Validation Loss: 0.6757504940032959\n",
      "Epoch 20: Train Loss: 0.685338294506073, Validation Loss: 0.6784878373146057\n",
      "Epoch 21: Train Loss: 0.6281525373458863, Validation Loss: 0.6784635186195374\n",
      "Epoch 22: Train Loss: 0.6539172530174255, Validation Loss: 0.6763094663619995\n",
      "Epoch 23: Train Loss: 0.6229600489139557, Validation Loss: 0.6748051047325134\n",
      "Epoch 24: Train Loss: 0.6989753723144532, Validation Loss: 0.6756832599639893\n",
      "Epoch 25: Train Loss: 0.654840087890625, Validation Loss: 0.6779677271842957\n",
      "Epoch 26: Train Loss: 0.6583597540855408, Validation Loss: 0.6787565350532532\n",
      "Epoch 27: Train Loss: 0.6701345086097718, Validation Loss: 0.6802443265914917\n",
      "Epoch 28: Train Loss: 0.601756501197815, Validation Loss: 0.6803306341171265\n",
      "Epoch 29: Train Loss: 0.6201702654361725, Validation Loss: 0.6797728538513184\n",
      "Epoch 30: Train Loss: 0.6139281749725342, Validation Loss: 0.6762479543685913\n",
      "Epoch 31: Train Loss: 0.6251941919326782, Validation Loss: 0.6741029620170593\n",
      "Epoch 32: Train Loss: 0.6112246870994568, Validation Loss: 0.669434130191803\n",
      "Epoch 33: Train Loss: 0.6336780786514282, Validation Loss: 0.6683992147445679\n",
      "Epoch 34: Train Loss: 0.578506076335907, Validation Loss: 0.6681749224662781\n",
      "Epoch 35: Train Loss: 0.5823869049549103, Validation Loss: 0.6681941747665405\n",
      "Epoch 36: Train Loss: 0.6339472651481628, Validation Loss: 0.6683689951896667\n",
      "Epoch 37: Train Loss: 0.6226069211959839, Validation Loss: 0.6685085296630859\n",
      "Epoch 38: Train Loss: 0.6087890028953552, Validation Loss: 0.6677232384681702\n",
      "Epoch 39: Train Loss: 0.5833507061004639, Validation Loss: 0.6699002981185913\n",
      "Epoch 40: Train Loss: 0.6213696122169494, Validation Loss: 0.6639151573181152\n",
      "Epoch 41: Train Loss: 0.5924211621284485, Validation Loss: 0.6605809926986694\n",
      "Epoch 42: Train Loss: 0.598562729358673, Validation Loss: 0.6563605070114136\n",
      "Epoch 43: Train Loss: 0.5918083488941193, Validation Loss: 0.6529697179794312\n",
      "Epoch 44: Train Loss: 0.5304915487766266, Validation Loss: 0.6479438543319702\n",
      "Epoch 45: Train Loss: 0.579064416885376, Validation Loss: 0.6465241312980652\n",
      "Epoch 46: Train Loss: 0.6132949948310852, Validation Loss: 0.6435181498527527\n",
      "Epoch 47: Train Loss: 0.5937337160110474, Validation Loss: 0.641965925693512\n",
      "Epoch 48: Train Loss: 0.6421132445335388, Validation Loss: 0.6402203440666199\n",
      "Epoch 49: Train Loss: 0.5588854551315308, Validation Loss: 0.6410190463066101\n",
      "Epoch 50: Train Loss: 0.5582756578922272, Validation Loss: 0.6433168053627014\n",
      "Epoch 51: Train Loss: 0.587350869178772, Validation Loss: 0.6394194960594177\n",
      "Epoch 52: Train Loss: 0.5933734059333802, Validation Loss: 0.6295744776725769\n",
      "Epoch 53: Train Loss: 0.6141581416130066, Validation Loss: 0.6188727021217346\n",
      "Epoch 54: Train Loss: 0.6171273946762085, Validation Loss: 0.6121971607208252\n",
      "Epoch 55: Train Loss: 0.5959086894989014, Validation Loss: 0.6129364967346191\n",
      "Epoch 56: Train Loss: 0.5475833177566528, Validation Loss: 0.6188760995864868\n",
      "Epoch 57: Train Loss: 0.5796642541885376, Validation Loss: 0.6173844933509827\n",
      "Epoch 58: Train Loss: 0.5649394869804383, Validation Loss: 0.6165270805358887\n",
      "Epoch 59: Train Loss: 0.5663337707519531, Validation Loss: 0.6165308952331543\n",
      "Epoch 60: Train Loss: 0.5719368278980255, Validation Loss: 0.6200274229049683\n",
      "Epoch 61: Train Loss: 0.5983119249343872, Validation Loss: 0.6201268434524536\n",
      "Epoch 62: Train Loss: 0.5628768563270569, Validation Loss: 0.624385416507721\n",
      "Epoch 63: Train Loss: 0.5501513779163361, Validation Loss: 0.6234084963798523\n",
      "Epoch 64: Train Loss: 0.5479913890361786, Validation Loss: 0.620766818523407\n",
      "Epoch 65: Train Loss: 0.5237335503101349, Validation Loss: 0.6229214072227478\n",
      "Epoch 66: Train Loss: 0.6299840092658997, Validation Loss: 0.6224645376205444\n",
      "Epoch 67: Train Loss: 0.5851851463317871, Validation Loss: 0.6194126009941101\n",
      "Epoch 68: Train Loss: 0.5279826879501343, Validation Loss: 0.6202032566070557\n",
      "Epoch 69: Train Loss: 0.5443008065223693, Validation Loss: 0.6177787780761719\n",
      "Epoch 70: Train Loss: 0.5336171627044678, Validation Loss: 0.616389811038971\n",
      "Epoch 71: Train Loss: 0.5314296960830689, Validation Loss: 0.6221019625663757\n",
      "Epoch 72: Train Loss: 0.5075746178627014, Validation Loss: 0.61790531873703\n",
      "Epoch 73: Train Loss: 0.5124334275722504, Validation Loss: 0.6140369772911072\n",
      "Epoch 74: Train Loss: 0.5778675079345703, Validation Loss: 0.6168923377990723\n",
      "Epoch 75: Train Loss: 0.5713535904884338, Validation Loss: 0.6098724007606506\n",
      "Epoch 76: Train Loss: 0.5139119446277618, Validation Loss: 0.6048315763473511\n",
      "Epoch 77: Train Loss: 0.5518263578414917, Validation Loss: 0.6015188097953796\n",
      "Epoch 78: Train Loss: 0.5062428474426269, Validation Loss: 0.5995012521743774\n",
      "Epoch 79: Train Loss: 0.5647511780261993, Validation Loss: 0.5981360077857971\n",
      "Epoch 80: Train Loss: 0.488850075006485, Validation Loss: 0.598580539226532\n",
      "Epoch 81: Train Loss: 0.5046876192092895, Validation Loss: 0.6058436036109924\n",
      "Epoch 82: Train Loss: 0.4905227065086365, Validation Loss: 0.6099159121513367\n",
      "Epoch 83: Train Loss: 0.5258642375469208, Validation Loss: 0.6100943684577942\n",
      "Epoch 84: Train Loss: 0.4938339114189148, Validation Loss: 0.6100891828536987\n",
      "Epoch 85: Train Loss: 0.505039381980896, Validation Loss: 0.610176682472229\n",
      "Epoch 86: Train Loss: 0.5197274327278137, Validation Loss: 0.6113349795341492\n",
      "Epoch 87: Train Loss: 0.5507886528968811, Validation Loss: 0.6117817759513855\n",
      "Epoch 88: Train Loss: 0.5071688234806061, Validation Loss: 0.6110022664070129\n",
      "Epoch 89: Train Loss: 0.5669030666351318, Validation Loss: 0.6122071743011475\n",
      "Epoch 90: Train Loss: 0.5337779760360718, Validation Loss: 0.6211552619934082\n",
      "Epoch 91: Train Loss: 0.564713305234909, Validation Loss: 0.6223224997520447\n",
      "Epoch 92: Train Loss: 0.48401992917060854, Validation Loss: 0.6182249784469604\n",
      "Epoch 93: Train Loss: 0.501136326789856, Validation Loss: 0.6104040741920471\n",
      "Epoch 94: Train Loss: 0.46893065571784975, Validation Loss: 0.6054142713546753\n",
      "Epoch 95: Train Loss: 0.4655317306518555, Validation Loss: 0.6034179329872131\n",
      "Epoch 96: Train Loss: 0.48340103030204773, Validation Loss: 0.6014438271522522\n",
      "Epoch 97: Train Loss: 0.547841876745224, Validation Loss: 0.5972813367843628\n",
      "Epoch 98: Train Loss: 0.4751345217227936, Validation Loss: 0.5965136289596558\n",
      "Epoch 99: Train Loss: 0.49451049566268923, Validation Loss: 0.5956819653511047\n",
      "Epoch 100: Train Loss: 0.48798556327819825, Validation Loss: 0.5927938222885132\n",
      "Epoch 101: Train Loss: 0.4908997118473053, Validation Loss: 0.5862327218055725\n",
      "Epoch 102: Train Loss: 0.45627763867378235, Validation Loss: 0.5880154967308044\n",
      "Epoch 103: Train Loss: 0.5438298285007477, Validation Loss: 0.5908281803131104\n",
      "Epoch 104: Train Loss: 0.5470736563205719, Validation Loss: 0.5849001407623291\n",
      "Epoch 105: Train Loss: 0.5183135688304901, Validation Loss: 0.5795275568962097\n",
      "Epoch 106: Train Loss: 0.43795704245567324, Validation Loss: 0.5755264759063721\n",
      "Epoch 107: Train Loss: 0.5356981039047242, Validation Loss: 0.5717445015907288\n",
      "Epoch 108: Train Loss: 0.5057872653007507, Validation Loss: 0.5689706802368164\n",
      "Epoch 109: Train Loss: 0.42817068099975586, Validation Loss: 0.5686717629432678\n",
      "Epoch 110: Train Loss: 0.4762335538864136, Validation Loss: 0.5722571015357971\n",
      "Epoch 111: Train Loss: 0.5078153610229492, Validation Loss: 0.5748361945152283\n",
      "Epoch 112: Train Loss: 0.4648320496082306, Validation Loss: 0.5822190046310425\n",
      "Epoch 113: Train Loss: 0.43299336433410646, Validation Loss: 0.5967493653297424\n",
      "Epoch 114: Train Loss: 0.4579309284687042, Validation Loss: 0.5949880480766296\n",
      "Epoch 115: Train Loss: 0.48891271352767945, Validation Loss: 0.5918608903884888\n",
      "Epoch 116: Train Loss: 0.4835387527942657, Validation Loss: 0.584864616394043\n",
      "Epoch 117: Train Loss: 0.5125809669494629, Validation Loss: 0.5784270763397217\n",
      "Epoch 118: Train Loss: 0.4380734324455261, Validation Loss: 0.5785050988197327\n",
      "Epoch 119: Train Loss: 0.4814022719860077, Validation Loss: 0.5742892026901245\n",
      "Epoch 120: Train Loss: 0.5006742775440216, Validation Loss: 0.5720592141151428\n",
      "Epoch 121: Train Loss: 0.4116157412528992, Validation Loss: 0.5704912543296814\n",
      "Epoch 122: Train Loss: 0.446307498216629, Validation Loss: 0.5646817684173584\n",
      "Epoch 123: Train Loss: 0.42381134033203127, Validation Loss: 0.5705673694610596\n",
      "Epoch 124: Train Loss: 0.4067262470722198, Validation Loss: 0.5738204717636108\n",
      "Epoch 125: Train Loss: 0.39278804659843447, Validation Loss: 0.5759254097938538\n",
      "Epoch 126: Train Loss: 0.37560046911239625, Validation Loss: 0.5758397579193115\n",
      "Epoch 127: Train Loss: 0.4209774613380432, Validation Loss: 0.571563720703125\n",
      "Epoch 128: Train Loss: 0.45865562558174133, Validation Loss: 0.5687653422355652\n",
      "Epoch 129: Train Loss: 0.392408674955368, Validation Loss: 0.5710979700088501\n",
      "Epoch 130: Train Loss: 0.44068863391876223, Validation Loss: 0.5575169324874878\n",
      "Epoch 131: Train Loss: 0.44782631993293764, Validation Loss: 0.5472866892814636\n",
      "Epoch 132: Train Loss: 0.49300641417503355, Validation Loss: 0.5596171021461487\n",
      "Epoch 133: Train Loss: 0.3982866942882538, Validation Loss: 0.5655993223190308\n",
      "Epoch 134: Train Loss: 0.4524540603160858, Validation Loss: 0.5696659684181213\n",
      "Epoch 135: Train Loss: 0.37797340750694275, Validation Loss: 0.576930046081543\n",
      "Epoch 136: Train Loss: 0.4295608103275299, Validation Loss: 0.5752809047698975\n",
      "Epoch 137: Train Loss: 0.36370575428009033, Validation Loss: 0.5686709880828857\n",
      "Epoch 138: Train Loss: 0.40304375886917115, Validation Loss: 0.5659493207931519\n",
      "Epoch 139: Train Loss: 0.4475161373615265, Validation Loss: 0.5617820024490356\n",
      "Epoch 140: Train Loss: 0.37021716237068175, Validation Loss: 0.5565115809440613\n",
      "Epoch 141: Train Loss: 0.4141013264656067, Validation Loss: 0.5507212281227112\n",
      "Epoch 142: Train Loss: 0.4364853262901306, Validation Loss: 0.5359300374984741\n",
      "Epoch 143: Train Loss: 0.3918211817741394, Validation Loss: 0.5289270281791687\n",
      "Epoch 144: Train Loss: 0.3747807860374451, Validation Loss: 0.5307205319404602\n",
      "Epoch 145: Train Loss: 0.4212421000003815, Validation Loss: 0.5290473103523254\n",
      "Epoch 146: Train Loss: 0.35709025859832766, Validation Loss: 0.5268542766571045\n",
      "Epoch 147: Train Loss: 0.37138100266456603, Validation Loss: 0.526699960231781\n",
      "Epoch 148: Train Loss: 0.35410248637199404, Validation Loss: 0.5289917588233948\n",
      "Epoch 149: Train Loss: 0.3618483543395996, Validation Loss: 0.5293921828269958\n",
      "Epoch 150: Train Loss: 0.35603761672973633, Validation Loss: 0.526780903339386\n",
      "Epoch 151: Train Loss: 0.3454809904098511, Validation Loss: 0.526933491230011\n",
      "Epoch 152: Train Loss: 0.3361868053674698, Validation Loss: 0.5252726078033447\n",
      "Epoch 153: Train Loss: 0.36301225423812866, Validation Loss: 0.5191444754600525\n",
      "Epoch 154: Train Loss: 0.30788304209709166, Validation Loss: 0.5100045800209045\n",
      "Epoch 155: Train Loss: 0.34790138006210325, Validation Loss: 0.506337583065033\n",
      "Epoch 156: Train Loss: 0.3110142707824707, Validation Loss: 0.5068562626838684\n",
      "Epoch 157: Train Loss: 0.3147081911563873, Validation Loss: 0.5124592781066895\n",
      "Epoch 158: Train Loss: 0.38854289054870605, Validation Loss: 0.5100826621055603\n",
      "Epoch 159: Train Loss: 0.3671952188014984, Validation Loss: 0.5134750008583069\n",
      "Epoch 160: Train Loss: 0.3374911487102509, Validation Loss: 0.5247974395751953\n",
      "Epoch 161: Train Loss: 0.3537995100021362, Validation Loss: 0.5245096683502197\n",
      "Epoch 162: Train Loss: 0.2888693153858185, Validation Loss: 0.5246841907501221\n",
      "Epoch 163: Train Loss: 0.34433495104312895, Validation Loss: 0.5179382562637329\n",
      "Epoch 164: Train Loss: 0.30757946372032163, Validation Loss: 0.5173901915550232\n",
      "Epoch 165: Train Loss: 0.2826977640390396, Validation Loss: 0.510993242263794\n",
      "Epoch 166: Train Loss: 0.30957993268966677, Validation Loss: 0.5116738677024841\n",
      "Epoch 167: Train Loss: 0.29997077882289885, Validation Loss: 0.5112071633338928\n",
      "Epoch 168: Train Loss: 0.2847019612789154, Validation Loss: 0.5108752250671387\n",
      "Epoch 169: Train Loss: 0.3141085386276245, Validation Loss: 0.5097977519035339\n",
      "Epoch 170: Train Loss: 0.319090136885643, Validation Loss: 0.5062232613563538\n",
      "Epoch 171: Train Loss: 0.33014513552188873, Validation Loss: 0.5017886757850647\n",
      "Epoch 172: Train Loss: 0.2729446142911911, Validation Loss: 0.5011764168739319\n",
      "Epoch 173: Train Loss: 0.3244328200817108, Validation Loss: 0.4866655170917511\n",
      "Epoch 174: Train Loss: 0.2882043421268463, Validation Loss: 0.47777411341667175\n",
      "Epoch 175: Train Loss: 0.28068341612815856, Validation Loss: 0.47651273012161255\n",
      "Epoch 176: Train Loss: 0.2897296786308289, Validation Loss: 0.4898146390914917\n",
      "Epoch 177: Train Loss: 0.2282722234725952, Validation Loss: 0.48541268706321716\n",
      "Epoch 178: Train Loss: 0.26040531098842623, Validation Loss: 0.47657692432403564\n",
      "Epoch 179: Train Loss: 0.25384650826454164, Validation Loss: 0.4828394651412964\n",
      "Epoch 180: Train Loss: 0.31148573458194734, Validation Loss: 0.4735332429409027\n",
      "Epoch 181: Train Loss: 0.23718400299549103, Validation Loss: 0.47883182764053345\n",
      "Epoch 182: Train Loss: 0.27873300909996035, Validation Loss: 0.4913432002067566\n",
      "Epoch 183: Train Loss: 0.23101958334445954, Validation Loss: 0.49688541889190674\n",
      "Epoch 184: Train Loss: 0.22635380029678345, Validation Loss: 0.5019072890281677\n",
      "Epoch 185: Train Loss: 0.21506804525852202, Validation Loss: 0.49903404712677\n",
      "Epoch 186: Train Loss: 0.21735121607780455, Validation Loss: 0.49421900510787964\n",
      "Epoch 187: Train Loss: 0.2172981321811676, Validation Loss: 0.4959118068218231\n",
      "Epoch 188: Train Loss: 0.2278119295835495, Validation Loss: 0.5048726201057434\n",
      "Epoch 189: Train Loss: 0.21932510733604432, Validation Loss: 0.5481735467910767\n",
      "Epoch 190: Train Loss: 0.2593019664287567, Validation Loss: 0.5873367786407471\n",
      "Epoch 191: Train Loss: 0.26422969698905946, Validation Loss: 0.5326816439628601\n",
      "Epoch 192: Train Loss: 0.2729834794998169, Validation Loss: 0.5551580190658569\n",
      "Epoch 193: Train Loss: 0.20241667181253434, Validation Loss: 0.5930047035217285\n",
      "Epoch 194: Train Loss: 0.23124670684337617, Validation Loss: 0.5801499485969543\n",
      "Epoch 195: Train Loss: 0.22054096758365632, Validation Loss: 0.5570929646492004\n",
      "Epoch 196: Train Loss: 0.21111803352832795, Validation Loss: 0.5921463966369629\n",
      "Epoch 197: Train Loss: 0.28893150091171266, Validation Loss: 0.5912858247756958\n",
      "Epoch 198: Train Loss: 0.2682074189186096, Validation Loss: 0.5412631034851074\n",
      "Epoch 199: Train Loss: 0.26412617564201357, Validation Loss: 0.5607967376708984\n",
      "Fold 7 Metrics:\n",
      "Accuracy: 0.6363636363636364, Precision: 0.75, Recall: 0.5, F1-score: 0.6, AUC: 0.6500000000000001\n",
      "Confusion Matrix:\n",
      "[[4 1]\n",
      " [3 3]]\n",
      "Completed fold 7\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples from subject 9 to test set\n",
      "Adding 6 truth samples from subject 9 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.7296681523323059, Validation Loss: 0.7146883010864258\n",
      "Epoch 1: Train Loss: 0.721018934249878, Validation Loss: 0.7197116613388062\n",
      "Epoch 2: Train Loss: 0.6858843803405762, Validation Loss: 0.7270112037658691\n",
      "Epoch 3: Train Loss: 0.7069640040397644, Validation Loss: 0.7289063930511475\n",
      "Epoch 4: Train Loss: 0.6853199124336242, Validation Loss: 0.7270730137825012\n",
      "Epoch 5: Train Loss: 0.6827606558799744, Validation Loss: 0.7290372848510742\n",
      "Epoch 6: Train Loss: 0.7113144636154175, Validation Loss: 0.7305886745452881\n",
      "Epoch 7: Train Loss: 0.6985451936721802, Validation Loss: 0.7310420274734497\n",
      "Epoch 8: Train Loss: 0.6516807198524475, Validation Loss: 0.730744481086731\n",
      "Epoch 9: Train Loss: 0.7151573777198792, Validation Loss: 0.7317813038825989\n",
      "Epoch 10: Train Loss: 0.6438732504844665, Validation Loss: 0.7289360165596008\n",
      "Epoch 11: Train Loss: 0.6528527021408081, Validation Loss: 0.7341403365135193\n",
      "Epoch 12: Train Loss: 0.6810095429420471, Validation Loss: 0.7338439226150513\n",
      "Epoch 13: Train Loss: 0.7078601598739624, Validation Loss: 0.7344626784324646\n",
      "Epoch 14: Train Loss: 0.6658796668052673, Validation Loss: 0.7328708171844482\n",
      "Epoch 15: Train Loss: 0.6497340798377991, Validation Loss: 0.7315261960029602\n",
      "Epoch 16: Train Loss: 0.6623559594154358, Validation Loss: 0.7305415868759155\n",
      "Epoch 17: Train Loss: 0.6032248318195343, Validation Loss: 0.7311801314353943\n",
      "Epoch 18: Train Loss: 0.6686269879341126, Validation Loss: 0.732194721698761\n",
      "Epoch 19: Train Loss: 0.7226815462112427, Validation Loss: 0.7323628664016724\n",
      "Epoch 20: Train Loss: 0.6721733927726745, Validation Loss: 0.7321898937225342\n",
      "Epoch 21: Train Loss: 0.6563032746315003, Validation Loss: 0.7388294339179993\n",
      "Epoch 22: Train Loss: 0.6309903025627136, Validation Loss: 0.7436597347259521\n",
      "Epoch 23: Train Loss: 0.6782673835754395, Validation Loss: 0.7489672899246216\n",
      "Epoch 24: Train Loss: 0.5974366545677186, Validation Loss: 0.7480387687683105\n",
      "Epoch 25: Train Loss: 0.6425139307975769, Validation Loss: 0.7420938014984131\n",
      "Epoch 26: Train Loss: 0.6524551153182984, Validation Loss: 0.7380568385124207\n",
      "Epoch 27: Train Loss: 0.5801950871944428, Validation Loss: 0.7365004420280457\n",
      "Epoch 28: Train Loss: 0.6532093644142151, Validation Loss: 0.7364642024040222\n",
      "Epoch 29: Train Loss: 0.5915514171123505, Validation Loss: 0.7357978224754333\n",
      "Epoch 30: Train Loss: 0.6391875267028808, Validation Loss: 0.7297284603118896\n",
      "Epoch 31: Train Loss: 0.6807749509811402, Validation Loss: 0.7179999351501465\n",
      "Epoch 32: Train Loss: 0.6352729678153992, Validation Loss: 0.7241109013557434\n",
      "Epoch 33: Train Loss: 0.583936733007431, Validation Loss: 0.7293092608451843\n",
      "Epoch 34: Train Loss: 0.5954279065132141, Validation Loss: 0.7326754927635193\n",
      "Epoch 35: Train Loss: 0.6184573411941529, Validation Loss: 0.7340494990348816\n",
      "Epoch 36: Train Loss: 0.5833358645439148, Validation Loss: 0.7352297902107239\n",
      "Epoch 37: Train Loss: 0.6308601021766662, Validation Loss: 0.7363765835762024\n",
      "Epoch 38: Train Loss: 0.5510006010532379, Validation Loss: 0.7374162077903748\n",
      "Epoch 39: Train Loss: 0.6691333413124084, Validation Loss: 0.7378103137016296\n",
      "Epoch 40: Train Loss: 0.6209511280059814, Validation Loss: 0.7406415343284607\n",
      "Epoch 41: Train Loss: 0.6437897682189941, Validation Loss: 0.7403596043586731\n",
      "Epoch 42: Train Loss: 0.6102646470069886, Validation Loss: 0.738979697227478\n",
      "Epoch 43: Train Loss: 0.6922968983650207, Validation Loss: 0.738793671131134\n",
      "Epoch 44: Train Loss: 0.6054559230804444, Validation Loss: 0.7403417229652405\n",
      "Epoch 45: Train Loss: 0.5836722791194916, Validation Loss: 0.7405202388763428\n",
      "Epoch 46: Train Loss: 0.6092426657676697, Validation Loss: 0.7439685463905334\n",
      "Epoch 47: Train Loss: 0.5821523427963257, Validation Loss: 0.747857391834259\n",
      "Epoch 48: Train Loss: 0.5991161823272705, Validation Loss: 0.7489538192749023\n",
      "Epoch 49: Train Loss: 0.5871398687362671, Validation Loss: 0.7495540380477905\n",
      "Epoch 50: Train Loss: 0.6003970623016357, Validation Loss: 0.7520759105682373\n",
      "Epoch 51: Train Loss: 0.6041141867637634, Validation Loss: 0.7491740584373474\n",
      "Epoch 52: Train Loss: 0.5488630533218384, Validation Loss: 0.7380792498588562\n",
      "Epoch 53: Train Loss: 0.5999202728271484, Validation Loss: 0.7277864217758179\n",
      "Epoch 54: Train Loss: 0.5961566925048828, Validation Loss: 0.7173292636871338\n",
      "Epoch 55: Train Loss: 0.59651620388031, Validation Loss: 0.7151111364364624\n",
      "Epoch 56: Train Loss: 0.545961570739746, Validation Loss: 0.716780424118042\n",
      "Epoch 57: Train Loss: 0.5857860803604126, Validation Loss: 0.7192081212997437\n",
      "Epoch 58: Train Loss: 0.6154567003250122, Validation Loss: 0.7202188372612\n",
      "Epoch 59: Train Loss: 0.5517550528049469, Validation Loss: 0.7206206917762756\n",
      "Epoch 60: Train Loss: 0.5547357141971588, Validation Loss: 0.7278556227684021\n",
      "Epoch 61: Train Loss: 0.5470821619033813, Validation Loss: 0.7281410098075867\n",
      "Epoch 62: Train Loss: 0.533169549703598, Validation Loss: 0.7231590151786804\n",
      "Epoch 63: Train Loss: 0.5537185430526733, Validation Loss: 0.7272742390632629\n",
      "Epoch 64: Train Loss: 0.5394847571849823, Validation Loss: 0.7320420145988464\n",
      "Epoch 65: Train Loss: 0.5732661724090576, Validation Loss: 0.7316091060638428\n",
      "Epoch 66: Train Loss: 0.5629753470420837, Validation Loss: 0.7325908541679382\n",
      "Epoch 67: Train Loss: 0.5685258507728577, Validation Loss: 0.7293202877044678\n",
      "Epoch 68: Train Loss: 0.5503216981887817, Validation Loss: 0.7293113470077515\n",
      "Epoch 69: Train Loss: 0.5794427514076232, Validation Loss: 0.7294778823852539\n",
      "Epoch 70: Train Loss: 0.5568448901176453, Validation Loss: 0.7418729066848755\n",
      "Epoch 71: Train Loss: 0.5882094204425812, Validation Loss: 0.7446621060371399\n",
      "Epoch 72: Train Loss: 0.5306031823158264, Validation Loss: 0.7365478277206421\n",
      "Epoch 73: Train Loss: 0.5205979645252228, Validation Loss: 0.7298738360404968\n",
      "Epoch 74: Train Loss: 0.5559153079986572, Validation Loss: 0.729052722454071\n",
      "Epoch 75: Train Loss: 0.53917555809021, Validation Loss: 0.7339562773704529\n",
      "Epoch 76: Train Loss: 0.49585933089256284, Validation Loss: 0.7421627044677734\n",
      "Epoch 77: Train Loss: 0.49833454489707946, Validation Loss: 0.7443385124206543\n",
      "Epoch 78: Train Loss: 0.5347667574882508, Validation Loss: 0.7447388768196106\n",
      "Epoch 79: Train Loss: 0.5371435403823852, Validation Loss: 0.744691789150238\n",
      "Epoch 80: Train Loss: 0.48377264142036436, Validation Loss: 0.7413191795349121\n",
      "Epoch 81: Train Loss: 0.5393062055110931, Validation Loss: 0.736194908618927\n",
      "Epoch 82: Train Loss: 0.5418496370315552, Validation Loss: 0.7388726472854614\n",
      "Epoch 83: Train Loss: 0.5100839793682098, Validation Loss: 0.7429942488670349\n",
      "Epoch 84: Train Loss: 0.48184001445770264, Validation Loss: 0.7474859356880188\n",
      "Epoch 85: Train Loss: 0.4904798030853271, Validation Loss: 0.7475631237030029\n",
      "Epoch 86: Train Loss: 0.5085142254829407, Validation Loss: 0.7474406361579895\n",
      "Epoch 87: Train Loss: 0.5274825155735016, Validation Loss: 0.7492026686668396\n",
      "Epoch 88: Train Loss: 0.485818886756897, Validation Loss: 0.7509726881980896\n",
      "Epoch 89: Train Loss: 0.5553746521472931, Validation Loss: 0.7515519261360168\n",
      "Epoch 90: Train Loss: 0.5295668780803681, Validation Loss: 0.7552659511566162\n",
      "Epoch 91: Train Loss: 0.5216568648815155, Validation Loss: 0.7470318675041199\n",
      "Epoch 92: Train Loss: 0.4948991298675537, Validation Loss: 0.7602707743644714\n",
      "Epoch 93: Train Loss: 0.46924378871917727, Validation Loss: 0.7721307873725891\n",
      "Epoch 94: Train Loss: 0.4857282698154449, Validation Loss: 0.7833133339881897\n",
      "Epoch 95: Train Loss: 0.4700560688972473, Validation Loss: 0.79072505235672\n",
      "Epoch 96: Train Loss: 0.4524933576583862, Validation Loss: 0.7928611040115356\n",
      "Epoch 97: Train Loss: 0.4780998587608337, Validation Loss: 0.7895889282226562\n",
      "Epoch 98: Train Loss: 0.4925933003425598, Validation Loss: 0.7892440557479858\n",
      "Epoch 99: Train Loss: 0.43934364914894103, Validation Loss: 0.7897162437438965\n",
      "Epoch 100: Train Loss: 0.4851818263530731, Validation Loss: 0.7713142037391663\n",
      "Epoch 101: Train Loss: 0.46585577726364136, Validation Loss: 0.7689124345779419\n",
      "Epoch 102: Train Loss: 0.43609994649887085, Validation Loss: 0.7771434187889099\n",
      "Epoch 103: Train Loss: 0.4432118356227875, Validation Loss: 0.7864893078804016\n",
      "Epoch 104: Train Loss: 0.5068518638610839, Validation Loss: 0.793428897857666\n",
      "Epoch 105: Train Loss: 0.4543012261390686, Validation Loss: 0.7918686866760254\n",
      "Epoch 106: Train Loss: 0.4330837488174438, Validation Loss: 0.78986656665802\n",
      "Epoch 107: Train Loss: 0.4348637700080872, Validation Loss: 0.7920306324958801\n",
      "Epoch 108: Train Loss: 0.430683308839798, Validation Loss: 0.7923440337181091\n",
      "Epoch 109: Train Loss: 0.40727804899215697, Validation Loss: 0.791102409362793\n",
      "Epoch 110: Train Loss: 0.47556103467941285, Validation Loss: 0.789905309677124\n",
      "Epoch 111: Train Loss: 0.4340516686439514, Validation Loss: 0.7831416726112366\n",
      "Epoch 112: Train Loss: 0.43144001960754397, Validation Loss: 0.7845918536186218\n",
      "Epoch 113: Train Loss: 0.3935595601797104, Validation Loss: 0.7869632840156555\n",
      "Epoch 114: Train Loss: 0.46689274311065676, Validation Loss: 0.7871286273002625\n",
      "Epoch 115: Train Loss: 0.42222451567649844, Validation Loss: 0.8000354170799255\n",
      "Epoch 116: Train Loss: 0.4215413749217987, Validation Loss: 0.7987281084060669\n",
      "Epoch 117: Train Loss: 0.45535662174224856, Validation Loss: 0.7962615489959717\n",
      "Epoch 118: Train Loss: 0.4406940400600433, Validation Loss: 0.7973087430000305\n",
      "Epoch 119: Train Loss: 0.43160867094993594, Validation Loss: 0.7988497018814087\n",
      "Epoch 120: Train Loss: 0.4125040888786316, Validation Loss: 0.8028947710990906\n",
      "Epoch 121: Train Loss: 0.4194892108440399, Validation Loss: 0.791867733001709\n",
      "Epoch 122: Train Loss: 0.4752551317214966, Validation Loss: 0.7881346940994263\n",
      "Epoch 123: Train Loss: 0.4382793724536896, Validation Loss: 0.8079978227615356\n",
      "Epoch 124: Train Loss: 0.4447076916694641, Validation Loss: 0.8289163708686829\n",
      "Epoch 125: Train Loss: 0.4224742233753204, Validation Loss: 0.837051510810852\n",
      "Epoch 126: Train Loss: 0.3748214662075043, Validation Loss: 0.8357275128364563\n",
      "Epoch 127: Train Loss: 0.45572246313095094, Validation Loss: 0.831260085105896\n",
      "Epoch 128: Train Loss: 0.3985357522964478, Validation Loss: 0.8291240930557251\n",
      "Epoch 129: Train Loss: 0.38495224714279175, Validation Loss: 0.8286492824554443\n",
      "Epoch 130: Train Loss: 0.3578671395778656, Validation Loss: 0.8076680898666382\n",
      "Epoch 131: Train Loss: 0.3659238576889038, Validation Loss: 0.8178887963294983\n",
      "Epoch 132: Train Loss: 0.38638541102409363, Validation Loss: 0.8375334739685059\n",
      "Epoch 133: Train Loss: 0.3990622222423553, Validation Loss: 0.843511164188385\n",
      "Epoch 134: Train Loss: 0.3678085207939148, Validation Loss: 0.876952588558197\n",
      "Epoch 135: Train Loss: 0.32845771312713623, Validation Loss: 0.8926690816879272\n",
      "Epoch 136: Train Loss: 0.38039037585258484, Validation Loss: 0.8901708126068115\n",
      "Epoch 137: Train Loss: 0.3565958857536316, Validation Loss: 0.882659375667572\n",
      "Epoch 138: Train Loss: 0.3726603388786316, Validation Loss: 0.8799012303352356\n",
      "Epoch 139: Train Loss: 0.38100472688674925, Validation Loss: 0.8806396126747131\n",
      "Epoch 140: Train Loss: 0.3903799533843994, Validation Loss: 0.8479353189468384\n",
      "Epoch 141: Train Loss: 0.41250957250595094, Validation Loss: 0.8116959929466248\n",
      "Epoch 142: Train Loss: 0.35124597549438474, Validation Loss: 0.8290786743164062\n",
      "Epoch 143: Train Loss: 0.35882648229599, Validation Loss: 0.8423394560813904\n",
      "Epoch 144: Train Loss: 0.39569448232650756, Validation Loss: 0.8414598107337952\n",
      "Epoch 145: Train Loss: 0.34989496469497683, Validation Loss: 0.8377423286437988\n",
      "Epoch 146: Train Loss: 0.3322103977203369, Validation Loss: 0.8524449467658997\n",
      "Epoch 147: Train Loss: 0.35355466306209565, Validation Loss: 0.8618791699409485\n",
      "Epoch 148: Train Loss: 0.33738515973091127, Validation Loss: 0.8695982098579407\n",
      "Epoch 149: Train Loss: 0.37643738389015197, Validation Loss: 0.8728960156440735\n",
      "Epoch 150: Train Loss: 0.2952184557914734, Validation Loss: 0.916248083114624\n",
      "Epoch 151: Train Loss: 0.3476050853729248, Validation Loss: 0.9316568374633789\n",
      "Epoch 152: Train Loss: 0.3401746034622192, Validation Loss: 0.9006537795066833\n",
      "Epoch 153: Train Loss: 0.28997347354888914, Validation Loss: 0.8827482461929321\n",
      "Epoch 154: Train Loss: 0.3799977838993073, Validation Loss: 0.9047647714614868\n",
      "Epoch 155: Train Loss: 0.31689565181732177, Validation Loss: 0.9294873476028442\n",
      "Epoch 156: Train Loss: 0.32462084889411924, Validation Loss: 0.9338027834892273\n",
      "Epoch 157: Train Loss: 0.32466940879821776, Validation Loss: 0.9298030138015747\n",
      "Epoch 158: Train Loss: 0.3223548322916031, Validation Loss: 0.9254164099693298\n",
      "Epoch 159: Train Loss: 0.3204162508249283, Validation Loss: 0.9220092296600342\n",
      "Epoch 160: Train Loss: 0.2885288387537003, Validation Loss: 0.9145861864089966\n",
      "Epoch 161: Train Loss: 0.35194400548934934, Validation Loss: 0.9182008504867554\n",
      "Epoch 162: Train Loss: 0.32269100546836854, Validation Loss: 0.883633553981781\n",
      "Epoch 163: Train Loss: 0.33218862414360045, Validation Loss: 0.878875195980072\n",
      "Epoch 164: Train Loss: 0.32355242371559145, Validation Loss: 0.8759856820106506\n",
      "Epoch 165: Train Loss: 0.2959262251853943, Validation Loss: 0.8664880394935608\n",
      "Epoch 166: Train Loss: 0.3425700068473816, Validation Loss: 0.8643701672554016\n",
      "Epoch 167: Train Loss: 0.31171936392784116, Validation Loss: 0.8646008968353271\n",
      "Epoch 168: Train Loss: 0.3853574812412262, Validation Loss: 0.8667460680007935\n",
      "Epoch 169: Train Loss: 0.3360564410686493, Validation Loss: 0.8695977330207825\n",
      "Epoch 170: Train Loss: 0.30482906103134155, Validation Loss: 0.9400813579559326\n",
      "Epoch 171: Train Loss: 0.336142760515213, Validation Loss: 0.9587879180908203\n",
      "Epoch 172: Train Loss: 0.2866306006908417, Validation Loss: 0.9022489786148071\n",
      "Epoch 173: Train Loss: 0.2728341668844223, Validation Loss: 0.8721129894256592\n",
      "Epoch 174: Train Loss: 0.3069665789604187, Validation Loss: 0.8882217407226562\n",
      "Epoch 175: Train Loss: 0.2708170831203461, Validation Loss: 0.9108064770698547\n",
      "Epoch 176: Train Loss: 0.29438010454177854, Validation Loss: 0.9406492710113525\n",
      "Epoch 177: Train Loss: 0.300358510017395, Validation Loss: 0.9607135653495789\n",
      "Epoch 178: Train Loss: 0.3126997917890549, Validation Loss: 0.9629619717597961\n",
      "Epoch 179: Train Loss: 0.32741091549396517, Validation Loss: 0.9662527441978455\n",
      "Epoch 180: Train Loss: 0.32431186735630035, Validation Loss: 0.9471132159233093\n",
      "Epoch 181: Train Loss: 0.2760313719511032, Validation Loss: 0.9421152472496033\n",
      "Epoch 182: Train Loss: 0.32111613154411317, Validation Loss: 0.9601694345474243\n",
      "Epoch 183: Train Loss: 0.2638867884874344, Validation Loss: 0.9686233997344971\n",
      "Epoch 184: Train Loss: 0.3174514681100845, Validation Loss: 1.0102121829986572\n",
      "Epoch 185: Train Loss: 0.2534554660320282, Validation Loss: 1.0267844200134277\n",
      "Epoch 186: Train Loss: 0.2543332904577255, Validation Loss: 1.0258959531784058\n",
      "Epoch 187: Train Loss: 0.2713691174983978, Validation Loss: 1.031891942024231\n",
      "Epoch 188: Train Loss: 0.25592151284217834, Validation Loss: 1.0360705852508545\n",
      "Epoch 189: Train Loss: 0.20475836396217345, Validation Loss: 1.0363764762878418\n",
      "Epoch 190: Train Loss: 0.2352233350276947, Validation Loss: 1.0239675045013428\n",
      "Epoch 191: Train Loss: 0.26596914380788805, Validation Loss: 1.0319175720214844\n",
      "Epoch 192: Train Loss: 0.23674336075782776, Validation Loss: 1.0592204332351685\n",
      "Epoch 193: Train Loss: 0.31691493690013883, Validation Loss: 1.0345937013626099\n",
      "Epoch 194: Train Loss: 0.27153016328811647, Validation Loss: 1.022359013557434\n",
      "Epoch 195: Train Loss: 0.22902256846427918, Validation Loss: 1.017797827720642\n",
      "Epoch 196: Train Loss: 0.25333704650402067, Validation Loss: 1.0452697277069092\n",
      "Epoch 197: Train Loss: 0.28169087767601014, Validation Loss: 1.058370590209961\n",
      "Epoch 198: Train Loss: 0.23466707468032838, Validation Loss: 1.059425711631775\n",
      "Epoch 199: Train Loss: 0.2397963136434555, Validation Loss: 1.0553334951400757\n",
      "Fold 8 Metrics:\n",
      "Accuracy: 0.45454545454545453, Precision: 0.5, Recall: 0.8333333333333334, F1-score: 0.625, AUC: 0.4166666666666667\n",
      "Confusion Matrix:\n",
      "[[0 5]\n",
      " [1 5]]\n",
      "Completed fold 8\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples from subject 4 to test set\n",
      "Adding 6 truth samples from subject 4 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.7433770179748536, Validation Loss: 0.6916094422340393\n",
      "Epoch 1: Train Loss: 0.7375926375389099, Validation Loss: 0.6949602365493774\n",
      "Epoch 2: Train Loss: 0.7497193574905395, Validation Loss: 0.6989099979400635\n",
      "Epoch 3: Train Loss: 0.727631151676178, Validation Loss: 0.704077422618866\n",
      "Epoch 4: Train Loss: 0.7089902877807617, Validation Loss: 0.7103923559188843\n",
      "Epoch 5: Train Loss: 0.7064481258392334, Validation Loss: 0.7137088775634766\n",
      "Epoch 6: Train Loss: 0.6972225069999695, Validation Loss: 0.7154728174209595\n",
      "Epoch 7: Train Loss: 0.6681892156600953, Validation Loss: 0.7154868841171265\n",
      "Epoch 8: Train Loss: 0.6720790982246398, Validation Loss: 0.7171375155448914\n",
      "Epoch 9: Train Loss: 0.7413094878196717, Validation Loss: 0.7177306413650513\n",
      "Epoch 10: Train Loss: 0.6747526049613952, Validation Loss: 0.718063473701477\n",
      "Epoch 11: Train Loss: 0.68531813621521, Validation Loss: 0.7211838960647583\n",
      "Epoch 12: Train Loss: 0.6935850858688355, Validation Loss: 0.7243549227714539\n",
      "Epoch 13: Train Loss: 0.6861333608627319, Validation Loss: 0.7309970259666443\n",
      "Epoch 14: Train Loss: 0.6583586454391479, Validation Loss: 0.7367361187934875\n",
      "Epoch 15: Train Loss: 0.6652376592159271, Validation Loss: 0.7396178841590881\n",
      "Epoch 16: Train Loss: 0.6994755148887635, Validation Loss: 0.7402957081794739\n",
      "Epoch 17: Train Loss: 0.6734634637832642, Validation Loss: 0.7411489486694336\n",
      "Epoch 18: Train Loss: 0.6514603853225708, Validation Loss: 0.7406322360038757\n",
      "Epoch 19: Train Loss: 0.6478672027587891, Validation Loss: 0.7417247891426086\n",
      "Epoch 20: Train Loss: 0.6200526595115662, Validation Loss: 0.7376385927200317\n",
      "Epoch 21: Train Loss: 0.6466940760612487, Validation Loss: 0.7391507029533386\n",
      "Epoch 22: Train Loss: 0.6919992446899415, Validation Loss: 0.7430009245872498\n",
      "Epoch 23: Train Loss: 0.6615094661712646, Validation Loss: 0.7446997761726379\n",
      "Epoch 24: Train Loss: 0.659762978553772, Validation Loss: 0.7447633743286133\n",
      "Epoch 25: Train Loss: 0.6097486972808838, Validation Loss: 0.7425680160522461\n",
      "Epoch 26: Train Loss: 0.6440173387527466, Validation Loss: 0.743880033493042\n",
      "Epoch 27: Train Loss: 0.6100255489349365, Validation Loss: 0.7442085146903992\n",
      "Epoch 28: Train Loss: 0.6661207914352417, Validation Loss: 0.7446816563606262\n",
      "Epoch 29: Train Loss: 0.6184084057807923, Validation Loss: 0.7458216547966003\n",
      "Epoch 30: Train Loss: 0.6045934796333313, Validation Loss: 0.7450084090232849\n",
      "Epoch 31: Train Loss: 0.607736599445343, Validation Loss: 0.7433432936668396\n",
      "Epoch 32: Train Loss: 0.611446762084961, Validation Loss: 0.7459676861763\n",
      "Epoch 33: Train Loss: 0.6362639546394349, Validation Loss: 0.7452839016914368\n",
      "Epoch 34: Train Loss: 0.5978180527687073, Validation Loss: 0.7471970915794373\n",
      "Epoch 35: Train Loss: 0.6092909336090088, Validation Loss: 0.7474400997161865\n",
      "Epoch 36: Train Loss: 0.6514487028121948, Validation Loss: 0.7490878105163574\n",
      "Epoch 37: Train Loss: 0.6348081350326538, Validation Loss: 0.749481737613678\n",
      "Epoch 38: Train Loss: 0.6399344563484192, Validation Loss: 0.7497190237045288\n",
      "Epoch 39: Train Loss: 0.5865357518196106, Validation Loss: 0.7498923540115356\n",
      "Epoch 40: Train Loss: 0.6139915108680725, Validation Loss: 0.7495128512382507\n",
      "Epoch 41: Train Loss: 0.6165873765945434, Validation Loss: 0.7527179718017578\n",
      "Epoch 42: Train Loss: 0.6252027869224548, Validation Loss: 0.7525860667228699\n",
      "Epoch 43: Train Loss: 0.6436613917350769, Validation Loss: 0.7565749287605286\n",
      "Epoch 44: Train Loss: 0.6017082810401917, Validation Loss: 0.7593464851379395\n",
      "Epoch 45: Train Loss: 0.5838135302066803, Validation Loss: 0.7603440880775452\n",
      "Epoch 46: Train Loss: 0.6552409410476685, Validation Loss: 0.7597636580467224\n",
      "Epoch 47: Train Loss: 0.5861133337020874, Validation Loss: 0.7576055526733398\n",
      "Epoch 48: Train Loss: 0.6025243282318116, Validation Loss: 0.7578271627426147\n",
      "Epoch 49: Train Loss: 0.655241072177887, Validation Loss: 0.7581426501274109\n",
      "Epoch 50: Train Loss: 0.5959912896156311, Validation Loss: 0.7627679705619812\n",
      "Epoch 51: Train Loss: 0.5725103259086609, Validation Loss: 0.7662429213523865\n",
      "Epoch 52: Train Loss: 0.5978700041770935, Validation Loss: 0.7716166973114014\n",
      "Epoch 53: Train Loss: 0.5833924412727356, Validation Loss: 0.7726804614067078\n",
      "Epoch 54: Train Loss: 0.5622168898582458, Validation Loss: 0.7729377150535583\n",
      "Epoch 55: Train Loss: 0.6060705661773682, Validation Loss: 0.77293461561203\n",
      "Epoch 56: Train Loss: 0.6026503801345825, Validation Loss: 0.7732692360877991\n",
      "Epoch 57: Train Loss: 0.539076691865921, Validation Loss: 0.7674189805984497\n",
      "Epoch 58: Train Loss: 0.576052212715149, Validation Loss: 0.770064651966095\n",
      "Epoch 59: Train Loss: 0.5562334597110749, Validation Loss: 0.7700098156929016\n",
      "Epoch 60: Train Loss: 0.5499205529689789, Validation Loss: 0.7705176472663879\n",
      "Epoch 61: Train Loss: 0.5721582651138306, Validation Loss: 0.7723593711853027\n",
      "Epoch 62: Train Loss: 0.5922553896903991, Validation Loss: 0.7743045091629028\n",
      "Epoch 63: Train Loss: 0.529474002122879, Validation Loss: 0.7748721837997437\n",
      "Epoch 64: Train Loss: 0.5404201984405518, Validation Loss: 0.7775101661682129\n",
      "Epoch 65: Train Loss: 0.5516659021377563, Validation Loss: 0.7805762887001038\n",
      "Epoch 66: Train Loss: 0.5239831149578095, Validation Loss: 0.7824686169624329\n",
      "Epoch 67: Train Loss: 0.5146688163280487, Validation Loss: 0.7842694520950317\n",
      "Epoch 68: Train Loss: 0.5002478182315826, Validation Loss: 0.7849197387695312\n",
      "Epoch 69: Train Loss: 0.5773838996887207, Validation Loss: 0.7834062576293945\n",
      "Epoch 70: Train Loss: 0.5417951822280884, Validation Loss: 0.7805467247962952\n",
      "Epoch 71: Train Loss: 0.549991661310196, Validation Loss: 0.7853425145149231\n",
      "Epoch 72: Train Loss: 0.5186675310134887, Validation Loss: 0.7909995317459106\n",
      "Epoch 73: Train Loss: 0.49340683221817017, Validation Loss: 0.8001385927200317\n",
      "Epoch 74: Train Loss: 0.5121856033802032, Validation Loss: 0.804003894329071\n",
      "Epoch 75: Train Loss: 0.5790855526924134, Validation Loss: 0.8058511018753052\n",
      "Epoch 76: Train Loss: 0.49379300475120547, Validation Loss: 0.8099275231361389\n",
      "Epoch 77: Train Loss: 0.5074802219867707, Validation Loss: 0.8116163015365601\n",
      "Epoch 78: Train Loss: 0.48694871068000795, Validation Loss: 0.8138734698295593\n",
      "Epoch 79: Train Loss: 0.5347367525100708, Validation Loss: 0.8161054253578186\n",
      "Epoch 80: Train Loss: 0.545726203918457, Validation Loss: 0.821918785572052\n",
      "Epoch 81: Train Loss: 0.4838405311107635, Validation Loss: 0.8359951972961426\n",
      "Epoch 82: Train Loss: 0.5066578805446624, Validation Loss: 0.8451289534568787\n",
      "Epoch 83: Train Loss: 0.5124123156070709, Validation Loss: 0.860745370388031\n",
      "Epoch 84: Train Loss: 0.5075054228305816, Validation Loss: 0.8758383989334106\n",
      "Epoch 85: Train Loss: 0.47018809914588927, Validation Loss: 0.8839244842529297\n",
      "Epoch 86: Train Loss: 0.4490161120891571, Validation Loss: 0.9031313061714172\n",
      "Epoch 87: Train Loss: 0.47277693152427674, Validation Loss: 0.9114245772361755\n",
      "Epoch 88: Train Loss: 0.4220123618841171, Validation Loss: 0.9145625829696655\n",
      "Epoch 89: Train Loss: 0.4546105206012726, Validation Loss: 0.9196295142173767\n",
      "Epoch 90: Train Loss: 0.5235198855400085, Validation Loss: 0.9489855170249939\n",
      "Epoch 91: Train Loss: 0.4887765467166901, Validation Loss: 0.954886794090271\n",
      "Epoch 92: Train Loss: 0.4605586051940918, Validation Loss: 0.9627878069877625\n",
      "Epoch 93: Train Loss: 0.49683486819267275, Validation Loss: 0.9738898873329163\n",
      "Epoch 94: Train Loss: 0.41553770899772646, Validation Loss: 0.9725647568702698\n",
      "Epoch 95: Train Loss: 0.46476494073867797, Validation Loss: 0.9930025339126587\n",
      "Epoch 96: Train Loss: 0.4381053149700165, Validation Loss: 1.0054882764816284\n",
      "Epoch 97: Train Loss: 0.427957284450531, Validation Loss: 1.0112181901931763\n",
      "Epoch 98: Train Loss: 0.41479021310806274, Validation Loss: 1.0114842653274536\n",
      "Epoch 99: Train Loss: 0.39482973217964173, Validation Loss: 1.0160208940505981\n",
      "Epoch 100: Train Loss: 0.4370493829250336, Validation Loss: 1.0503884553909302\n",
      "Epoch 101: Train Loss: 0.36108187437057493, Validation Loss: 1.0862282514572144\n",
      "Epoch 102: Train Loss: 0.35591243505477904, Validation Loss: 1.1309071779251099\n",
      "Epoch 103: Train Loss: 0.37613859176635744, Validation Loss: 1.1698541641235352\n",
      "Epoch 104: Train Loss: 0.3870509326457977, Validation Loss: 1.2003720998764038\n",
      "Epoch 105: Train Loss: 0.40655686855316164, Validation Loss: 1.2367044687271118\n",
      "Epoch 106: Train Loss: 0.44219434857368467, Validation Loss: 1.2643250226974487\n",
      "Epoch 107: Train Loss: 0.34945773482322695, Validation Loss: 1.2555094957351685\n",
      "Epoch 108: Train Loss: 0.3866083323955536, Validation Loss: 1.2559187412261963\n",
      "Epoch 109: Train Loss: 0.35098983645439147, Validation Loss: 1.2621182203292847\n",
      "Epoch 110: Train Loss: 0.3522934138774872, Validation Loss: 1.2977250814437866\n",
      "Epoch 111: Train Loss: 0.3560269415378571, Validation Loss: 1.3404905796051025\n",
      "Epoch 112: Train Loss: 0.36792114973068235, Validation Loss: 1.3632012605667114\n",
      "Epoch 113: Train Loss: 0.34942167401313784, Validation Loss: 1.3920844793319702\n",
      "Epoch 114: Train Loss: 0.33209410309791565, Validation Loss: 1.417890191078186\n",
      "Epoch 115: Train Loss: 0.31169080436229707, Validation Loss: 1.4022630453109741\n",
      "Epoch 116: Train Loss: 0.37421330213546755, Validation Loss: 1.4352970123291016\n",
      "Epoch 117: Train Loss: 0.4001322865486145, Validation Loss: 1.472360610961914\n",
      "Epoch 118: Train Loss: 0.323058956861496, Validation Loss: 1.4680646657943726\n",
      "Epoch 119: Train Loss: 0.3305910289287567, Validation Loss: 1.4604333639144897\n",
      "Epoch 120: Train Loss: 0.39960508942604067, Validation Loss: 1.5803065299987793\n",
      "Epoch 121: Train Loss: 0.33829694986343384, Validation Loss: 1.7370067834854126\n",
      "Epoch 122: Train Loss: 0.30054458379745486, Validation Loss: 1.7822322845458984\n",
      "Epoch 123: Train Loss: 0.29741708040237425, Validation Loss: 1.7805964946746826\n",
      "Epoch 124: Train Loss: 0.3509144902229309, Validation Loss: 1.7718136310577393\n",
      "Epoch 125: Train Loss: 0.3443209946155548, Validation Loss: 1.8124678134918213\n",
      "Epoch 126: Train Loss: 0.3347166210412979, Validation Loss: 1.814990520477295\n",
      "Epoch 127: Train Loss: 0.35974362790584563, Validation Loss: 1.7891349792480469\n",
      "Epoch 128: Train Loss: 0.32188905477523805, Validation Loss: 1.8309165239334106\n",
      "Epoch 129: Train Loss: 0.28979584872722625, Validation Loss: 1.8057619333267212\n",
      "Epoch 130: Train Loss: 0.3999706029891968, Validation Loss: 1.8281407356262207\n",
      "Epoch 131: Train Loss: 0.2998346209526062, Validation Loss: 1.8893227577209473\n",
      "Epoch 132: Train Loss: 0.3069590628147125, Validation Loss: 1.9010181427001953\n",
      "Epoch 133: Train Loss: 0.3286523163318634, Validation Loss: 1.9151504039764404\n",
      "Epoch 134: Train Loss: 0.27163860499858855, Validation Loss: 1.9421824216842651\n",
      "Epoch 135: Train Loss: 0.27425704300403597, Validation Loss: 1.9727721214294434\n",
      "Epoch 136: Train Loss: 0.33354329466819765, Validation Loss: 2.002877950668335\n",
      "Epoch 137: Train Loss: 0.2954147219657898, Validation Loss: 2.0254948139190674\n",
      "Epoch 138: Train Loss: 0.3028276115655899, Validation Loss: 2.000575304031372\n",
      "Epoch 139: Train Loss: 0.26057479083538054, Validation Loss: 2.031209945678711\n",
      "Epoch 140: Train Loss: 0.28100493252277375, Validation Loss: 2.0531060695648193\n",
      "Epoch 141: Train Loss: 0.26018117368221283, Validation Loss: 2.0205130577087402\n",
      "Epoch 142: Train Loss: 0.29318471252918243, Validation Loss: 2.0112054347991943\n",
      "Epoch 143: Train Loss: 0.27270529568195345, Validation Loss: 1.991227388381958\n",
      "Epoch 144: Train Loss: 0.210103028267622, Validation Loss: 1.9692721366882324\n",
      "Epoch 145: Train Loss: 0.2941254198551178, Validation Loss: 1.9832053184509277\n",
      "Epoch 146: Train Loss: 0.3201734095811844, Validation Loss: 2.0131828784942627\n",
      "Epoch 147: Train Loss: 0.22150869742035867, Validation Loss: 1.9954177141189575\n",
      "Epoch 148: Train Loss: 0.2468070089817047, Validation Loss: 2.024954080581665\n",
      "Epoch 149: Train Loss: 0.244308140873909, Validation Loss: 2.0387914180755615\n",
      "Epoch 150: Train Loss: 0.28067700266838075, Validation Loss: 2.0980474948883057\n",
      "Epoch 151: Train Loss: 0.27549631893634796, Validation Loss: 2.1512112617492676\n",
      "Epoch 152: Train Loss: 0.25798734426498415, Validation Loss: 2.1296708583831787\n",
      "Epoch 153: Train Loss: 0.27735104858875276, Validation Loss: 2.0836679935455322\n",
      "Epoch 154: Train Loss: 0.25239244699478147, Validation Loss: 2.1235687732696533\n",
      "Epoch 155: Train Loss: 0.3108109265565872, Validation Loss: 2.152031660079956\n",
      "Epoch 156: Train Loss: 0.24239911139011383, Validation Loss: 2.195061683654785\n",
      "Epoch 157: Train Loss: 0.24052201211452484, Validation Loss: 2.2341225147247314\n",
      "Epoch 158: Train Loss: 0.22241262793540956, Validation Loss: 2.202883005142212\n",
      "Epoch 159: Train Loss: 0.24824063777923583, Validation Loss: 2.179769515991211\n",
      "Epoch 160: Train Loss: 0.25818455815315244, Validation Loss: 2.278846502304077\n",
      "Epoch 161: Train Loss: 0.286586531996727, Validation Loss: 2.228407859802246\n",
      "Epoch 162: Train Loss: 0.24822098016738892, Validation Loss: 2.1392946243286133\n",
      "Epoch 163: Train Loss: 0.24939686059951782, Validation Loss: 2.159619092941284\n",
      "Epoch 164: Train Loss: 0.29107835292816164, Validation Loss: 2.2683603763580322\n",
      "Epoch 165: Train Loss: 0.3199364185333252, Validation Loss: 2.3428397178649902\n",
      "Epoch 166: Train Loss: 0.22753114998340607, Validation Loss: 2.380897045135498\n",
      "Epoch 167: Train Loss: 0.25981044173240664, Validation Loss: 2.4035801887512207\n",
      "Epoch 168: Train Loss: 0.22283340096473694, Validation Loss: 2.390909433364868\n",
      "Epoch 169: Train Loss: 0.2469406932592392, Validation Loss: 2.431955337524414\n",
      "Epoch 170: Train Loss: 0.18235913664102554, Validation Loss: 2.551215887069702\n",
      "Epoch 171: Train Loss: 0.22814365327358246, Validation Loss: 2.6715199947357178\n",
      "Epoch 172: Train Loss: 0.235415717959404, Validation Loss: 2.7815749645233154\n",
      "Epoch 173: Train Loss: 0.18666366785764693, Validation Loss: 2.8036537170410156\n",
      "Epoch 174: Train Loss: 0.2924783408641815, Validation Loss: 2.813535213470459\n",
      "Epoch 175: Train Loss: 0.22807727456092836, Validation Loss: 2.8093292713165283\n",
      "Epoch 176: Train Loss: 0.18115822076797486, Validation Loss: 2.789065361022949\n",
      "Epoch 177: Train Loss: 0.17335117757320403, Validation Loss: 2.8180110454559326\n",
      "Epoch 178: Train Loss: 0.27022424936294553, Validation Loss: 2.872760534286499\n",
      "Epoch 179: Train Loss: 0.16888283789157868, Validation Loss: 2.875769853591919\n",
      "Epoch 180: Train Loss: 0.22246630489826202, Validation Loss: 2.97363018989563\n",
      "Epoch 181: Train Loss: 0.20380084216594696, Validation Loss: 3.0329947471618652\n",
      "Epoch 182: Train Loss: 0.17023173570632935, Validation Loss: 3.061983108520508\n",
      "Epoch 183: Train Loss: 0.21749037206172944, Validation Loss: 2.981856346130371\n",
      "Epoch 184: Train Loss: 0.22025541067123414, Validation Loss: 2.9782073497772217\n",
      "Epoch 185: Train Loss: 0.22798071205615997, Validation Loss: 2.9512970447540283\n",
      "Epoch 186: Train Loss: 0.15949328541755675, Validation Loss: 2.8950557708740234\n",
      "Epoch 187: Train Loss: 0.17988502681255342, Validation Loss: 2.9019887447357178\n",
      "Epoch 188: Train Loss: 0.1852951690554619, Validation Loss: 2.9414989948272705\n",
      "Epoch 189: Train Loss: 0.1653728812932968, Validation Loss: 2.9090511798858643\n",
      "Epoch 190: Train Loss: 0.24649449288845063, Validation Loss: 2.973952054977417\n",
      "Epoch 191: Train Loss: 0.1883607268333435, Validation Loss: 2.9647300243377686\n",
      "Epoch 192: Train Loss: 0.19119516313076018, Validation Loss: 3.037041187286377\n",
      "Epoch 193: Train Loss: 0.27286378741264344, Validation Loss: 3.140998125076294\n",
      "Epoch 194: Train Loss: 0.14943283498287202, Validation Loss: 3.1785221099853516\n",
      "Epoch 195: Train Loss: 0.24728352427482606, Validation Loss: 3.1462562084198\n",
      "Epoch 196: Train Loss: 0.14728204980492593, Validation Loss: 3.0564253330230713\n",
      "Epoch 197: Train Loss: 0.17635473012924194, Validation Loss: 3.0275092124938965\n",
      "Epoch 198: Train Loss: 0.14258971363306044, Validation Loss: 3.0706145763397217\n",
      "Epoch 199: Train Loss: 0.3126910120248795, Validation Loss: 3.1098663806915283\n",
      "Fold 9 Metrics:\n",
      "Accuracy: 0.18181818181818182, Precision: 0.0, Recall: 0.0, F1-score: 0.0, AUC: 0.2\n",
      "Confusion Matrix:\n",
      "[[2 3]\n",
      " [6 0]]\n",
      "Completed fold 9\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples from subject 5 to test set\n",
      "Adding 6 truth samples from subject 5 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.6987921237945557, Validation Loss: 0.702927827835083\n",
      "Epoch 1: Train Loss: 0.7977140188217163, Validation Loss: 0.7050254344940186\n",
      "Epoch 2: Train Loss: 0.7961682081222534, Validation Loss: 0.7062767148017883\n",
      "Epoch 3: Train Loss: 0.7308110475540162, Validation Loss: 0.7084235548973083\n",
      "Epoch 4: Train Loss: 0.7147534132003784, Validation Loss: 0.7104523181915283\n",
      "Epoch 5: Train Loss: 0.7020824193954468, Validation Loss: 0.7105139493942261\n",
      "Epoch 6: Train Loss: 0.6805288791656494, Validation Loss: 0.71052485704422\n",
      "Epoch 7: Train Loss: 0.7357665419578552, Validation Loss: 0.7107361555099487\n",
      "Epoch 8: Train Loss: 0.7127554774284363, Validation Loss: 0.7110299468040466\n",
      "Epoch 9: Train Loss: 0.6957344532012939, Validation Loss: 0.7112502455711365\n",
      "Epoch 10: Train Loss: 0.7216965913772583, Validation Loss: 0.7081463932991028\n",
      "Epoch 11: Train Loss: 0.710916006565094, Validation Loss: 0.7046837210655212\n",
      "Epoch 12: Train Loss: 0.6593018054962159, Validation Loss: 0.7020475268363953\n",
      "Epoch 13: Train Loss: 0.6934437394142151, Validation Loss: 0.6994456052780151\n",
      "Epoch 14: Train Loss: 0.706141471862793, Validation Loss: 0.6979525685310364\n",
      "Epoch 15: Train Loss: 0.7164408087730407, Validation Loss: 0.6962279081344604\n",
      "Epoch 16: Train Loss: 0.6775145173072815, Validation Loss: 0.6948505640029907\n",
      "Epoch 17: Train Loss: 0.6889511704444885, Validation Loss: 0.6943575739860535\n",
      "Epoch 18: Train Loss: 0.6670297384262085, Validation Loss: 0.6946415901184082\n",
      "Epoch 19: Train Loss: 0.7478532075881958, Validation Loss: 0.6948412656784058\n",
      "Epoch 20: Train Loss: 0.6874538779258728, Validation Loss: 0.694754421710968\n",
      "Epoch 21: Train Loss: 0.6590254068374634, Validation Loss: 0.6943339705467224\n",
      "Epoch 22: Train Loss: 0.6891076326370239, Validation Loss: 0.6936660408973694\n",
      "Epoch 23: Train Loss: 0.6395382165908814, Validation Loss: 0.6940205097198486\n",
      "Epoch 24: Train Loss: 0.6468431711196899, Validation Loss: 0.6945843696594238\n",
      "Epoch 25: Train Loss: 0.6402955651283264, Validation Loss: 0.6936511993408203\n",
      "Epoch 26: Train Loss: 0.67984699010849, Validation Loss: 0.6934455037117004\n",
      "Epoch 27: Train Loss: 0.6283894658088685, Validation Loss: 0.6932578682899475\n",
      "Epoch 28: Train Loss: 0.629250991344452, Validation Loss: 0.6929219961166382\n",
      "Epoch 29: Train Loss: 0.648755818605423, Validation Loss: 0.6932272911071777\n",
      "Epoch 30: Train Loss: 0.6452650189399719, Validation Loss: 0.6906356811523438\n",
      "Epoch 31: Train Loss: 0.6498581171035767, Validation Loss: 0.6900118589401245\n",
      "Epoch 32: Train Loss: 0.6564493536949157, Validation Loss: 0.6899282336235046\n",
      "Epoch 33: Train Loss: 0.6188846349716186, Validation Loss: 0.6884589195251465\n",
      "Epoch 34: Train Loss: 0.6242400527000427, Validation Loss: 0.6875138878822327\n",
      "Epoch 35: Train Loss: 0.6782496213912964, Validation Loss: 0.6881455183029175\n",
      "Epoch 36: Train Loss: 0.6905051231384277, Validation Loss: 0.6880842447280884\n",
      "Epoch 37: Train Loss: 0.6485947370529175, Validation Loss: 0.6877040266990662\n",
      "Epoch 38: Train Loss: 0.6269046545028687, Validation Loss: 0.6875393390655518\n",
      "Epoch 39: Train Loss: 0.6537443399429321, Validation Loss: 0.6872513890266418\n",
      "Epoch 40: Train Loss: 0.6674671769142151, Validation Loss: 0.685135543346405\n",
      "Epoch 41: Train Loss: 0.6349371552467347, Validation Loss: 0.6838564276695251\n",
      "Epoch 42: Train Loss: 0.6463336229324341, Validation Loss: 0.6861539483070374\n",
      "Epoch 43: Train Loss: 0.6506399273872375, Validation Loss: 0.6883013844490051\n",
      "Epoch 44: Train Loss: 0.5921380162239075, Validation Loss: 0.6893048286437988\n",
      "Epoch 45: Train Loss: 0.6368627190589905, Validation Loss: 0.6909465789794922\n",
      "Epoch 46: Train Loss: 0.6318549633026123, Validation Loss: 0.6917638182640076\n",
      "Epoch 47: Train Loss: 0.5706929624080658, Validation Loss: 0.6922780871391296\n",
      "Epoch 48: Train Loss: 0.6389924883842468, Validation Loss: 0.6922135353088379\n",
      "Epoch 49: Train Loss: 0.6880539417266845, Validation Loss: 0.6920053958892822\n",
      "Epoch 50: Train Loss: 0.7048142075538635, Validation Loss: 0.6927909851074219\n",
      "Epoch 51: Train Loss: 0.629036259651184, Validation Loss: 0.6931464076042175\n",
      "Epoch 52: Train Loss: 0.6116238355636596, Validation Loss: 0.6955268383026123\n",
      "Epoch 53: Train Loss: 0.6413276791572571, Validation Loss: 0.6944096684455872\n",
      "Epoch 54: Train Loss: 0.6224798917770386, Validation Loss: 0.6914384365081787\n",
      "Epoch 55: Train Loss: 0.6140684247016907, Validation Loss: 0.6925325393676758\n",
      "Epoch 56: Train Loss: 0.6471657156944275, Validation Loss: 0.6924079656600952\n",
      "Epoch 57: Train Loss: 0.6076423406600953, Validation Loss: 0.6921793818473816\n",
      "Epoch 58: Train Loss: 0.5915724277496338, Validation Loss: 0.6925809383392334\n",
      "Epoch 59: Train Loss: 0.6090319633483887, Validation Loss: 0.6927213668823242\n",
      "Epoch 60: Train Loss: 0.5839282155036927, Validation Loss: 0.6928734183311462\n",
      "Epoch 61: Train Loss: 0.5749512195587159, Validation Loss: 0.6910117268562317\n",
      "Epoch 62: Train Loss: 0.5981577634811401, Validation Loss: 0.6891121864318848\n",
      "Epoch 63: Train Loss: 0.5804330706596375, Validation Loss: 0.684851884841919\n",
      "Epoch 64: Train Loss: 0.6018502593040467, Validation Loss: 0.678412139415741\n",
      "Epoch 65: Train Loss: 0.5946814298629761, Validation Loss: 0.6752157807350159\n",
      "Epoch 66: Train Loss: 0.5734023094177246, Validation Loss: 0.6767429709434509\n",
      "Epoch 67: Train Loss: 0.5829430818557739, Validation Loss: 0.67792147397995\n",
      "Epoch 68: Train Loss: 0.5608879685401916, Validation Loss: 0.6781580448150635\n",
      "Epoch 69: Train Loss: 0.556891942024231, Validation Loss: 0.6780732274055481\n",
      "Epoch 70: Train Loss: 0.6241672396659851, Validation Loss: 0.6752398610115051\n",
      "Epoch 71: Train Loss: 0.6146340548992157, Validation Loss: 0.6762697100639343\n",
      "Epoch 72: Train Loss: 0.6038005471229553, Validation Loss: 0.6784015893936157\n",
      "Epoch 73: Train Loss: 0.5614632964134216, Validation Loss: 0.6831978559494019\n",
      "Epoch 74: Train Loss: 0.5365308940410614, Validation Loss: 0.6851754784584045\n",
      "Epoch 75: Train Loss: 0.5528549790382385, Validation Loss: 0.6875357031822205\n",
      "Epoch 76: Train Loss: 0.5671477675437927, Validation Loss: 0.6883702874183655\n",
      "Epoch 77: Train Loss: 0.5983893513679505, Validation Loss: 0.6885541677474976\n",
      "Epoch 78: Train Loss: 0.5550671756267548, Validation Loss: 0.6879974603652954\n",
      "Epoch 79: Train Loss: 0.5434593856334686, Validation Loss: 0.687863290309906\n",
      "Epoch 80: Train Loss: 0.5628576397895813, Validation Loss: 0.6794862747192383\n",
      "Epoch 81: Train Loss: 0.57518550157547, Validation Loss: 0.675416886806488\n",
      "Epoch 82: Train Loss: 0.5432638168334961, Validation Loss: 0.6789641976356506\n",
      "Epoch 83: Train Loss: 0.5801278233528138, Validation Loss: 0.6798423528671265\n",
      "Epoch 84: Train Loss: 0.5625389218330383, Validation Loss: 0.6813718676567078\n",
      "Epoch 85: Train Loss: 0.5848378896713257, Validation Loss: 0.6840912103652954\n",
      "Epoch 86: Train Loss: 0.5448025524616241, Validation Loss: 0.6850007772445679\n",
      "Epoch 87: Train Loss: 0.5407063126564026, Validation Loss: 0.684777557849884\n",
      "Epoch 88: Train Loss: 0.531303733587265, Validation Loss: 0.685036838054657\n",
      "Epoch 89: Train Loss: 0.4864859938621521, Validation Loss: 0.6847979426383972\n",
      "Epoch 90: Train Loss: 0.5308200716972351, Validation Loss: 0.6825957298278809\n",
      "Epoch 91: Train Loss: 0.5258185148239136, Validation Loss: 0.6884329319000244\n",
      "Epoch 92: Train Loss: 0.5275991559028625, Validation Loss: 0.6895005106925964\n",
      "Epoch 93: Train Loss: 0.5343743026256561, Validation Loss: 0.6905573606491089\n",
      "Epoch 94: Train Loss: 0.5874292373657226, Validation Loss: 0.691136360168457\n",
      "Epoch 95: Train Loss: 0.5257918536663055, Validation Loss: 0.6892668008804321\n",
      "Epoch 96: Train Loss: 0.4778879821300507, Validation Loss: 0.6873496174812317\n",
      "Epoch 97: Train Loss: 0.5992264747619629, Validation Loss: 0.6867941617965698\n",
      "Epoch 98: Train Loss: 0.5463396668434143, Validation Loss: 0.6866273283958435\n",
      "Epoch 99: Train Loss: 0.4860692977905273, Validation Loss: 0.6859700083732605\n",
      "Epoch 100: Train Loss: 0.4930803418159485, Validation Loss: 0.6940698623657227\n",
      "Epoch 101: Train Loss: 0.5137342274188995, Validation Loss: 0.6938132643699646\n",
      "Epoch 102: Train Loss: 0.45099376440048217, Validation Loss: 0.6867768168449402\n",
      "Epoch 103: Train Loss: 0.47006657123565676, Validation Loss: 0.6869187355041504\n",
      "Epoch 104: Train Loss: 0.523697429895401, Validation Loss: 0.6882394552230835\n",
      "Epoch 105: Train Loss: 0.5123123288154602, Validation Loss: 0.6867758631706238\n",
      "Epoch 106: Train Loss: 0.4456240594387054, Validation Loss: 0.684317409992218\n",
      "Epoch 107: Train Loss: 0.41170390248298644, Validation Loss: 0.683129608631134\n",
      "Epoch 108: Train Loss: 0.46606560945510866, Validation Loss: 0.6822341680526733\n",
      "Epoch 109: Train Loss: 0.4492123067378998, Validation Loss: 0.6812793016433716\n",
      "Epoch 110: Train Loss: 0.49748754501342773, Validation Loss: 0.6702113151550293\n",
      "Epoch 111: Train Loss: 0.5367423295974731, Validation Loss: 0.6722562909126282\n",
      "Epoch 112: Train Loss: 0.4523568630218506, Validation Loss: 0.6678276062011719\n",
      "Epoch 113: Train Loss: 0.488053959608078, Validation Loss: 0.662445604801178\n",
      "Epoch 114: Train Loss: 0.41035438776016236, Validation Loss: 0.6574815511703491\n",
      "Epoch 115: Train Loss: 0.4660329043865204, Validation Loss: 0.654662013053894\n",
      "Epoch 116: Train Loss: 0.4202004551887512, Validation Loss: 0.6526265740394592\n",
      "Epoch 117: Train Loss: 0.4709735751152039, Validation Loss: 0.6522664427757263\n",
      "Epoch 118: Train Loss: 0.49039719700813295, Validation Loss: 0.6518890261650085\n",
      "Epoch 119: Train Loss: 0.40385034680366516, Validation Loss: 0.6515087485313416\n",
      "Epoch 120: Train Loss: 0.4013137698173523, Validation Loss: 0.6594477891921997\n",
      "Epoch 121: Train Loss: 0.4401210010051727, Validation Loss: 0.6628449559211731\n",
      "Epoch 122: Train Loss: 0.43051524758338927, Validation Loss: 0.6537532806396484\n",
      "Epoch 123: Train Loss: 0.442576014995575, Validation Loss: 0.6507068276405334\n",
      "Epoch 124: Train Loss: 0.4150181770324707, Validation Loss: 0.6533880829811096\n",
      "Epoch 125: Train Loss: 0.37665091156959535, Validation Loss: 0.6523143649101257\n",
      "Epoch 126: Train Loss: 0.39904730319976806, Validation Loss: 0.6488159894943237\n",
      "Epoch 127: Train Loss: 0.4476996660232544, Validation Loss: 0.6452664136886597\n",
      "Epoch 128: Train Loss: 0.37457333207130433, Validation Loss: 0.6425120234489441\n",
      "Epoch 129: Train Loss: 0.41231790781021116, Validation Loss: 0.6434061527252197\n",
      "Epoch 130: Train Loss: 0.35513917207717893, Validation Loss: 0.6316291093826294\n",
      "Epoch 131: Train Loss: 0.40599294304847716, Validation Loss: 0.6235440969467163\n",
      "Epoch 132: Train Loss: 0.38778235912323, Validation Loss: 0.6137334704399109\n",
      "Epoch 133: Train Loss: 0.3901606023311615, Validation Loss: 0.6228995323181152\n",
      "Epoch 134: Train Loss: 0.3313997328281403, Validation Loss: 0.6351533532142639\n",
      "Epoch 135: Train Loss: 0.3629223585128784, Validation Loss: 0.6402809023857117\n",
      "Epoch 136: Train Loss: 0.3490199983119965, Validation Loss: 0.6356198787689209\n",
      "Epoch 137: Train Loss: 0.3781708061695099, Validation Loss: 0.632685661315918\n",
      "Epoch 138: Train Loss: 0.3581139326095581, Validation Loss: 0.6332276463508606\n",
      "Epoch 139: Train Loss: 0.4098571836948395, Validation Loss: 0.6284738779067993\n",
      "Epoch 140: Train Loss: 0.2943016290664673, Validation Loss: 0.6116902232170105\n",
      "Epoch 141: Train Loss: 0.3885271668434143, Validation Loss: 0.6103624105453491\n",
      "Epoch 142: Train Loss: 0.32904120683670046, Validation Loss: 0.618630051612854\n",
      "Epoch 143: Train Loss: 0.33665685057640077, Validation Loss: 0.6243531107902527\n",
      "Epoch 144: Train Loss: 0.3622040987014771, Validation Loss: 0.6237484812736511\n",
      "Epoch 145: Train Loss: 0.3535156071186066, Validation Loss: 0.619987428188324\n",
      "Epoch 146: Train Loss: 0.36303542256355287, Validation Loss: 0.6101508736610413\n",
      "Epoch 147: Train Loss: 0.2991690844297409, Validation Loss: 0.6064435839653015\n",
      "Epoch 148: Train Loss: 0.3392281413078308, Validation Loss: 0.6092780828475952\n",
      "Epoch 149: Train Loss: 0.29430532455444336, Validation Loss: 0.6132112741470337\n",
      "Epoch 150: Train Loss: 0.28935452699661257, Validation Loss: 0.6206827759742737\n",
      "Epoch 151: Train Loss: 0.2970802843570709, Validation Loss: 0.6261749863624573\n",
      "Epoch 152: Train Loss: 0.280757749080658, Validation Loss: 0.6209736466407776\n",
      "Epoch 153: Train Loss: 0.33426777124404905, Validation Loss: 0.6177057027816772\n",
      "Epoch 154: Train Loss: 0.2916185617446899, Validation Loss: 0.6134804487228394\n",
      "Epoch 155: Train Loss: 0.2725879937410355, Validation Loss: 0.616690993309021\n",
      "Epoch 156: Train Loss: 0.34396933317184447, Validation Loss: 0.6173655986785889\n",
      "Epoch 157: Train Loss: 0.269906610250473, Validation Loss: 0.6146222949028015\n",
      "Epoch 158: Train Loss: 0.29752972424030305, Validation Loss: 0.6085861921310425\n",
      "Epoch 159: Train Loss: 0.3587020099163055, Validation Loss: 0.598852813243866\n",
      "Epoch 160: Train Loss: 0.2500535637140274, Validation Loss: 0.6009085178375244\n",
      "Epoch 161: Train Loss: 0.26410614550113676, Validation Loss: 0.615479052066803\n",
      "Epoch 162: Train Loss: 0.3013551503419876, Validation Loss: 0.6256930232048035\n",
      "Epoch 163: Train Loss: 0.30216549038887025, Validation Loss: 0.615112841129303\n",
      "Epoch 164: Train Loss: 0.29330079853534696, Validation Loss: 0.6064193248748779\n",
      "Epoch 165: Train Loss: 0.2415900006890297, Validation Loss: 0.6018294095993042\n",
      "Epoch 166: Train Loss: 0.3042374044656754, Validation Loss: 0.6000733971595764\n",
      "Epoch 167: Train Loss: 0.2952632874250412, Validation Loss: 0.6081099510192871\n",
      "Epoch 168: Train Loss: 0.285478413105011, Validation Loss: 0.6014936566352844\n",
      "Epoch 169: Train Loss: 0.2872547715902328, Validation Loss: 0.6007304787635803\n",
      "Epoch 170: Train Loss: 0.24885328114032745, Validation Loss: 0.5868127346038818\n",
      "Epoch 171: Train Loss: 0.3285511642694473, Validation Loss: 0.6037149429321289\n",
      "Epoch 172: Train Loss: 0.2955146968364716, Validation Loss: 0.5988995432853699\n",
      "Epoch 173: Train Loss: 0.2866035223007202, Validation Loss: 0.5912485122680664\n",
      "Epoch 174: Train Loss: 0.2761965960264206, Validation Loss: 0.5860908031463623\n",
      "Epoch 175: Train Loss: 0.2543538838624954, Validation Loss: 0.5830875039100647\n",
      "Epoch 176: Train Loss: 0.23487202525138856, Validation Loss: 0.582850456237793\n",
      "Epoch 177: Train Loss: 0.2582841545343399, Validation Loss: 0.5789695978164673\n",
      "Epoch 178: Train Loss: 0.23952525854110718, Validation Loss: 0.5760975480079651\n",
      "Epoch 179: Train Loss: 0.2947234630584717, Validation Loss: 0.5747413635253906\n",
      "Epoch 180: Train Loss: 0.2567779630422592, Validation Loss: 0.5688040852546692\n",
      "Epoch 181: Train Loss: 0.2693853497505188, Validation Loss: 0.5736409425735474\n",
      "Epoch 182: Train Loss: 0.23429670333862304, Validation Loss: 0.5822833180427551\n",
      "Epoch 183: Train Loss: 0.24325160384178163, Validation Loss: 0.5771821141242981\n",
      "Epoch 184: Train Loss: 0.2514273762702942, Validation Loss: 0.5808833241462708\n",
      "Epoch 185: Train Loss: 0.21780334413051605, Validation Loss: 0.5903220176696777\n",
      "Epoch 186: Train Loss: 0.24435697197914125, Validation Loss: 0.5854460597038269\n",
      "Epoch 187: Train Loss: 0.2339023232460022, Validation Loss: 0.5898410677909851\n",
      "Epoch 188: Train Loss: 0.3119892716407776, Validation Loss: 0.5956742167472839\n",
      "Epoch 189: Train Loss: 0.17107738554477692, Validation Loss: 0.6003485918045044\n",
      "Epoch 190: Train Loss: 0.21526796519756317, Validation Loss: 0.59343022108078\n",
      "Epoch 191: Train Loss: 0.2247353732585907, Validation Loss: 0.6014588475227356\n",
      "Epoch 192: Train Loss: 0.24470437169075013, Validation Loss: 0.6129322052001953\n",
      "Epoch 193: Train Loss: 0.24348070323467255, Validation Loss: 0.642757773399353\n",
      "Epoch 194: Train Loss: 0.296246325969696, Validation Loss: 0.676880419254303\n",
      "Epoch 195: Train Loss: 0.18326417803764344, Validation Loss: 0.6768840551376343\n",
      "Epoch 196: Train Loss: 0.20337504148483276, Validation Loss: 0.6624741554260254\n",
      "Epoch 197: Train Loss: 0.21249292194843292, Validation Loss: 0.6599833369255066\n",
      "Epoch 198: Train Loss: 0.1938986897468567, Validation Loss: 0.6584881544113159\n",
      "Epoch 199: Train Loss: 0.17887431383132935, Validation Loss: 0.6591305732727051\n",
      "Fold 10 Metrics:\n",
      "Accuracy: 0.5454545454545454, Precision: 0.6, Recall: 0.5, F1-score: 0.5454545454545454, AUC: 0.5499999999999999\n",
      "Confusion Matrix:\n",
      "[[3 2]\n",
      " [3 3]]\n",
      "Completed fold 10\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples from subject 1 to test set\n",
      "Adding 6 truth samples from subject 1 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.7191425561904907, Validation Loss: 0.7201075553894043\n",
      "Epoch 1: Train Loss: 0.7165671348571777, Validation Loss: 0.7379950881004333\n",
      "Epoch 2: Train Loss: 0.705767285823822, Validation Loss: 0.7572112083435059\n",
      "Epoch 3: Train Loss: 0.6927489399909973, Validation Loss: 0.7576415538787842\n",
      "Epoch 4: Train Loss: 0.6449353456497192, Validation Loss: 0.7657316327095032\n",
      "Epoch 5: Train Loss: 0.6473159551620483, Validation Loss: 0.7732729911804199\n",
      "Epoch 6: Train Loss: 0.6357650041580201, Validation Loss: 0.7743310332298279\n",
      "Epoch 7: Train Loss: 0.6510833144187927, Validation Loss: 0.7759611010551453\n",
      "Epoch 8: Train Loss: 0.6363924741744995, Validation Loss: 0.7770105004310608\n",
      "Epoch 9: Train Loss: 0.6510060429573059, Validation Loss: 0.7780348658561707\n",
      "Epoch 10: Train Loss: 0.6729993343353271, Validation Loss: 0.7787286639213562\n",
      "Epoch 11: Train Loss: 0.6583634376525879, Validation Loss: 0.7807138562202454\n",
      "Epoch 12: Train Loss: 0.6846316576004028, Validation Loss: 0.7870441675186157\n",
      "Epoch 13: Train Loss: 0.674710500240326, Validation Loss: 0.8079096078872681\n",
      "Epoch 14: Train Loss: 0.6805025815963746, Validation Loss: 0.822834312915802\n",
      "Epoch 15: Train Loss: 0.5604637086391449, Validation Loss: 0.818581759929657\n",
      "Epoch 16: Train Loss: 0.5993338704109192, Validation Loss: 0.8187490701675415\n",
      "Epoch 17: Train Loss: 0.6388045072555542, Validation Loss: 0.817345380783081\n",
      "Epoch 18: Train Loss: 0.574719363451004, Validation Loss: 0.8151419758796692\n",
      "Epoch 19: Train Loss: 0.6127285957336426, Validation Loss: 0.8142333030700684\n",
      "Epoch 20: Train Loss: 0.6100140571594238, Validation Loss: 0.8037099838256836\n",
      "Epoch 21: Train Loss: 0.695969033241272, Validation Loss: 0.7746665477752686\n",
      "Epoch 22: Train Loss: 0.6032825589179993, Validation Loss: 0.7623006701469421\n",
      "Epoch 23: Train Loss: 0.5888865351676941, Validation Loss: 0.7647223472595215\n",
      "Epoch 24: Train Loss: 0.5990484476089477, Validation Loss: 0.7791637182235718\n",
      "Epoch 25: Train Loss: 0.6146920442581176, Validation Loss: 0.7879390120506287\n",
      "Epoch 26: Train Loss: 0.6240718364715576, Validation Loss: 0.7939996123313904\n",
      "Epoch 27: Train Loss: 0.5816351294517517, Validation Loss: 0.7969779372215271\n",
      "Epoch 28: Train Loss: 0.577436101436615, Validation Loss: 0.7980329394340515\n",
      "Epoch 29: Train Loss: 0.6034440517425537, Validation Loss: 0.7982540130615234\n",
      "Epoch 30: Train Loss: 0.5844511747360229, Validation Loss: 0.8198406100273132\n",
      "Epoch 31: Train Loss: 0.6077616691589356, Validation Loss: 0.8399711847305298\n",
      "Epoch 32: Train Loss: 0.5988370895385742, Validation Loss: 0.8355180621147156\n",
      "Epoch 33: Train Loss: 0.5977483093738556, Validation Loss: 0.8310006856918335\n",
      "Epoch 34: Train Loss: 0.5654084324836731, Validation Loss: 0.8031133413314819\n",
      "Epoch 35: Train Loss: 0.5487367570400238, Validation Loss: 0.7897745370864868\n",
      "Epoch 36: Train Loss: 0.5436178505420685, Validation Loss: 0.7843581438064575\n",
      "Epoch 37: Train Loss: 0.5928528904914856, Validation Loss: 0.785061240196228\n",
      "Epoch 38: Train Loss: 0.5406659662723541, Validation Loss: 0.7857223153114319\n",
      "Epoch 39: Train Loss: 0.5685829222202301, Validation Loss: 0.786048412322998\n",
      "Epoch 40: Train Loss: 0.5511655688285828, Validation Loss: 0.8390353918075562\n",
      "Epoch 41: Train Loss: 0.5468330681324005, Validation Loss: 0.8696536421775818\n",
      "Epoch 42: Train Loss: 0.5879439830780029, Validation Loss: 0.9145816564559937\n",
      "Epoch 43: Train Loss: 0.6203532218933105, Validation Loss: 0.9281414151191711\n",
      "Epoch 44: Train Loss: 0.5108124732971191, Validation Loss: 0.910510241985321\n",
      "Epoch 45: Train Loss: 0.5365646123886109, Validation Loss: 0.8989104628562927\n",
      "Epoch 46: Train Loss: 0.5356138288974762, Validation Loss: 0.8893198370933533\n",
      "Epoch 47: Train Loss: 0.528162956237793, Validation Loss: 0.8846627473831177\n",
      "Epoch 48: Train Loss: 0.5534450888633728, Validation Loss: 0.8852753043174744\n",
      "Epoch 49: Train Loss: 0.5698871374130249, Validation Loss: 0.8849340677261353\n",
      "Epoch 50: Train Loss: 0.5549914479255676, Validation Loss: 0.8588985204696655\n",
      "Epoch 51: Train Loss: 0.5508244514465332, Validation Loss: 0.8658196926116943\n",
      "Epoch 52: Train Loss: 0.5180943310260773, Validation Loss: 0.8972360491752625\n",
      "Epoch 53: Train Loss: 0.5437315702438354, Validation Loss: 0.9273755550384521\n",
      "Epoch 54: Train Loss: 0.5321214437484741, Validation Loss: 0.9345632791519165\n",
      "Epoch 55: Train Loss: 0.6064674317836761, Validation Loss: 0.9524224996566772\n",
      "Epoch 56: Train Loss: 0.5218046128749847, Validation Loss: 0.9536110162734985\n",
      "Epoch 57: Train Loss: 0.5317392349243164, Validation Loss: 0.9525413513183594\n",
      "Epoch 58: Train Loss: 0.5649518728256225, Validation Loss: 0.950237512588501\n",
      "Epoch 59: Train Loss: 0.5185519814491272, Validation Loss: 0.951341986656189\n",
      "Epoch 60: Train Loss: 0.5190127730369568, Validation Loss: 0.920693039894104\n",
      "Epoch 61: Train Loss: 0.48567975163459776, Validation Loss: 0.8683193325996399\n",
      "Epoch 62: Train Loss: 0.5508215665817261, Validation Loss: 0.8496084213256836\n",
      "Epoch 63: Train Loss: 0.45611160397529604, Validation Loss: 0.8637078404426575\n",
      "Epoch 64: Train Loss: 0.4918367326259613, Validation Loss: 0.8868375420570374\n",
      "Epoch 65: Train Loss: 0.4736213147640228, Validation Loss: 0.9051946401596069\n",
      "Epoch 66: Train Loss: 0.5017362594604492, Validation Loss: 0.9329002499580383\n",
      "Epoch 67: Train Loss: 0.46715707182884214, Validation Loss: 0.9406694173812866\n",
      "Epoch 68: Train Loss: 0.5302004992961884, Validation Loss: 0.944016695022583\n",
      "Epoch 69: Train Loss: 0.49723501205444337, Validation Loss: 0.9398269653320312\n",
      "Epoch 70: Train Loss: 0.46898424029350283, Validation Loss: 0.8856677412986755\n",
      "Epoch 71: Train Loss: 0.49381060600280763, Validation Loss: 0.8674477934837341\n",
      "Epoch 72: Train Loss: 0.48808228969573975, Validation Loss: 0.8998329043388367\n",
      "Epoch 73: Train Loss: 0.5219467759132386, Validation Loss: 0.9347217082977295\n",
      "Epoch 74: Train Loss: 0.4776144683361053, Validation Loss: 0.9547452926635742\n",
      "Epoch 75: Train Loss: 0.4474378526210785, Validation Loss: 0.9639732241630554\n",
      "Epoch 76: Train Loss: 0.4601768314838409, Validation Loss: 0.9695159792900085\n",
      "Epoch 77: Train Loss: 0.47103399634361265, Validation Loss: 0.9740356206893921\n",
      "Epoch 78: Train Loss: 0.46415833234786985, Validation Loss: 0.9747693538665771\n",
      "Epoch 79: Train Loss: 0.46764035820961, Validation Loss: 0.9770107865333557\n",
      "Epoch 80: Train Loss: 0.46619864702224734, Validation Loss: 0.9617251753807068\n",
      "Epoch 81: Train Loss: 0.43549286723136904, Validation Loss: 1.002550482749939\n",
      "Epoch 82: Train Loss: 0.3965307056903839, Validation Loss: 1.0070579051971436\n",
      "Epoch 83: Train Loss: 0.4594289541244507, Validation Loss: 1.0007178783416748\n",
      "Epoch 84: Train Loss: 0.4489733874797821, Validation Loss: 1.005491852760315\n",
      "Epoch 85: Train Loss: 0.4573046088218689, Validation Loss: 1.012579083442688\n",
      "Epoch 86: Train Loss: 0.4336216986179352, Validation Loss: 1.0079361200332642\n",
      "Epoch 87: Train Loss: 0.45934065580368044, Validation Loss: 1.0018210411071777\n",
      "Epoch 88: Train Loss: 0.4372210085391998, Validation Loss: 1.0021487474441528\n",
      "Epoch 89: Train Loss: 0.45302723050117494, Validation Loss: 1.005388617515564\n",
      "Epoch 90: Train Loss: 0.48797495365142823, Validation Loss: 0.979744553565979\n",
      "Epoch 91: Train Loss: 0.45761469602584837, Validation Loss: 1.0076713562011719\n",
      "Epoch 92: Train Loss: 0.466560435295105, Validation Loss: 1.039008617401123\n",
      "Epoch 93: Train Loss: 0.40852075815200806, Validation Loss: 1.1273066997528076\n",
      "Epoch 94: Train Loss: 0.3935131192207336, Validation Loss: 1.1696263551712036\n",
      "Epoch 95: Train Loss: 0.3788428962230682, Validation Loss: 1.1661183834075928\n",
      "Epoch 96: Train Loss: 0.3681380569934845, Validation Loss: 1.1478828191757202\n",
      "Epoch 97: Train Loss: 0.421173232793808, Validation Loss: 1.1432286500930786\n",
      "Epoch 98: Train Loss: 0.38195779323577883, Validation Loss: 1.1513720750808716\n",
      "Epoch 99: Train Loss: 0.3883803427219391, Validation Loss: 1.1558665037155151\n",
      "Epoch 100: Train Loss: 0.4010009765625, Validation Loss: 1.1741164922714233\n",
      "Epoch 101: Train Loss: 0.39117962718009947, Validation Loss: 1.157320499420166\n",
      "Epoch 102: Train Loss: 0.3820234537124634, Validation Loss: 1.1649175882339478\n",
      "Epoch 103: Train Loss: 0.3645901262760162, Validation Loss: 1.1096488237380981\n",
      "Epoch 104: Train Loss: 0.4449197232723236, Validation Loss: 1.1018234491348267\n",
      "Epoch 105: Train Loss: 0.3419049233198166, Validation Loss: 1.0927742719650269\n",
      "Epoch 106: Train Loss: 0.38512585163116453, Validation Loss: 1.103060245513916\n",
      "Epoch 107: Train Loss: 0.3781274437904358, Validation Loss: 1.113287329673767\n",
      "Epoch 108: Train Loss: 0.37656842470169066, Validation Loss: 1.119712471961975\n",
      "Epoch 109: Train Loss: 0.357759553194046, Validation Loss: 1.12361478805542\n",
      "Epoch 110: Train Loss: 0.35598652958869936, Validation Loss: 1.132895588874817\n",
      "Epoch 111: Train Loss: 0.40148518681526185, Validation Loss: 1.1937285661697388\n",
      "Epoch 112: Train Loss: 0.34431185126304625, Validation Loss: 1.1556637287139893\n",
      "Epoch 113: Train Loss: 0.31870015859603884, Validation Loss: 1.0814003944396973\n",
      "Epoch 114: Train Loss: 0.3443861424922943, Validation Loss: 1.1247273683547974\n",
      "Epoch 115: Train Loss: 0.37986695766448975, Validation Loss: 1.1232080459594727\n",
      "Epoch 116: Train Loss: 0.3192311316728592, Validation Loss: 1.1037938594818115\n",
      "Epoch 117: Train Loss: 0.3899816691875458, Validation Loss: 1.1001698970794678\n",
      "Epoch 118: Train Loss: 0.34412687420845034, Validation Loss: 1.1017698049545288\n",
      "Epoch 119: Train Loss: 0.30916587710380555, Validation Loss: 1.1037744283676147\n",
      "Epoch 120: Train Loss: 0.358489453792572, Validation Loss: 1.1258089542388916\n",
      "Epoch 121: Train Loss: 0.33280159831047057, Validation Loss: 1.1842025518417358\n",
      "Epoch 122: Train Loss: 0.42688294053077697, Validation Loss: 1.1301881074905396\n",
      "Epoch 123: Train Loss: 0.3426799178123474, Validation Loss: 1.0761042833328247\n",
      "Epoch 124: Train Loss: 0.34514582753181455, Validation Loss: 1.2181785106658936\n",
      "Epoch 125: Train Loss: 0.3109553426504135, Validation Loss: 1.307002305984497\n",
      "Epoch 126: Train Loss: 0.3130035936832428, Validation Loss: 1.2688748836517334\n",
      "Epoch 127: Train Loss: 0.3264943718910217, Validation Loss: 1.2430795431137085\n",
      "Epoch 128: Train Loss: 0.28270489275455474, Validation Loss: 1.2424908876419067\n",
      "Epoch 129: Train Loss: 0.29258534908294676, Validation Loss: 1.2440294027328491\n",
      "Epoch 130: Train Loss: 0.3207916021347046, Validation Loss: 1.077637791633606\n",
      "Epoch 131: Train Loss: 0.3234992563724518, Validation Loss: 1.1917047500610352\n",
      "Epoch 132: Train Loss: 0.30044207274913787, Validation Loss: 1.3140543699264526\n",
      "Epoch 133: Train Loss: 0.32604814171791074, Validation Loss: 1.3580647706985474\n",
      "Epoch 134: Train Loss: 0.33501226305961607, Validation Loss: 1.3147071599960327\n",
      "Epoch 135: Train Loss: 0.3086888551712036, Validation Loss: 1.3118020296096802\n",
      "Epoch 136: Train Loss: 0.2984925925731659, Validation Loss: 1.274902582168579\n",
      "Epoch 137: Train Loss: 0.28551999628543856, Validation Loss: 1.222660779953003\n",
      "Epoch 138: Train Loss: 0.2871354162693024, Validation Loss: 1.2091575860977173\n",
      "Epoch 139: Train Loss: 0.2697078138589859, Validation Loss: 1.2125228643417358\n",
      "Epoch 140: Train Loss: 0.27438058257102965, Validation Loss: 1.1227881908416748\n",
      "Epoch 141: Train Loss: 0.2921671003103256, Validation Loss: 1.0877741575241089\n",
      "Epoch 142: Train Loss: 0.29541888236999514, Validation Loss: 1.270840048789978\n",
      "Epoch 143: Train Loss: 0.23714622855186462, Validation Loss: 1.5116691589355469\n",
      "Epoch 144: Train Loss: 0.2884405255317688, Validation Loss: 1.527927041053772\n",
      "Epoch 145: Train Loss: 0.26899273991584777, Validation Loss: 1.4363064765930176\n",
      "Epoch 146: Train Loss: 0.29263921082019806, Validation Loss: 1.3143439292907715\n",
      "Epoch 147: Train Loss: 0.23093158900737762, Validation Loss: 1.2477997541427612\n",
      "Epoch 148: Train Loss: 0.29561778008937833, Validation Loss: 1.2397072315216064\n",
      "Epoch 149: Train Loss: 0.23004009425640107, Validation Loss: 1.2492311000823975\n",
      "Epoch 150: Train Loss: 0.24787420332431792, Validation Loss: 1.3575830459594727\n",
      "Epoch 151: Train Loss: 0.2692586243152618, Validation Loss: 1.3092185258865356\n",
      "Epoch 152: Train Loss: 0.25055807530879975, Validation Loss: 1.2478432655334473\n",
      "Epoch 153: Train Loss: 0.27703708708286284, Validation Loss: 1.3186149597167969\n",
      "Epoch 154: Train Loss: 0.22082809805870057, Validation Loss: 1.3297128677368164\n",
      "Epoch 155: Train Loss: 0.23818936049938202, Validation Loss: 1.3388320207595825\n",
      "Epoch 156: Train Loss: 0.22369190454483032, Validation Loss: 1.3562942743301392\n",
      "Epoch 157: Train Loss: 0.20871722400188447, Validation Loss: 1.361168622970581\n",
      "Epoch 158: Train Loss: 0.22669849097728728, Validation Loss: 1.36796236038208\n",
      "Epoch 159: Train Loss: 0.19141591489315032, Validation Loss: 1.3769499063491821\n",
      "Epoch 160: Train Loss: 0.2057408422231674, Validation Loss: 1.2817906141281128\n",
      "Epoch 161: Train Loss: 0.23698603510856628, Validation Loss: 1.275954246520996\n",
      "Epoch 162: Train Loss: 0.2367365688085556, Validation Loss: 1.4190629720687866\n",
      "Epoch 163: Train Loss: 0.21886524856090545, Validation Loss: 1.3614836931228638\n",
      "Epoch 164: Train Loss: 0.210565747320652, Validation Loss: 1.310147762298584\n",
      "Epoch 165: Train Loss: 0.2148572623729706, Validation Loss: 1.2834529876708984\n",
      "Epoch 166: Train Loss: 0.22538871467113494, Validation Loss: 1.2785645723342896\n",
      "Epoch 167: Train Loss: 0.17457919120788573, Validation Loss: 1.3203790187835693\n",
      "Epoch 168: Train Loss: 0.2380793422460556, Validation Loss: 1.3095080852508545\n",
      "Epoch 169: Train Loss: 0.21936787366867067, Validation Loss: 1.2940499782562256\n",
      "Epoch 170: Train Loss: 0.17224555015563964, Validation Loss: 1.3516594171524048\n",
      "Epoch 171: Train Loss: 0.22189381420612336, Validation Loss: 1.265735387802124\n",
      "Epoch 172: Train Loss: 0.18314515203237533, Validation Loss: 1.1749683618545532\n",
      "Epoch 173: Train Loss: 0.18819569051265717, Validation Loss: 1.2448246479034424\n",
      "Epoch 174: Train Loss: 0.18641200363636018, Validation Loss: 1.338018536567688\n",
      "Epoch 175: Train Loss: 0.19671947062015532, Validation Loss: 1.3224363327026367\n",
      "Epoch 176: Train Loss: 0.14983129054307937, Validation Loss: 1.3179737329483032\n",
      "Epoch 177: Train Loss: 0.17505927681922911, Validation Loss: 1.3450021743774414\n",
      "Epoch 178: Train Loss: 0.20599903613328935, Validation Loss: 1.3113131523132324\n",
      "Epoch 179: Train Loss: 0.16669480502605438, Validation Loss: 1.3053748607635498\n",
      "Epoch 180: Train Loss: 0.17990079522132874, Validation Loss: 1.193231225013733\n",
      "Epoch 181: Train Loss: 0.18204505741596222, Validation Loss: 1.3856168985366821\n",
      "Epoch 182: Train Loss: 0.2887501806020737, Validation Loss: 1.62252676486969\n",
      "Epoch 183: Train Loss: 0.1643466256558895, Validation Loss: 1.4922277927398682\n",
      "Epoch 184: Train Loss: 0.233410981297493, Validation Loss: 1.4099082946777344\n",
      "Epoch 185: Train Loss: 0.1952874094247818, Validation Loss: 1.3080452680587769\n",
      "Epoch 186: Train Loss: 0.18159240186214448, Validation Loss: 1.2480604648590088\n",
      "Epoch 187: Train Loss: 0.17917667776346208, Validation Loss: 1.283438801765442\n",
      "Epoch 188: Train Loss: 0.1709953725337982, Validation Loss: 1.299917459487915\n",
      "Epoch 189: Train Loss: 0.15186891108751296, Validation Loss: 1.3013436794281006\n",
      "Epoch 190: Train Loss: 0.1605137676000595, Validation Loss: 1.5442194938659668\n",
      "Epoch 191: Train Loss: 0.16918658316135407, Validation Loss: 1.558251976966858\n",
      "Epoch 192: Train Loss: 0.17046436071395873, Validation Loss: 1.3998510837554932\n",
      "Epoch 193: Train Loss: 0.1334794670343399, Validation Loss: 1.4537326097488403\n",
      "Epoch 194: Train Loss: 0.13849000483751298, Validation Loss: 1.6278568506240845\n",
      "Epoch 195: Train Loss: 0.18108040392398833, Validation Loss: 1.565341830253601\n",
      "Epoch 196: Train Loss: 0.16847900152206421, Validation Loss: 1.5393874645233154\n",
      "Epoch 197: Train Loss: 0.17047151327133178, Validation Loss: 1.5171257257461548\n",
      "Epoch 198: Train Loss: 0.14760692566633224, Validation Loss: 1.5209442377090454\n",
      "Epoch 199: Train Loss: 0.1741904616355896, Validation Loss: 1.5188837051391602\n",
      "Fold 11 Metrics:\n",
      "Accuracy: 0.36363636363636365, Precision: 0.0, Recall: 0.0, F1-score: 0.0, AUC: 0.4\n",
      "Confusion Matrix:\n",
      "[[4 1]\n",
      " [6 0]]\n",
      "Completed fold 11\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples from subject 2 to test set\n",
      "Adding 6 truth samples from subject 2 to test set\n",
      "Adding 5 lie samples and 6 truth samples from subject 11 to train set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.771522331237793, Validation Loss: 0.6919515132904053\n",
      "Epoch 1: Train Loss: 0.7324942231178284, Validation Loss: 0.684967041015625\n",
      "Epoch 2: Train Loss: 0.672810685634613, Validation Loss: 0.686902642250061\n",
      "Epoch 3: Train Loss: 0.696137535572052, Validation Loss: 0.6917163729667664\n",
      "Epoch 4: Train Loss: 0.6915122270584106, Validation Loss: 0.6959101557731628\n",
      "Epoch 5: Train Loss: 0.6972333669662476, Validation Loss: 0.6971065998077393\n",
      "Epoch 6: Train Loss: 0.7018331050872803, Validation Loss: 0.6947353482246399\n",
      "Epoch 7: Train Loss: 0.6853553533554078, Validation Loss: 0.6927179098129272\n",
      "Epoch 8: Train Loss: 0.7351321697235107, Validation Loss: 0.6930740475654602\n",
      "Epoch 9: Train Loss: 0.6954913258552551, Validation Loss: 0.693134069442749\n",
      "Epoch 10: Train Loss: 0.694417941570282, Validation Loss: 0.6966084837913513\n",
      "Epoch 11: Train Loss: 0.6695207595825196, Validation Loss: 0.7053015232086182\n",
      "Epoch 12: Train Loss: 0.7543079018592834, Validation Loss: 0.709364652633667\n",
      "Epoch 13: Train Loss: 0.6475205659866333, Validation Loss: 0.70891934633255\n",
      "Epoch 14: Train Loss: 0.7829310655593872, Validation Loss: 0.7099635601043701\n",
      "Epoch 15: Train Loss: 0.6801285743713379, Validation Loss: 0.7095832228660583\n",
      "Epoch 16: Train Loss: 0.6587943077087403, Validation Loss: 0.706814169883728\n",
      "Epoch 17: Train Loss: 0.6355635762214661, Validation Loss: 0.704269289970398\n",
      "Epoch 18: Train Loss: 0.6574549674987793, Validation Loss: 0.7037913203239441\n",
      "Epoch 19: Train Loss: 0.7139358878135681, Validation Loss: 0.7035654783248901\n",
      "Epoch 20: Train Loss: 0.6368632793426514, Validation Loss: 0.7100609540939331\n",
      "Epoch 21: Train Loss: 0.6971966981887817, Validation Loss: 0.7112006545066833\n",
      "Epoch 22: Train Loss: 0.6789396166801452, Validation Loss: 0.7120022773742676\n",
      "Epoch 23: Train Loss: 0.6494977116584778, Validation Loss: 0.7165207862854004\n",
      "Epoch 24: Train Loss: 0.6325049638748169, Validation Loss: 0.7197819948196411\n",
      "Epoch 25: Train Loss: 0.6560154318809509, Validation Loss: 0.7215306162834167\n",
      "Epoch 26: Train Loss: 0.6185335516929626, Validation Loss: 0.7211456298828125\n",
      "Epoch 27: Train Loss: 0.7328880667686463, Validation Loss: 0.7230269312858582\n",
      "Epoch 28: Train Loss: 0.6632003068923951, Validation Loss: 0.7237677574157715\n",
      "Epoch 29: Train Loss: 0.6705344915390015, Validation Loss: 0.7239344716072083\n",
      "Epoch 30: Train Loss: 0.6291171073913574, Validation Loss: 0.7182061672210693\n",
      "Epoch 31: Train Loss: 0.6376657366752625, Validation Loss: 0.7158803343772888\n",
      "Epoch 32: Train Loss: 0.595445168018341, Validation Loss: 0.7204772233963013\n",
      "Epoch 33: Train Loss: 0.6725325226783753, Validation Loss: 0.7177636623382568\n",
      "Epoch 34: Train Loss: 0.6213786602020264, Validation Loss: 0.7179057002067566\n",
      "Epoch 35: Train Loss: 0.6016667366027832, Validation Loss: 0.719221830368042\n",
      "Epoch 36: Train Loss: 0.6291075587272644, Validation Loss: 0.7177350521087646\n",
      "Epoch 37: Train Loss: 0.6838688254356384, Validation Loss: 0.7166972160339355\n",
      "Epoch 38: Train Loss: 0.6356321334838867, Validation Loss: 0.716707170009613\n",
      "Epoch 39: Train Loss: 0.6177668094635009, Validation Loss: 0.7170196175575256\n",
      "Epoch 40: Train Loss: 0.5792226195335388, Validation Loss: 0.726627767086029\n",
      "Epoch 41: Train Loss: 0.6131071209907532, Validation Loss: 0.7386088371276855\n",
      "Epoch 42: Train Loss: 0.6274008274078369, Validation Loss: 0.743373692035675\n",
      "Epoch 43: Train Loss: 0.6807033896446228, Validation Loss: 0.7416602969169617\n",
      "Epoch 44: Train Loss: 0.6049029350280761, Validation Loss: 0.7408340573310852\n",
      "Epoch 45: Train Loss: 0.579941201210022, Validation Loss: 0.7338411211967468\n",
      "Epoch 46: Train Loss: 0.5956525087356568, Validation Loss: 0.7323938012123108\n",
      "Epoch 47: Train Loss: 0.5896048665046691, Validation Loss: 0.7311446666717529\n",
      "Epoch 48: Train Loss: 0.6559636712074279, Validation Loss: 0.7318519353866577\n",
      "Epoch 49: Train Loss: 0.5783522129058838, Validation Loss: 0.7325587868690491\n",
      "Epoch 50: Train Loss: 0.6759934425354004, Validation Loss: 0.7302523851394653\n",
      "Epoch 51: Train Loss: 0.593967866897583, Validation Loss: 0.7250210642814636\n",
      "Epoch 52: Train Loss: 0.6156499147415161, Validation Loss: 0.7175360918045044\n",
      "Epoch 53: Train Loss: 0.6292402148246765, Validation Loss: 0.7090421319007874\n",
      "Epoch 54: Train Loss: 0.5828786849975586, Validation Loss: 0.6978206634521484\n",
      "Epoch 55: Train Loss: 0.5974892020225525, Validation Loss: 0.7009239196777344\n",
      "Epoch 56: Train Loss: 0.6232766270637512, Validation Loss: 0.7052655220031738\n",
      "Epoch 57: Train Loss: 0.5810558795928955, Validation Loss: 0.7068572640419006\n",
      "Epoch 58: Train Loss: 0.5656136512756348, Validation Loss: 0.7070457935333252\n",
      "Epoch 59: Train Loss: 0.5763811826705932, Validation Loss: 0.7065183520317078\n",
      "Epoch 60: Train Loss: 0.5735777020454407, Validation Loss: 0.7108315825462341\n",
      "Epoch 61: Train Loss: 0.5730174779891968, Validation Loss: 0.7207476496696472\n",
      "Epoch 62: Train Loss: 0.6176640748977661, Validation Loss: 0.7271913886070251\n",
      "Epoch 63: Train Loss: 0.5431727170944214, Validation Loss: 0.7268887162208557\n",
      "Epoch 64: Train Loss: 0.5928377509117126, Validation Loss: 0.7278180718421936\n",
      "Epoch 65: Train Loss: 0.5324708819389343, Validation Loss: 0.7319473028182983\n",
      "Epoch 66: Train Loss: 0.624211597442627, Validation Loss: 0.7372004985809326\n",
      "Epoch 67: Train Loss: 0.5462845087051391, Validation Loss: 0.7378167510032654\n",
      "Epoch 68: Train Loss: 0.5452481627464294, Validation Loss: 0.7382711172103882\n",
      "Epoch 69: Train Loss: 0.59441739320755, Validation Loss: 0.7382270693778992\n",
      "Epoch 70: Train Loss: 0.5299059629440308, Validation Loss: 0.7432472109794617\n",
      "Epoch 71: Train Loss: 0.5859121561050415, Validation Loss: 0.7386495471000671\n",
      "Epoch 72: Train Loss: 0.6037661671638489, Validation Loss: 0.7347725033760071\n",
      "Epoch 73: Train Loss: 0.5619503021240234, Validation Loss: 0.7299022674560547\n",
      "Epoch 74: Train Loss: 0.5686930060386658, Validation Loss: 0.7311607003211975\n",
      "Epoch 75: Train Loss: 0.5218837976455688, Validation Loss: 0.7371503114700317\n",
      "Epoch 76: Train Loss: 0.6025205194950104, Validation Loss: 0.7388370037078857\n",
      "Epoch 77: Train Loss: 0.5471682131290436, Validation Loss: 0.7354634404182434\n",
      "Epoch 78: Train Loss: 0.5476737260818482, Validation Loss: 0.7355910539627075\n",
      "Epoch 79: Train Loss: 0.5496881365776062, Validation Loss: 0.735489547252655\n",
      "Epoch 80: Train Loss: 0.5745054244995117, Validation Loss: 0.7332497835159302\n",
      "Epoch 81: Train Loss: 0.5064550817012787, Validation Loss: 0.7360057234764099\n",
      "Epoch 82: Train Loss: 0.5519180357456207, Validation Loss: 0.7351020574569702\n",
      "Epoch 83: Train Loss: 0.5763872027397156, Validation Loss: 0.731783390045166\n",
      "Epoch 84: Train Loss: 0.543250322341919, Validation Loss: 0.7256979942321777\n",
      "Epoch 85: Train Loss: 0.5291956067085266, Validation Loss: 0.723526656627655\n",
      "Epoch 86: Train Loss: 0.5397385060787201, Validation Loss: 0.7258049845695496\n",
      "Epoch 87: Train Loss: 0.5406958222389221, Validation Loss: 0.7271139025688171\n",
      "Epoch 88: Train Loss: 0.5168636381626129, Validation Loss: 0.727553129196167\n",
      "Epoch 89: Train Loss: 0.5053798854351044, Validation Loss: 0.7284871339797974\n",
      "Epoch 90: Train Loss: 0.5002001583576202, Validation Loss: 0.7357382774353027\n",
      "Epoch 91: Train Loss: 0.5066785097122193, Validation Loss: 0.7431180477142334\n",
      "Epoch 92: Train Loss: 0.5187306880950928, Validation Loss: 0.7297847270965576\n",
      "Epoch 93: Train Loss: 0.5197412431240082, Validation Loss: 0.7229433059692383\n",
      "Epoch 94: Train Loss: 0.5345118045806885, Validation Loss: 0.7218356132507324\n",
      "Epoch 95: Train Loss: 0.5178891122341156, Validation Loss: 0.7225847244262695\n",
      "Epoch 96: Train Loss: 0.48522581458091735, Validation Loss: 0.7258718013763428\n",
      "Epoch 97: Train Loss: 0.5198682308197021, Validation Loss: 0.7270007729530334\n",
      "Epoch 98: Train Loss: 0.5228630423545837, Validation Loss: 0.7274826169013977\n",
      "Epoch 99: Train Loss: 0.488601541519165, Validation Loss: 0.7277711033821106\n",
      "Epoch 100: Train Loss: 0.5448065161705017, Validation Loss: 0.7305324673652649\n",
      "Epoch 101: Train Loss: 0.5463934719562531, Validation Loss: 0.7320895791053772\n",
      "Epoch 102: Train Loss: 0.4821079194545746, Validation Loss: 0.7152061462402344\n",
      "Epoch 103: Train Loss: 0.4673539996147156, Validation Loss: 0.7135411500930786\n",
      "Epoch 104: Train Loss: 0.4892545878887177, Validation Loss: 0.7250096201896667\n",
      "Epoch 105: Train Loss: 0.5101633369922638, Validation Loss: 0.7370659708976746\n",
      "Epoch 106: Train Loss: 0.4998347759246826, Validation Loss: 0.7406413555145264\n",
      "Epoch 107: Train Loss: 0.525300121307373, Validation Loss: 0.7400954961776733\n",
      "Epoch 108: Train Loss: 0.46427335143089293, Validation Loss: 0.7388135194778442\n",
      "Epoch 109: Train Loss: 0.4744118511676788, Validation Loss: 0.7391878962516785\n",
      "Epoch 110: Train Loss: 0.5079459071159362, Validation Loss: 0.747092068195343\n",
      "Epoch 111: Train Loss: 0.44587414860725405, Validation Loss: 0.7395930290222168\n",
      "Epoch 112: Train Loss: 0.5100805103778839, Validation Loss: 0.7444842457771301\n",
      "Epoch 113: Train Loss: 0.48587759733200075, Validation Loss: 0.7606339454650879\n",
      "Epoch 114: Train Loss: 0.44500415325164794, Validation Loss: 0.7680045962333679\n",
      "Epoch 115: Train Loss: 0.4475678324699402, Validation Loss: 0.7592275738716125\n",
      "Epoch 116: Train Loss: 0.44745492935180664, Validation Loss: 0.7561295628547668\n",
      "Epoch 117: Train Loss: 0.4630687892436981, Validation Loss: 0.7592176198959351\n",
      "Epoch 118: Train Loss: 0.41656285524368286, Validation Loss: 0.7558326721191406\n",
      "Epoch 119: Train Loss: 0.4656448781490326, Validation Loss: 0.7559944987297058\n",
      "Epoch 120: Train Loss: 0.45486273765563967, Validation Loss: 0.7351274490356445\n",
      "Epoch 121: Train Loss: 0.4668591976165771, Validation Loss: 0.7152209281921387\n",
      "Epoch 122: Train Loss: 0.46211575865745547, Validation Loss: 0.7366441488265991\n",
      "Epoch 123: Train Loss: 0.46752324104309084, Validation Loss: 0.7553389668464661\n",
      "Epoch 124: Train Loss: 0.4221204161643982, Validation Loss: 0.7623394131660461\n",
      "Epoch 125: Train Loss: 0.46352126002311705, Validation Loss: 0.760120153427124\n",
      "Epoch 126: Train Loss: 0.4089950382709503, Validation Loss: 0.7546388506889343\n",
      "Epoch 127: Train Loss: 0.4356076896190643, Validation Loss: 0.7496921420097351\n",
      "Epoch 128: Train Loss: 0.4484836578369141, Validation Loss: 0.7515562176704407\n",
      "Epoch 129: Train Loss: 0.49201017022132876, Validation Loss: 0.7490208745002747\n",
      "Epoch 130: Train Loss: 0.4445147693157196, Validation Loss: 0.793867826461792\n",
      "Epoch 131: Train Loss: 0.4564209282398224, Validation Loss: 0.801797091960907\n",
      "Epoch 132: Train Loss: 0.41260008215904237, Validation Loss: 0.7818012237548828\n",
      "Epoch 133: Train Loss: 0.4422536909580231, Validation Loss: 0.7845487594604492\n",
      "Epoch 134: Train Loss: 0.48587960600852964, Validation Loss: 0.8008643388748169\n",
      "Epoch 135: Train Loss: 0.4116445243358612, Validation Loss: 0.8133583068847656\n",
      "Epoch 136: Train Loss: 0.48724348545074464, Validation Loss: 0.8129871487617493\n",
      "Epoch 137: Train Loss: 0.4380820274353027, Validation Loss: 0.8064592480659485\n",
      "Epoch 138: Train Loss: 0.35859569907188416, Validation Loss: 0.8034106492996216\n",
      "Epoch 139: Train Loss: 0.4781272172927856, Validation Loss: 0.8044281005859375\n",
      "Epoch 140: Train Loss: 0.38096986413002015, Validation Loss: 0.7620536088943481\n",
      "Epoch 141: Train Loss: 0.4007040798664093, Validation Loss: 0.7704512476921082\n",
      "Epoch 142: Train Loss: 0.42249080538749695, Validation Loss: 0.7866715788841248\n",
      "Epoch 143: Train Loss: 0.44191343784332277, Validation Loss: 0.7681062817573547\n",
      "Epoch 144: Train Loss: 0.4095256745815277, Validation Loss: 0.7421123385429382\n",
      "Epoch 145: Train Loss: 0.4109591841697693, Validation Loss: 0.7514783143997192\n",
      "Epoch 146: Train Loss: 0.48348655700683596, Validation Loss: 0.7481486797332764\n",
      "Epoch 147: Train Loss: 0.39499591588974, Validation Loss: 0.7393584251403809\n",
      "Epoch 148: Train Loss: 0.3554060339927673, Validation Loss: 0.7396170496940613\n",
      "Epoch 149: Train Loss: 0.4342376053333282, Validation Loss: 0.7405393719673157\n",
      "Epoch 150: Train Loss: 0.3865547239780426, Validation Loss: 0.7535363435745239\n",
      "Epoch 151: Train Loss: 0.3909361094236374, Validation Loss: 0.7710482478141785\n",
      "Epoch 152: Train Loss: 0.3568889856338501, Validation Loss: 0.7776168584823608\n",
      "Epoch 153: Train Loss: 0.38460981845855713, Validation Loss: 0.7698705196380615\n",
      "Epoch 154: Train Loss: 0.3567628443241119, Validation Loss: 0.7522080540657043\n",
      "Epoch 155: Train Loss: 0.3587333232164383, Validation Loss: 0.7338584661483765\n",
      "Epoch 156: Train Loss: 0.3655107617378235, Validation Loss: 0.7396566271781921\n",
      "Epoch 157: Train Loss: 0.3548086702823639, Validation Loss: 0.7361332774162292\n",
      "Epoch 158: Train Loss: 0.38119807839393616, Validation Loss: 0.7405632138252258\n",
      "Epoch 159: Train Loss: 0.38038934469223024, Validation Loss: 0.7395566701889038\n",
      "Epoch 160: Train Loss: 0.3969200849533081, Validation Loss: 0.7669017314910889\n",
      "Epoch 161: Train Loss: 0.33637977838516236, Validation Loss: 0.7765336036682129\n",
      "Epoch 162: Train Loss: 0.42234091758728026, Validation Loss: 0.7601727843284607\n",
      "Epoch 163: Train Loss: 0.3398019284009933, Validation Loss: 0.7858855724334717\n",
      "Epoch 164: Train Loss: 0.3109876662492752, Validation Loss: 0.8163676261901855\n",
      "Epoch 165: Train Loss: 0.32275821566581725, Validation Loss: 0.8349931240081787\n",
      "Epoch 166: Train Loss: 0.34723864793777465, Validation Loss: 0.8325933218002319\n",
      "Epoch 167: Train Loss: 0.32713751792907714, Validation Loss: 0.8199241161346436\n",
      "Epoch 168: Train Loss: 0.33628302812576294, Validation Loss: 0.821430504322052\n",
      "Epoch 169: Train Loss: 0.39569740891456606, Validation Loss: 0.8224175572395325\n",
      "Epoch 170: Train Loss: 0.36493178009986876, Validation Loss: 0.7820612192153931\n",
      "Epoch 171: Train Loss: 0.34346734285354613, Validation Loss: 0.7954459190368652\n",
      "Epoch 172: Train Loss: 0.35532034039497373, Validation Loss: 0.7857499122619629\n",
      "Epoch 173: Train Loss: 0.32708183526992796, Validation Loss: 0.7698522210121155\n",
      "Epoch 174: Train Loss: 0.3434337705373764, Validation Loss: 0.7513155937194824\n",
      "Epoch 175: Train Loss: 0.36057076454162595, Validation Loss: 0.7486146092414856\n",
      "Epoch 176: Train Loss: 0.3192791372537613, Validation Loss: 0.7512601613998413\n",
      "Epoch 177: Train Loss: 0.30572503209114077, Validation Loss: 0.7591691613197327\n",
      "Epoch 178: Train Loss: 0.3158496618270874, Validation Loss: 0.7614439129829407\n",
      "Epoch 179: Train Loss: 0.2862994968891144, Validation Loss: 0.7621710896492004\n",
      "Epoch 180: Train Loss: 0.29564206302165985, Validation Loss: 0.7590382695198059\n",
      "Epoch 181: Train Loss: 0.296498966217041, Validation Loss: 0.7902686595916748\n",
      "Epoch 182: Train Loss: 0.32526344060897827, Validation Loss: 0.792001485824585\n",
      "Epoch 183: Train Loss: 0.30358693599700926, Validation Loss: 0.7343360781669617\n",
      "Epoch 184: Train Loss: 0.3308817625045776, Validation Loss: 0.707726240158081\n",
      "Epoch 185: Train Loss: 0.25380790829658506, Validation Loss: 0.7125213146209717\n",
      "Epoch 186: Train Loss: 0.339771443605423, Validation Loss: 0.7404888272285461\n",
      "Epoch 187: Train Loss: 0.2847809910774231, Validation Loss: 0.7770802974700928\n",
      "Epoch 188: Train Loss: 0.28635673224925995, Validation Loss: 0.7878128290176392\n",
      "Epoch 189: Train Loss: 0.347607421875, Validation Loss: 0.7863824367523193\n",
      "Epoch 190: Train Loss: 0.2985554188489914, Validation Loss: 0.7814741134643555\n",
      "Epoch 191: Train Loss: 0.32044662833213805, Validation Loss: 0.7632066607475281\n",
      "Epoch 192: Train Loss: 0.2998025596141815, Validation Loss: 0.7824862599372864\n",
      "Epoch 193: Train Loss: 0.31546708643436433, Validation Loss: 0.8453251123428345\n",
      "Epoch 194: Train Loss: 0.3112196445465088, Validation Loss: 0.8632040023803711\n",
      "Epoch 195: Train Loss: 0.3265900671482086, Validation Loss: 0.8627989888191223\n",
      "Epoch 196: Train Loss: 0.27780978977680204, Validation Loss: 0.8411582112312317\n",
      "Epoch 197: Train Loss: 0.3133456438779831, Validation Loss: 0.8207510113716125\n",
      "Epoch 198: Train Loss: 0.2758604407310486, Validation Loss: 0.8003217577934265\n",
      "Epoch 199: Train Loss: 0.27054189443588256, Validation Loss: 0.7974029779434204\n",
      "Fold 12 Metrics:\n",
      "Accuracy: 0.36363636363636365, Precision: 0.42857142857142855, Recall: 0.5, F1-score: 0.46153846153846156, AUC: 0.35\n",
      "Confusion Matrix:\n",
      "[[1 4]\n",
      " [3 3]]\n",
      "Completed fold 12\n",
      "--------------------------------------------------\n",
      "Adding 5 lie samples and 6 truth samples from subject 8 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 7 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 3 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 10 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 13 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 6 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 12 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 9 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 4 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 5 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 1 to train set\n",
      "Adding 5 lie samples and 6 truth samples from subject 2 to train set\n",
      "Adding 5 lie samples from subject 11 to test set\n",
      "Adding 6 truth samples from subject 11 to test set\n",
      "Total training samples: 132, Total test samples: 11\n",
      "(11, 65, 3750)\n",
      "(132, 65, 3750)\n",
      "Epoch 0: Train Loss: 0.712551474571228, Validation Loss: 0.6962872743606567\n",
      "Epoch 1: Train Loss: 0.6659480571746826, Validation Loss: 0.6982929706573486\n",
      "Epoch 2: Train Loss: 0.7041645288467407, Validation Loss: 0.7009397745132446\n",
      "Epoch 3: Train Loss: 0.6782374501228332, Validation Loss: 0.7030095458030701\n",
      "Epoch 4: Train Loss: 0.6523099422454834, Validation Loss: 0.703084409236908\n",
      "Epoch 5: Train Loss: 0.6573689699172973, Validation Loss: 0.7012971639633179\n",
      "Epoch 6: Train Loss: 0.7048212647438049, Validation Loss: 0.6997042894363403\n",
      "Epoch 7: Train Loss: 0.6616073369979858, Validation Loss: 0.6989747881889343\n",
      "Epoch 8: Train Loss: 0.7469065070152283, Validation Loss: 0.6984883546829224\n",
      "Epoch 9: Train Loss: 0.6563191533088684, Validation Loss: 0.6972466111183167\n",
      "Epoch 10: Train Loss: 0.6812379002571106, Validation Loss: 0.6968488693237305\n",
      "Epoch 11: Train Loss: 0.7437458038330078, Validation Loss: 0.691093921661377\n",
      "Epoch 12: Train Loss: 0.6565497994422913, Validation Loss: 0.686642587184906\n",
      "Epoch 13: Train Loss: 0.6497909903526307, Validation Loss: 0.6815932989120483\n",
      "Epoch 14: Train Loss: 0.6416848421096801, Validation Loss: 0.676835298538208\n",
      "Epoch 15: Train Loss: 0.6575536131858826, Validation Loss: 0.6747168898582458\n",
      "Epoch 16: Train Loss: 0.6312081456184387, Validation Loss: 0.6721410751342773\n",
      "Epoch 17: Train Loss: 0.6540582299232482, Validation Loss: 0.6710758209228516\n",
      "Epoch 18: Train Loss: 0.6548824906349182, Validation Loss: 0.6708123683929443\n",
      "Epoch 19: Train Loss: 0.6625169038772583, Validation Loss: 0.6706036925315857\n",
      "Epoch 20: Train Loss: 0.6800575375556945, Validation Loss: 0.672217071056366\n",
      "Epoch 21: Train Loss: 0.6813342213630676, Validation Loss: 0.6749019026756287\n",
      "Epoch 22: Train Loss: 0.6770765066146851, Validation Loss: 0.6748777627944946\n",
      "Epoch 23: Train Loss: 0.6892507314682007, Validation Loss: 0.6720956563949585\n",
      "Epoch 24: Train Loss: 0.6114139318466186, Validation Loss: 0.6685271263122559\n",
      "Epoch 25: Train Loss: 0.6197867751121521, Validation Loss: 0.6661387085914612\n",
      "Epoch 26: Train Loss: 0.6534061074256897, Validation Loss: 0.6639129519462585\n",
      "Epoch 27: Train Loss: 0.6061890482902527, Validation Loss: 0.6618075966835022\n",
      "Epoch 28: Train Loss: 0.6081559300422669, Validation Loss: 0.6608855128288269\n",
      "Epoch 29: Train Loss: 0.626399576663971, Validation Loss: 0.6609833836555481\n",
      "Epoch 30: Train Loss: 0.6097071766853333, Validation Loss: 0.654296338558197\n",
      "Epoch 31: Train Loss: 0.6390194177627564, Validation Loss: 0.6481232643127441\n",
      "Epoch 32: Train Loss: 0.6102899551391602, Validation Loss: 0.6423725485801697\n",
      "Epoch 33: Train Loss: 0.6296465992927551, Validation Loss: 0.6394728422164917\n",
      "Epoch 34: Train Loss: 0.6084185600280761, Validation Loss: 0.635107696056366\n",
      "Epoch 35: Train Loss: 0.5839169502258301, Validation Loss: 0.6291018724441528\n",
      "Epoch 36: Train Loss: 0.5745774269104004, Validation Loss: 0.6250793933868408\n",
      "Epoch 37: Train Loss: 0.6160365343093872, Validation Loss: 0.6223966479301453\n",
      "Epoch 38: Train Loss: 0.5847413420677186, Validation Loss: 0.6227694153785706\n",
      "Epoch 39: Train Loss: 0.5743677079677582, Validation Loss: 0.6242945194244385\n",
      "Epoch 40: Train Loss: 0.6010959386825562, Validation Loss: 0.6175655126571655\n",
      "Epoch 41: Train Loss: 0.5734748244285583, Validation Loss: 0.6102950572967529\n",
      "Epoch 42: Train Loss: 0.5985244512557983, Validation Loss: 0.6036117076873779\n",
      "Epoch 43: Train Loss: 0.5960548043251037, Validation Loss: 0.5989276766777039\n",
      "Epoch 44: Train Loss: 0.6439944386482239, Validation Loss: 0.5949275493621826\n",
      "Epoch 45: Train Loss: 0.6638376832008361, Validation Loss: 0.5924340486526489\n",
      "Epoch 46: Train Loss: 0.6421859741210938, Validation Loss: 0.589752733707428\n",
      "Epoch 47: Train Loss: 0.5935654640197754, Validation Loss: 0.5892764329910278\n",
      "Epoch 48: Train Loss: 0.5965372562408447, Validation Loss: 0.5890316367149353\n",
      "Epoch 49: Train Loss: 0.5656448364257812, Validation Loss: 0.5883994698524475\n",
      "Epoch 50: Train Loss: 0.5600618183612823, Validation Loss: 0.5812454223632812\n",
      "Epoch 51: Train Loss: 0.5452923715114594, Validation Loss: 0.5719980597496033\n",
      "Epoch 52: Train Loss: 0.5670152902603149, Validation Loss: 0.5631082057952881\n",
      "Epoch 53: Train Loss: 0.5995184779167175, Validation Loss: 0.5585373044013977\n",
      "Epoch 54: Train Loss: 0.564254242181778, Validation Loss: 0.5544127821922302\n",
      "Epoch 55: Train Loss: 0.5442828953266143, Validation Loss: 0.5491400957107544\n",
      "Epoch 56: Train Loss: 0.5362259685993195, Validation Loss: 0.5465382933616638\n",
      "Epoch 57: Train Loss: 0.6503059566020966, Validation Loss: 0.5417007207870483\n",
      "Epoch 58: Train Loss: 0.49942916035652163, Validation Loss: 0.5419473648071289\n",
      "Epoch 59: Train Loss: 0.5582159757614136, Validation Loss: 0.538640022277832\n",
      "Epoch 60: Train Loss: 0.5609277486801147, Validation Loss: 0.5362008810043335\n",
      "Epoch 61: Train Loss: 0.5512245893478394, Validation Loss: 0.5366641283035278\n",
      "Epoch 62: Train Loss: 0.5195662081241608, Validation Loss: 0.5242716670036316\n",
      "Epoch 63: Train Loss: 0.5734404921531677, Validation Loss: 0.5153194665908813\n",
      "Epoch 64: Train Loss: 0.5002597630023956, Validation Loss: 0.5193192362785339\n",
      "Epoch 65: Train Loss: 0.5557380855083466, Validation Loss: 0.5184957981109619\n",
      "Epoch 66: Train Loss: 0.49921839833259585, Validation Loss: 0.5124240517616272\n",
      "Epoch 67: Train Loss: 0.5295743107795715, Validation Loss: 0.5031111240386963\n",
      "Epoch 68: Train Loss: 0.5386593997478485, Validation Loss: 0.4975428879261017\n",
      "Epoch 69: Train Loss: 0.5903536736965179, Validation Loss: 0.4934312701225281\n",
      "Epoch 70: Train Loss: 0.4995108485221863, Validation Loss: 0.533630907535553\n",
      "Epoch 71: Train Loss: 0.515180104970932, Validation Loss: 0.5508555173873901\n",
      "Epoch 72: Train Loss: 0.5122605383396148, Validation Loss: 0.5420360565185547\n",
      "Epoch 73: Train Loss: 0.5430266976356506, Validation Loss: 0.5225965976715088\n",
      "Epoch 74: Train Loss: 0.4764634907245636, Validation Loss: 0.496705025434494\n",
      "Epoch 75: Train Loss: 0.4853716492652893, Validation Loss: 0.48194456100463867\n",
      "Epoch 76: Train Loss: 0.5327706694602966, Validation Loss: 0.47198736667633057\n",
      "Epoch 77: Train Loss: 0.5094850301742554, Validation Loss: 0.4643191993236542\n",
      "Epoch 78: Train Loss: 0.4825313091278076, Validation Loss: 0.4602336883544922\n",
      "Epoch 79: Train Loss: 0.4936081349849701, Validation Loss: 0.4594123959541321\n",
      "Epoch 80: Train Loss: 0.47865983843803406, Validation Loss: 0.45815905928611755\n",
      "Epoch 81: Train Loss: 0.512725967168808, Validation Loss: 0.4431402385234833\n",
      "Epoch 82: Train Loss: 0.45614043474197385, Validation Loss: 0.41124653816223145\n",
      "Epoch 83: Train Loss: 0.4756325244903564, Validation Loss: 0.4117066264152527\n",
      "Epoch 84: Train Loss: 0.5058811604976654, Validation Loss: 0.40855884552001953\n",
      "Epoch 85: Train Loss: 0.4548426330089569, Validation Loss: 0.4031607210636139\n",
      "Epoch 86: Train Loss: 0.40615593194961547, Validation Loss: 0.4012051522731781\n",
      "Epoch 87: Train Loss: 0.4305853843688965, Validation Loss: 0.3928786814212799\n",
      "Epoch 88: Train Loss: 0.4507316827774048, Validation Loss: 0.3913438320159912\n",
      "Epoch 89: Train Loss: 0.4501428246498108, Validation Loss: 0.3860888183116913\n",
      "Epoch 90: Train Loss: 0.4444044291973114, Validation Loss: 0.36069560050964355\n",
      "Epoch 91: Train Loss: 0.5042371690273285, Validation Loss: 0.3517058789730072\n",
      "Epoch 92: Train Loss: 0.4502363562583923, Validation Loss: 0.32719287276268005\n",
      "Epoch 93: Train Loss: 0.47492260932922364, Validation Loss: 0.31080296635627747\n",
      "Epoch 94: Train Loss: 0.4296876311302185, Validation Loss: 0.2924420237541199\n",
      "Epoch 95: Train Loss: 0.4332865118980408, Validation Loss: 0.28205323219299316\n",
      "Epoch 96: Train Loss: 0.4708088517189026, Validation Loss: 0.2759869396686554\n",
      "Epoch 97: Train Loss: 0.430428546667099, Validation Loss: 0.27387839555740356\n",
      "Epoch 98: Train Loss: 0.4222444772720337, Validation Loss: 0.2699924111366272\n",
      "Epoch 99: Train Loss: 0.48783127665519715, Validation Loss: 0.26799458265304565\n",
      "Epoch 100: Train Loss: 0.42734999656677247, Validation Loss: 0.27731460332870483\n",
      "Epoch 101: Train Loss: 0.470847088098526, Validation Loss: 0.25851666927337646\n",
      "Epoch 102: Train Loss: 0.4276499390602112, Validation Loss: 0.25270381569862366\n",
      "Epoch 103: Train Loss: 0.42551941275596616, Validation Loss: 0.2501116096973419\n",
      "Epoch 104: Train Loss: 0.41653427481651306, Validation Loss: 0.25396600365638733\n",
      "Epoch 105: Train Loss: 0.3827620565891266, Validation Loss: 0.2512414753437042\n",
      "Epoch 106: Train Loss: 0.38801628947257993, Validation Loss: 0.24018052220344543\n",
      "Epoch 107: Train Loss: 0.4584064781665802, Validation Loss: 0.23340362310409546\n",
      "Epoch 108: Train Loss: 0.391311252117157, Validation Loss: 0.23247352242469788\n",
      "Epoch 109: Train Loss: 0.44145538210868834, Validation Loss: 0.23394040763378143\n",
      "Epoch 110: Train Loss: 0.45931693315505984, Validation Loss: 0.24476438760757446\n",
      "Epoch 111: Train Loss: 0.4186219573020935, Validation Loss: 0.24658074975013733\n",
      "Epoch 112: Train Loss: 0.37562631964683535, Validation Loss: 0.24480845034122467\n",
      "Epoch 113: Train Loss: 0.35642249286174776, Validation Loss: 0.23764708638191223\n",
      "Epoch 114: Train Loss: 0.3876135528087616, Validation Loss: 0.2248545140028\n",
      "Epoch 115: Train Loss: 0.40612736344337463, Validation Loss: 0.2243981808423996\n",
      "Epoch 116: Train Loss: 0.3809072494506836, Validation Loss: 0.2297704517841339\n",
      "Epoch 117: Train Loss: 0.38721947073936464, Validation Loss: 0.22244380414485931\n",
      "Epoch 118: Train Loss: 0.415973836183548, Validation Loss: 0.2220466583967209\n",
      "Epoch 119: Train Loss: 0.3729400336742401, Validation Loss: 0.21274800598621368\n",
      "Epoch 120: Train Loss: 0.37406319975852964, Validation Loss: 0.2189662754535675\n",
      "Epoch 121: Train Loss: 0.40487256050109866, Validation Loss: 0.23556457459926605\n",
      "Epoch 122: Train Loss: 0.4638113796710968, Validation Loss: 0.23874005675315857\n",
      "Epoch 123: Train Loss: 0.4008149683475494, Validation Loss: 0.2318097949028015\n",
      "Epoch 124: Train Loss: 0.3858096420764923, Validation Loss: 0.216065913438797\n",
      "Epoch 125: Train Loss: 0.3306249976158142, Validation Loss: 0.20548619329929352\n",
      "Epoch 126: Train Loss: 0.3985302448272705, Validation Loss: 0.2027202844619751\n",
      "Epoch 127: Train Loss: 0.34480956196784973, Validation Loss: 0.204924538731575\n",
      "Epoch 128: Train Loss: 0.4070308983325958, Validation Loss: 0.19949395954608917\n",
      "Epoch 129: Train Loss: 0.31795315742492675, Validation Loss: 0.19583749771118164\n",
      "Epoch 130: Train Loss: 0.3634640038013458, Validation Loss: 0.2050497978925705\n",
      "Epoch 131: Train Loss: 0.3371241331100464, Validation Loss: 0.1902332305908203\n",
      "Epoch 132: Train Loss: 0.35653159618377683, Validation Loss: 0.17880702018737793\n",
      "Epoch 133: Train Loss: 0.29983174204826357, Validation Loss: 0.16541481018066406\n",
      "Epoch 134: Train Loss: 0.37579128742218015, Validation Loss: 0.168855220079422\n",
      "Epoch 135: Train Loss: 0.38695874214172366, Validation Loss: 0.17525766789913177\n",
      "Epoch 136: Train Loss: 0.3041036456823349, Validation Loss: 0.18377289175987244\n",
      "Epoch 137: Train Loss: 0.29501685202121736, Validation Loss: 0.17062678933143616\n",
      "Epoch 138: Train Loss: 0.29026252329349517, Validation Loss: 0.15790967643260956\n",
      "Epoch 139: Train Loss: 0.35407479405403136, Validation Loss: 0.15318451821804047\n",
      "Epoch 140: Train Loss: 0.2960012137889862, Validation Loss: 0.13883885741233826\n",
      "Epoch 141: Train Loss: 0.32367870807647703, Validation Loss: 0.13745225965976715\n",
      "Epoch 142: Train Loss: 0.33097587823867797, Validation Loss: 0.13595050573349\n",
      "Epoch 143: Train Loss: 0.27171785831451417, Validation Loss: 0.13884511590003967\n",
      "Epoch 144: Train Loss: 0.3161722034215927, Validation Loss: 0.14229385554790497\n",
      "Epoch 145: Train Loss: 0.31027719378471375, Validation Loss: 0.1352970451116562\n",
      "Epoch 146: Train Loss: 0.3463172197341919, Validation Loss: 0.1353335976600647\n",
      "Epoch 147: Train Loss: 0.2950966715812683, Validation Loss: 0.13040761649608612\n",
      "Epoch 148: Train Loss: 0.2427776038646698, Validation Loss: 0.12958797812461853\n",
      "Epoch 149: Train Loss: 0.3248218297958374, Validation Loss: 0.12583592534065247\n",
      "Epoch 150: Train Loss: 0.26046920716762545, Validation Loss: 0.14097364246845245\n",
      "Epoch 151: Train Loss: 0.34543814063072203, Validation Loss: 0.14616450667381287\n",
      "Epoch 152: Train Loss: 0.2948683202266693, Validation Loss: 0.13559629023075104\n",
      "Epoch 153: Train Loss: 0.26701171398162843, Validation Loss: 0.12656362354755402\n",
      "Epoch 154: Train Loss: 0.28297117054462434, Validation Loss: 0.12338082492351532\n",
      "Epoch 155: Train Loss: 0.29869251847267153, Validation Loss: 0.12501130998134613\n",
      "Epoch 156: Train Loss: 0.3281023144721985, Validation Loss: 0.13451069593429565\n",
      "Epoch 157: Train Loss: 0.29012196362018583, Validation Loss: 0.14557085931301117\n",
      "Epoch 158: Train Loss: 0.29830444753170016, Validation Loss: 0.13635829091072083\n",
      "Epoch 159: Train Loss: 0.3257416784763336, Validation Loss: 0.13573835790157318\n",
      "Epoch 160: Train Loss: 0.263990518450737, Validation Loss: 0.17406697571277618\n",
      "Epoch 161: Train Loss: 0.24879602789878846, Validation Loss: 0.17620216310024261\n",
      "Epoch 162: Train Loss: 0.255841389298439, Validation Loss: 0.15700237452983856\n",
      "Epoch 163: Train Loss: 0.28319181203842164, Validation Loss: 0.1467386782169342\n",
      "Epoch 164: Train Loss: 0.35085600316524507, Validation Loss: 0.13148361444473267\n",
      "Epoch 165: Train Loss: 0.21980804949998856, Validation Loss: 0.12213890999555588\n",
      "Epoch 166: Train Loss: 0.2891716241836548, Validation Loss: 0.11328906565904617\n",
      "Epoch 167: Train Loss: 0.24923412799835204, Validation Loss: 0.11556555330753326\n",
      "Epoch 168: Train Loss: 0.26157276928424833, Validation Loss: 0.11101706326007843\n",
      "Epoch 169: Train Loss: 0.24514006376266478, Validation Loss: 0.1095060333609581\n",
      "Epoch 170: Train Loss: 0.2755292147397995, Validation Loss: 0.09599977731704712\n",
      "Epoch 171: Train Loss: 0.20886640548706054, Validation Loss: 0.09710504114627838\n",
      "Epoch 172: Train Loss: 0.2942592859268188, Validation Loss: 0.09975377470254898\n",
      "Epoch 173: Train Loss: 0.23903357982635498, Validation Loss: 0.09921319037675858\n",
      "Epoch 174: Train Loss: 0.4694381654262543, Validation Loss: 0.10787777602672577\n",
      "Epoch 175: Train Loss: 0.23670195639133454, Validation Loss: 0.12963972985744476\n",
      "Epoch 176: Train Loss: 0.2136475622653961, Validation Loss: 0.13354073464870453\n",
      "Epoch 177: Train Loss: 0.27035907804965975, Validation Loss: 0.1385730654001236\n",
      "Epoch 178: Train Loss: 0.2546740859746933, Validation Loss: 0.1342802196741104\n",
      "Epoch 179: Train Loss: 0.26254130303859713, Validation Loss: 0.13836298882961273\n",
      "Epoch 180: Train Loss: 0.22449717819690704, Validation Loss: 0.15842480957508087\n",
      "Epoch 181: Train Loss: 0.3694067120552063, Validation Loss: 0.13100117444992065\n",
      "Epoch 182: Train Loss: 0.23323639929294587, Validation Loss: 0.18078428506851196\n",
      "Epoch 183: Train Loss: 0.2083159163594246, Validation Loss: 0.20804962515830994\n",
      "Epoch 184: Train Loss: 0.28659080862998965, Validation Loss: 0.18635176122188568\n",
      "Epoch 185: Train Loss: 0.23679344952106476, Validation Loss: 0.1752128303050995\n",
      "Epoch 186: Train Loss: 0.25301678776741027, Validation Loss: 0.17500151693820953\n",
      "Epoch 187: Train Loss: 0.21653063595294952, Validation Loss: 0.16801904141902924\n",
      "Epoch 188: Train Loss: 0.22052846252918243, Validation Loss: 0.16796834766864777\n",
      "Epoch 189: Train Loss: 0.2426576465368271, Validation Loss: 0.16154508292675018\n",
      "Epoch 190: Train Loss: 0.2936673045158386, Validation Loss: 0.1388104110956192\n",
      "Epoch 191: Train Loss: 0.26662119925022126, Validation Loss: 0.12686240673065186\n",
      "Epoch 192: Train Loss: 0.19065011739730836, Validation Loss: 0.1400994062423706\n",
      "Epoch 193: Train Loss: 0.24917386770248412, Validation Loss: 0.13346803188323975\n",
      "Epoch 194: Train Loss: 0.21190972924232482, Validation Loss: 0.13786588609218597\n",
      "Epoch 195: Train Loss: 0.19317694902420043, Validation Loss: 0.14600971341133118\n",
      "Epoch 196: Train Loss: 0.26608461141586304, Validation Loss: 0.15587559342384338\n",
      "Epoch 197: Train Loss: 0.2867152735590935, Validation Loss: 0.16189998388290405\n",
      "Epoch 198: Train Loss: 0.1852908968925476, Validation Loss: 0.1639462411403656\n",
      "Epoch 199: Train Loss: 0.17860291600227357, Validation Loss: 0.16371694207191467\n",
      "Fold 13 Metrics:\n",
      "Accuracy: 1.0, Precision: 1.0, Recall: 1.0, F1-score: 1.0, AUC: 1.0\n",
      "Confusion Matrix:\n",
      "[[5 0]\n",
      " [0 6]]\n",
      "Completed fold 13\n",
      "--------------------------------------------------\n",
      "Overall Metrics:\n",
      "Accuracy: 0.5524475524475524, Precision: 0.5945945945945946, Recall: 0.5641025641025641, F1-score: 0.5789473684210527, AUC: 0.5512820512820513\n",
      "Overall Confusion Matrix:\n",
      "[[35 30]\n",
      " [34 44]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "model_save_dir = r'C:\\Users\\User\\Documents\\Lie detect data\\Model'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion Matrix'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='viridis', xticklabels=['lie', 'truth'], yticklabels=['lie', 'truth'])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.show()\n",
    "\n",
    "# Define a function to load and pad data\n",
    "def load_data(data_dir, max_length):\n",
    "    subject_data = {'lie': {}, 'truth': {}}\n",
    "    \n",
    "    file_list = os.listdir(data_dir)\n",
    "    \n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Determine if the file is 'truth' or 'lie'\n",
    "        label_type = 'truth' if 'truth' in file else 'lie'\n",
    "        subj_id = int(file.split('_')[-1].split('.')[0])\n",
    "        \n",
    "        # Grouping logic\n",
    "        if label_type == 'lie':\n",
    "            # Mapping each 5 lie samples to one subject\n",
    "            subject_key = (subj_id - 1) // 5 + 1\n",
    "        else:  # 'truth'\n",
    "            # Mapping each 6 truth samples to one subject\n",
    "            subject_key = (subj_id - 1) // 6 + 1\n",
    "        \n",
    "        # Initialize the subject's list if it doesn't exist\n",
    "        if subject_key not in subject_data[label_type]:\n",
    "            subject_data[label_type][subject_key] = []\n",
    "        \n",
    "        # Pad or truncate the data to match max_length\n",
    "        if data.shape[1] > max_length:\n",
    "            processed_data = data[:, :max_length]  # Truncate if it exceeds max_length\n",
    "        else:\n",
    "            processed_data = np.zeros((data.shape[0], max_length))\n",
    "            processed_data[:, :data.shape[1]] = data  # Pad if it is shorter than max_length\n",
    "        \n",
    "        # Add the processed data to the appropriate list\n",
    "        subject_data[label_type][subject_key].append(processed_data)\n",
    "    \n",
    "    return subject_data\n",
    "\n",
    "# Load dataset and pad/truncate the data\n",
    "data_dir = \"C:\\\\Users\\\\User\\\\Documents\\\\Lie detect data\\\\7M_EEGData\"\n",
    "max_length = 3750 # Define maximum length for padding\n",
    "subject_data = load_data(data_dir, max_length)\n",
    "\n",
    "# Count the total number of samples\n",
    "num_lie_samples = sum(len(subject_data['lie'][subject_key]) for subject_key in subject_data['lie'])\n",
    "num_truth_samples = sum(len(subject_data['truth'][subject_key]) for subject_key in subject_data['truth'])\n",
    "\n",
    "print(f\"Number of 'lie' samples: {num_lie_samples}\")\n",
    "print(f\"Number of 'truth' samples: {num_truth_samples}\")\n",
    "print(f\"Total number of samples: {num_lie_samples + num_truth_samples}\")\n",
    "\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure the data is reshaped to [1, Chans, Samples]\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# Define EEGNet model\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int, kernel_size: tuple, padding: tuple = 0):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.depthwise_conv = nn.Conv2d(self.c_in, self.c_in, kernel_size=self.kernel_size,\n",
    "                                        padding=self.padding, groups=self.c_in)\n",
    "        self.conv2d_1x1 = nn.Conv2d(self.c_in, self.c_out, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y = self.depthwise_conv(x)\n",
    "        y = self.conv2d_1x1(y)\n",
    "        return y\n",
    "\n",
    "class SeparableConv1d(nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int, kernel_size: tuple, padding: tuple = 0):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.depthwise_conv = nn.Conv1d(self.c_in, self.c_in, kernel_size=self.kernel_size,\n",
    "                                        padding=self.padding, groups=self.c_in)\n",
    "        self.conv1d_1x1 = nn.Conv1d(self.c_in, self.c_out, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y = self.depthwise_conv(x)\n",
    "        y = self.conv1d_1x1(y)\n",
    "        return y\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, nb_classes: int = 2, Chans: int = 65, Samples: int = 3750,\n",
    "                 dropoutRate: float = 0.7, kernLength: int = 63,\n",
    "                 F1:int = 8, D:int = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        F2 = F1 * D\n",
    "\n",
    "        # Make kernel size and odd number\n",
    "        try:\n",
    "            assert kernLength % 2 != 0\n",
    "        except AssertionError:\n",
    "            raise ValueError(\"ERROR: kernLength must be odd number\")\n",
    "\n",
    "        # In: (B, Chans, Samples, 1)\n",
    "        # Out: (B, F1, Samples, 1)\n",
    "        self.conv1 = nn.Conv1d(Chans, F1, kernLength, padding=(kernLength // 2))\n",
    "        self.bn1 = nn.BatchNorm1d(F1) # (B, F1, Samples, 1)\n",
    "        # In: (B, F1, Samples, 1)\n",
    "        # Out: (B, F2, Samples - Chans + 1, 1)\n",
    "        self.conv2 = nn.Conv1d(F1, F2, Chans, groups=F1)\n",
    "        self.bn2 = nn.BatchNorm1d(F2) # (B, F2, Samples - Chans + 1, 1)\n",
    "        # In: (B, F2, Samples - Chans + 1, 1)\n",
    "        # Out: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        self.avg_pool = nn.AvgPool1d(4)\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "\n",
    "        # In: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        # Out: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        self.conv3 = SeparableConv1d(F2, F2, kernel_size=31, padding=15)\n",
    "        self.bn3 = nn.BatchNorm1d(F2)\n",
    "        # In: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        # Out: (B, F2, (Samples - Chans + 1) / 32, 1)\n",
    "        self.avg_pool2 = nn.AvgPool1d(8)\n",
    "        # In: (B, F2 *  (Samples - Chans + 1) / 32)\n",
    "        self.fc = nn.Linear(F2 * ((Samples - Chans + 1) // 32), nb_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Block 1\n",
    "        y1 = self.conv1(x)\n",
    "        #print(\"conv1: \", y1.shape)\n",
    "        y1 = self.bn1(y1)\n",
    "        #print(\"bn1: \", y1.shape)\n",
    "        y1 = self.conv2(y1)\n",
    "        #print(\"conv2\", y1.shape)\n",
    "        y1 = F.elu(self.bn2(y1))\n",
    "        #print(\"bn2\", y1.shape)\n",
    "        y1 = self.avg_pool(y1)\n",
    "        #print(\"avg_pool\", y1.shape)\n",
    "        y1 = self.dropout(y1)\n",
    "        #print(\"dropout\", y1.shape)\n",
    "\n",
    "        # Block 2\n",
    "        y2 = self.conv3(y1)\n",
    "        #print(\"conv3\", y2.shape)\n",
    "        y2 = F.elu(self.bn3(y2))\n",
    "        #print(\"bn3\", y2.shape)\n",
    "        y2 = self.avg_pool2(y2)\n",
    "        #print(\"avg_pool2\", y2.shape)\n",
    "        y2 = self.dropout(y2)\n",
    "        #print(\"dropout\", y2.shape)\n",
    "        y2 = torch.flatten(y2, 1)\n",
    "        #print(\"flatten\", y2.shape)\n",
    "        y2 = self.fc(y2)\n",
    "        #print(\"fc\", y2.shape)\n",
    "\n",
    "        return y2\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_and_evaluate(train_loader, val_loader, y_train):\n",
    "    model = EEGNet().to(device)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-2)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "    num_epochs = 200\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 200\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "            fold_model_path = os.path.join(model_save_dir, f'revise_model_fold_{fold_idx}.pth')\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': best_val_loss,\n",
    "            }, fold_model_path)\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize the cross-validation\n",
    "subject_ids = list(range(1, 14))  # Since there are 13 subjects for lie and truth\n",
    "random.shuffle(subject_ids)  # Shuffle to ensure random selection\n",
    "\n",
    "# Initialize arrays to store all labels, predictions, and metrics\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "fold_accuracies = []\n",
    "fold_precisions = []\n",
    "fold_recalls = []\n",
    "fold_f1_scores = []\n",
    "fold_aucs = []\n",
    "fold_conf_matrices = []\n",
    "\n",
    "for fold_idx in range(13):\n",
    "    # Split subjects into test and training sets\n",
    "    test_subject = subject_ids[fold_idx]\n",
    "\n",
    "    # Prepare training and test data\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for subject_id in subject_ids:\n",
    "        lie_samples = subject_data['lie'].get(subject_id, [])\n",
    "        truth_samples = subject_data['truth'].get(subject_id, [])\n",
    "\n",
    "        if subject_id == test_subject:\n",
    "            X_test.extend(lie_samples)\n",
    "            y_test.extend([0] * len(lie_samples))\n",
    "            print(f\"Adding {len(lie_samples)} lie samples from subject {subject_id} to test set\")\n",
    "            X_test.extend(truth_samples)\n",
    "            y_test.extend([1] * len(truth_samples))\n",
    "            print(f\"Adding {len(truth_samples)} truth samples from subject {subject_id} to test set\")\n",
    "        else:\n",
    "            X_train.extend(lie_samples)\n",
    "            y_train.extend([0] * len(lie_samples))\n",
    "            X_train.extend(truth_samples)\n",
    "            y_train.extend([1] * len(truth_samples))\n",
    "            print(f\"Adding {len(lie_samples)} lie samples and {len(truth_samples)} truth samples from subject {subject_id} to train set\")\n",
    "\n",
    "    print(f\"Total training samples: {len(X_train)}, Total test samples: {len(X_test)}\")\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    # Normalize data using scaler fitted on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_test = scaler.transform(X_test.reshape(X_test.shape[0], -1))\n",
    "    X_train = X_train.reshape(-1, 65, max_length)\n",
    "    X_test = X_test.reshape(-1, 65, max_length)\n",
    "\n",
    "    print(X_test.shape)\n",
    "    print(X_train.shape)\n",
    "\n",
    "    # Save the scaler to a file\n",
    "    with open(r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\RevisedEEGNet_scaler_{}.pkl'.format(fold_idx), 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = EEGDataset(X_train, y_train)\n",
    "    test_dataset = EEGDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    model = train_and_evaluate(train_loader, test_loader, y_train)\n",
    "\n",
    "    # Evaluate on the test set and calculate metrics\n",
    "    model.eval()\n",
    "    fold_labels = []\n",
    "    fold_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            fold_labels.extend(y_batch.cpu().numpy())\n",
    "            fold_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics for the current fold\n",
    "    accuracy = accuracy_score(fold_labels, fold_predictions)\n",
    "    precision = precision_score(fold_labels, fold_predictions)\n",
    "    recall = recall_score(fold_labels, fold_predictions)\n",
    "    f1 = f1_score(fold_labels, fold_predictions)\n",
    "    auc = roc_auc_score(fold_labels, fold_predictions)\n",
    "    conf_matrix = confusion_matrix(fold_labels, fold_predictions)\n",
    "\n",
    "    # Store fold metrics\n",
    "    fold_accuracies.append(accuracy)\n",
    "    fold_precisions.append(precision)\n",
    "    fold_recalls.append(recall)\n",
    "    fold_f1_scores.append(f1)\n",
    "    fold_aucs.append(auc)\n",
    "    fold_conf_matrices.append(conf_matrix)\n",
    "\n",
    "    # Print metrics for the current fold\n",
    "    print(f'Fold {fold_idx + 1} Metrics:')\n",
    "    print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Aggregate all labels and predictions for final evaluation across all folds\n",
    "    all_labels.extend(fold_labels)\n",
    "    all_predictions.extend(fold_predictions)\n",
    "\n",
    "    print(f'Completed fold {fold_idx + 1}')\n",
    "    print('-' * 50)\n",
    "\n",
    "# Calculate overall metrics across all folds\n",
    "overall_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "overall_precision = precision_score(all_labels, all_predictions)\n",
    "overall_recall = recall_score(all_labels, all_predictions)\n",
    "overall_f1 = f1_score(all_labels, all_predictions)\n",
    "overall_auc = roc_auc_score(all_labels, all_predictions)\n",
    "overall_conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(f'Overall Metrics:')\n",
    "print(f'Accuracy: {overall_accuracy}, Precision: {overall_precision}, Recall: {overall_recall}, F1-score: {overall_f1}, AUC: {overall_auc}')\n",
    "print('Overall Confusion Matrix:')\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "plot_confusion_matrix(overall_conf_matrix, title='Overall Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae639325-1800-4763-ad37-b5931e4391c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abbb309-50a0-43fa-bcc0-05e6d62e6cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
