{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29d5ee57-cf0b-4330-985e-ada385863b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.7191913723945618, Validation Loss: 0.6939796209335327\n",
      "Epoch 1: Train Loss: 0.7561951577663422, Validation Loss: 0.6962319612503052\n",
      "Epoch 2: Train Loss: 0.7123105078935623, Validation Loss: 0.6957748532295227\n",
      "Epoch 3: Train Loss: 0.6844078600406647, Validation Loss: 0.6944807767868042\n",
      "Epoch 4: Train Loss: 0.6975468993186951, Validation Loss: 0.6937009692192078\n",
      "Epoch 5: Train Loss: 0.7248170971870422, Validation Loss: 0.6936878561973572\n",
      "Epoch 6: Train Loss: 0.7037608623504639, Validation Loss: 0.6941452622413635\n",
      "Epoch 7: Train Loss: 0.6803794056177139, Validation Loss: 0.6941950917243958\n",
      "Epoch 8: Train Loss: 0.7037793844938278, Validation Loss: 0.6942302584648132\n",
      "Epoch 9: Train Loss: 0.6633947938680649, Validation Loss: 0.694333553314209\n",
      "Epoch 10: Train Loss: 0.7113292217254639, Validation Loss: 0.6927846670150757\n",
      "Epoch 11: Train Loss: 0.6625527143478394, Validation Loss: 0.691508412361145\n",
      "Epoch 12: Train Loss: 0.711758092045784, Validation Loss: 0.6879533529281616\n",
      "Epoch 13: Train Loss: 0.6802583932876587, Validation Loss: 0.6831730604171753\n",
      "Epoch 14: Train Loss: 0.6298822462558746, Validation Loss: 0.6802965998649597\n",
      "Epoch 15: Train Loss: 0.6954577565193176, Validation Loss: 0.6785736083984375\n",
      "Epoch 16: Train Loss: 0.6698554754257202, Validation Loss: 0.6779805421829224\n",
      "Epoch 17: Train Loss: 0.6812174618244171, Validation Loss: 0.6774153709411621\n",
      "Epoch 18: Train Loss: 0.6544696390628815, Validation Loss: 0.6767711043357849\n",
      "Epoch 19: Train Loss: 0.6538769751787186, Validation Loss: 0.6763930916786194\n",
      "Epoch 20: Train Loss: 0.6702069640159607, Validation Loss: 0.6761331558227539\n",
      "Epoch 21: Train Loss: 0.6992835849523544, Validation Loss: 0.6780146956443787\n",
      "Epoch 22: Train Loss: 0.65842704474926, Validation Loss: 0.6790025234222412\n",
      "Epoch 23: Train Loss: 0.6477153301239014, Validation Loss: 0.6789450645446777\n",
      "Epoch 24: Train Loss: 0.633917972445488, Validation Loss: 0.6784220933914185\n",
      "Epoch 25: Train Loss: 0.6534321457147598, Validation Loss: 0.6782894730567932\n",
      "Epoch 26: Train Loss: 0.6910873204469681, Validation Loss: 0.6780731081962585\n",
      "Epoch 27: Train Loss: 0.6325592398643494, Validation Loss: 0.6781318783760071\n",
      "Epoch 28: Train Loss: 0.6223856806755066, Validation Loss: 0.6782772541046143\n",
      "Epoch 29: Train Loss: 0.6532330065965652, Validation Loss: 0.6782612204551697\n",
      "Epoch 30: Train Loss: 0.628060907125473, Validation Loss: 0.6824420690536499\n",
      "Epoch 31: Train Loss: 0.609014168381691, Validation Loss: 0.6847286820411682\n",
      "Epoch 32: Train Loss: 0.6354487836360931, Validation Loss: 0.674964964389801\n",
      "Epoch 33: Train Loss: 0.6459844708442688, Validation Loss: 0.6669124364852905\n",
      "Epoch 34: Train Loss: 0.6175233423709869, Validation Loss: 0.6645062565803528\n",
      "Epoch 35: Train Loss: 0.5715236067771912, Validation Loss: 0.664061427116394\n",
      "Epoch 36: Train Loss: 0.5968325138092041, Validation Loss: 0.6658141613006592\n",
      "Epoch 37: Train Loss: 0.5978442281484604, Validation Loss: 0.667877197265625\n",
      "Epoch 38: Train Loss: 0.6298797875642776, Validation Loss: 0.668775737285614\n",
      "Epoch 39: Train Loss: 0.6141563504934311, Validation Loss: 0.668668270111084\n",
      "Epoch 40: Train Loss: 0.5718330591917038, Validation Loss: 0.6766948103904724\n",
      "Epoch 41: Train Loss: 0.5518044680356979, Validation Loss: 0.6846029162406921\n",
      "Epoch 42: Train Loss: 0.6137797981500626, Validation Loss: 0.6771825551986694\n",
      "Epoch 43: Train Loss: 0.6044090688228607, Validation Loss: 0.670818030834198\n",
      "Epoch 44: Train Loss: 0.6260157078504562, Validation Loss: 0.6668959856033325\n",
      "Epoch 45: Train Loss: 0.6230148077011108, Validation Loss: 0.668938159942627\n",
      "Epoch 46: Train Loss: 0.6175971031188965, Validation Loss: 0.6701872944831848\n",
      "Epoch 47: Train Loss: 0.5783637538552284, Validation Loss: 0.6697198748588562\n",
      "Epoch 48: Train Loss: 0.5409249812364578, Validation Loss: 0.6692199110984802\n",
      "Epoch 49: Train Loss: 0.5378205552697182, Validation Loss: 0.6690467000007629\n",
      "Epoch 50: Train Loss: 0.5306508392095566, Validation Loss: 0.6617172956466675\n",
      "Epoch 51: Train Loss: 0.5513112246990204, Validation Loss: 0.6577280759811401\n",
      "Epoch 52: Train Loss: 0.5562165081501007, Validation Loss: 0.6607025265693665\n",
      "Epoch 53: Train Loss: 0.5657481551170349, Validation Loss: 0.6651460528373718\n",
      "Epoch 54: Train Loss: 0.5321620628237724, Validation Loss: 0.659674346446991\n",
      "Epoch 55: Train Loss: 0.5215176418423653, Validation Loss: 0.6570911407470703\n",
      "Epoch 56: Train Loss: 0.5018209293484688, Validation Loss: 0.6529894471168518\n",
      "Epoch 57: Train Loss: 0.4831709861755371, Validation Loss: 0.6535083651542664\n",
      "Epoch 58: Train Loss: 0.5002190992236137, Validation Loss: 0.6538148522377014\n",
      "Epoch 59: Train Loss: 0.4792792424559593, Validation Loss: 0.653501570224762\n",
      "Epoch 60: Train Loss: 0.5152653381228447, Validation Loss: 0.6612825393676758\n",
      "Epoch 61: Train Loss: 0.5030759274959564, Validation Loss: 0.643894374370575\n",
      "Epoch 62: Train Loss: 0.5235275998711586, Validation Loss: 0.6264050602912903\n",
      "Epoch 63: Train Loss: 0.46856430917978287, Validation Loss: 0.651093602180481\n",
      "Epoch 64: Train Loss: 0.4684366136789322, Validation Loss: 0.6654693484306335\n",
      "Epoch 65: Train Loss: 0.5055734887719154, Validation Loss: 0.6521323919296265\n",
      "Epoch 66: Train Loss: 0.45428138971328735, Validation Loss: 0.6354455351829529\n",
      "Epoch 67: Train Loss: 0.45615391433238983, Validation Loss: 0.6290922164916992\n",
      "Epoch 68: Train Loss: 0.5236594378948212, Validation Loss: 0.6292451024055481\n",
      "Epoch 69: Train Loss: 0.4523277282714844, Validation Loss: 0.6312466263771057\n",
      "Epoch 70: Train Loss: 0.46107616275548935, Validation Loss: 0.6595795750617981\n",
      "Epoch 71: Train Loss: 0.472762830555439, Validation Loss: 0.6271718740463257\n",
      "Epoch 72: Train Loss: 0.4546177238225937, Validation Loss: 0.6447750926017761\n",
      "Epoch 73: Train Loss: 0.4432377964258194, Validation Loss: 0.6433529853820801\n",
      "Epoch 74: Train Loss: 0.41889115422964096, Validation Loss: 0.6240271329879761\n",
      "Epoch 75: Train Loss: 0.3896806314587593, Validation Loss: 0.6083580255508423\n",
      "Epoch 76: Train Loss: 0.40581539273262024, Validation Loss: 0.6149145364761353\n",
      "Epoch 77: Train Loss: 0.40509676933288574, Validation Loss: 0.6157702207565308\n",
      "Epoch 78: Train Loss: 0.4001086577773094, Validation Loss: 0.6187008619308472\n",
      "Epoch 79: Train Loss: 0.3859872594475746, Validation Loss: 0.6178256869316101\n",
      "Epoch 80: Train Loss: 0.3902113661170006, Validation Loss: 0.5912919640541077\n",
      "Epoch 81: Train Loss: 0.3913761377334595, Validation Loss: 0.5892985463142395\n",
      "Epoch 82: Train Loss: 0.38409573584795, Validation Loss: 0.6072248220443726\n",
      "Epoch 83: Train Loss: 0.36134878545999527, Validation Loss: 0.604702353477478\n",
      "Epoch 84: Train Loss: 0.34994807839393616, Validation Loss: 0.5945401787757874\n",
      "Epoch 85: Train Loss: 0.3505413755774498, Validation Loss: 0.597354531288147\n",
      "Epoch 86: Train Loss: 0.32187944650650024, Validation Loss: 0.5922639966011047\n",
      "Epoch 87: Train Loss: 0.35591935366392136, Validation Loss: 0.5822386145591736\n",
      "Epoch 88: Train Loss: 0.3157965689897537, Validation Loss: 0.5837215185165405\n",
      "Epoch 89: Train Loss: 0.3216801807284355, Validation Loss: 0.5844788551330566\n",
      "Epoch 90: Train Loss: 0.3184664696455002, Validation Loss: 0.5595158934593201\n",
      "Epoch 91: Train Loss: 0.32095281034708023, Validation Loss: 0.5622419714927673\n",
      "Epoch 92: Train Loss: 0.28404631465673447, Validation Loss: 0.5765255689620972\n",
      "Epoch 93: Train Loss: 0.2944590635597706, Validation Loss: 0.5444295406341553\n",
      "Epoch 94: Train Loss: 0.32008014991879463, Validation Loss: 0.5728558301925659\n",
      "Epoch 95: Train Loss: 0.29094186425209045, Validation Loss: 0.6184455156326294\n",
      "Epoch 96: Train Loss: 0.2878582552075386, Validation Loss: 0.5655444860458374\n",
      "Epoch 97: Train Loss: 0.29524020105600357, Validation Loss: 0.5476737022399902\n",
      "Epoch 98: Train Loss: 0.3078553155064583, Validation Loss: 0.5470346212387085\n",
      "Epoch 99: Train Loss: 0.35229259356856346, Validation Loss: 0.5490087866783142\n",
      "Epoch 100: Train Loss: 0.27410297468304634, Validation Loss: 0.5492456555366516\n",
      "Epoch 101: Train Loss: 0.28929200023412704, Validation Loss: 0.4960237741470337\n",
      "Epoch 102: Train Loss: 0.2529262453317642, Validation Loss: 0.5135980248451233\n",
      "Epoch 103: Train Loss: 0.2571925185620785, Validation Loss: 0.5180033445358276\n",
      "Epoch 104: Train Loss: 0.22500881925225258, Validation Loss: 0.4935930073261261\n",
      "Epoch 105: Train Loss: 0.25207025557756424, Validation Loss: 0.4812104105949402\n",
      "Epoch 106: Train Loss: 0.2251572087407112, Validation Loss: 0.49769508838653564\n",
      "Epoch 107: Train Loss: 0.21435603499412537, Validation Loss: 0.5075108408927917\n",
      "Epoch 108: Train Loss: 0.2558565214276314, Validation Loss: 0.499169260263443\n",
      "Epoch 109: Train Loss: 0.22388452664017677, Validation Loss: 0.4949508011341095\n",
      "Epoch 110: Train Loss: 0.24615876376628876, Validation Loss: 0.46454426646232605\n",
      "Epoch 111: Train Loss: 0.266291756182909, Validation Loss: 0.5024845600128174\n",
      "Epoch 112: Train Loss: 0.2378217913210392, Validation Loss: 0.4789397120475769\n",
      "Epoch 113: Train Loss: 0.24779312685132027, Validation Loss: 0.47940340638160706\n",
      "Epoch 114: Train Loss: 0.21793371811509132, Validation Loss: 0.46182918548583984\n",
      "Epoch 115: Train Loss: 0.25118956342339516, Validation Loss: 0.45558613538742065\n",
      "Epoch 116: Train Loss: 0.20320044085383415, Validation Loss: 0.4678763151168823\n",
      "Epoch 117: Train Loss: 0.22825577482581139, Validation Loss: 0.47408100962638855\n",
      "Epoch 118: Train Loss: 0.205120038241148, Validation Loss: 0.47947531938552856\n",
      "Epoch 119: Train Loss: 0.20676696673035622, Validation Loss: 0.47962120175361633\n",
      "Epoch 120: Train Loss: 0.191293153911829, Validation Loss: 0.4813605546951294\n",
      "Epoch 121: Train Loss: 0.19584058970212936, Validation Loss: 0.4901217222213745\n",
      "Epoch 122: Train Loss: 0.2241816222667694, Validation Loss: 0.47104594111442566\n",
      "Epoch 123: Train Loss: 0.17936019226908684, Validation Loss: 0.4748455882072449\n",
      "Epoch 124: Train Loss: 0.1939801089465618, Validation Loss: 0.47446906566619873\n",
      "Epoch 125: Train Loss: 0.1931978389620781, Validation Loss: 0.48413363099098206\n",
      "Epoch 126: Train Loss: 0.21533214673399925, Validation Loss: 0.48380517959594727\n",
      "Epoch 127: Train Loss: 0.15279361978173256, Validation Loss: 0.4902941584587097\n",
      "Epoch 128: Train Loss: 0.19289879500865936, Validation Loss: 0.4847303330898285\n",
      "Epoch 129: Train Loss: 0.18152036145329475, Validation Loss: 0.48160114884376526\n",
      "Epoch 130: Train Loss: 0.2159457989037037, Validation Loss: 0.4603746235370636\n",
      "Epoch 131: Train Loss: 0.18389370664954185, Validation Loss: 0.4660794138908386\n",
      "Epoch 132: Train Loss: 0.1964389719069004, Validation Loss: 0.44760799407958984\n",
      "Epoch 133: Train Loss: 0.1919529903680086, Validation Loss: 0.4170511066913605\n",
      "Epoch 134: Train Loss: 0.17210078611969948, Validation Loss: 0.42809316515922546\n",
      "Epoch 135: Train Loss: 0.16010180488228798, Validation Loss: 0.4478994309902191\n",
      "Epoch 136: Train Loss: 0.17966414242982864, Validation Loss: 0.41897493600845337\n",
      "Epoch 137: Train Loss: 0.14849796146154404, Validation Loss: 0.41491755843162537\n",
      "Epoch 138: Train Loss: 0.13759936578571796, Validation Loss: 0.4168236553668976\n",
      "Epoch 139: Train Loss: 0.1685129813849926, Validation Loss: 0.4176293909549713\n",
      "Epoch 140: Train Loss: 0.16442526504397392, Validation Loss: 0.448552668094635\n",
      "Epoch 141: Train Loss: 0.19874410703778267, Validation Loss: 0.4469640552997589\n",
      "Epoch 142: Train Loss: 0.1751096025109291, Validation Loss: 0.45869380235671997\n",
      "Epoch 143: Train Loss: 0.18524808064103127, Validation Loss: 0.44190698862075806\n",
      "Epoch 144: Train Loss: 0.1491044070571661, Validation Loss: 0.4380106031894684\n",
      "Epoch 145: Train Loss: 0.16029118187725544, Validation Loss: 0.43999508023262024\n",
      "Epoch 146: Train Loss: 0.15369844436645508, Validation Loss: 0.4444749653339386\n",
      "Epoch 147: Train Loss: 0.15829694271087646, Validation Loss: 0.4468572735786438\n",
      "Epoch 148: Train Loss: 0.1128753274679184, Validation Loss: 0.4517829716205597\n",
      "Epoch 149: Train Loss: 0.1822221614420414, Validation Loss: 0.44856175780296326\n",
      "Epoch 150: Train Loss: 0.18080591782927513, Validation Loss: 0.44484174251556396\n",
      "Epoch 151: Train Loss: 0.16571572422981262, Validation Loss: 0.450718492269516\n",
      "Epoch 152: Train Loss: 0.13427702151238918, Validation Loss: 0.4700186848640442\n",
      "Epoch 153: Train Loss: 0.1453688908368349, Validation Loss: 0.500779926776886\n",
      "Epoch 154: Train Loss: 0.1334593091160059, Validation Loss: 0.4905163049697876\n",
      "Epoch 155: Train Loss: 0.14652136713266373, Validation Loss: 0.4705909788608551\n",
      "Epoch 156: Train Loss: 0.14707252755761147, Validation Loss: 0.476276695728302\n",
      "Epoch 157: Train Loss: 0.11308205500245094, Validation Loss: 0.48271963000297546\n",
      "Epoch 158: Train Loss: 0.12842646799981594, Validation Loss: 0.4843095541000366\n",
      "Epoch 159: Train Loss: 0.1274157166481018, Validation Loss: 0.4776502549648285\n",
      "Epoch 160: Train Loss: 0.14256749115884304, Validation Loss: 0.4905957877635956\n",
      "Epoch 161: Train Loss: 0.15333900228142738, Validation Loss: 0.4895774722099304\n",
      "Epoch 162: Train Loss: 0.15076157450675964, Validation Loss: 0.46629008650779724\n",
      "Epoch 163: Train Loss: 0.11099043302237988, Validation Loss: 0.47795799374580383\n",
      "Epoch 164: Train Loss: 0.11695674248039722, Validation Loss: 0.46826493740081787\n",
      "Epoch 165: Train Loss: 0.13059203512966633, Validation Loss: 0.46479013562202454\n",
      "Epoch 166: Train Loss: 0.15339849889278412, Validation Loss: 0.4717857539653778\n",
      "Epoch 167: Train Loss: 0.10282849613577127, Validation Loss: 0.4669997990131378\n",
      "Epoch 168: Train Loss: 0.15250488184392452, Validation Loss: 0.4743597209453583\n",
      "Epoch 169: Train Loss: 0.1154550090432167, Validation Loss: 0.4706692695617676\n",
      "Epoch 170: Train Loss: 0.13258845917880535, Validation Loss: 0.47112369537353516\n",
      "Epoch 171: Train Loss: 0.13192708045244217, Validation Loss: 0.45633724331855774\n",
      "Epoch 172: Train Loss: 0.10253708530217409, Validation Loss: 0.4597134590148926\n",
      "Epoch 173: Train Loss: 0.14498213678598404, Validation Loss: 0.5108794569969177\n",
      "Epoch 174: Train Loss: 0.14082490652799606, Validation Loss: 0.5292230844497681\n",
      "Epoch 175: Train Loss: 0.1453536693006754, Validation Loss: 0.51082843542099\n",
      "Epoch 176: Train Loss: 0.11964290216565132, Validation Loss: 0.4884709119796753\n",
      "Epoch 177: Train Loss: 0.12134037911891937, Validation Loss: 0.49082043766975403\n",
      "Epoch 178: Train Loss: 0.11828187294304371, Validation Loss: 0.49297669529914856\n",
      "Epoch 179: Train Loss: 0.11696343310177326, Validation Loss: 0.4945516884326935\n",
      "Epoch 180: Train Loss: 0.11995602771639824, Validation Loss: 0.5139164328575134\n",
      "Epoch 181: Train Loss: 0.10905461944639683, Validation Loss: 0.49600544571876526\n",
      "Epoch 182: Train Loss: 0.1775669027119875, Validation Loss: 0.49429580569267273\n",
      "Epoch 183: Train Loss: 0.11456037499010563, Validation Loss: 0.49835795164108276\n",
      "Epoch 184: Train Loss: 0.15098229050636292, Validation Loss: 0.5238330364227295\n",
      "Epoch 185: Train Loss: 0.10755560547113419, Validation Loss: 0.4847772419452667\n",
      "Epoch 186: Train Loss: 0.10994954034686089, Validation Loss: 0.4953082203865051\n",
      "Epoch 187: Train Loss: 0.15297810174524784, Validation Loss: 0.4887676537036896\n",
      "Epoch 188: Train Loss: 0.12610649038106203, Validation Loss: 0.4889540374279022\n",
      "Epoch 189: Train Loss: 0.11578079871833324, Validation Loss: 0.4911148250102997\n",
      "Epoch 190: Train Loss: 0.12090845312923193, Validation Loss: 0.5333393216133118\n",
      "Epoch 191: Train Loss: 0.11728663370013237, Validation Loss: 0.5288565754890442\n",
      "Epoch 192: Train Loss: 0.11959709599614143, Validation Loss: 0.48938730359077454\n",
      "Epoch 193: Train Loss: 0.14160865917801857, Validation Loss: 0.4644368290901184\n",
      "Epoch 194: Train Loss: 0.14259875379502773, Validation Loss: 0.458374947309494\n",
      "Epoch 195: Train Loss: 0.12946053221821785, Validation Loss: 0.47592946887016296\n",
      "Epoch 196: Train Loss: 0.09293353743851185, Validation Loss: 0.49461060762405396\n",
      "Epoch 197: Train Loss: 0.08881849143654108, Validation Loss: 0.5003392100334167\n",
      "Epoch 198: Train Loss: 0.08950375393033028, Validation Loss: 0.5008744597434998\n",
      "Epoch 199: Train Loss: 0.08224497176706791, Validation Loss: 0.5124540328979492\n",
      "Epoch 200: Train Loss: 0.10968926921486855, Validation Loss: 0.5111820697784424\n",
      "Epoch 201: Train Loss: 0.12018624693155289, Validation Loss: 0.5081030130386353\n",
      "Epoch 202: Train Loss: 0.11067059077322483, Validation Loss: 0.5495406985282898\n",
      "Epoch 203: Train Loss: 0.16298391204327345, Validation Loss: 0.5079339146614075\n",
      "Epoch 204: Train Loss: 0.10052480176091194, Validation Loss: 0.4703945219516754\n",
      "Epoch 205: Train Loss: 0.09291173703968525, Validation Loss: 0.46277475357055664\n",
      "Epoch 206: Train Loss: 0.0869705006480217, Validation Loss: 0.4824783205986023\n",
      "Epoch 207: Train Loss: 0.11429456621408463, Validation Loss: 0.49560099840164185\n",
      "Epoch 208: Train Loss: 0.10068892501294613, Validation Loss: 0.4964781701564789\n",
      "Epoch 209: Train Loss: 0.0751949492841959, Validation Loss: 0.4989908039569855\n",
      "Epoch 210: Train Loss: 0.08090387843549252, Validation Loss: 0.5021523833274841\n",
      "Epoch 211: Train Loss: 0.12840156257152557, Validation Loss: 0.5215335488319397\n",
      "Epoch 212: Train Loss: 0.0878952844068408, Validation Loss: 0.5779683589935303\n",
      "Epoch 213: Train Loss: 0.09871882107108831, Validation Loss: 0.5374277830123901\n",
      "Epoch 214: Train Loss: 0.11037391796708107, Validation Loss: 0.5182594656944275\n",
      "Epoch 215: Train Loss: 0.09159475471824408, Validation Loss: 0.5124750137329102\n",
      "Epoch 216: Train Loss: 0.08703193999826908, Validation Loss: 0.5200162529945374\n",
      "Epoch 217: Train Loss: 0.11336869187653065, Validation Loss: 0.5411504507064819\n",
      "Epoch 218: Train Loss: 0.08705205004662275, Validation Loss: 0.5460644960403442\n",
      "Epoch 219: Train Loss: 0.08472555596381426, Validation Loss: 0.541402280330658\n",
      "Epoch 220: Train Loss: 0.09769698604941368, Validation Loss: 0.5784734487533569\n",
      "Epoch 221: Train Loss: 0.08896885626018047, Validation Loss: 0.547630786895752\n",
      "Epoch 222: Train Loss: 0.10593420267105103, Validation Loss: 0.5276083946228027\n",
      "Epoch 223: Train Loss: 0.11430957727134228, Validation Loss: 0.5455244779586792\n",
      "Epoch 224: Train Loss: 0.12389599531888962, Validation Loss: 0.5610592365264893\n",
      "Epoch 225: Train Loss: 0.11772401258349419, Validation Loss: 0.5757084488868713\n",
      "Epoch 226: Train Loss: 0.12073998712003231, Validation Loss: 0.550910472869873\n",
      "Epoch 227: Train Loss: 0.11749413050711155, Validation Loss: 0.5399539470672607\n",
      "Epoch 228: Train Loss: 0.09055894240736961, Validation Loss: 0.5471519231796265\n",
      "Epoch 229: Train Loss: 0.09457169100642204, Validation Loss: 0.5429600477218628\n",
      "Epoch 230: Train Loss: 0.07119576772674918, Validation Loss: 0.5146521925926208\n",
      "Epoch 231: Train Loss: 0.12513739988207817, Validation Loss: 0.5034547448158264\n",
      "Epoch 232: Train Loss: 0.08843406569212675, Validation Loss: 0.4901205003261566\n",
      "Epoch 233: Train Loss: 0.10326487571001053, Validation Loss: 0.4818301200866699\n",
      "Epoch 234: Train Loss: 0.09306405857205391, Validation Loss: 0.48377302289009094\n",
      "Epoch 235: Train Loss: 0.08205961249768734, Validation Loss: 0.4719775319099426\n",
      "Epoch 236: Train Loss: 0.12192195653915405, Validation Loss: 0.45290687680244446\n",
      "Early stopping at epoch 237\n",
      "Completed fold 1\n",
      "Epoch 0: Train Loss: 0.7381101846694946, Validation Loss: 0.6963738203048706\n",
      "Epoch 1: Train Loss: 0.7058376371860504, Validation Loss: 0.6972231864929199\n",
      "Epoch 2: Train Loss: 0.701752245426178, Validation Loss: 0.6992090940475464\n",
      "Epoch 3: Train Loss: 0.7486326098442078, Validation Loss: 0.7006838917732239\n",
      "Epoch 4: Train Loss: 0.7030697613954544, Validation Loss: 0.7027244567871094\n",
      "Epoch 5: Train Loss: 0.6933360546827316, Validation Loss: 0.7049651145935059\n",
      "Epoch 6: Train Loss: 0.6827983558177948, Validation Loss: 0.706972062587738\n",
      "Epoch 7: Train Loss: 0.7068737000226974, Validation Loss: 0.7084324955940247\n",
      "Epoch 8: Train Loss: 0.725012481212616, Validation Loss: 0.7095229029655457\n",
      "Epoch 9: Train Loss: 0.7174769937992096, Validation Loss: 0.7102224826812744\n",
      "Epoch 10: Train Loss: 0.7047393321990967, Validation Loss: 0.7131274342536926\n",
      "Epoch 11: Train Loss: 0.6410516500473022, Validation Loss: 0.7154894471168518\n",
      "Epoch 12: Train Loss: 0.6896178275346756, Validation Loss: 0.7154533267021179\n",
      "Epoch 13: Train Loss: 0.6590478122234344, Validation Loss: 0.7166505455970764\n",
      "Epoch 14: Train Loss: 0.6969994157552719, Validation Loss: 0.7164930701255798\n",
      "Epoch 15: Train Loss: 0.6843086332082748, Validation Loss: 0.7163742780685425\n",
      "Epoch 16: Train Loss: 0.6281330287456512, Validation Loss: 0.7155333757400513\n",
      "Epoch 17: Train Loss: 0.7178018689155579, Validation Loss: 0.7154449820518494\n",
      "Epoch 18: Train Loss: 0.7079712152481079, Validation Loss: 0.7155982851982117\n",
      "Epoch 19: Train Loss: 0.7057739496231079, Validation Loss: 0.7156196236610413\n",
      "Epoch 20: Train Loss: 0.6089754998683929, Validation Loss: 0.7166264057159424\n",
      "Epoch 21: Train Loss: 0.6851132810115814, Validation Loss: 0.718377411365509\n",
      "Epoch 22: Train Loss: 0.6366997361183167, Validation Loss: 0.7191869616508484\n",
      "Epoch 23: Train Loss: 0.6309356391429901, Validation Loss: 0.7185025811195374\n",
      "Epoch 24: Train Loss: 0.6593273878097534, Validation Loss: 0.7182613015174866\n",
      "Epoch 25: Train Loss: 0.6815899014472961, Validation Loss: 0.7184371948242188\n",
      "Epoch 26: Train Loss: 0.6521160155534744, Validation Loss: 0.7186066508293152\n",
      "Epoch 27: Train Loss: 0.7197531163692474, Validation Loss: 0.7180827856063843\n",
      "Epoch 28: Train Loss: 0.6459683477878571, Validation Loss: 0.7182998657226562\n",
      "Epoch 29: Train Loss: 0.7197139859199524, Validation Loss: 0.7186148762702942\n",
      "Epoch 30: Train Loss: 0.6599573343992233, Validation Loss: 0.7173647880554199\n",
      "Epoch 31: Train Loss: 0.6221757233142853, Validation Loss: 0.7157349586486816\n",
      "Epoch 32: Train Loss: 0.6230176985263824, Validation Loss: 0.7163378596305847\n",
      "Epoch 33: Train Loss: 0.6095722317695618, Validation Loss: 0.7162807583808899\n",
      "Epoch 34: Train Loss: 0.6568669825792313, Validation Loss: 0.7165906429290771\n",
      "Epoch 35: Train Loss: 0.6676234006881714, Validation Loss: 0.7167326807975769\n",
      "Epoch 36: Train Loss: 0.6416040062904358, Validation Loss: 0.7181487083435059\n",
      "Epoch 37: Train Loss: 0.6257075369358063, Validation Loss: 0.7186219096183777\n",
      "Epoch 38: Train Loss: 0.6370198130607605, Validation Loss: 0.719025731086731\n",
      "Epoch 39: Train Loss: 0.6515468508005142, Validation Loss: 0.7189956307411194\n",
      "Epoch 40: Train Loss: 0.6577264219522476, Validation Loss: 0.7203607559204102\n",
      "Epoch 41: Train Loss: 0.6354811638593674, Validation Loss: 0.721531093120575\n",
      "Epoch 42: Train Loss: 0.6305123269557953, Validation Loss: 0.722257137298584\n",
      "Epoch 43: Train Loss: 0.6311226636171341, Validation Loss: 0.7236887812614441\n",
      "Epoch 44: Train Loss: 0.6412336826324463, Validation Loss: 0.7258041501045227\n",
      "Epoch 45: Train Loss: 0.6375042498111725, Validation Loss: 0.7287705540657043\n",
      "Epoch 46: Train Loss: 0.6040418148040771, Validation Loss: 0.7300281524658203\n",
      "Epoch 47: Train Loss: 0.6268276125192642, Validation Loss: 0.7308699488639832\n",
      "Epoch 48: Train Loss: 0.645111694931984, Validation Loss: 0.7308816313743591\n",
      "Epoch 49: Train Loss: 0.5840275436639786, Validation Loss: 0.7307894825935364\n",
      "Epoch 50: Train Loss: 0.6528576463460922, Validation Loss: 0.7298469543457031\n",
      "Epoch 51: Train Loss: 0.6321661919355392, Validation Loss: 0.730511486530304\n",
      "Epoch 52: Train Loss: 0.6366516798734665, Validation Loss: 0.7297388911247253\n",
      "Epoch 53: Train Loss: 0.6010851562023163, Validation Loss: 0.7289263606071472\n",
      "Epoch 54: Train Loss: 0.599907785654068, Validation Loss: 0.7276349067687988\n",
      "Epoch 55: Train Loss: 0.6402936428785324, Validation Loss: 0.7275378704071045\n",
      "Epoch 56: Train Loss: 0.621466264128685, Validation Loss: 0.7283374667167664\n",
      "Epoch 57: Train Loss: 0.6056205034255981, Validation Loss: 0.7292230725288391\n",
      "Epoch 58: Train Loss: 0.5937988460063934, Validation Loss: 0.7293285131454468\n",
      "Epoch 59: Train Loss: 0.5822185277938843, Validation Loss: 0.7295390367507935\n",
      "Epoch 60: Train Loss: 0.6064813286066055, Validation Loss: 0.731126606464386\n",
      "Epoch 61: Train Loss: 0.5861280709505081, Validation Loss: 0.730975329875946\n",
      "Epoch 62: Train Loss: 0.6248138695955276, Validation Loss: 0.7296962141990662\n",
      "Epoch 63: Train Loss: 0.6076411306858063, Validation Loss: 0.7278121113777161\n",
      "Epoch 64: Train Loss: 0.6609268039464951, Validation Loss: 0.7285969853401184\n",
      "Epoch 65: Train Loss: 0.6296846866607666, Validation Loss: 0.7314841747283936\n",
      "Epoch 66: Train Loss: 0.6452550739049911, Validation Loss: 0.7346692085266113\n",
      "Epoch 67: Train Loss: 0.6038383990526199, Validation Loss: 0.7363774180412292\n",
      "Epoch 68: Train Loss: 0.6004514247179031, Validation Loss: 0.736650288105011\n",
      "Epoch 69: Train Loss: 0.6235412210226059, Validation Loss: 0.7365806698799133\n",
      "Epoch 70: Train Loss: 0.6138311326503754, Validation Loss: 0.7376410961151123\n",
      "Epoch 71: Train Loss: 0.5903868675231934, Validation Loss: 0.737261950969696\n",
      "Epoch 72: Train Loss: 0.6078001856803894, Validation Loss: 0.7396504282951355\n",
      "Epoch 73: Train Loss: 0.6129691302776337, Validation Loss: 0.7391800880432129\n",
      "Epoch 74: Train Loss: 0.6257218271493912, Validation Loss: 0.7374276518821716\n",
      "Epoch 75: Train Loss: 0.5525365471839905, Validation Loss: 0.7347896099090576\n",
      "Epoch 76: Train Loss: 0.5881885439157486, Validation Loss: 0.7337058782577515\n",
      "Epoch 77: Train Loss: 0.6170885562896729, Validation Loss: 0.7327143549919128\n",
      "Epoch 78: Train Loss: 0.5754348784685135, Validation Loss: 0.7314000129699707\n",
      "Epoch 79: Train Loss: 0.6103436797857285, Validation Loss: 0.7304688692092896\n",
      "Epoch 80: Train Loss: 0.5775303840637207, Validation Loss: 0.7271535396575928\n",
      "Epoch 81: Train Loss: 0.5437651351094246, Validation Loss: 0.7272389531135559\n",
      "Epoch 82: Train Loss: 0.5544339269399643, Validation Loss: 0.7265545725822449\n",
      "Epoch 83: Train Loss: 0.5481079667806625, Validation Loss: 0.7282478213310242\n",
      "Epoch 84: Train Loss: 0.5852165967226028, Validation Loss: 0.7300484776496887\n",
      "Epoch 85: Train Loss: 0.5784966796636581, Validation Loss: 0.7315824627876282\n",
      "Epoch 86: Train Loss: 0.5722164958715439, Validation Loss: 0.7313442230224609\n",
      "Epoch 87: Train Loss: 0.5286378860473633, Validation Loss: 0.7311347723007202\n",
      "Epoch 88: Train Loss: 0.5600447505712509, Validation Loss: 0.7314807176589966\n",
      "Epoch 89: Train Loss: 0.5918676555156708, Validation Loss: 0.7309438586235046\n",
      "Epoch 90: Train Loss: 0.5697377026081085, Validation Loss: 0.7265490889549255\n",
      "Epoch 91: Train Loss: 0.5949373990297318, Validation Loss: 0.7286239266395569\n",
      "Epoch 92: Train Loss: 0.5610340088605881, Validation Loss: 0.7298610806465149\n",
      "Epoch 93: Train Loss: 0.572889007627964, Validation Loss: 0.7294641137123108\n",
      "Epoch 94: Train Loss: 0.5514368116855621, Validation Loss: 0.7284018397331238\n",
      "Epoch 95: Train Loss: 0.586115300655365, Validation Loss: 0.7296067476272583\n",
      "Epoch 96: Train Loss: 0.5657527893781662, Validation Loss: 0.7303656935691833\n",
      "Epoch 97: Train Loss: 0.5616668239235878, Validation Loss: 0.7317151427268982\n",
      "Epoch 98: Train Loss: 0.5878796726465225, Validation Loss: 0.7325284481048584\n",
      "Epoch 99: Train Loss: 0.5517566651105881, Validation Loss: 0.7325614094734192\n",
      "Early stopping at epoch 100\n",
      "Completed fold 2\n",
      "Epoch 0: Train Loss: 0.7258498668670654, Validation Loss: 0.6890873908996582\n",
      "Epoch 1: Train Loss: 0.7172833234071732, Validation Loss: 0.6865978240966797\n",
      "Epoch 2: Train Loss: 0.7163143157958984, Validation Loss: 0.6818656921386719\n",
      "Epoch 3: Train Loss: 0.7538084238767624, Validation Loss: 0.6788344979286194\n",
      "Epoch 4: Train Loss: 0.6907266825437546, Validation Loss: 0.676274299621582\n",
      "Epoch 5: Train Loss: 0.7612433731555939, Validation Loss: 0.6741413474082947\n",
      "Epoch 6: Train Loss: 0.7747215032577515, Validation Loss: 0.6722328066825867\n",
      "Epoch 7: Train Loss: 0.7079162448644638, Validation Loss: 0.6710942387580872\n",
      "Epoch 8: Train Loss: 0.7025045603513718, Validation Loss: 0.6699560880661011\n",
      "Epoch 9: Train Loss: 0.7140044569969177, Validation Loss: 0.6690731644630432\n",
      "Epoch 10: Train Loss: 0.6985612660646439, Validation Loss: 0.670324981212616\n",
      "Epoch 11: Train Loss: 0.6891718804836273, Validation Loss: 0.6711421608924866\n",
      "Epoch 12: Train Loss: 0.7367314100265503, Validation Loss: 0.6765586733818054\n",
      "Epoch 13: Train Loss: 0.6989584565162659, Validation Loss: 0.6772328615188599\n",
      "Epoch 14: Train Loss: 0.7626871764659882, Validation Loss: 0.6768924593925476\n",
      "Epoch 15: Train Loss: 0.6727229356765747, Validation Loss: 0.674756646156311\n",
      "Epoch 16: Train Loss: 0.7337913662195206, Validation Loss: 0.6728635430335999\n",
      "Epoch 17: Train Loss: 0.6943233609199524, Validation Loss: 0.6721817851066589\n",
      "Epoch 18: Train Loss: 0.6841918528079987, Validation Loss: 0.6718565821647644\n",
      "Epoch 19: Train Loss: 0.7071160078048706, Validation Loss: 0.6716115474700928\n",
      "Epoch 20: Train Loss: 0.7242517918348312, Validation Loss: 0.6743859648704529\n",
      "Epoch 21: Train Loss: 0.6896714419126511, Validation Loss: 0.6743525862693787\n",
      "Epoch 22: Train Loss: 0.6968478709459305, Validation Loss: 0.6747695803642273\n",
      "Epoch 23: Train Loss: 0.7082248032093048, Validation Loss: 0.6755342483520508\n",
      "Epoch 24: Train Loss: 0.6659869253635406, Validation Loss: 0.6771681904792786\n",
      "Epoch 25: Train Loss: 0.7085895985364914, Validation Loss: 0.677191972732544\n",
      "Epoch 26: Train Loss: 0.6867657899856567, Validation Loss: 0.6770925521850586\n",
      "Epoch 27: Train Loss: 0.6301579773426056, Validation Loss: 0.6767364740371704\n",
      "Epoch 28: Train Loss: 0.6620461940765381, Validation Loss: 0.676487922668457\n",
      "Epoch 29: Train Loss: 0.6858705133199692, Validation Loss: 0.6762102842330933\n",
      "Epoch 30: Train Loss: 0.6800061911344528, Validation Loss: 0.6784970164299011\n",
      "Epoch 31: Train Loss: 0.7074141204357147, Validation Loss: 0.6818951964378357\n",
      "Epoch 32: Train Loss: 0.709478348493576, Validation Loss: 0.681470513343811\n",
      "Epoch 33: Train Loss: 0.7057847678661346, Validation Loss: 0.6824877858161926\n",
      "Epoch 34: Train Loss: 0.6628807336091995, Validation Loss: 0.6808652877807617\n",
      "Epoch 35: Train Loss: 0.6859786957502365, Validation Loss: 0.6803317666053772\n",
      "Epoch 36: Train Loss: 0.662160724401474, Validation Loss: 0.6795463562011719\n",
      "Epoch 37: Train Loss: 0.6625837087631226, Validation Loss: 0.6793355941772461\n",
      "Epoch 38: Train Loss: 0.6904253363609314, Validation Loss: 0.6793355941772461\n",
      "Epoch 39: Train Loss: 0.6738748252391815, Validation Loss: 0.6793680787086487\n",
      "Epoch 40: Train Loss: 0.6873526722192764, Validation Loss: 0.678054928779602\n",
      "Epoch 41: Train Loss: 0.6362281739711761, Validation Loss: 0.6728658080101013\n",
      "Epoch 42: Train Loss: 0.6846571266651154, Validation Loss: 0.671412467956543\n",
      "Epoch 43: Train Loss: 0.6718219518661499, Validation Loss: 0.6700198650360107\n",
      "Epoch 44: Train Loss: 0.6460956037044525, Validation Loss: 0.6707361936569214\n",
      "Epoch 45: Train Loss: 0.6634672731161118, Validation Loss: 0.6692371368408203\n",
      "Epoch 46: Train Loss: 0.647333949804306, Validation Loss: 0.6687831282615662\n",
      "Epoch 47: Train Loss: 0.6508672386407852, Validation Loss: 0.6684695482254028\n",
      "Epoch 48: Train Loss: 0.6251043677330017, Validation Loss: 0.6676966547966003\n",
      "Epoch 49: Train Loss: 0.6310994774103165, Validation Loss: 0.667310357093811\n",
      "Epoch 50: Train Loss: 0.6327539086341858, Validation Loss: 0.6618687510490417\n",
      "Epoch 51: Train Loss: 0.6537493318319321, Validation Loss: 0.6576417088508606\n",
      "Epoch 52: Train Loss: 0.6438688784837723, Validation Loss: 0.6536360383033752\n",
      "Epoch 53: Train Loss: 0.6260640472173691, Validation Loss: 0.6498085260391235\n",
      "Epoch 54: Train Loss: 0.6074555069208145, Validation Loss: 0.6473867893218994\n",
      "Epoch 55: Train Loss: 0.6559747010469437, Validation Loss: 0.6456395387649536\n",
      "Epoch 56: Train Loss: 0.6054497361183167, Validation Loss: 0.6442636251449585\n",
      "Epoch 57: Train Loss: 0.6317509561777115, Validation Loss: 0.6438309550285339\n",
      "Epoch 58: Train Loss: 0.6593837738037109, Validation Loss: 0.6439487338066101\n",
      "Epoch 59: Train Loss: 0.6350769400596619, Validation Loss: 0.6441431045532227\n",
      "Epoch 60: Train Loss: 0.5985189080238342, Validation Loss: 0.6452165842056274\n",
      "Epoch 61: Train Loss: 0.6040587574243546, Validation Loss: 0.6468108296394348\n",
      "Epoch 62: Train Loss: 0.6313488334417343, Validation Loss: 0.6459103226661682\n",
      "Epoch 63: Train Loss: 0.6113597899675369, Validation Loss: 0.6346789002418518\n",
      "Epoch 64: Train Loss: 0.6328516751527786, Validation Loss: 0.6266170740127563\n",
      "Epoch 65: Train Loss: 0.5926670879125595, Validation Loss: 0.6249333620071411\n",
      "Epoch 66: Train Loss: 0.6011184751987457, Validation Loss: 0.6236631274223328\n",
      "Epoch 67: Train Loss: 0.572955772280693, Validation Loss: 0.6247147917747498\n",
      "Epoch 68: Train Loss: 0.5846759229898453, Validation Loss: 0.6260603666305542\n",
      "Epoch 69: Train Loss: 0.5703978687524796, Validation Loss: 0.6271742582321167\n",
      "Epoch 70: Train Loss: 0.5902104377746582, Validation Loss: 0.6331116557121277\n",
      "Epoch 71: Train Loss: 0.6698901355266571, Validation Loss: 0.6337845921516418\n",
      "Epoch 72: Train Loss: 0.6067570447921753, Validation Loss: 0.6200348138809204\n",
      "Epoch 73: Train Loss: 0.5893021821975708, Validation Loss: 0.6078504323959351\n",
      "Epoch 74: Train Loss: 0.5713270753622055, Validation Loss: 0.5960199236869812\n",
      "Epoch 75: Train Loss: 0.6126380562782288, Validation Loss: 0.590967059135437\n",
      "Epoch 76: Train Loss: 0.6334826201200485, Validation Loss: 0.5911179184913635\n",
      "Epoch 77: Train Loss: 0.5971147865056992, Validation Loss: 0.5930430293083191\n",
      "Epoch 78: Train Loss: 0.5792694985866547, Validation Loss: 0.5942950248718262\n",
      "Epoch 79: Train Loss: 0.5777152925729752, Validation Loss: 0.5942086577415466\n",
      "Epoch 80: Train Loss: 0.6034343987703323, Validation Loss: 0.6091504693031311\n",
      "Epoch 81: Train Loss: 0.6012363582849503, Validation Loss: 0.6115075945854187\n",
      "Epoch 82: Train Loss: 0.5887548923492432, Validation Loss: 0.6083193421363831\n",
      "Epoch 83: Train Loss: 0.5883603990077972, Validation Loss: 0.5951986908912659\n",
      "Epoch 84: Train Loss: 0.5474286675453186, Validation Loss: 0.5819447040557861\n",
      "Epoch 85: Train Loss: 0.5523086786270142, Validation Loss: 0.5767896175384521\n",
      "Epoch 86: Train Loss: 0.581853836774826, Validation Loss: 0.5796588659286499\n",
      "Epoch 87: Train Loss: 0.5617159456014633, Validation Loss: 0.5809944272041321\n",
      "Epoch 88: Train Loss: 0.5469841063022614, Validation Loss: 0.5829002857208252\n",
      "Epoch 89: Train Loss: 0.577739492058754, Validation Loss: 0.5826184749603271\n",
      "Epoch 90: Train Loss: 0.5721899718046188, Validation Loss: 0.5975898504257202\n",
      "Epoch 91: Train Loss: 0.5728083848953247, Validation Loss: 0.5920127630233765\n",
      "Epoch 92: Train Loss: 0.5432674139738083, Validation Loss: 0.567559540271759\n",
      "Epoch 93: Train Loss: 0.5255820900201797, Validation Loss: 0.5455625057220459\n",
      "Epoch 94: Train Loss: 0.4987564757466316, Validation Loss: 0.5443289279937744\n",
      "Epoch 95: Train Loss: 0.5324402302503586, Validation Loss: 0.5427971482276917\n",
      "Epoch 96: Train Loss: 0.5235124453902245, Validation Loss: 0.5441631078720093\n",
      "Epoch 97: Train Loss: 0.5467484146356583, Validation Loss: 0.5463586449623108\n",
      "Epoch 98: Train Loss: 0.5423468500375748, Validation Loss: 0.5470107197761536\n",
      "Epoch 99: Train Loss: 0.529580719769001, Validation Loss: 0.5466960668563843\n",
      "Epoch 100: Train Loss: 0.5034681782126427, Validation Loss: 0.5497393608093262\n",
      "Epoch 101: Train Loss: 0.5348699688911438, Validation Loss: 0.5514342784881592\n",
      "Epoch 102: Train Loss: 0.5191123932600021, Validation Loss: 0.5433149337768555\n",
      "Epoch 103: Train Loss: 0.4994162693619728, Validation Loss: 0.5205250978469849\n",
      "Epoch 104: Train Loss: 0.5015309602022171, Validation Loss: 0.5187929272651672\n",
      "Epoch 105: Train Loss: 0.49005553871393204, Validation Loss: 0.5225834846496582\n",
      "Epoch 106: Train Loss: 0.4779019057750702, Validation Loss: 0.5227109789848328\n",
      "Epoch 107: Train Loss: 0.503997340798378, Validation Loss: 0.5229365825653076\n",
      "Epoch 108: Train Loss: 0.5081253796815872, Validation Loss: 0.5240870118141174\n",
      "Epoch 109: Train Loss: 0.4699884206056595, Validation Loss: 0.5234642624855042\n",
      "Epoch 110: Train Loss: 0.5081176161766052, Validation Loss: 0.5232011079788208\n",
      "Epoch 111: Train Loss: 0.4643234759569168, Validation Loss: 0.5015000700950623\n",
      "Epoch 112: Train Loss: 0.495571494102478, Validation Loss: 0.5057043433189392\n",
      "Epoch 113: Train Loss: 0.4953923970460892, Validation Loss: 0.5042939782142639\n",
      "Epoch 114: Train Loss: 0.45766598731279373, Validation Loss: 0.4977370500564575\n",
      "Epoch 115: Train Loss: 0.461328350007534, Validation Loss: 0.48756110668182373\n",
      "Epoch 116: Train Loss: 0.4632667154073715, Validation Loss: 0.47824084758758545\n",
      "Epoch 117: Train Loss: 0.49236729741096497, Validation Loss: 0.48255836963653564\n",
      "Epoch 118: Train Loss: 0.45551618933677673, Validation Loss: 0.4858607351779938\n",
      "Epoch 119: Train Loss: 0.43918783217668533, Validation Loss: 0.4849396049976349\n",
      "Epoch 120: Train Loss: 0.46578293293714523, Validation Loss: 0.5161918997764587\n",
      "Epoch 121: Train Loss: 0.44564805179834366, Validation Loss: 0.4640537202358246\n",
      "Epoch 122: Train Loss: 0.43674492835998535, Validation Loss: 0.46055859327316284\n",
      "Epoch 123: Train Loss: 0.42465387284755707, Validation Loss: 0.4724433124065399\n",
      "Epoch 124: Train Loss: 0.4187877029180527, Validation Loss: 0.45921629667282104\n",
      "Epoch 125: Train Loss: 0.3989196717739105, Validation Loss: 0.4557327926158905\n",
      "Epoch 126: Train Loss: 0.4460028409957886, Validation Loss: 0.463049054145813\n",
      "Epoch 127: Train Loss: 0.41633908450603485, Validation Loss: 0.46446487307548523\n",
      "Epoch 128: Train Loss: 0.39034274965524673, Validation Loss: 0.4636545777320862\n",
      "Epoch 129: Train Loss: 0.3985196053981781, Validation Loss: 0.4604596793651581\n",
      "Epoch 130: Train Loss: 0.4499863237142563, Validation Loss: 0.43885934352874756\n",
      "Epoch 131: Train Loss: 0.4226202741265297, Validation Loss: 0.4580349624156952\n",
      "Epoch 132: Train Loss: 0.39601127803325653, Validation Loss: 0.44280025362968445\n",
      "Epoch 133: Train Loss: 0.3972025215625763, Validation Loss: 0.44200870394706726\n",
      "Epoch 134: Train Loss: 0.3480362370610237, Validation Loss: 0.43891841173171997\n",
      "Epoch 135: Train Loss: 0.34265562891960144, Validation Loss: 0.4484870731830597\n",
      "Epoch 136: Train Loss: 0.36732108891010284, Validation Loss: 0.4626847505569458\n",
      "Epoch 137: Train Loss: 0.3507929742336273, Validation Loss: 0.4543122351169586\n",
      "Epoch 138: Train Loss: 0.36371761560440063, Validation Loss: 0.44526511430740356\n",
      "Epoch 139: Train Loss: 0.34449057281017303, Validation Loss: 0.44334274530410767\n",
      "Epoch 140: Train Loss: 0.3481737747788429, Validation Loss: 0.4366464614868164\n",
      "Epoch 141: Train Loss: 0.3378448039293289, Validation Loss: 0.4388486444950104\n",
      "Epoch 142: Train Loss: 0.3175899460911751, Validation Loss: 0.4656315743923187\n",
      "Epoch 143: Train Loss: 0.3130647763609886, Validation Loss: 0.4601057767868042\n",
      "Epoch 144: Train Loss: 0.28713008016347885, Validation Loss: 0.46786099672317505\n",
      "Epoch 145: Train Loss: 0.3290133476257324, Validation Loss: 0.45960676670074463\n",
      "Epoch 146: Train Loss: 0.2783248834311962, Validation Loss: 0.46778175234794617\n",
      "Epoch 147: Train Loss: 0.2874576821923256, Validation Loss: 0.4798329174518585\n",
      "Epoch 148: Train Loss: 0.3058345764875412, Validation Loss: 0.4796028137207031\n",
      "Epoch 149: Train Loss: 0.310459204018116, Validation Loss: 0.47783783078193665\n",
      "Epoch 150: Train Loss: 0.2885870337486267, Validation Loss: 0.4553540349006653\n",
      "Epoch 151: Train Loss: 0.2749606668949127, Validation Loss: 0.47216087579727173\n",
      "Epoch 152: Train Loss: 0.27801477536559105, Validation Loss: 0.551298975944519\n",
      "Epoch 153: Train Loss: 0.2775438353419304, Validation Loss: 0.5799643993377686\n",
      "Epoch 154: Train Loss: 0.28635023161768913, Validation Loss: 0.5690854787826538\n",
      "Epoch 155: Train Loss: 0.23424622043967247, Validation Loss: 0.5584302544593811\n",
      "Epoch 156: Train Loss: 0.23197390139102936, Validation Loss: 0.5605180859565735\n",
      "Epoch 157: Train Loss: 0.23242663964629173, Validation Loss: 0.5508416891098022\n",
      "Epoch 158: Train Loss: 0.2341643124818802, Validation Loss: 0.5472471117973328\n",
      "Epoch 159: Train Loss: 0.22322065010666847, Validation Loss: 0.5478445291519165\n",
      "Epoch 160: Train Loss: 0.2566334269940853, Validation Loss: 0.5875810384750366\n",
      "Epoch 161: Train Loss: 0.23012785241007805, Validation Loss: 0.6773068904876709\n",
      "Epoch 162: Train Loss: 0.2067074179649353, Validation Loss: 0.5927064418792725\n",
      "Epoch 163: Train Loss: 0.232100497931242, Validation Loss: 0.5697950124740601\n",
      "Epoch 164: Train Loss: 0.23444679751992226, Validation Loss: 0.6466702222824097\n",
      "Epoch 165: Train Loss: 0.21698815003037453, Validation Loss: 0.6324225664138794\n",
      "Epoch 166: Train Loss: 0.21989846602082253, Validation Loss: 0.6091933250427246\n",
      "Epoch 167: Train Loss: 0.20147984102368355, Validation Loss: 0.625267744064331\n",
      "Epoch 168: Train Loss: 0.20214930176734924, Validation Loss: 0.6392185688018799\n",
      "Epoch 169: Train Loss: 0.20366816967725754, Validation Loss: 0.650626540184021\n",
      "Epoch 170: Train Loss: 0.23509511351585388, Validation Loss: 0.5935041904449463\n",
      "Epoch 171: Train Loss: 0.21628589183092117, Validation Loss: 0.7295271754264832\n",
      "Epoch 172: Train Loss: 0.19035294838249683, Validation Loss: 0.7727654576301575\n",
      "Epoch 173: Train Loss: 0.22140754014253616, Validation Loss: 0.7356282472610474\n",
      "Epoch 174: Train Loss: 0.20870528742671013, Validation Loss: 0.7028533816337585\n",
      "Epoch 175: Train Loss: 0.18947641924023628, Validation Loss: 0.73619145154953\n",
      "Epoch 176: Train Loss: 0.17654958367347717, Validation Loss: 0.760900616645813\n",
      "Epoch 177: Train Loss: 0.173856470733881, Validation Loss: 0.7805801033973694\n",
      "Epoch 178: Train Loss: 0.17474698647856712, Validation Loss: 0.7905307412147522\n",
      "Epoch 179: Train Loss: 0.17638959363102913, Validation Loss: 0.7916125655174255\n",
      "Epoch 180: Train Loss: 0.16419617272913456, Validation Loss: 0.818381130695343\n",
      "Epoch 181: Train Loss: 0.16663677245378494, Validation Loss: 0.8421680331230164\n",
      "Epoch 182: Train Loss: 0.1774913053959608, Validation Loss: 0.9297992587089539\n",
      "Epoch 183: Train Loss: 0.16600216925144196, Validation Loss: 0.932692289352417\n",
      "Epoch 184: Train Loss: 0.17359057441353798, Validation Loss: 0.9104129076004028\n",
      "Epoch 185: Train Loss: 0.14580032788217068, Validation Loss: 0.9009203314781189\n",
      "Epoch 186: Train Loss: 0.1377688366919756, Validation Loss: 0.9434685111045837\n",
      "Epoch 187: Train Loss: 0.17976543493568897, Validation Loss: 0.9541966915130615\n",
      "Epoch 188: Train Loss: 0.19594955444335938, Validation Loss: 0.9682800769805908\n",
      "Epoch 189: Train Loss: 0.1791891474276781, Validation Loss: 0.9561362862586975\n",
      "Epoch 190: Train Loss: 0.1759197786450386, Validation Loss: 0.9319698214530945\n",
      "Epoch 191: Train Loss: 0.13947858102619648, Validation Loss: 0.9845539331436157\n",
      "Epoch 192: Train Loss: 0.14169545657932758, Validation Loss: 1.0283221006393433\n",
      "Epoch 193: Train Loss: 0.20758447796106339, Validation Loss: 0.9680055975914001\n",
      "Epoch 194: Train Loss: 0.14538057334721088, Validation Loss: 0.9526487588882446\n",
      "Epoch 195: Train Loss: 0.13928698375821114, Validation Loss: 1.0045167207717896\n",
      "Epoch 196: Train Loss: 0.135015482082963, Validation Loss: 1.0458929538726807\n",
      "Epoch 197: Train Loss: 0.13011951930820942, Validation Loss: 1.039355754852295\n",
      "Epoch 198: Train Loss: 0.14065915159881115, Validation Loss: 1.0293680429458618\n",
      "Epoch 199: Train Loss: 0.15790725499391556, Validation Loss: 1.0421228408813477\n",
      "Epoch 200: Train Loss: 0.1332283541560173, Validation Loss: 1.1496236324310303\n",
      "Epoch 201: Train Loss: 0.1269614789634943, Validation Loss: 1.0495814085006714\n",
      "Epoch 202: Train Loss: 0.13486018404364586, Validation Loss: 1.100847601890564\n",
      "Epoch 203: Train Loss: 0.13920957408845425, Validation Loss: 1.2090402841567993\n",
      "Epoch 204: Train Loss: 0.16558985225856304, Validation Loss: 1.263697624206543\n",
      "Epoch 205: Train Loss: 0.1662723906338215, Validation Loss: 1.191105604171753\n",
      "Epoch 206: Train Loss: 0.13494400307536125, Validation Loss: 1.1700259447097778\n",
      "Epoch 207: Train Loss: 0.1518169455230236, Validation Loss: 1.177682638168335\n",
      "Epoch 208: Train Loss: 0.1177641898393631, Validation Loss: 1.1493207216262817\n",
      "Epoch 209: Train Loss: 0.1353979166597128, Validation Loss: 1.141792893409729\n",
      "Epoch 210: Train Loss: 0.12760852836072445, Validation Loss: 1.31678307056427\n",
      "Epoch 211: Train Loss: 0.16074436902999878, Validation Loss: 1.0598160028457642\n",
      "Epoch 212: Train Loss: 0.13290611188858747, Validation Loss: 1.0420135259628296\n",
      "Epoch 213: Train Loss: 0.12380688637495041, Validation Loss: 1.1082698106765747\n",
      "Epoch 214: Train Loss: 0.16621800512075424, Validation Loss: 1.2005306482315063\n",
      "Epoch 215: Train Loss: 0.12390447966754436, Validation Loss: 1.146289587020874\n",
      "Epoch 216: Train Loss: 0.11677832342684269, Validation Loss: 1.1208423376083374\n",
      "Epoch 217: Train Loss: 0.13704872876405716, Validation Loss: 1.1266518831253052\n",
      "Epoch 218: Train Loss: 0.10240524634718895, Validation Loss: 1.1369397640228271\n",
      "Epoch 219: Train Loss: 0.12794115394353867, Validation Loss: 1.1465831995010376\n",
      "Epoch 220: Train Loss: 0.1459398865699768, Validation Loss: 1.3533977270126343\n",
      "Epoch 221: Train Loss: 0.14406823925673962, Validation Loss: 1.2118165493011475\n",
      "Epoch 222: Train Loss: 0.09198971465229988, Validation Loss: 1.153893232345581\n",
      "Epoch 223: Train Loss: 0.12440920621156693, Validation Loss: 1.1465026140213013\n",
      "Epoch 224: Train Loss: 0.1161919105798006, Validation Loss: 1.2630184888839722\n",
      "Epoch 225: Train Loss: 0.11930187977850437, Validation Loss: 1.428674578666687\n",
      "Epoch 226: Train Loss: 0.10873783007264137, Validation Loss: 1.4335604906082153\n",
      "Epoch 227: Train Loss: 0.10382096283137798, Validation Loss: 1.431492567062378\n",
      "Epoch 228: Train Loss: 0.11935314908623695, Validation Loss: 1.448883295059204\n",
      "Epoch 229: Train Loss: 0.11195062566548586, Validation Loss: 1.4530755281448364\n",
      "Epoch 230: Train Loss: 0.10238387621939182, Validation Loss: 1.2204970121383667\n",
      "Epoch 231: Train Loss: 0.1292530857026577, Validation Loss: 1.0284394025802612\n",
      "Epoch 232: Train Loss: 0.11658291332423687, Validation Loss: 1.2288099527359009\n",
      "Epoch 233: Train Loss: 0.10567016527056694, Validation Loss: 1.3735699653625488\n",
      "Epoch 234: Train Loss: 0.1063532829284668, Validation Loss: 1.343377709388733\n",
      "Epoch 235: Train Loss: 0.1037311926484108, Validation Loss: 1.3257485628128052\n",
      "Epoch 236: Train Loss: 0.12316401302814484, Validation Loss: 1.2948371171951294\n",
      "Epoch 237: Train Loss: 0.13143515028059483, Validation Loss: 1.2472338676452637\n",
      "Epoch 238: Train Loss: 0.1242820043116808, Validation Loss: 1.252570390701294\n",
      "Epoch 239: Train Loss: 0.09332890994846821, Validation Loss: 1.2485240697860718\n",
      "Early stopping at epoch 240\n",
      "Completed fold 3\n",
      "Epoch 0: Train Loss: 0.7439953982830048, Validation Loss: 0.7006570100784302\n",
      "Epoch 1: Train Loss: 0.7391908317804337, Validation Loss: 0.7028898000717163\n",
      "Epoch 2: Train Loss: 0.7570274919271469, Validation Loss: 0.7064752578735352\n",
      "Epoch 3: Train Loss: 0.7509307861328125, Validation Loss: 0.7092593312263489\n",
      "Epoch 4: Train Loss: 0.6766777485609055, Validation Loss: 0.7108845710754395\n",
      "Epoch 5: Train Loss: 0.720988929271698, Validation Loss: 0.7127995491027832\n",
      "Epoch 6: Train Loss: 0.767406553030014, Validation Loss: 0.7141351699829102\n",
      "Epoch 7: Train Loss: 0.6744600981473923, Validation Loss: 0.7155598402023315\n",
      "Epoch 8: Train Loss: 0.7216483354568481, Validation Loss: 0.7164109349250793\n",
      "Epoch 9: Train Loss: 0.7310989499092102, Validation Loss: 0.7170352935791016\n",
      "Epoch 10: Train Loss: 0.6826086342334747, Validation Loss: 0.7176265716552734\n",
      "Epoch 11: Train Loss: 0.7292841970920563, Validation Loss: 0.7177214026451111\n",
      "Epoch 12: Train Loss: 0.6915628612041473, Validation Loss: 0.7186098694801331\n",
      "Epoch 13: Train Loss: 0.6541653573513031, Validation Loss: 0.719466507434845\n",
      "Epoch 14: Train Loss: 0.6928019225597382, Validation Loss: 0.720838725566864\n",
      "Epoch 15: Train Loss: 0.7066425979137421, Validation Loss: 0.7209720611572266\n",
      "Epoch 16: Train Loss: 0.6973412781953812, Validation Loss: 0.7216824889183044\n",
      "Epoch 17: Train Loss: 0.7098877429962158, Validation Loss: 0.7224693298339844\n",
      "Epoch 18: Train Loss: 0.6773802638053894, Validation Loss: 0.72263503074646\n",
      "Epoch 19: Train Loss: 0.7046610862016678, Validation Loss: 0.7228056788444519\n",
      "Epoch 20: Train Loss: 0.7186886519193649, Validation Loss: 0.7227686047554016\n",
      "Epoch 21: Train Loss: 0.6626793444156647, Validation Loss: 0.7223588228225708\n",
      "Epoch 22: Train Loss: 0.7034690231084824, Validation Loss: 0.7212370038032532\n",
      "Epoch 23: Train Loss: 0.6591624766588211, Validation Loss: 0.7200593948364258\n",
      "Epoch 24: Train Loss: 0.6651552021503448, Validation Loss: 0.7191920876502991\n",
      "Epoch 25: Train Loss: 0.6347495168447495, Validation Loss: 0.718416690826416\n",
      "Epoch 26: Train Loss: 0.697754368185997, Validation Loss: 0.7190327644348145\n",
      "Epoch 27: Train Loss: 0.7006112933158875, Validation Loss: 0.7188990712165833\n",
      "Epoch 28: Train Loss: 0.6778071075677872, Validation Loss: 0.7191718220710754\n",
      "Epoch 29: Train Loss: 0.6311159282922745, Validation Loss: 0.719571590423584\n",
      "Epoch 30: Train Loss: 0.6788907796144485, Validation Loss: 0.7235892415046692\n",
      "Epoch 31: Train Loss: 0.6941554546356201, Validation Loss: 0.7275239825248718\n",
      "Epoch 32: Train Loss: 0.6728901416063309, Validation Loss: 0.7303171157836914\n",
      "Epoch 33: Train Loss: 0.6469951570034027, Validation Loss: 0.7322692275047302\n",
      "Epoch 34: Train Loss: 0.6539979875087738, Validation Loss: 0.7351789474487305\n",
      "Epoch 35: Train Loss: 0.6831084787845612, Validation Loss: 0.7373538613319397\n",
      "Epoch 36: Train Loss: 0.7021541893482208, Validation Loss: 0.7387952208518982\n",
      "Epoch 37: Train Loss: 0.7011855393648148, Validation Loss: 0.739072322845459\n",
      "Epoch 38: Train Loss: 0.6103395819664001, Validation Loss: 0.7390769720077515\n",
      "Epoch 39: Train Loss: 0.6654523611068726, Validation Loss: 0.7389608025550842\n",
      "Epoch 40: Train Loss: 0.6422978341579437, Validation Loss: 0.7375870943069458\n",
      "Epoch 41: Train Loss: 0.6315395534038544, Validation Loss: 0.7366305589675903\n",
      "Epoch 42: Train Loss: 0.6092193275690079, Validation Loss: 0.7375591397285461\n",
      "Epoch 43: Train Loss: 0.6456100642681122, Validation Loss: 0.7396184802055359\n",
      "Epoch 44: Train Loss: 0.6620812565088272, Validation Loss: 0.7395713329315186\n",
      "Epoch 45: Train Loss: 0.6683710664510727, Validation Loss: 0.7396533489227295\n",
      "Epoch 46: Train Loss: 0.6863909959793091, Validation Loss: 0.7399192452430725\n",
      "Epoch 47: Train Loss: 0.6442234516143799, Validation Loss: 0.7398101687431335\n",
      "Epoch 48: Train Loss: 0.632894441485405, Validation Loss: 0.7401105165481567\n",
      "Epoch 49: Train Loss: 0.6662322729825974, Validation Loss: 0.7399579286575317\n",
      "Epoch 50: Train Loss: 0.6170942634344101, Validation Loss: 0.7393364906311035\n",
      "Epoch 51: Train Loss: 0.6624668389558792, Validation Loss: 0.737910270690918\n",
      "Epoch 52: Train Loss: 0.6258872151374817, Validation Loss: 0.7379603385925293\n",
      "Epoch 53: Train Loss: 0.6264462172985077, Validation Loss: 0.7402558326721191\n",
      "Epoch 54: Train Loss: 0.6489941626787186, Validation Loss: 0.7416999936103821\n",
      "Epoch 55: Train Loss: 0.6269555389881134, Validation Loss: 0.7422119975090027\n",
      "Epoch 56: Train Loss: 0.580497495830059, Validation Loss: 0.7424623370170593\n",
      "Epoch 57: Train Loss: 0.6112417280673981, Validation Loss: 0.7428418397903442\n",
      "Epoch 58: Train Loss: 0.6456359624862671, Validation Loss: 0.7432329058647156\n",
      "Epoch 59: Train Loss: 0.657228410243988, Validation Loss: 0.7434678673744202\n",
      "Epoch 60: Train Loss: 0.6366310566663742, Validation Loss: 0.7461356520652771\n",
      "Epoch 61: Train Loss: 0.6392675191164017, Validation Loss: 0.7474506497383118\n",
      "Epoch 62: Train Loss: 0.661462739109993, Validation Loss: 0.747285783290863\n",
      "Epoch 63: Train Loss: 0.6132356226444244, Validation Loss: 0.7454325556755066\n",
      "Epoch 64: Train Loss: 0.6428841352462769, Validation Loss: 0.7433694005012512\n",
      "Epoch 65: Train Loss: 0.6225526481866837, Validation Loss: 0.7418387532234192\n",
      "Epoch 66: Train Loss: 0.5932488739490509, Validation Loss: 0.7405015826225281\n",
      "Epoch 67: Train Loss: 0.6157791465520859, Validation Loss: 0.7399604320526123\n",
      "Epoch 68: Train Loss: 0.6256721019744873, Validation Loss: 0.7397222518920898\n",
      "Epoch 69: Train Loss: 0.6134115755558014, Validation Loss: 0.7400420904159546\n",
      "Epoch 70: Train Loss: 0.5906336605548859, Validation Loss: 0.7360421419143677\n",
      "Epoch 71: Train Loss: 0.6105301529169083, Validation Loss: 0.734414279460907\n",
      "Epoch 72: Train Loss: 0.6141457706689835, Validation Loss: 0.7361113429069519\n",
      "Epoch 73: Train Loss: 0.5682664960622787, Validation Loss: 0.7391741871833801\n",
      "Epoch 74: Train Loss: 0.5953716486692429, Validation Loss: 0.7419126033782959\n",
      "Epoch 75: Train Loss: 0.6115842461585999, Validation Loss: 0.7431588768959045\n",
      "Epoch 76: Train Loss: 0.5580572932958603, Validation Loss: 0.7437783479690552\n",
      "Epoch 77: Train Loss: 0.5840371698141098, Validation Loss: 0.7436769008636475\n",
      "Epoch 78: Train Loss: 0.6452490240335464, Validation Loss: 0.7435365319252014\n",
      "Epoch 79: Train Loss: 0.597552940249443, Validation Loss: 0.7438985705375671\n",
      "Epoch 80: Train Loss: 0.6001468151807785, Validation Loss: 0.7402005791664124\n",
      "Epoch 81: Train Loss: 0.6280763149261475, Validation Loss: 0.7386252284049988\n",
      "Epoch 82: Train Loss: 0.6197038739919662, Validation Loss: 0.7355270385742188\n",
      "Epoch 83: Train Loss: 0.5865049809217453, Validation Loss: 0.7318315505981445\n",
      "Epoch 84: Train Loss: 0.5831204205751419, Validation Loss: 0.7322913408279419\n",
      "Epoch 85: Train Loss: 0.5645439922809601, Validation Loss: 0.7333816289901733\n",
      "Epoch 86: Train Loss: 0.5589127391576767, Validation Loss: 0.7344711422920227\n",
      "Epoch 87: Train Loss: 0.6001807451248169, Validation Loss: 0.7352301478385925\n",
      "Epoch 88: Train Loss: 0.5739589482545853, Validation Loss: 0.7360727190971375\n",
      "Epoch 89: Train Loss: 0.5994736403226852, Validation Loss: 0.7362782955169678\n",
      "Epoch 90: Train Loss: 0.526619479060173, Validation Loss: 0.7391504049301147\n",
      "Epoch 91: Train Loss: 0.5829540565609932, Validation Loss: 0.7385894656181335\n",
      "Epoch 92: Train Loss: 0.5788652077317238, Validation Loss: 0.7357801198959351\n",
      "Epoch 93: Train Loss: 0.5821574479341507, Validation Loss: 0.7295476794242859\n",
      "Epoch 94: Train Loss: 0.5326042622327805, Validation Loss: 0.7252593040466309\n",
      "Epoch 95: Train Loss: 0.547615647315979, Validation Loss: 0.7237594127655029\n",
      "Epoch 96: Train Loss: 0.5399827063083649, Validation Loss: 0.7224203944206238\n",
      "Epoch 97: Train Loss: 0.5591741502285004, Validation Loss: 0.722101628780365\n",
      "Epoch 98: Train Loss: 0.6104633659124374, Validation Loss: 0.7208347320556641\n",
      "Epoch 99: Train Loss: 0.5352203026413918, Validation Loss: 0.7207026481628418\n",
      "Early stopping at epoch 100\n",
      "Completed fold 4\n",
      "Epoch 0: Train Loss: 0.7260177582502365, Validation Loss: 0.7047926783561707\n",
      "Epoch 1: Train Loss: 0.7695159912109375, Validation Loss: 0.7085380554199219\n",
      "Epoch 2: Train Loss: 0.7580126523971558, Validation Loss: 0.7110564112663269\n",
      "Epoch 3: Train Loss: 0.6874780356884003, Validation Loss: 0.714760959148407\n",
      "Epoch 4: Train Loss: 0.7181697636842728, Validation Loss: 0.7198009490966797\n",
      "Epoch 5: Train Loss: 0.7245328724384308, Validation Loss: 0.7233759164810181\n",
      "Epoch 6: Train Loss: 0.6963243633508682, Validation Loss: 0.7256994247436523\n",
      "Epoch 7: Train Loss: 0.6780546307563782, Validation Loss: 0.7277219295501709\n",
      "Epoch 8: Train Loss: 0.730144739151001, Validation Loss: 0.7292105555534363\n",
      "Epoch 9: Train Loss: 0.6695753335952759, Validation Loss: 0.7303165197372437\n",
      "Epoch 10: Train Loss: 0.7434707880020142, Validation Loss: 0.7291566729545593\n",
      "Epoch 11: Train Loss: 0.6801596879959106, Validation Loss: 0.7253850102424622\n",
      "Epoch 12: Train Loss: 0.7027545720338821, Validation Loss: 0.7208429574966431\n",
      "Epoch 13: Train Loss: 0.644374743103981, Validation Loss: 0.7187588214874268\n",
      "Epoch 14: Train Loss: 0.726015642285347, Validation Loss: 0.7191149592399597\n",
      "Epoch 15: Train Loss: 0.6476304233074188, Validation Loss: 0.7191724181175232\n",
      "Epoch 16: Train Loss: 0.7009826749563217, Validation Loss: 0.719526469707489\n",
      "Epoch 17: Train Loss: 0.6643294095993042, Validation Loss: 0.7199621200561523\n",
      "Epoch 18: Train Loss: 0.7389757335186005, Validation Loss: 0.7206842303276062\n",
      "Epoch 19: Train Loss: 0.632434606552124, Validation Loss: 0.721202552318573\n",
      "Epoch 20: Train Loss: 0.6641290783882141, Validation Loss: 0.7213574647903442\n",
      "Epoch 21: Train Loss: 0.6924572885036469, Validation Loss: 0.7246180176734924\n",
      "Epoch 22: Train Loss: 0.6763518452644348, Validation Loss: 0.7267705202102661\n",
      "Epoch 23: Train Loss: 0.6573060601949692, Validation Loss: 0.7259896397590637\n",
      "Epoch 24: Train Loss: 0.6905108094215393, Validation Loss: 0.7259729504585266\n",
      "Epoch 25: Train Loss: 0.6472761482000351, Validation Loss: 0.7259247303009033\n",
      "Epoch 26: Train Loss: 0.6316794902086258, Validation Loss: 0.7266656756401062\n",
      "Epoch 27: Train Loss: 0.6926675289869308, Validation Loss: 0.7276614308357239\n",
      "Epoch 28: Train Loss: 0.6542372703552246, Validation Loss: 0.7282624244689941\n",
      "Epoch 29: Train Loss: 0.6531113535165787, Validation Loss: 0.7281894087791443\n",
      "Epoch 30: Train Loss: 0.6286207586526871, Validation Loss: 0.7283019423484802\n",
      "Epoch 31: Train Loss: 0.6400987654924393, Validation Loss: 0.7267382740974426\n",
      "Epoch 32: Train Loss: 0.6705532073974609, Validation Loss: 0.7292364239692688\n",
      "Epoch 33: Train Loss: 0.6404148042201996, Validation Loss: 0.7326037287712097\n",
      "Epoch 34: Train Loss: 0.6644125282764435, Validation Loss: 0.7336255311965942\n",
      "Epoch 35: Train Loss: 0.6876629739999771, Validation Loss: 0.7345222234725952\n",
      "Epoch 36: Train Loss: 0.6483960449695587, Validation Loss: 0.7332202792167664\n",
      "Epoch 37: Train Loss: 0.63992078602314, Validation Loss: 0.732880175113678\n",
      "Epoch 38: Train Loss: 0.6565683484077454, Validation Loss: 0.7328155040740967\n",
      "Epoch 39: Train Loss: 0.6814316213130951, Validation Loss: 0.7324029803276062\n",
      "Epoch 40: Train Loss: 0.654475212097168, Validation Loss: 0.7297477722167969\n",
      "Epoch 41: Train Loss: 0.6139826625585556, Validation Loss: 0.7259006500244141\n",
      "Epoch 42: Train Loss: 0.6578586995601654, Validation Loss: 0.7233285903930664\n",
      "Epoch 43: Train Loss: 0.6930689215660095, Validation Loss: 0.7251724004745483\n",
      "Epoch 44: Train Loss: 0.6225061565637589, Validation Loss: 0.7267969250679016\n",
      "Epoch 45: Train Loss: 0.6599536687135696, Validation Loss: 0.7292929887771606\n",
      "Epoch 46: Train Loss: 0.6125174164772034, Validation Loss: 0.7300402522087097\n",
      "Epoch 47: Train Loss: 0.6107934862375259, Validation Loss: 0.7299195528030396\n",
      "Epoch 48: Train Loss: 0.6510348170995712, Validation Loss: 0.7300273180007935\n",
      "Epoch 49: Train Loss: 0.6206772029399872, Validation Loss: 0.7303063273429871\n",
      "Epoch 50: Train Loss: 0.6305618137121201, Validation Loss: 0.7317836880683899\n",
      "Epoch 51: Train Loss: 0.6351052224636078, Validation Loss: 0.7283120155334473\n",
      "Epoch 52: Train Loss: 0.6261033415794373, Validation Loss: 0.7262604832649231\n",
      "Epoch 53: Train Loss: 0.6297573000192642, Validation Loss: 0.7273510098457336\n",
      "Epoch 54: Train Loss: 0.6081770956516266, Validation Loss: 0.7285070419311523\n",
      "Epoch 55: Train Loss: 0.5730183720588684, Validation Loss: 0.7306275367736816\n",
      "Epoch 56: Train Loss: 0.6010308861732483, Validation Loss: 0.7336760759353638\n",
      "Epoch 57: Train Loss: 0.5931601077318192, Validation Loss: 0.7350310683250427\n",
      "Epoch 58: Train Loss: 0.5847530961036682, Validation Loss: 0.7354913353919983\n",
      "Epoch 59: Train Loss: 0.608019694685936, Validation Loss: 0.7359262108802795\n",
      "Epoch 60: Train Loss: 0.6175122261047363, Validation Loss: 0.7400068044662476\n",
      "Epoch 61: Train Loss: 0.624560683965683, Validation Loss: 0.74500972032547\n",
      "Epoch 62: Train Loss: 0.5789704769849777, Validation Loss: 0.7489433288574219\n",
      "Epoch 63: Train Loss: 0.6166123598814011, Validation Loss: 0.7515853047370911\n",
      "Epoch 64: Train Loss: 0.5790395885705948, Validation Loss: 0.7566275596618652\n",
      "Epoch 65: Train Loss: 0.5766597911715508, Validation Loss: 0.7605908513069153\n",
      "Epoch 66: Train Loss: 0.5618352144956589, Validation Loss: 0.7620835900306702\n",
      "Epoch 67: Train Loss: 0.5529602319002151, Validation Loss: 0.7633103728294373\n",
      "Epoch 68: Train Loss: 0.5563278943300247, Validation Loss: 0.7641398310661316\n",
      "Epoch 69: Train Loss: 0.5974413156509399, Validation Loss: 0.7644253969192505\n",
      "Epoch 70: Train Loss: 0.6463433653116226, Validation Loss: 0.7648561596870422\n",
      "Epoch 71: Train Loss: 0.561734676361084, Validation Loss: 0.7589457631111145\n",
      "Epoch 72: Train Loss: 0.5605430155992508, Validation Loss: 0.7562827467918396\n",
      "Epoch 73: Train Loss: 0.5470879450440407, Validation Loss: 0.7561599612236023\n",
      "Epoch 74: Train Loss: 0.5568094700574875, Validation Loss: 0.7588325142860413\n",
      "Epoch 75: Train Loss: 0.5689293593168259, Validation Loss: 0.763102114200592\n",
      "Epoch 76: Train Loss: 0.5474558100104332, Validation Loss: 0.765457272529602\n",
      "Epoch 77: Train Loss: 0.5642220079898834, Validation Loss: 0.7662268877029419\n",
      "Epoch 78: Train Loss: 0.555869996547699, Validation Loss: 0.7662436962127686\n",
      "Epoch 79: Train Loss: 0.48517216742038727, Validation Loss: 0.7664949297904968\n",
      "Epoch 80: Train Loss: 0.5695282220840454, Validation Loss: 0.7685269713401794\n",
      "Epoch 81: Train Loss: 0.5589662790298462, Validation Loss: 0.7713727355003357\n",
      "Epoch 82: Train Loss: 0.5446862727403641, Validation Loss: 0.7826575636863708\n",
      "Epoch 83: Train Loss: 0.5635606944561005, Validation Loss: 0.7945696115493774\n",
      "Epoch 84: Train Loss: 0.5632631033658981, Validation Loss: 0.7966468334197998\n",
      "Epoch 85: Train Loss: 0.53744176030159, Validation Loss: 0.7945454716682434\n",
      "Epoch 86: Train Loss: 0.5181780755519867, Validation Loss: 0.8032407760620117\n",
      "Epoch 87: Train Loss: 0.5414123833179474, Validation Loss: 0.8084720373153687\n",
      "Epoch 88: Train Loss: 0.5386082381010056, Validation Loss: 0.8106753826141357\n",
      "Epoch 89: Train Loss: 0.5292487889528275, Validation Loss: 0.8110805749893188\n",
      "Epoch 90: Train Loss: 0.5189426615834236, Validation Loss: 0.8034646511077881\n",
      "Epoch 91: Train Loss: 0.48590606451034546, Validation Loss: 0.8119590282440186\n",
      "Epoch 92: Train Loss: 0.5122938826680183, Validation Loss: 0.832084596157074\n",
      "Epoch 93: Train Loss: 0.4960915222764015, Validation Loss: 0.8440629839897156\n",
      "Epoch 94: Train Loss: 0.4675923138856888, Validation Loss: 0.8356075286865234\n",
      "Epoch 95: Train Loss: 0.5131945013999939, Validation Loss: 0.8352925777435303\n",
      "Epoch 96: Train Loss: 0.5310378447175026, Validation Loss: 0.8371272087097168\n",
      "Epoch 97: Train Loss: 0.49970556050539017, Validation Loss: 0.8378403782844543\n",
      "Epoch 98: Train Loss: 0.5324629992246628, Validation Loss: 0.8401012420654297\n",
      "Epoch 99: Train Loss: 0.44326768070459366, Validation Loss: 0.8402982950210571\n",
      "Early stopping at epoch 100\n",
      "Completed fold 5\n",
      "Epoch 0: Train Loss: 0.7257713824510574, Validation Loss: 0.7002086639404297\n",
      "Epoch 1: Train Loss: 0.6987607181072235, Validation Loss: 0.7039578557014465\n",
      "Epoch 2: Train Loss: 0.7283293157815933, Validation Loss: 0.7065712213516235\n",
      "Epoch 3: Train Loss: 0.7078441232442856, Validation Loss: 0.7107260823249817\n",
      "Epoch 4: Train Loss: 0.669744998216629, Validation Loss: 0.7140820026397705\n",
      "Epoch 5: Train Loss: 0.7951060384511948, Validation Loss: 0.717237651348114\n",
      "Epoch 6: Train Loss: 0.7073839753866196, Validation Loss: 0.7203567624092102\n",
      "Epoch 7: Train Loss: 0.6781316995620728, Validation Loss: 0.7224997878074646\n",
      "Epoch 8: Train Loss: 0.6486719399690628, Validation Loss: 0.7232863306999207\n",
      "Epoch 9: Train Loss: 0.6973445564508438, Validation Loss: 0.7241649031639099\n",
      "Epoch 10: Train Loss: 0.7080217748880386, Validation Loss: 0.7240634560585022\n",
      "Epoch 11: Train Loss: 0.7149929255247116, Validation Loss: 0.7224545478820801\n",
      "Epoch 12: Train Loss: 0.7105381488800049, Validation Loss: 0.7217972278594971\n",
      "Epoch 13: Train Loss: 0.7071183174848557, Validation Loss: 0.7208251953125\n",
      "Epoch 14: Train Loss: 0.7258737683296204, Validation Loss: 0.7195566296577454\n",
      "Epoch 15: Train Loss: 0.6755190044641495, Validation Loss: 0.7188912630081177\n",
      "Epoch 16: Train Loss: 0.6947333961725235, Validation Loss: 0.7185096144676208\n",
      "Epoch 17: Train Loss: 0.6965272724628448, Validation Loss: 0.7183921933174133\n",
      "Epoch 18: Train Loss: 0.6848527789115906, Validation Loss: 0.7178037762641907\n",
      "Epoch 19: Train Loss: 0.6443023383617401, Validation Loss: 0.7180978655815125\n",
      "Epoch 20: Train Loss: 0.696736067533493, Validation Loss: 0.7120428681373596\n",
      "Epoch 21: Train Loss: 0.6892971992492676, Validation Loss: 0.7040481567382812\n",
      "Epoch 22: Train Loss: 0.7001515030860901, Validation Loss: 0.698662519454956\n",
      "Epoch 23: Train Loss: 0.6180044710636139, Validation Loss: 0.6978433132171631\n",
      "Epoch 24: Train Loss: 0.639094740152359, Validation Loss: 0.7000166773796082\n",
      "Epoch 25: Train Loss: 0.6979919970035553, Validation Loss: 0.7013785243034363\n",
      "Epoch 26: Train Loss: 0.6360425800085068, Validation Loss: 0.7020939588546753\n",
      "Epoch 27: Train Loss: 0.6766515970230103, Validation Loss: 0.7032918930053711\n",
      "Epoch 28: Train Loss: 0.6467817574739456, Validation Loss: 0.7040250301361084\n",
      "Epoch 29: Train Loss: 0.7028117775917053, Validation Loss: 0.7043794393539429\n",
      "Epoch 30: Train Loss: 0.6790046989917755, Validation Loss: 0.7104437947273254\n",
      "Epoch 31: Train Loss: 0.6557735055685043, Validation Loss: 0.715739905834198\n",
      "Epoch 32: Train Loss: 0.6773573160171509, Validation Loss: 0.7184543013572693\n",
      "Epoch 33: Train Loss: 0.6728098094463348, Validation Loss: 0.719768226146698\n",
      "Epoch 34: Train Loss: 0.6483491212129593, Validation Loss: 0.7150849103927612\n",
      "Epoch 35: Train Loss: 0.6742111146450043, Validation Loss: 0.7092477679252625\n",
      "Epoch 36: Train Loss: 0.6591161340475082, Validation Loss: 0.7067219614982605\n",
      "Epoch 37: Train Loss: 0.6006322950124741, Validation Loss: 0.7058149576187134\n",
      "Epoch 38: Train Loss: 0.6374014317989349, Validation Loss: 0.7061147689819336\n",
      "Epoch 39: Train Loss: 0.6362175792455673, Validation Loss: 0.7066400647163391\n",
      "Epoch 40: Train Loss: 0.6208555996417999, Validation Loss: 0.7081187963485718\n",
      "Epoch 41: Train Loss: 0.6413241624832153, Validation Loss: 0.7069547176361084\n",
      "Epoch 42: Train Loss: 0.6490604430437088, Validation Loss: 0.7033566236495972\n",
      "Epoch 43: Train Loss: 0.6286604851484299, Validation Loss: 0.7059651613235474\n",
      "Epoch 44: Train Loss: 0.6124939322471619, Validation Loss: 0.7071534991264343\n",
      "Epoch 45: Train Loss: 0.5750889182090759, Validation Loss: 0.708619236946106\n",
      "Epoch 46: Train Loss: 0.6287700235843658, Validation Loss: 0.7101476788520813\n",
      "Epoch 47: Train Loss: 0.599564403295517, Validation Loss: 0.7127628326416016\n",
      "Epoch 48: Train Loss: 0.60287144780159, Validation Loss: 0.7141678929328918\n",
      "Epoch 49: Train Loss: 0.622935339808464, Validation Loss: 0.7145317792892456\n",
      "Epoch 50: Train Loss: 0.5842568576335907, Validation Loss: 0.7144659757614136\n",
      "Epoch 51: Train Loss: 0.6360044181346893, Validation Loss: 0.7056209444999695\n",
      "Epoch 52: Train Loss: 0.5902126580476761, Validation Loss: 0.6928591728210449\n",
      "Epoch 53: Train Loss: 0.6123532652854919, Validation Loss: 0.6871967315673828\n",
      "Epoch 54: Train Loss: 0.5887651592493057, Validation Loss: 0.6847349405288696\n",
      "Epoch 55: Train Loss: 0.605497270822525, Validation Loss: 0.6859320402145386\n",
      "Epoch 56: Train Loss: 0.6009640544652939, Validation Loss: 0.6848964095115662\n",
      "Epoch 57: Train Loss: 0.5705968141555786, Validation Loss: 0.6829434633255005\n",
      "Epoch 58: Train Loss: 0.5756056904792786, Validation Loss: 0.681433379650116\n",
      "Epoch 59: Train Loss: 0.5728604644536972, Validation Loss: 0.681219220161438\n",
      "Epoch 60: Train Loss: 0.5741288736462593, Validation Loss: 0.6646366119384766\n",
      "Epoch 61: Train Loss: 0.5390491187572479, Validation Loss: 0.6575564742088318\n",
      "Epoch 62: Train Loss: 0.5459148585796356, Validation Loss: 0.647703230381012\n",
      "Epoch 63: Train Loss: 0.595597505569458, Validation Loss: 0.6377268433570862\n",
      "Epoch 64: Train Loss: 0.5538433566689491, Validation Loss: 0.62238609790802\n",
      "Epoch 65: Train Loss: 0.5469558387994766, Validation Loss: 0.6152034997940063\n",
      "Epoch 66: Train Loss: 0.5044723004102707, Validation Loss: 0.6094421148300171\n",
      "Epoch 67: Train Loss: 0.5366288870573044, Validation Loss: 0.6079150438308716\n",
      "Epoch 68: Train Loss: 0.5112257078289986, Validation Loss: 0.6092315912246704\n",
      "Epoch 69: Train Loss: 0.5379699543118477, Validation Loss: 0.6102128624916077\n",
      "Epoch 70: Train Loss: 0.5462040901184082, Validation Loss: 0.6003705859184265\n",
      "Epoch 71: Train Loss: 0.5217322483658791, Validation Loss: 0.5981764793395996\n",
      "Epoch 72: Train Loss: 0.4913954511284828, Validation Loss: 0.5945802927017212\n",
      "Epoch 73: Train Loss: 0.49369634687900543, Validation Loss: 0.5890823006629944\n",
      "Epoch 74: Train Loss: 0.47043778002262115, Validation Loss: 0.5813788175582886\n",
      "Epoch 75: Train Loss: 0.5150787085294724, Validation Loss: 0.5806453824043274\n",
      "Epoch 76: Train Loss: 0.4681210219860077, Validation Loss: 0.5810973644256592\n",
      "Epoch 77: Train Loss: 0.4872000366449356, Validation Loss: 0.5797276496887207\n",
      "Epoch 78: Train Loss: 0.49761005491018295, Validation Loss: 0.5771534442901611\n",
      "Epoch 79: Train Loss: 0.49290870130062103, Validation Loss: 0.5778124332427979\n",
      "Epoch 80: Train Loss: 0.4812671169638634, Validation Loss: 0.5620384812355042\n",
      "Epoch 81: Train Loss: 0.46554166823625565, Validation Loss: 0.543907880783081\n",
      "Epoch 82: Train Loss: 0.40790797770023346, Validation Loss: 0.5399501323699951\n",
      "Epoch 83: Train Loss: 0.4826135188341141, Validation Loss: 0.5560435056686401\n",
      "Epoch 84: Train Loss: 0.4255838692188263, Validation Loss: 0.5692397952079773\n",
      "Epoch 85: Train Loss: 0.3916679248213768, Validation Loss: 0.5699912905693054\n",
      "Epoch 86: Train Loss: 0.461176261305809, Validation Loss: 0.5668392777442932\n",
      "Epoch 87: Train Loss: 0.4491512179374695, Validation Loss: 0.5664063096046448\n",
      "Epoch 88: Train Loss: 0.43565163761377335, Validation Loss: 0.5650197863578796\n",
      "Epoch 89: Train Loss: 0.4571898952126503, Validation Loss: 0.5657045841217041\n",
      "Epoch 90: Train Loss: 0.4156786873936653, Validation Loss: 0.5501143336296082\n",
      "Epoch 91: Train Loss: 0.3853223994374275, Validation Loss: 0.5471900105476379\n",
      "Epoch 92: Train Loss: 0.40018119663000107, Validation Loss: 0.5510861873626709\n",
      "Epoch 93: Train Loss: 0.4065762758255005, Validation Loss: 0.5691606998443604\n",
      "Epoch 94: Train Loss: 0.39622659236192703, Validation Loss: 0.5750700235366821\n",
      "Epoch 95: Train Loss: 0.40192098915576935, Validation Loss: 0.5821776986122131\n",
      "Epoch 96: Train Loss: 0.39712851494550705, Validation Loss: 0.5848976969718933\n",
      "Epoch 97: Train Loss: 0.3968614935874939, Validation Loss: 0.5860105156898499\n",
      "Epoch 98: Train Loss: 0.37015800923109055, Validation Loss: 0.584214448928833\n",
      "Epoch 99: Train Loss: 0.39738061279058456, Validation Loss: 0.5826990604400635\n",
      "Epoch 100: Train Loss: 0.36923718452453613, Validation Loss: 0.5398390889167786\n",
      "Epoch 101: Train Loss: 0.3669874295592308, Validation Loss: 0.5401345491409302\n",
      "Epoch 102: Train Loss: 0.3547217845916748, Validation Loss: 0.551801860332489\n",
      "Epoch 103: Train Loss: 0.37495120614767075, Validation Loss: 0.5610858798027039\n",
      "Epoch 104: Train Loss: 0.33420271426439285, Validation Loss: 0.5848586559295654\n",
      "Epoch 105: Train Loss: 0.305039644241333, Validation Loss: 0.569495677947998\n",
      "Epoch 106: Train Loss: 0.31640833616256714, Validation Loss: 0.559283435344696\n",
      "Epoch 107: Train Loss: 0.3143801987171173, Validation Loss: 0.5544402599334717\n",
      "Epoch 108: Train Loss: 0.31925056129693985, Validation Loss: 0.5526522994041443\n",
      "Epoch 109: Train Loss: 0.31369782239198685, Validation Loss: 0.5521221160888672\n",
      "Epoch 110: Train Loss: 0.3031161427497864, Validation Loss: 0.49570730328559875\n",
      "Epoch 111: Train Loss: 0.29594794660806656, Validation Loss: 0.4683911204338074\n",
      "Epoch 112: Train Loss: 0.2868841104209423, Validation Loss: 0.4922015964984894\n",
      "Epoch 113: Train Loss: 0.2857045456767082, Validation Loss: 0.4974287152290344\n",
      "Epoch 114: Train Loss: 0.3026815727353096, Validation Loss: 0.4746975898742676\n",
      "Epoch 115: Train Loss: 0.28919459879398346, Validation Loss: 0.4721384644508362\n",
      "Epoch 116: Train Loss: 0.27135486528277397, Validation Loss: 0.48626062273979187\n",
      "Epoch 117: Train Loss: 0.26408110558986664, Validation Loss: 0.49188289046287537\n",
      "Epoch 118: Train Loss: 0.2544061951339245, Validation Loss: 0.4913528561592102\n",
      "Epoch 119: Train Loss: 0.25699012726545334, Validation Loss: 0.49020710587501526\n",
      "Epoch 120: Train Loss: 0.26484164223074913, Validation Loss: 0.49964410066604614\n",
      "Epoch 121: Train Loss: 0.23293504863977432, Validation Loss: 0.5311563611030579\n",
      "Epoch 122: Train Loss: 0.2558077462017536, Validation Loss: 0.5131964087486267\n",
      "Epoch 123: Train Loss: 0.24231013655662537, Validation Loss: 0.5017883777618408\n",
      "Epoch 124: Train Loss: 0.24649889767169952, Validation Loss: 0.5097954869270325\n",
      "Epoch 125: Train Loss: 0.22954192012548447, Validation Loss: 0.4992654621601105\n",
      "Epoch 126: Train Loss: 0.22203267365694046, Validation Loss: 0.4753707945346832\n",
      "Epoch 127: Train Loss: 0.263181209564209, Validation Loss: 0.46606355905532837\n",
      "Epoch 128: Train Loss: 0.22482647374272346, Validation Loss: 0.4603154957294464\n",
      "Epoch 129: Train Loss: 0.19705532118678093, Validation Loss: 0.46069103479385376\n",
      "Epoch 130: Train Loss: 0.21753234788775444, Validation Loss: 0.5144428014755249\n",
      "Epoch 131: Train Loss: 0.2448963262140751, Validation Loss: 0.48599255084991455\n",
      "Epoch 132: Train Loss: 0.19576997682452202, Validation Loss: 0.4141106605529785\n",
      "Epoch 133: Train Loss: 0.19594567641615868, Validation Loss: 0.4026433527469635\n",
      "Epoch 134: Train Loss: 0.24947497248649597, Validation Loss: 0.45767927169799805\n",
      "Epoch 135: Train Loss: 0.20077762380242348, Validation Loss: 0.4777691066265106\n",
      "Epoch 136: Train Loss: 0.2110837884247303, Validation Loss: 0.45951205492019653\n",
      "Epoch 137: Train Loss: 0.19835729897022247, Validation Loss: 0.45195600390434265\n",
      "Epoch 138: Train Loss: 0.18619468063116074, Validation Loss: 0.45413658022880554\n",
      "Epoch 139: Train Loss: 0.1940949410200119, Validation Loss: 0.45505377650260925\n",
      "Epoch 140: Train Loss: 0.19767948985099792, Validation Loss: 0.4871681332588196\n",
      "Epoch 141: Train Loss: 0.17301782220602036, Validation Loss: 0.424887090921402\n",
      "Epoch 142: Train Loss: 0.19093554094433784, Validation Loss: 0.43193376064300537\n",
      "Epoch 143: Train Loss: 0.18428250961005688, Validation Loss: 0.4677642583847046\n",
      "Epoch 144: Train Loss: 0.16798659414052963, Validation Loss: 0.49043992161750793\n",
      "Epoch 145: Train Loss: 0.1834927462041378, Validation Loss: 0.47937947511672974\n",
      "Epoch 146: Train Loss: 0.16751745343208313, Validation Loss: 0.470009446144104\n",
      "Epoch 147: Train Loss: 0.15520133450627327, Validation Loss: 0.44941097497940063\n",
      "Epoch 148: Train Loss: 0.16763558238744736, Validation Loss: 0.4474770426750183\n",
      "Epoch 149: Train Loss: 0.18282858282327652, Validation Loss: 0.44293642044067383\n",
      "Epoch 150: Train Loss: 0.17110464721918106, Validation Loss: 0.42309606075286865\n",
      "Epoch 151: Train Loss: 0.1879577934741974, Validation Loss: 0.502555787563324\n",
      "Epoch 152: Train Loss: 0.20694445446133614, Validation Loss: 0.5311986207962036\n",
      "Epoch 153: Train Loss: 0.16797997243702412, Validation Loss: 0.4152943193912506\n",
      "Epoch 154: Train Loss: 0.14790700003504753, Validation Loss: 0.38283753395080566\n",
      "Epoch 155: Train Loss: 0.15099204145371914, Validation Loss: 0.3717408776283264\n",
      "Epoch 156: Train Loss: 0.13758039101958275, Validation Loss: 0.38669562339782715\n",
      "Epoch 157: Train Loss: 0.15800799056887627, Validation Loss: 0.3979584276676178\n",
      "Epoch 158: Train Loss: 0.13192573375999928, Validation Loss: 0.39938884973526\n",
      "Epoch 159: Train Loss: 0.18706902116537094, Validation Loss: 0.38787660002708435\n",
      "Epoch 160: Train Loss: 0.16584743559360504, Validation Loss: 0.36748501658439636\n",
      "Epoch 161: Train Loss: 0.16034267470240593, Validation Loss: 0.39194709062576294\n",
      "Epoch 162: Train Loss: 0.1440731380134821, Validation Loss: 0.4741719961166382\n",
      "Epoch 163: Train Loss: 0.14137106016278267, Validation Loss: 0.4165859818458557\n",
      "Epoch 164: Train Loss: 0.1970218513160944, Validation Loss: 0.4108467996120453\n",
      "Epoch 165: Train Loss: 0.13383583165705204, Validation Loss: 0.4384341239929199\n",
      "Epoch 166: Train Loss: 0.13774277083575726, Validation Loss: 0.46669891476631165\n",
      "Epoch 167: Train Loss: 0.1378746386617422, Validation Loss: 0.4557098150253296\n",
      "Epoch 168: Train Loss: 0.13453899137675762, Validation Loss: 0.4566430151462555\n",
      "Epoch 169: Train Loss: 0.13731647841632366, Validation Loss: 0.4537312686443329\n",
      "Epoch 170: Train Loss: 0.13364327698946, Validation Loss: 0.3692885935306549\n",
      "Epoch 171: Train Loss: 0.13162964396178722, Validation Loss: 0.342736154794693\n",
      "Epoch 172: Train Loss: 0.1255402658134699, Validation Loss: 0.43383291363716125\n",
      "Epoch 173: Train Loss: 0.15594067983329296, Validation Loss: 0.45311811566352844\n",
      "Epoch 174: Train Loss: 0.15820607729256153, Validation Loss: 0.40024638175964355\n",
      "Epoch 175: Train Loss: 0.13164043240249157, Validation Loss: 0.3909969925880432\n",
      "Epoch 176: Train Loss: 0.13957923837006092, Validation Loss: 0.3876946270465851\n",
      "Epoch 177: Train Loss: 0.15237963013350964, Validation Loss: 0.38863739371299744\n",
      "Epoch 178: Train Loss: 0.10701354406774044, Validation Loss: 0.3824838697910309\n",
      "Epoch 179: Train Loss: 0.134533803910017, Validation Loss: 0.3823876976966858\n",
      "Epoch 180: Train Loss: 0.1271444745361805, Validation Loss: 0.4328691065311432\n",
      "Epoch 181: Train Loss: 0.1298782378435135, Validation Loss: 0.4180041253566742\n",
      "Epoch 182: Train Loss: 0.11941243521869183, Validation Loss: 0.39069756865501404\n",
      "Epoch 183: Train Loss: 0.12400233559310436, Validation Loss: 0.41706714034080505\n",
      "Epoch 184: Train Loss: 0.12354718148708344, Validation Loss: 0.40436720848083496\n",
      "Epoch 185: Train Loss: 0.11290336772799492, Validation Loss: 0.3645474314689636\n",
      "Epoch 186: Train Loss: 0.11677803844213486, Validation Loss: 0.3608029782772064\n",
      "Epoch 187: Train Loss: 0.10995334759354591, Validation Loss: 0.379802405834198\n",
      "Epoch 188: Train Loss: 0.13616107124835253, Validation Loss: 0.3879939019680023\n",
      "Epoch 189: Train Loss: 0.1202527116984129, Validation Loss: 0.38577041029930115\n",
      "Epoch 190: Train Loss: 0.10645141452550888, Validation Loss: 0.41393986344337463\n",
      "Epoch 191: Train Loss: 0.10675621218979359, Validation Loss: 0.46793442964553833\n",
      "Epoch 192: Train Loss: 0.1248395312577486, Validation Loss: 0.4319383502006531\n",
      "Epoch 193: Train Loss: 0.1283073015511036, Validation Loss: 0.3806981146335602\n",
      "Epoch 194: Train Loss: 0.1080491729080677, Validation Loss: 0.3628513216972351\n",
      "Epoch 195: Train Loss: 0.11748162843286991, Validation Loss: 0.356344610452652\n",
      "Epoch 196: Train Loss: 0.12560003995895386, Validation Loss: 0.3532494604587555\n",
      "Epoch 197: Train Loss: 0.13518415577709675, Validation Loss: 0.3671797513961792\n",
      "Epoch 198: Train Loss: 0.10897392220795155, Validation Loss: 0.3699966073036194\n",
      "Epoch 199: Train Loss: 0.10010495595633984, Validation Loss: 0.3736017048358917\n",
      "Epoch 200: Train Loss: 0.1274836203083396, Validation Loss: 0.37093767523765564\n",
      "Epoch 201: Train Loss: 0.10899807885289192, Validation Loss: 0.36934876441955566\n",
      "Epoch 202: Train Loss: 0.11556987464427948, Validation Loss: 0.3615017533302307\n",
      "Epoch 203: Train Loss: 0.11773292161524296, Validation Loss: 0.38774624466896057\n",
      "Epoch 204: Train Loss: 0.09993787482380867, Validation Loss: 0.421602725982666\n",
      "Epoch 205: Train Loss: 0.11214817315340042, Validation Loss: 0.4310629069805145\n",
      "Epoch 206: Train Loss: 0.1066906712949276, Validation Loss: 0.3830885589122772\n",
      "Epoch 207: Train Loss: 0.10403578728437424, Validation Loss: 0.35655903816223145\n",
      "Epoch 208: Train Loss: 0.09984906949102879, Validation Loss: 0.3606596291065216\n",
      "Epoch 209: Train Loss: 0.09043099079281092, Validation Loss: 0.3638491630554199\n",
      "Epoch 210: Train Loss: 0.12038610689342022, Validation Loss: 0.3979716897010803\n",
      "Epoch 211: Train Loss: 0.0888661714270711, Validation Loss: 0.38979458808898926\n",
      "Epoch 212: Train Loss: 0.10246758162975311, Validation Loss: 0.3952040374279022\n",
      "Epoch 213: Train Loss: 0.09393629990518093, Validation Loss: 0.37257781624794006\n",
      "Epoch 214: Train Loss: 0.10347112454473972, Validation Loss: 0.3497423827648163\n",
      "Epoch 215: Train Loss: 0.11496910452842712, Validation Loss: 0.38363179564476013\n",
      "Epoch 216: Train Loss: 0.0952443890273571, Validation Loss: 0.3706301748752594\n",
      "Epoch 217: Train Loss: 0.12582562491297722, Validation Loss: 0.33172330260276794\n",
      "Epoch 218: Train Loss: 0.09582343883812428, Validation Loss: 0.32659271359443665\n",
      "Epoch 219: Train Loss: 0.08381755650043488, Validation Loss: 0.3320837616920471\n",
      "Epoch 220: Train Loss: 0.10256042517721653, Validation Loss: 0.3372812271118164\n",
      "Epoch 221: Train Loss: 0.11137715447694063, Validation Loss: 0.37722939252853394\n",
      "Epoch 222: Train Loss: 0.10855306033045053, Validation Loss: 0.48315298557281494\n",
      "Epoch 223: Train Loss: 0.1056236494332552, Validation Loss: 0.3644747734069824\n",
      "Epoch 224: Train Loss: 0.0968639450147748, Validation Loss: 0.2972097396850586\n",
      "Epoch 225: Train Loss: 0.13309368304908276, Validation Loss: 0.30166786909103394\n",
      "Epoch 226: Train Loss: 0.09553675167262554, Validation Loss: 0.30127599835395813\n",
      "Epoch 227: Train Loss: 0.09332697466015816, Validation Loss: 0.3108423352241516\n",
      "Epoch 228: Train Loss: 0.09376267250627279, Validation Loss: 0.3085083067417145\n",
      "Epoch 229: Train Loss: 0.10190065391361713, Validation Loss: 0.3134499192237854\n",
      "Epoch 230: Train Loss: 0.10148341953754425, Validation Loss: 0.4192298948764801\n",
      "Epoch 231: Train Loss: 0.11365329846739769, Validation Loss: 0.390198290348053\n",
      "Epoch 232: Train Loss: 0.11701965890824795, Validation Loss: 0.3320390284061432\n",
      "Epoch 233: Train Loss: 0.09900558739900589, Validation Loss: 0.34080013632774353\n",
      "Epoch 234: Train Loss: 0.08498475048691034, Validation Loss: 0.35016053915023804\n",
      "Epoch 235: Train Loss: 0.11122641805559397, Validation Loss: 0.4135865867137909\n",
      "Epoch 236: Train Loss: 0.12295357044786215, Validation Loss: 0.4346406161785126\n",
      "Epoch 237: Train Loss: 0.08965370804071426, Validation Loss: 0.4591771066188812\n",
      "Epoch 238: Train Loss: 0.09090927802026272, Validation Loss: 0.465582937002182\n",
      "Epoch 239: Train Loss: 0.09635572507977486, Validation Loss: 0.4755261540412903\n",
      "Epoch 240: Train Loss: 0.11582199484109879, Validation Loss: 0.32531675696372986\n",
      "Epoch 241: Train Loss: 0.12030315957963467, Validation Loss: 0.3136608898639679\n",
      "Epoch 242: Train Loss: 0.15251843258738518, Validation Loss: 0.5019415616989136\n",
      "Epoch 243: Train Loss: 0.11412830837070942, Validation Loss: 0.4216654896736145\n",
      "Epoch 244: Train Loss: 0.13017904851585627, Validation Loss: 0.4011436402797699\n",
      "Epoch 245: Train Loss: 0.08111921790987253, Validation Loss: 0.34944289922714233\n",
      "Epoch 246: Train Loss: 0.09174333047121763, Validation Loss: 0.3285152316093445\n",
      "Epoch 247: Train Loss: 0.10239360202103853, Validation Loss: 0.32295066118240356\n",
      "Epoch 248: Train Loss: 0.10521251615136862, Validation Loss: 0.3156938850879669\n",
      "Epoch 249: Train Loss: 0.10619224235415459, Validation Loss: 0.324307382106781\n",
      "Epoch 250: Train Loss: 0.10738910548388958, Validation Loss: 0.38595300912857056\n",
      "Epoch 251: Train Loss: 0.14342844858765602, Validation Loss: 0.34810900688171387\n",
      "Epoch 252: Train Loss: 0.12932446040213108, Validation Loss: 0.39000463485717773\n",
      "Epoch 253: Train Loss: 0.09752449207007885, Validation Loss: 0.44921866059303284\n",
      "Epoch 254: Train Loss: 0.0907287672162056, Validation Loss: 0.5373121500015259\n",
      "Epoch 255: Train Loss: 0.08931365143507719, Validation Loss: 0.43549680709838867\n",
      "Epoch 256: Train Loss: 0.10511732660233974, Validation Loss: 0.37393078207969666\n",
      "Epoch 257: Train Loss: 0.08188499137759209, Validation Loss: 0.36899885535240173\n",
      "Epoch 258: Train Loss: 0.09201742056757212, Validation Loss: 0.3651161193847656\n",
      "Epoch 259: Train Loss: 0.10338414553552866, Validation Loss: 0.3622821271419525\n",
      "Epoch 260: Train Loss: 0.1174045791849494, Validation Loss: 0.4058261811733246\n",
      "Epoch 261: Train Loss: 0.12406634353101254, Validation Loss: 0.37744709849357605\n",
      "Epoch 262: Train Loss: 0.08555619791150093, Validation Loss: 0.3556712865829468\n",
      "Epoch 263: Train Loss: 0.09863974060863256, Validation Loss: 0.32081764936447144\n",
      "Epoch 264: Train Loss: 0.11818675883114338, Validation Loss: 0.3049604892730713\n",
      "Epoch 265: Train Loss: 0.08070258423686028, Validation Loss: 0.33219894766807556\n",
      "Epoch 266: Train Loss: 0.09831870067864656, Validation Loss: 0.37055841088294983\n",
      "Epoch 267: Train Loss: 0.0970750106498599, Validation Loss: 0.39041975140571594\n",
      "Epoch 268: Train Loss: 0.12034998089075089, Validation Loss: 0.3992529809474945\n",
      "Epoch 269: Train Loss: 0.09666128922253847, Validation Loss: 0.38212764263153076\n",
      "Epoch 270: Train Loss: 0.0775516927242279, Validation Loss: 0.48935040831565857\n",
      "Epoch 271: Train Loss: 0.08342410530894995, Validation Loss: 0.676777720451355\n",
      "Epoch 272: Train Loss: 0.06294877734035254, Validation Loss: 0.6361697316169739\n",
      "Epoch 273: Train Loss: 0.1004105918109417, Validation Loss: 0.5260531902313232\n",
      "Epoch 274: Train Loss: 0.11703872215002775, Validation Loss: 0.4331386685371399\n",
      "Epoch 275: Train Loss: 0.07181893941015005, Validation Loss: 0.35642552375793457\n",
      "Epoch 276: Train Loss: 0.09452184941619635, Validation Loss: 0.3134361207485199\n",
      "Epoch 277: Train Loss: 0.09238754585385323, Validation Loss: 0.29245325922966003\n",
      "Epoch 278: Train Loss: 0.07237385306507349, Validation Loss: 0.29349449276924133\n",
      "Epoch 279: Train Loss: 0.07539752591401339, Validation Loss: 0.2973785102367401\n",
      "Epoch 280: Train Loss: 0.08161905314773321, Validation Loss: 0.3426722288131714\n",
      "Epoch 281: Train Loss: 0.09056997299194336, Validation Loss: 0.42643803358078003\n",
      "Epoch 282: Train Loss: 0.09183008782565594, Validation Loss: 0.43206271529197693\n",
      "Epoch 283: Train Loss: 0.06760163139551878, Validation Loss: 0.4963153898715973\n",
      "Epoch 284: Train Loss: 0.1123258676379919, Validation Loss: 0.442715048789978\n",
      "Epoch 285: Train Loss: 0.07911910209804773, Validation Loss: 0.40622639656066895\n",
      "Epoch 286: Train Loss: 0.1219128961674869, Validation Loss: 0.4035370349884033\n",
      "Epoch 287: Train Loss: 0.09233623556792736, Validation Loss: 0.41184353828430176\n",
      "Epoch 288: Train Loss: 0.09450475592166185, Validation Loss: 0.39317482709884644\n",
      "Epoch 289: Train Loss: 0.08309179730713367, Validation Loss: 0.3968581259250641\n",
      "Epoch 290: Train Loss: 0.08150627464056015, Validation Loss: 0.3205792307853699\n",
      "Epoch 291: Train Loss: 0.08017468731850386, Validation Loss: 0.33814114332199097\n",
      "Epoch 292: Train Loss: 0.07366531668230891, Validation Loss: 0.3445504307746887\n",
      "Epoch 293: Train Loss: 0.09658901067450643, Validation Loss: 0.35961297154426575\n",
      "Epoch 294: Train Loss: 0.11964196246117353, Validation Loss: 0.4253012239933014\n",
      "Epoch 295: Train Loss: 0.08204062702134252, Validation Loss: 0.548157811164856\n",
      "Epoch 296: Train Loss: 0.09050850477069616, Validation Loss: 0.5422980189323425\n",
      "Epoch 297: Train Loss: 0.10258387587964535, Validation Loss: 0.4786089062690735\n",
      "Epoch 298: Train Loss: 0.08080593589693308, Validation Loss: 0.44354549050331116\n",
      "Epoch 299: Train Loss: 0.09027476236224174, Validation Loss: 0.4380694329738617\n",
      "Epoch 300: Train Loss: 0.07664807979017496, Validation Loss: 0.2711869478225708\n",
      "Epoch 301: Train Loss: 0.06809127703309059, Validation Loss: 0.2658425569534302\n",
      "Epoch 302: Train Loss: 0.10114884749054909, Validation Loss: 0.28701159358024597\n",
      "Epoch 303: Train Loss: 0.09325653407722712, Validation Loss: 0.339420884847641\n",
      "Epoch 304: Train Loss: 0.07688751677051187, Validation Loss: 0.3883897662162781\n",
      "Epoch 305: Train Loss: 0.08225125260651112, Validation Loss: 0.3739936351776123\n",
      "Epoch 306: Train Loss: 0.07035250728949904, Validation Loss: 0.35528895258903503\n",
      "Epoch 307: Train Loss: 0.09399770386517048, Validation Loss: 0.36718881130218506\n",
      "Epoch 308: Train Loss: 0.09866383112967014, Validation Loss: 0.3606109023094177\n",
      "Epoch 309: Train Loss: 0.0794096882455051, Validation Loss: 0.37769970297813416\n",
      "Epoch 310: Train Loss: 0.06693365424871445, Validation Loss: 0.4948306977748871\n",
      "Epoch 311: Train Loss: 0.10534260049462318, Validation Loss: 0.5302139520645142\n",
      "Epoch 312: Train Loss: 0.1081654280424118, Validation Loss: 0.499698281288147\n",
      "Epoch 313: Train Loss: 0.07237831689417362, Validation Loss: 0.45070281624794006\n",
      "Epoch 314: Train Loss: 0.09752448089420795, Validation Loss: 0.23046936094760895\n",
      "Epoch 315: Train Loss: 0.060632290318608284, Validation Loss: 0.20500293374061584\n",
      "Epoch 316: Train Loss: 0.084802838973701, Validation Loss: 0.20820434391498566\n",
      "Epoch 317: Train Loss: 0.07262005656957626, Validation Loss: 0.2109224796295166\n",
      "Epoch 318: Train Loss: 0.10151006560772657, Validation Loss: 0.2182951271533966\n",
      "Epoch 319: Train Loss: 0.06572482455521822, Validation Loss: 0.22614847123622894\n",
      "Epoch 320: Train Loss: 0.06418891903012991, Validation Loss: 0.5397911667823792\n",
      "Epoch 321: Train Loss: 0.09820569958537817, Validation Loss: 0.7195843458175659\n",
      "Epoch 322: Train Loss: 0.0789993698708713, Validation Loss: 0.7660042643547058\n",
      "Epoch 323: Train Loss: 0.12384699238464236, Validation Loss: 0.5495620965957642\n",
      "Epoch 324: Train Loss: 0.08803912810981274, Validation Loss: 0.312002956867218\n",
      "Epoch 325: Train Loss: 0.09314021561294794, Validation Loss: 0.2509099543094635\n",
      "Epoch 326: Train Loss: 0.10061626695096493, Validation Loss: 0.2697131633758545\n",
      "Epoch 327: Train Loss: 0.09933703392744064, Validation Loss: 0.2903020977973938\n",
      "Epoch 328: Train Loss: 0.06948246154934168, Validation Loss: 0.3099954128265381\n",
      "Epoch 329: Train Loss: 0.07578153256326914, Validation Loss: 0.30848315358161926\n",
      "Epoch 330: Train Loss: 0.09355171117931604, Validation Loss: 0.3579476773738861\n",
      "Epoch 331: Train Loss: 0.09213563054800034, Validation Loss: 0.41100597381591797\n",
      "Epoch 332: Train Loss: 0.08885021042078733, Validation Loss: 0.48044997453689575\n",
      "Epoch 333: Train Loss: 0.057644191198050976, Validation Loss: 0.4221861660480499\n",
      "Epoch 334: Train Loss: 0.0758276516571641, Validation Loss: 0.45126229524612427\n",
      "Epoch 335: Train Loss: 0.06917219050228596, Validation Loss: 0.4906761944293976\n",
      "Epoch 336: Train Loss: 0.06951470882631838, Validation Loss: 0.5476888418197632\n",
      "Epoch 337: Train Loss: 0.07754308078438044, Validation Loss: 0.506931722164154\n",
      "Epoch 338: Train Loss: 0.0952328834682703, Validation Loss: 0.5127079486846924\n",
      "Epoch 339: Train Loss: 0.06999619584530592, Validation Loss: 0.4889806807041168\n",
      "Epoch 340: Train Loss: 0.06664443202316761, Validation Loss: 0.3696066737174988\n",
      "Epoch 341: Train Loss: 0.07081942819058895, Validation Loss: 0.34831398725509644\n",
      "Epoch 342: Train Loss: 0.11208027601242065, Validation Loss: 0.4131515324115753\n",
      "Epoch 343: Train Loss: 0.07882046140730381, Validation Loss: 0.3619302213191986\n",
      "Epoch 344: Train Loss: 0.07396005280315876, Validation Loss: 0.2955154478549957\n",
      "Epoch 345: Train Loss: 0.09854753967374563, Validation Loss: 0.24566644430160522\n",
      "Epoch 346: Train Loss: 0.08187557198107243, Validation Loss: 0.21806345880031586\n",
      "Epoch 347: Train Loss: 0.07760618906468153, Validation Loss: 0.20743216574192047\n",
      "Epoch 348: Train Loss: 0.10289619117975235, Validation Loss: 0.22428077459335327\n",
      "Epoch 349: Train Loss: 0.07210973929613829, Validation Loss: 0.23607443273067474\n",
      "Epoch 350: Train Loss: 0.08820221293717623, Validation Loss: 0.29139021039009094\n",
      "Epoch 351: Train Loss: 0.07225183863192797, Validation Loss: 0.41830673813819885\n",
      "Epoch 352: Train Loss: 0.06613665726035833, Validation Loss: 0.5901997685432434\n",
      "Epoch 353: Train Loss: 0.07915961742401123, Validation Loss: 0.4188014566898346\n",
      "Epoch 354: Train Loss: 0.06257517077028751, Validation Loss: 0.28743353486061096\n",
      "Epoch 355: Train Loss: 0.0950216418132186, Validation Loss: 0.23057779669761658\n",
      "Epoch 356: Train Loss: 0.07943734712898731, Validation Loss: 0.22120851278305054\n",
      "Epoch 357: Train Loss: 0.08000572258606553, Validation Loss: 0.2157616913318634\n",
      "Epoch 358: Train Loss: 0.08106453809887171, Validation Loss: 0.2266564667224884\n",
      "Epoch 359: Train Loss: 0.11471164040267467, Validation Loss: 0.23163484036922455\n",
      "Epoch 360: Train Loss: 0.09715993609279394, Validation Loss: 0.33393558859825134\n",
      "Epoch 361: Train Loss: 0.06577855721116066, Validation Loss: 0.44435355067253113\n",
      "Epoch 362: Train Loss: 0.06990379514172673, Validation Loss: 0.43279018998146057\n",
      "Epoch 363: Train Loss: 0.07411974668502808, Validation Loss: 0.40185546875\n",
      "Epoch 364: Train Loss: 0.0945996604859829, Validation Loss: 0.37472906708717346\n",
      "Epoch 365: Train Loss: 0.07268356112763286, Validation Loss: 0.3721144497394562\n",
      "Epoch 366: Train Loss: 0.08558853669092059, Validation Loss: 0.4082624018192291\n",
      "Epoch 367: Train Loss: 0.09807685576379299, Validation Loss: 0.41667771339416504\n",
      "Epoch 368: Train Loss: 0.07249526958912611, Validation Loss: 0.4130222797393799\n",
      "Epoch 369: Train Loss: 0.06614059302955866, Validation Loss: 0.41225871443748474\n",
      "Epoch 370: Train Loss: 0.08356627635657787, Validation Loss: 0.24554742872714996\n",
      "Epoch 371: Train Loss: 0.07986088749021292, Validation Loss: 0.22525067627429962\n",
      "Epoch 372: Train Loss: 0.07973824604414403, Validation Loss: 0.2422948032617569\n",
      "Epoch 373: Train Loss: 0.07542680483311415, Validation Loss: 0.22446046769618988\n",
      "Epoch 374: Train Loss: 0.09371712058782578, Validation Loss: 0.22221359610557556\n",
      "Epoch 375: Train Loss: 0.08797382842749357, Validation Loss: 0.23228944838047028\n",
      "Epoch 376: Train Loss: 0.06502680759876966, Validation Loss: 0.2461784929037094\n",
      "Epoch 377: Train Loss: 0.09012486413121223, Validation Loss: 0.24227142333984375\n",
      "Epoch 378: Train Loss: 0.08460618834942579, Validation Loss: 0.22979632019996643\n",
      "Epoch 379: Train Loss: 0.08323893137276173, Validation Loss: 0.22605380415916443\n",
      "Epoch 380: Train Loss: 0.07434327283408493, Validation Loss: 0.15765731036663055\n",
      "Epoch 381: Train Loss: 0.10222841147333384, Validation Loss: 0.15130488574504852\n",
      "Epoch 382: Train Loss: 0.10714397393167019, Validation Loss: 0.19372156262397766\n",
      "Epoch 383: Train Loss: 0.11292167287319899, Validation Loss: 0.21580739319324493\n",
      "Epoch 384: Train Loss: 0.07514781691133976, Validation Loss: 0.266091525554657\n",
      "Epoch 385: Train Loss: 0.05779276601970196, Validation Loss: 0.3141014873981476\n",
      "Epoch 386: Train Loss: 0.08159145060926676, Validation Loss: 0.32947322726249695\n",
      "Epoch 387: Train Loss: 0.08004482463002205, Validation Loss: 0.31151512265205383\n",
      "Epoch 388: Train Loss: 0.07642744248732924, Validation Loss: 0.2642629146575928\n",
      "Epoch 389: Train Loss: 0.07942966977134347, Validation Loss: 0.25511741638183594\n",
      "Epoch 390: Train Loss: 0.08519443729892373, Validation Loss: 0.1825750172138214\n",
      "Epoch 391: Train Loss: 0.09109836909919977, Validation Loss: 0.18282866477966309\n",
      "Epoch 392: Train Loss: 0.09152502287179232, Validation Loss: 0.24847310781478882\n",
      "Epoch 393: Train Loss: 0.09353703306987882, Validation Loss: 0.26949137449264526\n",
      "Epoch 394: Train Loss: 0.12066304497420788, Validation Loss: 0.26276251673698425\n",
      "Epoch 395: Train Loss: 0.07187810237519443, Validation Loss: 0.31855079531669617\n",
      "Epoch 396: Train Loss: 0.09011444635689259, Validation Loss: 0.38234812021255493\n",
      "Epoch 397: Train Loss: 0.07825559610500932, Validation Loss: 0.3961021304130554\n",
      "Epoch 398: Train Loss: 0.06050168816000223, Validation Loss: 0.3931095004081726\n",
      "Epoch 399: Train Loss: 0.11593156680464745, Validation Loss: 0.3673436939716339\n",
      "Epoch 400: Train Loss: 0.06595606449991465, Validation Loss: 0.5606588125228882\n",
      "Epoch 401: Train Loss: 0.07161370012909174, Validation Loss: 0.5584393739700317\n",
      "Epoch 402: Train Loss: 0.0819803886115551, Validation Loss: 0.2991693317890167\n",
      "Epoch 403: Train Loss: 0.06753193610347807, Validation Loss: 0.22011326253414154\n",
      "Epoch 404: Train Loss: 0.07216171687468886, Validation Loss: 0.17096950113773346\n",
      "Epoch 405: Train Loss: 0.06595808593556285, Validation Loss: 0.15318600833415985\n",
      "Epoch 406: Train Loss: 0.06556476466357708, Validation Loss: 0.1597795933485031\n",
      "Epoch 407: Train Loss: 0.07922395318746567, Validation Loss: 0.16434916853904724\n",
      "Epoch 408: Train Loss: 0.06901017692871392, Validation Loss: 0.17127208411693573\n",
      "Epoch 409: Train Loss: 0.06254441663622856, Validation Loss: 0.17700797319412231\n",
      "Epoch 410: Train Loss: 0.057165331207215786, Validation Loss: 0.22750963270664215\n",
      "Epoch 411: Train Loss: 0.07136542908847332, Validation Loss: 0.2767508327960968\n",
      "Epoch 412: Train Loss: 0.0648194421082735, Validation Loss: 0.24736180901527405\n",
      "Epoch 413: Train Loss: 0.09031748306006193, Validation Loss: 0.23425768315792084\n",
      "Epoch 414: Train Loss: 0.10483134165406227, Validation Loss: 0.2030421793460846\n",
      "Epoch 415: Train Loss: 0.06506428914144635, Validation Loss: 0.17733190953731537\n",
      "Epoch 416: Train Loss: 0.09564446285367012, Validation Loss: 0.18931764364242554\n",
      "Epoch 417: Train Loss: 0.07673756219446659, Validation Loss: 0.20787248015403748\n",
      "Epoch 418: Train Loss: 0.0690201511606574, Validation Loss: 0.22664351761341095\n",
      "Epoch 419: Train Loss: 0.06495886296033859, Validation Loss: 0.22973385453224182\n",
      "Epoch 420: Train Loss: 0.11397285340353847, Validation Loss: 0.2510020434856415\n",
      "Epoch 421: Train Loss: 0.07760999538004398, Validation Loss: 0.4016072154045105\n",
      "Epoch 422: Train Loss: 0.07488765195012093, Validation Loss: 0.5280671119689941\n",
      "Epoch 423: Train Loss: 0.06801956426352262, Validation Loss: 0.47817981243133545\n",
      "Epoch 424: Train Loss: 0.05927694961428642, Validation Loss: 0.4114382565021515\n",
      "Epoch 425: Train Loss: 0.09236433543264866, Validation Loss: 0.2675098478794098\n",
      "Epoch 426: Train Loss: 0.0700597979594022, Validation Loss: 0.23494532704353333\n",
      "Epoch 427: Train Loss: 0.07549505215138197, Validation Loss: 0.23545421659946442\n",
      "Epoch 428: Train Loss: 0.08425071276724339, Validation Loss: 0.24006184935569763\n",
      "Epoch 429: Train Loss: 0.09923728182911873, Validation Loss: 0.27250850200653076\n",
      "Epoch 430: Train Loss: 0.0789289977401495, Validation Loss: 0.20878252387046814\n",
      "Epoch 431: Train Loss: 0.06927772797644138, Validation Loss: 0.18344472348690033\n",
      "Epoch 432: Train Loss: 0.07941728364676237, Validation Loss: 0.15321843326091766\n",
      "Epoch 433: Train Loss: 0.06487946957349777, Validation Loss: 0.22256194055080414\n",
      "Epoch 434: Train Loss: 0.07329028332605958, Validation Loss: 0.2015669345855713\n",
      "Epoch 435: Train Loss: 0.073343844152987, Validation Loss: 0.1736110895872116\n",
      "Epoch 436: Train Loss: 0.0669036814942956, Validation Loss: 0.18377922475337982\n",
      "Epoch 437: Train Loss: 0.07859174208715558, Validation Loss: 0.1964133083820343\n",
      "Epoch 438: Train Loss: 0.06316235149279237, Validation Loss: 0.19147877395153046\n",
      "Epoch 439: Train Loss: 0.07874484220519662, Validation Loss: 0.19059669971466064\n",
      "Epoch 440: Train Loss: 0.07741605304181576, Validation Loss: 0.22595477104187012\n",
      "Epoch 441: Train Loss: 0.06926769111305475, Validation Loss: 0.2080657035112381\n",
      "Epoch 442: Train Loss: 0.06673498172312975, Validation Loss: 0.2225184291601181\n",
      "Epoch 443: Train Loss: 0.08127125911414623, Validation Loss: 0.22153602540493011\n",
      "Epoch 444: Train Loss: 0.07689942140132189, Validation Loss: 0.2104707509279251\n",
      "Epoch 445: Train Loss: 0.0660358127206564, Validation Loss: 0.1875602751970291\n",
      "Epoch 446: Train Loss: 0.06555982073768973, Validation Loss: 0.1956673562526703\n",
      "Epoch 447: Train Loss: 0.11216325871646404, Validation Loss: 0.20464420318603516\n",
      "Epoch 448: Train Loss: 0.07730196695774794, Validation Loss: 0.20489010214805603\n",
      "Epoch 449: Train Loss: 0.052226195111870766, Validation Loss: 0.20203948020935059\n",
      "Epoch 450: Train Loss: 0.08909034263342619, Validation Loss: 0.23310135304927826\n",
      "Epoch 451: Train Loss: 0.07564861513674259, Validation Loss: 0.22561290860176086\n",
      "Epoch 452: Train Loss: 0.07774326298385859, Validation Loss: 0.15887396037578583\n",
      "Epoch 453: Train Loss: 0.06006161496043205, Validation Loss: 0.14385411143302917\n",
      "Epoch 454: Train Loss: 0.09165444830432534, Validation Loss: 0.14141708612442017\n",
      "Epoch 455: Train Loss: 0.0910042803734541, Validation Loss: 0.13199390470981598\n",
      "Epoch 456: Train Loss: 0.08954596146941185, Validation Loss: 0.16113345324993134\n",
      "Epoch 457: Train Loss: 0.055024921894073486, Validation Loss: 0.17463816702365875\n",
      "Epoch 458: Train Loss: 0.06486815121024847, Validation Loss: 0.1677893102169037\n",
      "Epoch 459: Train Loss: 0.0719307754188776, Validation Loss: 0.1628877967596054\n",
      "Epoch 460: Train Loss: 0.0762713385047391, Validation Loss: 0.1492256075143814\n",
      "Epoch 461: Train Loss: 0.0786434942856431, Validation Loss: 0.2630201280117035\n",
      "Epoch 462: Train Loss: 0.08940043020993471, Validation Loss: 0.2172425240278244\n",
      "Epoch 463: Train Loss: 0.06499178614467382, Validation Loss: 0.19348065555095673\n",
      "Epoch 464: Train Loss: 0.08838438615202904, Validation Loss: 0.1703866422176361\n",
      "Epoch 465: Train Loss: 0.07172033563256264, Validation Loss: 0.1876424103975296\n",
      "Epoch 466: Train Loss: 0.08054997958242893, Validation Loss: 0.20371034741401672\n",
      "Epoch 467: Train Loss: 0.05727932695299387, Validation Loss: 0.21849125623703003\n",
      "Epoch 468: Train Loss: 0.08775746077299118, Validation Loss: 0.23060566186904907\n",
      "Epoch 469: Train Loss: 0.08005938236601651, Validation Loss: 0.21658310294151306\n",
      "Epoch 470: Train Loss: 0.08946772152557969, Validation Loss: 0.1720862090587616\n",
      "Epoch 471: Train Loss: 0.07442426728084683, Validation Loss: 0.13912206888198853\n",
      "Epoch 472: Train Loss: 0.07151506468653679, Validation Loss: 0.18121260404586792\n",
      "Epoch 473: Train Loss: 0.0765566136687994, Validation Loss: 0.23536734282970428\n",
      "Epoch 474: Train Loss: 0.0709292721003294, Validation Loss: 0.21126961708068848\n",
      "Epoch 475: Train Loss: 0.0693965086247772, Validation Loss: 0.18021535873413086\n",
      "Epoch 476: Train Loss: 0.07489215163514018, Validation Loss: 0.178169846534729\n",
      "Epoch 477: Train Loss: 0.08215696271508932, Validation Loss: 0.19409073889255524\n",
      "Epoch 478: Train Loss: 0.07108531007543206, Validation Loss: 0.1854538917541504\n",
      "Epoch 479: Train Loss: 0.07090342603623867, Validation Loss: 0.17470337450504303\n",
      "Epoch 480: Train Loss: 0.06955039035528898, Validation Loss: 0.14943386614322662\n",
      "Epoch 481: Train Loss: 0.09370147436857224, Validation Loss: 0.1611155867576599\n",
      "Epoch 482: Train Loss: 0.04993437975645065, Validation Loss: 0.14468219876289368\n",
      "Epoch 483: Train Loss: 0.07612468209117651, Validation Loss: 0.13309675455093384\n",
      "Epoch 484: Train Loss: 0.06878007156774402, Validation Loss: 0.13995273411273956\n",
      "Epoch 485: Train Loss: 0.07080518919974566, Validation Loss: 0.14115270972251892\n",
      "Epoch 486: Train Loss: 0.1144620948471129, Validation Loss: 0.13370688259601593\n",
      "Epoch 487: Train Loss: 0.07015890907496214, Validation Loss: 0.1431233286857605\n",
      "Epoch 488: Train Loss: 0.10304972995072603, Validation Loss: 0.1484503298997879\n",
      "Epoch 489: Train Loss: 0.06020473362877965, Validation Loss: 0.1393928974866867\n",
      "Epoch 490: Train Loss: 0.07565745199099183, Validation Loss: 0.09651631116867065\n",
      "Epoch 491: Train Loss: 0.07795951701700687, Validation Loss: 0.07944781333208084\n",
      "Epoch 492: Train Loss: 0.07560402248054743, Validation Loss: 0.08673091977834702\n",
      "Epoch 493: Train Loss: 0.07648082356899977, Validation Loss: 0.10189101845026016\n",
      "Epoch 494: Train Loss: 0.08017469849437475, Validation Loss: 0.12072062492370605\n",
      "Epoch 495: Train Loss: 0.07731092162430286, Validation Loss: 0.15022636950016022\n",
      "Epoch 496: Train Loss: 0.06985087366774678, Validation Loss: 0.1720200777053833\n",
      "Epoch 497: Train Loss: 0.08792876731604338, Validation Loss: 0.1568528413772583\n",
      "Epoch 498: Train Loss: 0.05973220686428249, Validation Loss: 0.1686130315065384\n",
      "Epoch 499: Train Loss: 0.07611454464495182, Validation Loss: 0.1757388859987259\n",
      "Completed fold 6\n",
      "Epoch 0: Train Loss: 0.7789043933153152, Validation Loss: 0.6997075080871582\n",
      "Epoch 1: Train Loss: 0.7255588173866272, Validation Loss: 0.6999297142028809\n",
      "Epoch 2: Train Loss: 0.7109363377094269, Validation Loss: 0.7008016705513\n",
      "Epoch 3: Train Loss: 0.7102099508047104, Validation Loss: 0.7025842070579529\n",
      "Epoch 4: Train Loss: 0.696296438574791, Validation Loss: 0.7055529356002808\n",
      "Epoch 5: Train Loss: 0.7066311687231064, Validation Loss: 0.7091682553291321\n",
      "Epoch 6: Train Loss: 0.683263435959816, Validation Loss: 0.7126369476318359\n",
      "Epoch 7: Train Loss: 0.6695044040679932, Validation Loss: 0.7149695158004761\n",
      "Epoch 8: Train Loss: 0.7305085510015488, Validation Loss: 0.7166346907615662\n",
      "Epoch 9: Train Loss: 0.6497923731803894, Validation Loss: 0.7176169157028198\n",
      "Epoch 10: Train Loss: 0.6734986454248428, Validation Loss: 0.7245126366615295\n",
      "Epoch 11: Train Loss: 0.727900430560112, Validation Loss: 0.7304181456565857\n",
      "Epoch 12: Train Loss: 0.6781800389289856, Validation Loss: 0.732252836227417\n",
      "Epoch 13: Train Loss: 0.6779332160949707, Validation Loss: 0.7318978905677795\n",
      "Epoch 14: Train Loss: 0.67176254093647, Validation Loss: 0.7314785122871399\n",
      "Epoch 15: Train Loss: 0.7059098482131958, Validation Loss: 0.7314252853393555\n",
      "Epoch 16: Train Loss: 0.6571226567029953, Validation Loss: 0.7308903932571411\n",
      "Epoch 17: Train Loss: 0.6800667643547058, Validation Loss: 0.7302910685539246\n",
      "Epoch 18: Train Loss: 0.6333465129137039, Validation Loss: 0.7301958203315735\n",
      "Epoch 19: Train Loss: 0.6271606087684631, Validation Loss: 0.7302889823913574\n",
      "Epoch 20: Train Loss: 0.6733024716377258, Validation Loss: 0.7332019209861755\n",
      "Epoch 21: Train Loss: 0.6650714427232742, Validation Loss: 0.7344866991043091\n",
      "Epoch 22: Train Loss: 0.6655947417020798, Validation Loss: 0.7350233197212219\n",
      "Epoch 23: Train Loss: 0.6602070927619934, Validation Loss: 0.7342190742492676\n",
      "Epoch 24: Train Loss: 0.6716257184743881, Validation Loss: 0.7318326234817505\n",
      "Epoch 25: Train Loss: 0.6596536189317703, Validation Loss: 0.7303838729858398\n",
      "Epoch 26: Train Loss: 0.6613539457321167, Validation Loss: 0.7298563122749329\n",
      "Epoch 27: Train Loss: 0.6457456648349762, Validation Loss: 0.7294527888298035\n",
      "Epoch 28: Train Loss: 0.6684910655021667, Validation Loss: 0.7294692397117615\n",
      "Epoch 29: Train Loss: 0.6593317091464996, Validation Loss: 0.7297484874725342\n",
      "Epoch 30: Train Loss: 0.6759600043296814, Validation Loss: 0.7269553542137146\n",
      "Epoch 31: Train Loss: 0.6092340797185898, Validation Loss: 0.7284068465232849\n",
      "Epoch 32: Train Loss: 0.6312278062105179, Validation Loss: 0.7280671000480652\n",
      "Epoch 33: Train Loss: 0.6730692386627197, Validation Loss: 0.7264209985733032\n",
      "Epoch 34: Train Loss: 0.6773595362901688, Validation Loss: 0.7248314023017883\n",
      "Epoch 35: Train Loss: 0.6228041350841522, Validation Loss: 0.7241608500480652\n",
      "Epoch 36: Train Loss: 0.6217896640300751, Validation Loss: 0.7240799069404602\n",
      "Epoch 37: Train Loss: 0.634877935051918, Validation Loss: 0.7246772646903992\n",
      "Epoch 38: Train Loss: 0.6525194048881531, Validation Loss: 0.7246128916740417\n",
      "Epoch 39: Train Loss: 0.6336176842451096, Validation Loss: 0.7245550155639648\n",
      "Epoch 40: Train Loss: 0.6057967394590378, Validation Loss: 0.7236732244491577\n",
      "Epoch 41: Train Loss: 0.6122189611196518, Validation Loss: 0.7239466905593872\n",
      "Epoch 42: Train Loss: 0.6632868647575378, Validation Loss: 0.7235834002494812\n",
      "Epoch 43: Train Loss: 0.6276336312294006, Validation Loss: 0.7222693562507629\n",
      "Epoch 44: Train Loss: 0.6146421730518341, Validation Loss: 0.7225754857063293\n",
      "Epoch 45: Train Loss: 0.6524383723735809, Validation Loss: 0.7231215834617615\n",
      "Epoch 46: Train Loss: 0.6273323148488998, Validation Loss: 0.7234883904457092\n",
      "Epoch 47: Train Loss: 0.6048226207494736, Validation Loss: 0.7236754298210144\n",
      "Epoch 48: Train Loss: 0.5984993427991867, Validation Loss: 0.7238892912864685\n",
      "Epoch 49: Train Loss: 0.6382217109203339, Validation Loss: 0.7240703105926514\n",
      "Epoch 50: Train Loss: 0.5720864236354828, Validation Loss: 0.7221598029136658\n",
      "Epoch 51: Train Loss: 0.5832934379577637, Validation Loss: 0.7178460955619812\n",
      "Epoch 52: Train Loss: 0.5503990650177002, Validation Loss: 0.7187708616256714\n",
      "Epoch 53: Train Loss: 0.5389528498053551, Validation Loss: 0.7168634533882141\n",
      "Epoch 54: Train Loss: 0.5620288401842117, Validation Loss: 0.7165533900260925\n",
      "Epoch 55: Train Loss: 0.6144983023405075, Validation Loss: 0.7167484164237976\n",
      "Epoch 56: Train Loss: 0.5512615591287613, Validation Loss: 0.7148259878158569\n",
      "Epoch 57: Train Loss: 0.5910773575305939, Validation Loss: 0.7138651609420776\n",
      "Epoch 58: Train Loss: 0.5886664986610413, Validation Loss: 0.7136572599411011\n",
      "Epoch 59: Train Loss: 0.6013181060552597, Validation Loss: 0.7137207984924316\n",
      "Epoch 60: Train Loss: 0.5810014754533768, Validation Loss: 0.7112777829170227\n",
      "Epoch 61: Train Loss: 0.6079986393451691, Validation Loss: 0.7103943824768066\n",
      "Epoch 62: Train Loss: 0.5549964755773544, Validation Loss: 0.7037881016731262\n",
      "Epoch 63: Train Loss: 0.5420206487178802, Validation Loss: 0.6973329782485962\n",
      "Epoch 64: Train Loss: 0.5861808955669403, Validation Loss: 0.6921903491020203\n",
      "Epoch 65: Train Loss: 0.5368486791849136, Validation Loss: 0.6913498044013977\n",
      "Epoch 66: Train Loss: 0.5889730453491211, Validation Loss: 0.6925764679908752\n",
      "Epoch 67: Train Loss: 0.5178540274500847, Validation Loss: 0.6947028040885925\n",
      "Epoch 68: Train Loss: 0.522606611251831, Validation Loss: 0.6959308385848999\n",
      "Epoch 69: Train Loss: 0.5650390535593033, Validation Loss: 0.696601390838623\n",
      "Epoch 70: Train Loss: 0.524289183318615, Validation Loss: 0.7032637596130371\n",
      "Epoch 71: Train Loss: 0.5108121857047081, Validation Loss: 0.693114161491394\n",
      "Epoch 72: Train Loss: 0.5092504024505615, Validation Loss: 0.6757053136825562\n",
      "Epoch 73: Train Loss: 0.5103306323289871, Validation Loss: 0.676927924156189\n",
      "Epoch 74: Train Loss: 0.5318287536501884, Validation Loss: 0.6802825927734375\n",
      "Epoch 75: Train Loss: 0.47655361890792847, Validation Loss: 0.6788324117660522\n",
      "Epoch 76: Train Loss: 0.49765197932720184, Validation Loss: 0.6748784184455872\n",
      "Epoch 77: Train Loss: 0.5127853974699974, Validation Loss: 0.6749165654182434\n",
      "Epoch 78: Train Loss: 0.48573700338602066, Validation Loss: 0.6751381754875183\n",
      "Epoch 79: Train Loss: 0.48009300231933594, Validation Loss: 0.6751276850700378\n",
      "Epoch 80: Train Loss: 0.44081004709005356, Validation Loss: 0.6879148483276367\n",
      "Epoch 81: Train Loss: 0.4683259427547455, Validation Loss: 0.6786302924156189\n",
      "Epoch 82: Train Loss: 0.5000557005405426, Validation Loss: 0.6757763028144836\n",
      "Epoch 83: Train Loss: 0.4606366753578186, Validation Loss: 0.6885740160942078\n",
      "Epoch 84: Train Loss: 0.45505939424037933, Validation Loss: 0.7117106318473816\n",
      "Epoch 85: Train Loss: 0.40916816145181656, Validation Loss: 0.7131089568138123\n",
      "Epoch 86: Train Loss: 0.462869256734848, Validation Loss: 0.7052552700042725\n",
      "Epoch 87: Train Loss: 0.45935986936092377, Validation Loss: 0.6980084776878357\n",
      "Epoch 88: Train Loss: 0.432917557656765, Validation Loss: 0.6949652433395386\n",
      "Epoch 89: Train Loss: 0.4260725602507591, Validation Loss: 0.6961212158203125\n",
      "Epoch 90: Train Loss: 0.41245635598897934, Validation Loss: 0.7028923034667969\n",
      "Epoch 91: Train Loss: 0.45181288570165634, Validation Loss: 0.6917068362236023\n",
      "Epoch 92: Train Loss: 0.3920615538954735, Validation Loss: 0.7014114260673523\n",
      "Epoch 93: Train Loss: 0.3462938144803047, Validation Loss: 0.7044934034347534\n",
      "Epoch 94: Train Loss: 0.4093378931283951, Validation Loss: 0.6981507539749146\n",
      "Epoch 95: Train Loss: 0.3499745726585388, Validation Loss: 0.7256993651390076\n",
      "Epoch 96: Train Loss: 0.3274924159049988, Validation Loss: 0.7325208783149719\n",
      "Epoch 97: Train Loss: 0.36760060489177704, Validation Loss: 0.7299279570579529\n",
      "Epoch 98: Train Loss: 0.3661499470472336, Validation Loss: 0.7306059002876282\n",
      "Epoch 99: Train Loss: 0.3354618698358536, Validation Loss: 0.7286521792411804\n",
      "Epoch 100: Train Loss: 0.35424377769231796, Validation Loss: 0.7489268183708191\n",
      "Epoch 101: Train Loss: 0.3033124804496765, Validation Loss: 0.7558541297912598\n",
      "Epoch 102: Train Loss: 0.3411524295806885, Validation Loss: 0.7423964738845825\n",
      "Epoch 103: Train Loss: 0.2989789992570877, Validation Loss: 0.765099287033081\n",
      "Epoch 104: Train Loss: 0.2997175119817257, Validation Loss: 0.7801143527030945\n",
      "Epoch 105: Train Loss: 0.3036477193236351, Validation Loss: 0.7816182374954224\n",
      "Epoch 106: Train Loss: 0.2588814087212086, Validation Loss: 0.7871317863464355\n",
      "Epoch 107: Train Loss: 0.3396475613117218, Validation Loss: 0.7905951738357544\n",
      "Epoch 108: Train Loss: 0.29772674292325974, Validation Loss: 0.7917291522026062\n",
      "Epoch 109: Train Loss: 0.32615379989147186, Validation Loss: 0.7989635467529297\n",
      "Epoch 110: Train Loss: 0.2965673506259918, Validation Loss: 0.8369807600975037\n",
      "Epoch 111: Train Loss: 0.27527767419815063, Validation Loss: 0.8240591287612915\n",
      "Epoch 112: Train Loss: 0.23027736321091652, Validation Loss: 0.8664026260375977\n",
      "Epoch 113: Train Loss: 0.25681063905358315, Validation Loss: 0.9082632064819336\n",
      "Epoch 114: Train Loss: 0.24912524223327637, Validation Loss: 0.9189518094062805\n",
      "Epoch 115: Train Loss: 0.23224930837750435, Validation Loss: 0.9560500383377075\n",
      "Epoch 116: Train Loss: 0.2500978820025921, Validation Loss: 0.9553712606430054\n",
      "Epoch 117: Train Loss: 0.22592340409755707, Validation Loss: 0.9541782140731812\n",
      "Epoch 118: Train Loss: 0.20850299298763275, Validation Loss: 0.9636982083320618\n",
      "Epoch 119: Train Loss: 0.2310672551393509, Validation Loss: 0.9696757793426514\n",
      "Epoch 120: Train Loss: 0.22808147594332695, Validation Loss: 1.0317646265029907\n",
      "Epoch 121: Train Loss: 0.19233591854572296, Validation Loss: 1.0340046882629395\n",
      "Epoch 122: Train Loss: 0.19314881041646004, Validation Loss: 1.056130290031433\n",
      "Epoch 123: Train Loss: 0.1966697759926319, Validation Loss: 1.1326242685317993\n",
      "Epoch 124: Train Loss: 0.17750158160924911, Validation Loss: 1.1518712043762207\n",
      "Epoch 125: Train Loss: 0.2074858397245407, Validation Loss: 1.139979362487793\n",
      "Epoch 126: Train Loss: 0.1805294193327427, Validation Loss: 1.136249303817749\n",
      "Epoch 127: Train Loss: 0.18019451573491096, Validation Loss: 1.14871346950531\n",
      "Epoch 128: Train Loss: 0.17188075929880142, Validation Loss: 1.1565871238708496\n",
      "Epoch 129: Train Loss: 0.1718764714896679, Validation Loss: 1.1501212120056152\n",
      "Epoch 130: Train Loss: 0.1850651018321514, Validation Loss: 1.2447398900985718\n",
      "Epoch 131: Train Loss: 0.1505904197692871, Validation Loss: 1.2604990005493164\n",
      "Epoch 132: Train Loss: 0.16073458641767502, Validation Loss: 1.275814175605774\n",
      "Epoch 133: Train Loss: 0.1327046938240528, Validation Loss: 1.351263403892517\n",
      "Epoch 134: Train Loss: 0.15709483996033669, Validation Loss: 1.3945797681808472\n",
      "Epoch 135: Train Loss: 0.16503869369626045, Validation Loss: 1.3919668197631836\n",
      "Epoch 136: Train Loss: 0.13393894955515862, Validation Loss: 1.367849588394165\n",
      "Epoch 137: Train Loss: 0.14267854765057564, Validation Loss: 1.3619461059570312\n",
      "Epoch 138: Train Loss: 0.12627478502690792, Validation Loss: 1.3751760721206665\n",
      "Epoch 139: Train Loss: 0.12950957380235195, Validation Loss: 1.3849148750305176\n",
      "Epoch 140: Train Loss: 0.12359911575913429, Validation Loss: 1.421164631843567\n",
      "Epoch 141: Train Loss: 0.10910199210047722, Validation Loss: 1.4136101007461548\n",
      "Epoch 142: Train Loss: 0.1380601841956377, Validation Loss: 1.4960355758666992\n",
      "Epoch 143: Train Loss: 0.11257260292768478, Validation Loss: 1.5892504453659058\n",
      "Epoch 144: Train Loss: 0.1483486369252205, Validation Loss: 1.5983781814575195\n",
      "Epoch 145: Train Loss: 0.10200310684740543, Validation Loss: 1.5710246562957764\n",
      "Epoch 146: Train Loss: 0.10455132834613323, Validation Loss: 1.569676160812378\n",
      "Epoch 147: Train Loss: 0.098174799233675, Validation Loss: 1.5931520462036133\n",
      "Epoch 148: Train Loss: 0.1182366106659174, Validation Loss: 1.5783770084381104\n",
      "Epoch 149: Train Loss: 0.0987372174859047, Validation Loss: 1.574580192565918\n",
      "Epoch 150: Train Loss: 0.11945931613445282, Validation Loss: 1.6249271631240845\n",
      "Epoch 151: Train Loss: 0.11237506195902824, Validation Loss: 1.5697654485702515\n",
      "Epoch 152: Train Loss: 0.09176263958215714, Validation Loss: 1.5374236106872559\n",
      "Epoch 153: Train Loss: 0.09999958891421556, Validation Loss: 1.6104636192321777\n",
      "Epoch 154: Train Loss: 0.08062962163239717, Validation Loss: 1.645275354385376\n",
      "Epoch 155: Train Loss: 0.07473717909306288, Validation Loss: 1.6368945837020874\n",
      "Epoch 156: Train Loss: 0.11814397946000099, Validation Loss: 1.6430877447128296\n",
      "Epoch 157: Train Loss: 0.08267692103981972, Validation Loss: 1.7130359411239624\n",
      "Epoch 158: Train Loss: 0.07007163763046265, Validation Loss: 1.7468459606170654\n",
      "Epoch 159: Train Loss: 0.07325911242514849, Validation Loss: 1.7710120677947998\n",
      "Epoch 160: Train Loss: 0.07408408261835575, Validation Loss: 1.764271855354309\n",
      "Epoch 161: Train Loss: 0.08273099921643734, Validation Loss: 1.830329418182373\n",
      "Epoch 162: Train Loss: 0.08961446210741997, Validation Loss: 1.8377467393875122\n",
      "Epoch 163: Train Loss: 0.0818487387150526, Validation Loss: 1.8243597745895386\n",
      "Epoch 164: Train Loss: 0.06824960932135582, Validation Loss: 1.8579559326171875\n",
      "Epoch 165: Train Loss: 0.06441214866936207, Validation Loss: 1.8709536790847778\n",
      "Epoch 166: Train Loss: 0.05811150837689638, Validation Loss: 1.8807291984558105\n",
      "Epoch 167: Train Loss: 0.1074893856421113, Validation Loss: 1.9072871208190918\n",
      "Epoch 168: Train Loss: 0.06404476147145033, Validation Loss: 1.9078106880187988\n",
      "Epoch 169: Train Loss: 0.07117250841110945, Validation Loss: 1.9346903562545776\n",
      "Epoch 170: Train Loss: 0.06365806050598621, Validation Loss: 1.9381320476531982\n",
      "Epoch 171: Train Loss: 0.06648464687168598, Validation Loss: 1.8258854150772095\n",
      "Epoch 172: Train Loss: 0.07560496404767036, Validation Loss: 1.8570387363433838\n",
      "Epoch 173: Train Loss: 0.07129610795527697, Validation Loss: 1.9988837242126465\n",
      "Epoch 174: Train Loss: 0.05965775903314352, Validation Loss: 2.0674219131469727\n",
      "Epoch 175: Train Loss: 0.048001193441450596, Validation Loss: 2.035054922103882\n",
      "Early stopping at epoch 176\n",
      "Completed fold 7\n",
      "Epoch 0: Train Loss: 0.7496482580900192, Validation Loss: 0.6663456559181213\n",
      "Epoch 1: Train Loss: 0.8011213392019272, Validation Loss: 0.6560606360435486\n",
      "Epoch 2: Train Loss: 0.6611373275518417, Validation Loss: 0.6494165062904358\n",
      "Epoch 3: Train Loss: 0.725297138094902, Validation Loss: 0.6465904712677002\n",
      "Epoch 4: Train Loss: 0.7635467648506165, Validation Loss: 0.6512787938117981\n",
      "Epoch 5: Train Loss: 0.7560980319976807, Validation Loss: 0.6518572568893433\n",
      "Epoch 6: Train Loss: 0.7518827468156815, Validation Loss: 0.6510568261146545\n",
      "Epoch 7: Train Loss: 0.7147953361272812, Validation Loss: 0.652462899684906\n",
      "Epoch 8: Train Loss: 0.7176051735877991, Validation Loss: 0.6531340479850769\n",
      "Epoch 9: Train Loss: 0.7188097834587097, Validation Loss: 0.6537618637084961\n",
      "Epoch 10: Train Loss: 0.6868017315864563, Validation Loss: 0.6635788679122925\n",
      "Epoch 11: Train Loss: 0.6662947088479996, Validation Loss: 0.6732261776924133\n",
      "Epoch 12: Train Loss: 0.7052733302116394, Validation Loss: 0.6833299398422241\n",
      "Epoch 13: Train Loss: 0.7281763106584549, Validation Loss: 0.6810163259506226\n",
      "Epoch 14: Train Loss: 0.6604184061288834, Validation Loss: 0.6763488054275513\n",
      "Epoch 15: Train Loss: 0.6685283780097961, Validation Loss: 0.6756924986839294\n",
      "Epoch 16: Train Loss: 0.6859583854675293, Validation Loss: 0.673677921295166\n",
      "Epoch 17: Train Loss: 0.6903641074895859, Validation Loss: 0.6718649864196777\n",
      "Epoch 18: Train Loss: 0.7017713040113449, Validation Loss: 0.6701143980026245\n",
      "Epoch 19: Train Loss: 0.6944549679756165, Validation Loss: 0.6689616441726685\n",
      "Epoch 20: Train Loss: 0.7359382957220078, Validation Loss: 0.6602494120597839\n",
      "Epoch 21: Train Loss: 0.6534611582756042, Validation Loss: 0.6647211909294128\n",
      "Epoch 22: Train Loss: 0.6926125138998032, Validation Loss: 0.6654438376426697\n",
      "Epoch 23: Train Loss: 0.6467804163694382, Validation Loss: 0.6679240465164185\n",
      "Epoch 24: Train Loss: 0.6574961394071579, Validation Loss: 0.6672099232673645\n",
      "Epoch 25: Train Loss: 0.6716756820678711, Validation Loss: 0.6664296388626099\n",
      "Epoch 26: Train Loss: 0.6919257938861847, Validation Loss: 0.6697419881820679\n",
      "Epoch 27: Train Loss: 0.6627003699541092, Validation Loss: 0.6680396795272827\n",
      "Epoch 28: Train Loss: 0.6959573328495026, Validation Loss: 0.66631019115448\n",
      "Epoch 29: Train Loss: 0.6326464265584946, Validation Loss: 0.6658684611320496\n",
      "Epoch 30: Train Loss: 0.6714444309473038, Validation Loss: 0.6669727563858032\n",
      "Epoch 31: Train Loss: 0.6659672260284424, Validation Loss: 0.6661364436149597\n",
      "Epoch 32: Train Loss: 0.6732621937990189, Validation Loss: 0.6668957471847534\n",
      "Epoch 33: Train Loss: 0.6918725818395615, Validation Loss: 0.6666379570960999\n",
      "Epoch 34: Train Loss: 0.6408586949110031, Validation Loss: 0.6629176139831543\n",
      "Epoch 35: Train Loss: 0.6647357642650604, Validation Loss: 0.6637913584709167\n",
      "Epoch 36: Train Loss: 0.6491190791130066, Validation Loss: 0.6662372350692749\n",
      "Epoch 37: Train Loss: 0.6125321388244629, Validation Loss: 0.668189525604248\n",
      "Epoch 38: Train Loss: 0.592563733458519, Validation Loss: 0.6689400672912598\n",
      "Epoch 39: Train Loss: 0.636401429772377, Validation Loss: 0.6690330505371094\n",
      "Epoch 40: Train Loss: 0.6270983219146729, Validation Loss: 0.6610898971557617\n",
      "Epoch 41: Train Loss: 0.6645808815956116, Validation Loss: 0.6475664973258972\n",
      "Epoch 42: Train Loss: 0.6136590391397476, Validation Loss: 0.6407544016838074\n",
      "Epoch 43: Train Loss: 0.642568975687027, Validation Loss: 0.6385387182235718\n",
      "Epoch 44: Train Loss: 0.666453406214714, Validation Loss: 0.638522207736969\n",
      "Epoch 45: Train Loss: 0.6077480316162109, Validation Loss: 0.6357935070991516\n",
      "Epoch 46: Train Loss: 0.5951320976018906, Validation Loss: 0.6365939974784851\n",
      "Epoch 47: Train Loss: 0.5666478872299194, Validation Loss: 0.6366695165634155\n",
      "Epoch 48: Train Loss: 0.6072249859571457, Validation Loss: 0.6371052861213684\n",
      "Epoch 49: Train Loss: 0.6393680721521378, Validation Loss: 0.6379758715629578\n",
      "Epoch 50: Train Loss: 0.6255298107862473, Validation Loss: 0.6412508487701416\n",
      "Epoch 51: Train Loss: 0.6133835464715958, Validation Loss: 0.6521889567375183\n",
      "Epoch 52: Train Loss: 0.6154359430074692, Validation Loss: 0.6646296381950378\n",
      "Epoch 53: Train Loss: 0.5634055733680725, Validation Loss: 0.6734060049057007\n",
      "Epoch 54: Train Loss: 0.5908572822809219, Validation Loss: 0.6781651973724365\n",
      "Epoch 55: Train Loss: 0.6369824111461639, Validation Loss: 0.6772518754005432\n",
      "Epoch 56: Train Loss: 0.6344498544931412, Validation Loss: 0.6736400127410889\n",
      "Epoch 57: Train Loss: 0.6352014243602753, Validation Loss: 0.6727829575538635\n",
      "Epoch 58: Train Loss: 0.6318946331739426, Validation Loss: 0.672680139541626\n",
      "Epoch 59: Train Loss: 0.6215045750141144, Validation Loss: 0.6724641919136047\n",
      "Epoch 60: Train Loss: 0.6392501890659332, Validation Loss: 0.6860021352767944\n",
      "Epoch 61: Train Loss: 0.6033926159143448, Validation Loss: 0.7011841535568237\n",
      "Epoch 62: Train Loss: 0.6156017035245895, Validation Loss: 0.7052643895149231\n",
      "Epoch 63: Train Loss: 0.5599852353334427, Validation Loss: 0.7127665877342224\n",
      "Epoch 64: Train Loss: 0.5785038769245148, Validation Loss: 0.7174710631370544\n",
      "Epoch 65: Train Loss: 0.6117270290851593, Validation Loss: 0.7161151170730591\n",
      "Epoch 66: Train Loss: 0.5931380838155746, Validation Loss: 0.7139106392860413\n",
      "Epoch 67: Train Loss: 0.6247633993625641, Validation Loss: 0.7108743190765381\n",
      "Epoch 68: Train Loss: 0.5780595988035202, Validation Loss: 0.7098543047904968\n",
      "Epoch 69: Train Loss: 0.5772997736930847, Validation Loss: 0.7089918851852417\n",
      "Epoch 70: Train Loss: 0.5633677244186401, Validation Loss: 0.6931507587432861\n",
      "Epoch 71: Train Loss: 0.6356909722089767, Validation Loss: 0.6829337477684021\n",
      "Epoch 72: Train Loss: 0.5642343163490295, Validation Loss: 0.6750432848930359\n",
      "Epoch 73: Train Loss: 0.5437173694372177, Validation Loss: 0.6778494119644165\n",
      "Epoch 74: Train Loss: 0.5654326230287552, Validation Loss: 0.6812335848808289\n",
      "Epoch 75: Train Loss: 0.58948914706707, Validation Loss: 0.6833003163337708\n",
      "Epoch 76: Train Loss: 0.5951771587133408, Validation Loss: 0.6811221241950989\n",
      "Epoch 77: Train Loss: 0.601830005645752, Validation Loss: 0.6805790662765503\n",
      "Epoch 78: Train Loss: 0.5807553380727768, Validation Loss: 0.6805071234703064\n",
      "Epoch 79: Train Loss: 0.5710679963231087, Validation Loss: 0.6809006333351135\n",
      "Epoch 80: Train Loss: 0.5638457238674164, Validation Loss: 0.6914873123168945\n",
      "Epoch 81: Train Loss: 0.5616677030920982, Validation Loss: 0.6932789087295532\n",
      "Epoch 82: Train Loss: 0.5618695020675659, Validation Loss: 0.7061794400215149\n",
      "Epoch 83: Train Loss: 0.5597501248121262, Validation Loss: 0.7072914838790894\n",
      "Epoch 84: Train Loss: 0.5450191497802734, Validation Loss: 0.6990199685096741\n",
      "Epoch 85: Train Loss: 0.5568074434995651, Validation Loss: 0.6939420104026794\n",
      "Epoch 86: Train Loss: 0.6120886951684952, Validation Loss: 0.6873022317886353\n",
      "Epoch 87: Train Loss: 0.5685847401618958, Validation Loss: 0.6849365234375\n",
      "Epoch 88: Train Loss: 0.5393739491701126, Validation Loss: 0.6863322257995605\n",
      "Epoch 89: Train Loss: 0.5513645261526108, Validation Loss: 0.6878390312194824\n",
      "Epoch 90: Train Loss: 0.5447111427783966, Validation Loss: 0.6904497146606445\n",
      "Epoch 91: Train Loss: 0.5330322310328484, Validation Loss: 0.6855956315994263\n",
      "Epoch 92: Train Loss: 0.5911701321601868, Validation Loss: 0.6905543804168701\n",
      "Epoch 93: Train Loss: 0.5558000653982162, Validation Loss: 0.6920512914657593\n",
      "Epoch 94: Train Loss: 0.5542477965354919, Validation Loss: 0.6910357475280762\n",
      "Epoch 95: Train Loss: 0.5542108938097954, Validation Loss: 0.6972876787185669\n",
      "Epoch 96: Train Loss: 0.661041259765625, Validation Loss: 0.7048459053039551\n",
      "Epoch 97: Train Loss: 0.5782641023397446, Validation Loss: 0.7074576020240784\n",
      "Epoch 98: Train Loss: 0.5149444192647934, Validation Loss: 0.7073478698730469\n",
      "Epoch 99: Train Loss: 0.5496615469455719, Validation Loss: 0.7067990303039551\n",
      "Epoch 100: Train Loss: 0.5130939036607742, Validation Loss: 0.7181521654129028\n",
      "Epoch 101: Train Loss: 0.5493696331977844, Validation Loss: 0.7296463847160339\n",
      "Epoch 102: Train Loss: 0.5075653791427612, Validation Loss: 0.7237954139709473\n",
      "Epoch 103: Train Loss: 0.5349702835083008, Validation Loss: 0.6974738240242004\n",
      "Epoch 104: Train Loss: 0.5226466283202171, Validation Loss: 0.6900410652160645\n",
      "Epoch 105: Train Loss: 0.5170858278870583, Validation Loss: 0.693875253200531\n",
      "Epoch 106: Train Loss: 0.5463074073195457, Validation Loss: 0.7058579325675964\n",
      "Epoch 107: Train Loss: 0.5287872329354286, Validation Loss: 0.712350070476532\n",
      "Epoch 108: Train Loss: 0.549478068947792, Validation Loss: 0.7152811884880066\n",
      "Epoch 109: Train Loss: 0.5228003412485123, Validation Loss: 0.7164773344993591\n",
      "Epoch 110: Train Loss: 0.5253143534064293, Validation Loss: 0.7528670430183411\n",
      "Epoch 111: Train Loss: 0.5215850174427032, Validation Loss: 0.7490201592445374\n",
      "Epoch 112: Train Loss: 0.5290650948882103, Validation Loss: 0.7418919801712036\n",
      "Epoch 113: Train Loss: 0.5065263509750366, Validation Loss: 0.7301352620124817\n",
      "Epoch 114: Train Loss: 0.48536553978919983, Validation Loss: 0.7165316343307495\n",
      "Epoch 115: Train Loss: 0.4863356500864029, Validation Loss: 0.7119006514549255\n",
      "Epoch 116: Train Loss: 0.5054584816098213, Validation Loss: 0.7132067680358887\n",
      "Epoch 117: Train Loss: 0.49775321781635284, Validation Loss: 0.7159477472305298\n",
      "Epoch 118: Train Loss: 0.4932756870985031, Validation Loss: 0.7186020016670227\n",
      "Epoch 119: Train Loss: 0.504921168088913, Validation Loss: 0.7191149592399597\n",
      "Epoch 120: Train Loss: 0.5605313926935196, Validation Loss: 0.7515340447425842\n",
      "Epoch 121: Train Loss: 0.508052334189415, Validation Loss: 0.7355079054832458\n",
      "Epoch 122: Train Loss: 0.4751875177025795, Validation Loss: 0.733921229839325\n",
      "Epoch 123: Train Loss: 0.4818277657032013, Validation Loss: 0.7182238101959229\n",
      "Epoch 124: Train Loss: 0.4999835565686226, Validation Loss: 0.7127505540847778\n",
      "Epoch 125: Train Loss: 0.5079455226659775, Validation Loss: 0.7262061834335327\n",
      "Epoch 126: Train Loss: 0.48297085613012314, Validation Loss: 0.7405183911323547\n",
      "Epoch 127: Train Loss: 0.5215079262852669, Validation Loss: 0.7508015632629395\n",
      "Epoch 128: Train Loss: 0.48799270391464233, Validation Loss: 0.7514154314994812\n",
      "Epoch 129: Train Loss: 0.49965517222881317, Validation Loss: 0.7518271803855896\n",
      "Epoch 130: Train Loss: 0.45554301142692566, Validation Loss: 0.7189397811889648\n",
      "Epoch 131: Train Loss: 0.49219927936792374, Validation Loss: 0.6874451637268066\n",
      "Epoch 132: Train Loss: 0.49995923787355423, Validation Loss: 0.6810786128044128\n",
      "Epoch 133: Train Loss: 0.4845517724752426, Validation Loss: 0.6801647543907166\n",
      "Epoch 134: Train Loss: 0.4862935021519661, Validation Loss: 0.691840410232544\n",
      "Epoch 135: Train Loss: 0.45532987266778946, Validation Loss: 0.709054172039032\n",
      "Epoch 136: Train Loss: 0.4591498598456383, Validation Loss: 0.7226119637489319\n",
      "Epoch 137: Train Loss: 0.4772724434733391, Validation Loss: 0.7262670397758484\n",
      "Epoch 138: Train Loss: 0.49709419161081314, Validation Loss: 0.7224103808403015\n",
      "Epoch 139: Train Loss: 0.4515831023454666, Validation Loss: 0.7214112877845764\n",
      "Epoch 140: Train Loss: 0.47055430710315704, Validation Loss: 0.6797202229499817\n",
      "Epoch 141: Train Loss: 0.4526379257440567, Validation Loss: 0.6958182454109192\n",
      "Epoch 142: Train Loss: 0.4555211290717125, Validation Loss: 0.7348520159721375\n",
      "Epoch 143: Train Loss: 0.4501059427857399, Validation Loss: 0.7473466396331787\n",
      "Epoch 144: Train Loss: 0.435575932264328, Validation Loss: 0.7441713213920593\n",
      "Early stopping at epoch 145\n",
      "Completed fold 8\n",
      "Epoch 0: Train Loss: 0.745497465133667, Validation Loss: 0.6890214085578918\n",
      "Epoch 1: Train Loss: 0.8076637536287308, Validation Loss: 0.6874736547470093\n",
      "Epoch 2: Train Loss: 0.7288081645965576, Validation Loss: 0.682884156703949\n",
      "Epoch 3: Train Loss: 0.6894317716360092, Validation Loss: 0.6799135208129883\n",
      "Epoch 4: Train Loss: 0.7459210604429245, Validation Loss: 0.6798126101493835\n",
      "Epoch 5: Train Loss: 0.7306883782148361, Validation Loss: 0.678544282913208\n",
      "Epoch 6: Train Loss: 0.7222192287445068, Validation Loss: 0.6777505874633789\n",
      "Epoch 7: Train Loss: 0.6910108178853989, Validation Loss: 0.6770320534706116\n",
      "Epoch 8: Train Loss: 0.6883444488048553, Validation Loss: 0.67658531665802\n",
      "Epoch 9: Train Loss: 0.7287033945322037, Validation Loss: 0.6763541102409363\n",
      "Epoch 10: Train Loss: 0.7573488503694534, Validation Loss: 0.6800415515899658\n",
      "Epoch 11: Train Loss: 0.676278606057167, Validation Loss: 0.6855942606925964\n",
      "Epoch 12: Train Loss: 0.7340046167373657, Validation Loss: 0.6915570497512817\n",
      "Epoch 13: Train Loss: 0.7387568056583405, Validation Loss: 0.6956242322921753\n",
      "Epoch 14: Train Loss: 0.7045383602380753, Validation Loss: 0.7005788087844849\n",
      "Epoch 15: Train Loss: 0.6637421250343323, Validation Loss: 0.7044919729232788\n",
      "Epoch 16: Train Loss: 0.6911142766475677, Validation Loss: 0.7066953182220459\n",
      "Epoch 17: Train Loss: 0.6883675009012222, Validation Loss: 0.7076341509819031\n",
      "Epoch 18: Train Loss: 0.6668077409267426, Validation Loss: 0.7079362273216248\n",
      "Epoch 19: Train Loss: 0.6601814925670624, Validation Loss: 0.7080130577087402\n",
      "Epoch 20: Train Loss: 0.7133922427892685, Validation Loss: 0.7114502787590027\n",
      "Epoch 21: Train Loss: 0.6682628393173218, Validation Loss: 0.7176612615585327\n",
      "Epoch 22: Train Loss: 0.6842615157365799, Validation Loss: 0.7231019735336304\n",
      "Epoch 23: Train Loss: 0.706228107213974, Validation Loss: 0.7259994149208069\n",
      "Epoch 24: Train Loss: 0.6719353049993515, Validation Loss: 0.7265737652778625\n",
      "Epoch 25: Train Loss: 0.6763199865818024, Validation Loss: 0.7261461615562439\n",
      "Epoch 26: Train Loss: 0.6802010387182236, Validation Loss: 0.7264267802238464\n",
      "Epoch 27: Train Loss: 0.6563526540994644, Validation Loss: 0.7266891002655029\n",
      "Epoch 28: Train Loss: 0.6763714104890823, Validation Loss: 0.7266899943351746\n",
      "Epoch 29: Train Loss: 0.7031018435955048, Validation Loss: 0.7265092134475708\n",
      "Epoch 30: Train Loss: 0.6658052057027817, Validation Loss: 0.7257639765739441\n",
      "Epoch 31: Train Loss: 0.6846567690372467, Validation Loss: 0.7247398495674133\n",
      "Epoch 32: Train Loss: 0.6901711225509644, Validation Loss: 0.7249054312705994\n",
      "Epoch 33: Train Loss: 0.655325397849083, Validation Loss: 0.7260976433753967\n",
      "Epoch 34: Train Loss: 0.6420591771602631, Validation Loss: 0.7294504046440125\n",
      "Epoch 35: Train Loss: 0.6755576878786087, Validation Loss: 0.7310824990272522\n",
      "Epoch 36: Train Loss: 0.6491865962743759, Validation Loss: 0.7329209446907043\n",
      "Epoch 37: Train Loss: 0.6348337680101395, Validation Loss: 0.7340689897537231\n",
      "Epoch 38: Train Loss: 0.6334454268217087, Validation Loss: 0.7347230315208435\n",
      "Epoch 39: Train Loss: 0.6351379007101059, Validation Loss: 0.7347701787948608\n",
      "Epoch 40: Train Loss: 0.6725306808948517, Validation Loss: 0.7350453734397888\n",
      "Epoch 41: Train Loss: 0.6526703238487244, Validation Loss: 0.7369170188903809\n",
      "Epoch 42: Train Loss: 0.6367984116077423, Validation Loss: 0.7362169027328491\n",
      "Epoch 43: Train Loss: 0.6096774786710739, Validation Loss: 0.7352609634399414\n",
      "Epoch 44: Train Loss: 0.6748866587877274, Validation Loss: 0.7360744476318359\n",
      "Epoch 45: Train Loss: 0.6486925929784775, Validation Loss: 0.7392320036888123\n",
      "Epoch 46: Train Loss: 0.6546672582626343, Validation Loss: 0.7425311803817749\n",
      "Epoch 47: Train Loss: 0.6158713847398758, Validation Loss: 0.7443758845329285\n",
      "Epoch 48: Train Loss: 0.6583383083343506, Validation Loss: 0.7453944087028503\n",
      "Epoch 49: Train Loss: 0.598842591047287, Validation Loss: 0.7458848357200623\n",
      "Epoch 50: Train Loss: 0.6256477385759354, Validation Loss: 0.7615832686424255\n",
      "Epoch 51: Train Loss: 0.6238701194524765, Validation Loss: 0.7681253552436829\n",
      "Epoch 52: Train Loss: 0.6377519220113754, Validation Loss: 0.7697446942329407\n",
      "Epoch 53: Train Loss: 0.6396540254354477, Validation Loss: 0.7683038115501404\n",
      "Epoch 54: Train Loss: 0.5934776067733765, Validation Loss: 0.7697620391845703\n",
      "Epoch 55: Train Loss: 0.6272988468408585, Validation Loss: 0.7705272436141968\n",
      "Epoch 56: Train Loss: 0.6292860209941864, Validation Loss: 0.772408664226532\n",
      "Epoch 57: Train Loss: 0.6417326629161835, Validation Loss: 0.7739615440368652\n",
      "Epoch 58: Train Loss: 0.6442350596189499, Validation Loss: 0.7741355299949646\n",
      "Epoch 59: Train Loss: 0.6331534683704376, Validation Loss: 0.7747071385383606\n",
      "Epoch 60: Train Loss: 0.6668300181627274, Validation Loss: 0.7774894833564758\n",
      "Epoch 61: Train Loss: 0.6193322539329529, Validation Loss: 0.7789216041564941\n",
      "Epoch 62: Train Loss: 0.6536298990249634, Validation Loss: 0.7818341255187988\n",
      "Epoch 63: Train Loss: 0.6609698086977005, Validation Loss: 0.7853134274482727\n",
      "Epoch 64: Train Loss: 0.5932777523994446, Validation Loss: 0.7879469990730286\n",
      "Epoch 65: Train Loss: 0.6183934509754181, Validation Loss: 0.7891626358032227\n",
      "Epoch 66: Train Loss: 0.5807762742042542, Validation Loss: 0.789624810218811\n",
      "Epoch 67: Train Loss: 0.5686284154653549, Validation Loss: 0.7902011871337891\n",
      "Epoch 68: Train Loss: 0.6126798689365387, Validation Loss: 0.790738046169281\n",
      "Epoch 69: Train Loss: 0.5834913700819016, Validation Loss: 0.7913562059402466\n",
      "Epoch 70: Train Loss: 0.6078394651412964, Validation Loss: 0.8009974956512451\n",
      "Epoch 71: Train Loss: 0.6293971836566925, Validation Loss: 0.8131581544876099\n",
      "Epoch 72: Train Loss: 0.6075597107410431, Validation Loss: 0.8277822136878967\n",
      "Epoch 73: Train Loss: 0.5885506272315979, Validation Loss: 0.8334234356880188\n",
      "Epoch 74: Train Loss: 0.6078091710805893, Validation Loss: 0.8375723958015442\n",
      "Epoch 75: Train Loss: 0.5802743136882782, Validation Loss: 0.8399308323860168\n",
      "Epoch 76: Train Loss: 0.5713634341955185, Validation Loss: 0.8406620025634766\n",
      "Epoch 77: Train Loss: 0.5811875611543655, Validation Loss: 0.8397550582885742\n",
      "Epoch 78: Train Loss: 0.5961318910121918, Validation Loss: 0.8396552801132202\n",
      "Epoch 79: Train Loss: 0.6043605208396912, Validation Loss: 0.8408433794975281\n",
      "Epoch 80: Train Loss: 0.5685879662632942, Validation Loss: 0.826860785484314\n",
      "Epoch 81: Train Loss: 0.5751684904098511, Validation Loss: 0.8257166147232056\n",
      "Epoch 82: Train Loss: 0.5788907408714294, Validation Loss: 0.8232633471488953\n",
      "Epoch 83: Train Loss: 0.5041943937540054, Validation Loss: 0.8219890594482422\n",
      "Epoch 84: Train Loss: 0.6053087264299393, Validation Loss: 0.8231469988822937\n",
      "Epoch 85: Train Loss: 0.5935143455862999, Validation Loss: 0.8218281865119934\n",
      "Epoch 86: Train Loss: 0.5719531774520874, Validation Loss: 0.821067750453949\n",
      "Epoch 87: Train Loss: 0.5501072108745575, Validation Loss: 0.8224141597747803\n",
      "Epoch 88: Train Loss: 0.5735095888376236, Validation Loss: 0.8216344118118286\n",
      "Epoch 89: Train Loss: 0.5443227589130402, Validation Loss: 0.8218696713447571\n",
      "Epoch 90: Train Loss: 0.5378963947296143, Validation Loss: 0.8221591114997864\n",
      "Epoch 91: Train Loss: 0.5358235090970993, Validation Loss: 0.8313912749290466\n",
      "Epoch 92: Train Loss: 0.5524149239063263, Validation Loss: 0.8355979323387146\n",
      "Epoch 93: Train Loss: 0.5033583268523216, Validation Loss: 0.845782995223999\n",
      "Epoch 94: Train Loss: 0.49632713198661804, Validation Loss: 0.8525485396385193\n",
      "Epoch 95: Train Loss: 0.542755164206028, Validation Loss: 0.8496320247650146\n",
      "Epoch 96: Train Loss: 0.4774770960211754, Validation Loss: 0.8530630469322205\n",
      "Epoch 97: Train Loss: 0.5183826386928558, Validation Loss: 0.8579303622245789\n",
      "Epoch 98: Train Loss: 0.5228178426623344, Validation Loss: 0.8597617745399475\n",
      "Epoch 99: Train Loss: 0.5371362417936325, Validation Loss: 0.8597441911697388\n",
      "Epoch 100: Train Loss: 0.5440145581960678, Validation Loss: 0.8657370805740356\n",
      "Epoch 101: Train Loss: 0.5130040869116783, Validation Loss: 0.8704894185066223\n",
      "Epoch 102: Train Loss: 0.45233330875635147, Validation Loss: 0.8519410490989685\n",
      "Epoch 103: Train Loss: 0.4710855782032013, Validation Loss: 0.832794725894928\n",
      "Epoch 104: Train Loss: 0.48817693442106247, Validation Loss: 0.8253840804100037\n",
      "Epoch 105: Train Loss: 0.49237243086099625, Validation Loss: 0.8228665590286255\n",
      "Epoch 106: Train Loss: 0.44243571907281876, Validation Loss: 0.8282830715179443\n",
      "Epoch 107: Train Loss: 0.4691020995378494, Validation Loss: 0.8301912546157837\n",
      "Epoch 108: Train Loss: 0.4742007330060005, Validation Loss: 0.8305780291557312\n",
      "Early stopping at epoch 109\n",
      "Completed fold 9\n",
      "Epoch 0: Train Loss: 0.7264710068702698, Validation Loss: 0.6870110630989075\n",
      "Epoch 1: Train Loss: 0.7477567195892334, Validation Loss: 0.6783650517463684\n",
      "Epoch 2: Train Loss: 0.6823471635580063, Validation Loss: 0.671869158744812\n",
      "Epoch 3: Train Loss: 0.7170098721981049, Validation Loss: 0.6645810008049011\n",
      "Epoch 4: Train Loss: 0.7136385142803192, Validation Loss: 0.6583473682403564\n",
      "Epoch 5: Train Loss: 0.7343059033155441, Validation Loss: 0.6540641188621521\n",
      "Epoch 6: Train Loss: 0.7182639539241791, Validation Loss: 0.651323676109314\n",
      "Epoch 7: Train Loss: 0.7136928141117096, Validation Loss: 0.6490246653556824\n",
      "Epoch 8: Train Loss: 0.7113898396492004, Validation Loss: 0.6483504176139832\n",
      "Epoch 9: Train Loss: 0.6880392134189606, Validation Loss: 0.6472927927970886\n",
      "Epoch 10: Train Loss: 0.7065881192684174, Validation Loss: 0.6491864323616028\n",
      "Epoch 11: Train Loss: 0.6887156069278717, Validation Loss: 0.6487844586372375\n",
      "Epoch 12: Train Loss: 0.6997591704130173, Validation Loss: 0.6479893922805786\n",
      "Epoch 13: Train Loss: 0.7168659716844559, Validation Loss: 0.6486498713493347\n",
      "Epoch 14: Train Loss: 0.6711238622665405, Validation Loss: 0.6493033170700073\n",
      "Epoch 15: Train Loss: 0.6900893151760101, Validation Loss: 0.6495951414108276\n",
      "Epoch 16: Train Loss: 0.6854414939880371, Validation Loss: 0.6498874425888062\n",
      "Epoch 17: Train Loss: 0.660652369260788, Validation Loss: 0.6496888399124146\n",
      "Epoch 18: Train Loss: 0.6890663802623749, Validation Loss: 0.6488368511199951\n",
      "Epoch 19: Train Loss: 0.6713464856147766, Validation Loss: 0.6485335230827332\n",
      "Epoch 20: Train Loss: 0.6733512878417969, Validation Loss: 0.6452193260192871\n",
      "Epoch 21: Train Loss: 0.6523868143558502, Validation Loss: 0.6452934145927429\n",
      "Epoch 22: Train Loss: 0.6898663938045502, Validation Loss: 0.6450523138046265\n",
      "Epoch 23: Train Loss: 0.6486934125423431, Validation Loss: 0.6455366611480713\n",
      "Epoch 24: Train Loss: 0.6493720859289169, Validation Loss: 0.6459336876869202\n",
      "Epoch 25: Train Loss: 0.6440159231424332, Validation Loss: 0.644500732421875\n",
      "Epoch 26: Train Loss: 0.7022983431816101, Validation Loss: 0.6428338885307312\n",
      "Epoch 27: Train Loss: 0.6669159382581711, Validation Loss: 0.6410925984382629\n",
      "Epoch 28: Train Loss: 0.6487684696912766, Validation Loss: 0.6403080821037292\n",
      "Epoch 29: Train Loss: 0.6868800669908524, Validation Loss: 0.6402432322502136\n",
      "Epoch 30: Train Loss: 0.6185728758573532, Validation Loss: 0.632830023765564\n",
      "Epoch 31: Train Loss: 0.6770768016576767, Validation Loss: 0.6267557740211487\n",
      "Epoch 32: Train Loss: 0.6208384335041046, Validation Loss: 0.625199019908905\n",
      "Epoch 33: Train Loss: 0.6621560752391815, Validation Loss: 0.623890221118927\n",
      "Epoch 34: Train Loss: 0.6324205845594406, Validation Loss: 0.6235552430152893\n",
      "Epoch 35: Train Loss: 0.6387622803449631, Validation Loss: 0.624394953250885\n",
      "Epoch 36: Train Loss: 0.6554504185914993, Validation Loss: 0.6263120770454407\n",
      "Epoch 37: Train Loss: 0.6328631788492203, Validation Loss: 0.6254558563232422\n",
      "Epoch 38: Train Loss: 0.6533944308757782, Validation Loss: 0.6248282194137573\n",
      "Epoch 39: Train Loss: 0.6397803127765656, Validation Loss: 0.6239344477653503\n",
      "Epoch 40: Train Loss: 0.6127962619066238, Validation Loss: 0.6190875172615051\n",
      "Epoch 41: Train Loss: 0.6881733387708664, Validation Loss: 0.6117414236068726\n",
      "Epoch 42: Train Loss: 0.6019888371229172, Validation Loss: 0.6053569316864014\n",
      "Epoch 43: Train Loss: 0.6208441406488419, Validation Loss: 0.6016626358032227\n",
      "Epoch 44: Train Loss: 0.6141291856765747, Validation Loss: 0.6013213396072388\n",
      "Epoch 45: Train Loss: 0.6330294460058212, Validation Loss: 0.602277398109436\n",
      "Epoch 46: Train Loss: 0.6056948751211166, Validation Loss: 0.6029697060585022\n",
      "Epoch 47: Train Loss: 0.5952831208705902, Validation Loss: 0.6019158959388733\n",
      "Epoch 48: Train Loss: 0.5871503353118896, Validation Loss: 0.6027368307113647\n",
      "Epoch 49: Train Loss: 0.6215984970331192, Validation Loss: 0.6013336777687073\n",
      "Epoch 50: Train Loss: 0.5967469364404678, Validation Loss: 0.6185771226882935\n",
      "Epoch 51: Train Loss: 0.6362401098012924, Validation Loss: 0.625760018825531\n",
      "Epoch 52: Train Loss: 0.615383967757225, Validation Loss: 0.6199150681495667\n",
      "Epoch 53: Train Loss: 0.5688734352588654, Validation Loss: 0.6091648936271667\n",
      "Epoch 54: Train Loss: 0.5835279524326324, Validation Loss: 0.5950911045074463\n",
      "Epoch 55: Train Loss: 0.5890000462532043, Validation Loss: 0.588070809841156\n",
      "Epoch 56: Train Loss: 0.5686767250299454, Validation Loss: 0.5876340270042419\n",
      "Epoch 57: Train Loss: 0.581017330288887, Validation Loss: 0.5858147740364075\n",
      "Epoch 58: Train Loss: 0.5959964692592621, Validation Loss: 0.5844776630401611\n",
      "Epoch 59: Train Loss: 0.5662485361099243, Validation Loss: 0.5833770036697388\n",
      "Epoch 60: Train Loss: 0.5681125521659851, Validation Loss: 0.5811272263526917\n",
      "Epoch 61: Train Loss: 0.5610259175300598, Validation Loss: 0.5909818410873413\n",
      "Epoch 62: Train Loss: 0.5291385725140572, Validation Loss: 0.6062144041061401\n",
      "Epoch 63: Train Loss: 0.5938845425844193, Validation Loss: 0.6081662178039551\n",
      "Epoch 64: Train Loss: 0.5407287329435349, Validation Loss: 0.5936062335968018\n",
      "Epoch 65: Train Loss: 0.5254418402910233, Validation Loss: 0.5661812424659729\n",
      "Epoch 66: Train Loss: 0.5262213870882988, Validation Loss: 0.5550726652145386\n",
      "Epoch 67: Train Loss: 0.5408282428979874, Validation Loss: 0.5534257888793945\n",
      "Epoch 68: Train Loss: 0.5427191182971001, Validation Loss: 0.5544238686561584\n",
      "Epoch 69: Train Loss: 0.5414406135678291, Validation Loss: 0.5541325807571411\n",
      "Epoch 70: Train Loss: 0.5230199918150902, Validation Loss: 0.5648249387741089\n",
      "Epoch 71: Train Loss: 0.5428550690412521, Validation Loss: 0.563395619392395\n",
      "Epoch 72: Train Loss: 0.48528873920440674, Validation Loss: 0.569439172744751\n",
      "Epoch 73: Train Loss: 0.5164646282792091, Validation Loss: 0.5875799655914307\n",
      "Epoch 74: Train Loss: 0.5035634562373161, Validation Loss: 0.5768647193908691\n",
      "Epoch 75: Train Loss: 0.47952701151371, Validation Loss: 0.5578488707542419\n",
      "Epoch 76: Train Loss: 0.5009808093309402, Validation Loss: 0.5517552495002747\n",
      "Epoch 77: Train Loss: 0.5123323947191238, Validation Loss: 0.5580263733863831\n",
      "Epoch 78: Train Loss: 0.48952075093984604, Validation Loss: 0.5577449798583984\n",
      "Epoch 79: Train Loss: 0.4567360356450081, Validation Loss: 0.5561952590942383\n",
      "Epoch 80: Train Loss: 0.4518660083413124, Validation Loss: 0.5566096901893616\n",
      "Epoch 81: Train Loss: 0.4360717684030533, Validation Loss: 0.5445020794868469\n",
      "Epoch 82: Train Loss: 0.4650719165802002, Validation Loss: 0.5460315942764282\n",
      "Epoch 83: Train Loss: 0.4563381299376488, Validation Loss: 0.5439766645431519\n",
      "Epoch 84: Train Loss: 0.4209432378411293, Validation Loss: 0.5754302144050598\n",
      "Epoch 85: Train Loss: 0.4225843772292137, Validation Loss: 0.573609471321106\n",
      "Epoch 86: Train Loss: 0.46822357922792435, Validation Loss: 0.5526775121688843\n",
      "Epoch 87: Train Loss: 0.4164879098534584, Validation Loss: 0.5385572910308838\n",
      "Epoch 88: Train Loss: 0.4054163321852684, Validation Loss: 0.534917950630188\n",
      "Epoch 89: Train Loss: 0.4267048090696335, Validation Loss: 0.5353067517280579\n",
      "Epoch 90: Train Loss: 0.4057890251278877, Validation Loss: 0.5666343569755554\n",
      "Epoch 91: Train Loss: 0.38190487772226334, Validation Loss: 0.5506141781806946\n",
      "Epoch 92: Train Loss: 0.41123558580875397, Validation Loss: 0.5378031730651855\n",
      "Epoch 93: Train Loss: 0.4033503755927086, Validation Loss: 0.583091676235199\n",
      "Epoch 94: Train Loss: 0.34821025282144547, Validation Loss: 0.5763680934906006\n",
      "Epoch 95: Train Loss: 0.36384518444538116, Validation Loss: 0.5437377095222473\n",
      "Epoch 96: Train Loss: 0.37189536541700363, Validation Loss: 0.5249873399734497\n",
      "Epoch 97: Train Loss: 0.40349142998456955, Validation Loss: 0.533837080001831\n",
      "Epoch 98: Train Loss: 0.39372018724679947, Validation Loss: 0.5466805100440979\n",
      "Epoch 99: Train Loss: 0.37615835666656494, Validation Loss: 0.5487642884254456\n",
      "Epoch 100: Train Loss: 0.3489314094185829, Validation Loss: 0.530563235282898\n",
      "Epoch 101: Train Loss: 0.3251070976257324, Validation Loss: 0.5172774791717529\n",
      "Epoch 102: Train Loss: 0.3433830142021179, Validation Loss: 0.5275463461875916\n",
      "Epoch 103: Train Loss: 0.33158762753009796, Validation Loss: 0.5313795804977417\n",
      "Epoch 104: Train Loss: 0.3104352355003357, Validation Loss: 0.5242193937301636\n",
      "Epoch 105: Train Loss: 0.3012152314186096, Validation Loss: 0.5369961261749268\n",
      "Epoch 106: Train Loss: 0.3580593541264534, Validation Loss: 0.5407124757766724\n",
      "Epoch 107: Train Loss: 0.31504467874765396, Validation Loss: 0.5338484644889832\n",
      "Epoch 108: Train Loss: 0.3028480187058449, Validation Loss: 0.5324319005012512\n",
      "Epoch 109: Train Loss: 0.3049706518650055, Validation Loss: 0.5342686772346497\n",
      "Epoch 110: Train Loss: 0.2911412715911865, Validation Loss: 0.5884299874305725\n",
      "Epoch 111: Train Loss: 0.28413453325629234, Validation Loss: 0.5589695572853088\n",
      "Epoch 112: Train Loss: 0.3001435697078705, Validation Loss: 0.549972414970398\n",
      "Epoch 113: Train Loss: 0.2782520651817322, Validation Loss: 0.5923609733581543\n",
      "Epoch 114: Train Loss: 0.24908772855997086, Validation Loss: 0.5849519371986389\n",
      "Epoch 115: Train Loss: 0.2982279062271118, Validation Loss: 0.5422356128692627\n",
      "Epoch 116: Train Loss: 0.2591983340680599, Validation Loss: 0.5555285215377808\n",
      "Epoch 117: Train Loss: 0.2541748434305191, Validation Loss: 0.5946225523948669\n",
      "Epoch 118: Train Loss: 0.2632947862148285, Validation Loss: 0.6023056507110596\n",
      "Epoch 119: Train Loss: 0.241575226187706, Validation Loss: 0.6014676690101624\n",
      "Epoch 120: Train Loss: 0.23109236359596252, Validation Loss: 0.5661167502403259\n",
      "Epoch 121: Train Loss: 0.23863548785448074, Validation Loss: 0.5808145999908447\n",
      "Epoch 122: Train Loss: 0.23619580268859863, Validation Loss: 0.5600802898406982\n",
      "Epoch 123: Train Loss: 0.24376702308654785, Validation Loss: 0.5859442353248596\n",
      "Epoch 124: Train Loss: 0.2237478457391262, Validation Loss: 0.6388434767723083\n",
      "Epoch 125: Train Loss: 0.26044242456555367, Validation Loss: 0.5913122892379761\n",
      "Epoch 126: Train Loss: 0.22269313409924507, Validation Loss: 0.5644285678863525\n",
      "Epoch 127: Train Loss: 0.26046939939260483, Validation Loss: 0.5804369449615479\n",
      "Epoch 128: Train Loss: 0.20159878209233284, Validation Loss: 0.5958219170570374\n",
      "Epoch 129: Train Loss: 0.2023424208164215, Validation Loss: 0.6018780469894409\n",
      "Epoch 130: Train Loss: 0.219728484749794, Validation Loss: 0.7347281575202942\n",
      "Epoch 131: Train Loss: 0.22057907655835152, Validation Loss: 0.6062183976173401\n",
      "Epoch 132: Train Loss: 0.21017437055706978, Validation Loss: 0.6252232789993286\n",
      "Epoch 133: Train Loss: 0.20332615450024605, Validation Loss: 0.7543584108352661\n",
      "Epoch 134: Train Loss: 0.2015846222639084, Validation Loss: 0.6379523873329163\n",
      "Epoch 135: Train Loss: 0.2186596542596817, Validation Loss: 0.5783036947250366\n",
      "Epoch 136: Train Loss: 0.17118649184703827, Validation Loss: 0.6212972402572632\n",
      "Epoch 137: Train Loss: 0.1897742673754692, Validation Loss: 0.6580579280853271\n",
      "Epoch 138: Train Loss: 0.1943289451301098, Validation Loss: 0.6595346331596375\n",
      "Epoch 139: Train Loss: 0.19428719952702522, Validation Loss: 0.6632500290870667\n",
      "Epoch 140: Train Loss: 0.17970238253474236, Validation Loss: 0.6585116982460022\n",
      "Epoch 141: Train Loss: 0.18578602373600006, Validation Loss: 0.6894658803939819\n",
      "Epoch 142: Train Loss: 0.16995887830853462, Validation Loss: 0.7104901075363159\n",
      "Epoch 143: Train Loss: 0.1643102653324604, Validation Loss: 0.6811355948448181\n",
      "Epoch 144: Train Loss: 0.18323742970824242, Validation Loss: 0.6826457977294922\n",
      "Epoch 145: Train Loss: 0.15377135388553143, Validation Loss: 0.7263057827949524\n",
      "Epoch 146: Train Loss: 0.1574871502816677, Validation Loss: 0.767740786075592\n",
      "Epoch 147: Train Loss: 0.17568806558847427, Validation Loss: 0.7642219066619873\n",
      "Epoch 148: Train Loss: 0.17619572766125202, Validation Loss: 0.7509233355522156\n",
      "Epoch 149: Train Loss: 0.16207565926015377, Validation Loss: 0.7455936074256897\n",
      "Epoch 150: Train Loss: 0.160491231828928, Validation Loss: 0.6727023124694824\n",
      "Epoch 151: Train Loss: 0.1566503345966339, Validation Loss: 0.7311145067214966\n",
      "Epoch 152: Train Loss: 0.15219660103321075, Validation Loss: 0.7923173308372498\n",
      "Epoch 153: Train Loss: 0.14686493948101997, Validation Loss: 0.7466315627098083\n",
      "Epoch 154: Train Loss: 0.13028566911816597, Validation Loss: 0.740951657295227\n",
      "Epoch 155: Train Loss: 0.14113208651542664, Validation Loss: 0.7812494039535522\n",
      "Epoch 156: Train Loss: 0.15498069673776627, Validation Loss: 0.7586537599563599\n",
      "Epoch 157: Train Loss: 0.13391361944377422, Validation Loss: 0.7401928901672363\n",
      "Epoch 158: Train Loss: 0.1351670864969492, Validation Loss: 0.7257178425788879\n",
      "Epoch 159: Train Loss: 0.17115319147706032, Validation Loss: 0.7207351326942444\n",
      "Epoch 160: Train Loss: 0.1356469076126814, Validation Loss: 0.6901964545249939\n",
      "Epoch 161: Train Loss: 0.13439667597413063, Validation Loss: 0.7753075957298279\n",
      "Epoch 162: Train Loss: 0.136689193546772, Validation Loss: 0.8982660174369812\n",
      "Epoch 163: Train Loss: 0.13773776404559612, Validation Loss: 0.838771402835846\n",
      "Epoch 164: Train Loss: 0.1494984794408083, Validation Loss: 0.813459575176239\n",
      "Epoch 165: Train Loss: 0.11660998500883579, Validation Loss: 0.7771106362342834\n",
      "Epoch 166: Train Loss: 0.10772891528904438, Validation Loss: 0.7724959850311279\n",
      "Epoch 167: Train Loss: 0.12220635451376438, Validation Loss: 0.7977044582366943\n",
      "Epoch 168: Train Loss: 0.10760024189949036, Validation Loss: 0.8170180320739746\n",
      "Epoch 169: Train Loss: 0.13501943834125996, Validation Loss: 0.8212152719497681\n",
      "Epoch 170: Train Loss: 0.18366487696766853, Validation Loss: 0.9614315032958984\n",
      "Epoch 171: Train Loss: 0.12813139148056507, Validation Loss: 0.8390552401542664\n",
      "Epoch 172: Train Loss: 0.1450232770293951, Validation Loss: 0.8136616945266724\n",
      "Epoch 173: Train Loss: 0.1248140949755907, Validation Loss: 0.8465338945388794\n",
      "Epoch 174: Train Loss: 0.14387152716517448, Validation Loss: 0.8825737237930298\n",
      "Epoch 175: Train Loss: 0.1315146442502737, Validation Loss: 0.8791806697845459\n",
      "Epoch 176: Train Loss: 0.14442167803645134, Validation Loss: 0.8621463179588318\n",
      "Epoch 177: Train Loss: 0.11710200086236, Validation Loss: 0.8486509323120117\n",
      "Epoch 178: Train Loss: 0.15480306558310986, Validation Loss: 0.8462451696395874\n",
      "Epoch 179: Train Loss: 0.16016442887485027, Validation Loss: 0.8338445425033569\n",
      "Epoch 180: Train Loss: 0.09997874218970537, Validation Loss: 0.8117532730102539\n",
      "Epoch 181: Train Loss: 0.10350068286061287, Validation Loss: 0.7869687676429749\n",
      "Epoch 182: Train Loss: 0.11840360425412655, Validation Loss: 0.8331072926521301\n",
      "Epoch 183: Train Loss: 0.1160142756998539, Validation Loss: 0.8531489968299866\n",
      "Epoch 184: Train Loss: 0.12282570451498032, Validation Loss: 0.8467031121253967\n",
      "Epoch 185: Train Loss: 0.11204531416296959, Validation Loss: 0.9086906313896179\n",
      "Epoch 186: Train Loss: 0.10289246309548616, Validation Loss: 0.9686523079872131\n",
      "Epoch 187: Train Loss: 0.1002103267237544, Validation Loss: 0.9913906455039978\n",
      "Epoch 188: Train Loss: 0.10692024324089289, Validation Loss: 0.9940431118011475\n",
      "Epoch 189: Train Loss: 0.1321105808019638, Validation Loss: 0.9898576140403748\n",
      "Epoch 190: Train Loss: 0.09732359647750854, Validation Loss: 0.901479184627533\n",
      "Epoch 191: Train Loss: 0.09756132867187262, Validation Loss: 0.9574479460716248\n",
      "Epoch 192: Train Loss: 0.1046151053160429, Validation Loss: 0.9830828309059143\n",
      "Epoch 193: Train Loss: 0.11063391901552677, Validation Loss: 1.0438910722732544\n",
      "Epoch 194: Train Loss: 0.1295140404254198, Validation Loss: 1.000104308128357\n",
      "Epoch 195: Train Loss: 0.09938791953027248, Validation Loss: 0.9805777072906494\n",
      "Epoch 196: Train Loss: 0.11982804164290428, Validation Loss: 0.9432243704795837\n",
      "Epoch 197: Train Loss: 0.1263653915375471, Validation Loss: 0.9280750751495361\n",
      "Epoch 198: Train Loss: 0.11508106626570225, Validation Loss: 0.9278480410575867\n",
      "Epoch 199: Train Loss: 0.11144202575087547, Validation Loss: 0.9264367818832397\n",
      "Epoch 200: Train Loss: 0.10028350166976452, Validation Loss: 0.9531549215316772\n",
      "Early stopping at epoch 201\n",
      "Completed fold 10\n",
      "Epoch 0: Train Loss: 0.7733889818191528, Validation Loss: 0.6933637857437134\n",
      "Epoch 1: Train Loss: 0.7232359498739243, Validation Loss: 0.6915549039840698\n",
      "Epoch 2: Train Loss: 0.7242516279220581, Validation Loss: 0.6903230547904968\n",
      "Epoch 3: Train Loss: 0.790126621723175, Validation Loss: 0.6902582049369812\n",
      "Epoch 4: Train Loss: 0.7007344961166382, Validation Loss: 0.6904972195625305\n",
      "Epoch 5: Train Loss: 0.7377524226903915, Validation Loss: 0.6908010244369507\n",
      "Epoch 6: Train Loss: 0.6744154244661331, Validation Loss: 0.69120854139328\n",
      "Epoch 7: Train Loss: 0.705067440867424, Validation Loss: 0.691289484500885\n",
      "Epoch 8: Train Loss: 0.6972067207098007, Validation Loss: 0.691182553768158\n",
      "Epoch 9: Train Loss: 0.7089871466159821, Validation Loss: 0.6911011338233948\n",
      "Epoch 10: Train Loss: 0.6809049695730209, Validation Loss: 0.6907961964607239\n",
      "Epoch 11: Train Loss: 0.7273142337799072, Validation Loss: 0.6904265880584717\n",
      "Epoch 12: Train Loss: 0.7068495601415634, Validation Loss: 0.6897153854370117\n",
      "Epoch 13: Train Loss: 0.6984625160694122, Validation Loss: 0.6887431144714355\n",
      "Epoch 14: Train Loss: 0.6719692200422287, Validation Loss: 0.6884803175926208\n",
      "Epoch 15: Train Loss: 0.695678636431694, Validation Loss: 0.6880413889884949\n",
      "Epoch 16: Train Loss: 0.6991192102432251, Validation Loss: 0.6889861226081848\n",
      "Epoch 17: Train Loss: 0.6854786574840546, Validation Loss: 0.6895068287849426\n",
      "Epoch 18: Train Loss: 0.7025898694992065, Validation Loss: 0.6897966265678406\n",
      "Epoch 19: Train Loss: 0.6916690468788147, Validation Loss: 0.6898912191390991\n",
      "Epoch 20: Train Loss: 0.6787299066781998, Validation Loss: 0.6947404742240906\n",
      "Epoch 21: Train Loss: 0.6515925824642181, Validation Loss: 0.6996831893920898\n",
      "Epoch 22: Train Loss: 0.6522042453289032, Validation Loss: 0.7030794620513916\n",
      "Epoch 23: Train Loss: 0.6663414537906647, Validation Loss: 0.7033578157424927\n",
      "Epoch 24: Train Loss: 0.6825156658887863, Validation Loss: 0.7027705907821655\n",
      "Epoch 25: Train Loss: 0.6184599995613098, Validation Loss: 0.7018280625343323\n",
      "Epoch 26: Train Loss: 0.6351862251758575, Validation Loss: 0.7013177871704102\n",
      "Epoch 27: Train Loss: 0.6814810782670975, Validation Loss: 0.7009533643722534\n",
      "Epoch 28: Train Loss: 0.6629530191421509, Validation Loss: 0.7010762095451355\n",
      "Epoch 29: Train Loss: 0.6473174542188644, Validation Loss: 0.7011405825614929\n",
      "Epoch 30: Train Loss: 0.6417330503463745, Validation Loss: 0.7020595073699951\n",
      "Epoch 31: Train Loss: 0.6459991037845612, Validation Loss: 0.7029538154602051\n",
      "Epoch 32: Train Loss: 0.6575395166873932, Validation Loss: 0.7038607001304626\n",
      "Epoch 33: Train Loss: 0.6429493725299835, Validation Loss: 0.704745888710022\n",
      "Epoch 34: Train Loss: 0.6208924800157547, Validation Loss: 0.7058336734771729\n",
      "Epoch 35: Train Loss: 0.6631260961294174, Validation Loss: 0.7066491842269897\n",
      "Epoch 36: Train Loss: 0.6451393514871597, Validation Loss: 0.707043468952179\n",
      "Epoch 37: Train Loss: 0.7023405134677887, Validation Loss: 0.7069976329803467\n",
      "Epoch 38: Train Loss: 0.6718815118074417, Validation Loss: 0.7070795893669128\n",
      "Epoch 39: Train Loss: 0.6548147946596146, Validation Loss: 0.7072522044181824\n",
      "Epoch 40: Train Loss: 0.6616557538509369, Validation Loss: 0.7089225053787231\n",
      "Epoch 41: Train Loss: 0.6500423103570938, Validation Loss: 0.7089217901229858\n",
      "Epoch 42: Train Loss: 0.6291832327842712, Validation Loss: 0.7094956636428833\n",
      "Epoch 43: Train Loss: 0.6134971380233765, Validation Loss: 0.7110565900802612\n",
      "Epoch 44: Train Loss: 0.656534880399704, Validation Loss: 0.7117950916290283\n",
      "Epoch 45: Train Loss: 0.660032108426094, Validation Loss: 0.7120833992958069\n",
      "Epoch 46: Train Loss: 0.7018927037715912, Validation Loss: 0.7121514081954956\n",
      "Epoch 47: Train Loss: 0.6292765885591507, Validation Loss: 0.7121207118034363\n",
      "Epoch 48: Train Loss: 0.6534874588251114, Validation Loss: 0.7121137380599976\n",
      "Epoch 49: Train Loss: 0.6144189685583115, Validation Loss: 0.7119901180267334\n",
      "Epoch 50: Train Loss: 0.6465615928173065, Validation Loss: 0.7085031867027283\n",
      "Epoch 51: Train Loss: 0.6088393777608871, Validation Loss: 0.706337571144104\n",
      "Epoch 52: Train Loss: 0.6583966910839081, Validation Loss: 0.7053026556968689\n",
      "Epoch 53: Train Loss: 0.6426046043634415, Validation Loss: 0.7046689391136169\n",
      "Epoch 54: Train Loss: 0.6198737621307373, Validation Loss: 0.7042670249938965\n",
      "Epoch 55: Train Loss: 0.6029122471809387, Validation Loss: 0.7045745253562927\n",
      "Epoch 56: Train Loss: 0.5728520154953003, Validation Loss: 0.7044519186019897\n",
      "Epoch 57: Train Loss: 0.6278269439935684, Validation Loss: 0.7044553160667419\n",
      "Epoch 58: Train Loss: 0.6200271099805832, Validation Loss: 0.7045555710792542\n",
      "Epoch 59: Train Loss: 0.6121046543121338, Validation Loss: 0.7045474648475647\n",
      "Epoch 60: Train Loss: 0.630140870809555, Validation Loss: 0.7037461400032043\n",
      "Epoch 61: Train Loss: 0.6058756709098816, Validation Loss: 0.7033114433288574\n",
      "Epoch 62: Train Loss: 0.6098833978176117, Validation Loss: 0.7031549215316772\n",
      "Epoch 63: Train Loss: 0.658406063914299, Validation Loss: 0.7034586668014526\n",
      "Epoch 64: Train Loss: 0.5961767286062241, Validation Loss: 0.7036609649658203\n",
      "Epoch 65: Train Loss: 0.6145589798688889, Validation Loss: 0.7042579650878906\n",
      "Epoch 66: Train Loss: 0.6028871089220047, Validation Loss: 0.7043863534927368\n",
      "Epoch 67: Train Loss: 0.5813644975423813, Validation Loss: 0.7041681408882141\n",
      "Epoch 68: Train Loss: 0.6421287059783936, Validation Loss: 0.7040805816650391\n",
      "Epoch 69: Train Loss: 0.6122027933597565, Validation Loss: 0.7039357423782349\n",
      "Epoch 70: Train Loss: 0.5520803034305573, Validation Loss: 0.7051314115524292\n",
      "Epoch 71: Train Loss: 0.5421035215258598, Validation Loss: 0.706534743309021\n",
      "Epoch 72: Train Loss: 0.5867217555642128, Validation Loss: 0.706553041934967\n",
      "Epoch 73: Train Loss: 0.6148822903633118, Validation Loss: 0.7053321599960327\n",
      "Epoch 74: Train Loss: 0.6005558744072914, Validation Loss: 0.7043236494064331\n",
      "Epoch 75: Train Loss: 0.5732046067714691, Validation Loss: 0.7031131982803345\n",
      "Epoch 76: Train Loss: 0.5743146687746048, Validation Loss: 0.7023182511329651\n",
      "Epoch 77: Train Loss: 0.6034985035657883, Validation Loss: 0.7018125057220459\n",
      "Epoch 78: Train Loss: 0.5757841244339943, Validation Loss: 0.701655924320221\n",
      "Epoch 79: Train Loss: 0.554033637046814, Validation Loss: 0.7016268372535706\n",
      "Epoch 80: Train Loss: 0.6224429607391357, Validation Loss: 0.6981850266456604\n",
      "Epoch 81: Train Loss: 0.60315902531147, Validation Loss: 0.6979088187217712\n",
      "Epoch 82: Train Loss: 0.5716185122728348, Validation Loss: 0.6982859969139099\n",
      "Epoch 83: Train Loss: 0.5746343582868576, Validation Loss: 0.6980783343315125\n",
      "Epoch 84: Train Loss: 0.5639673471450806, Validation Loss: 0.6979765295982361\n",
      "Epoch 85: Train Loss: 0.5634114444255829, Validation Loss: 0.6984917521476746\n",
      "Epoch 86: Train Loss: 0.5779897570610046, Validation Loss: 0.6990665793418884\n",
      "Epoch 87: Train Loss: 0.6025547236204147, Validation Loss: 0.6993207335472107\n",
      "Epoch 88: Train Loss: 0.586885005235672, Validation Loss: 0.6997796893119812\n",
      "Epoch 89: Train Loss: 0.5515267848968506, Validation Loss: 0.6999249458312988\n",
      "Epoch 90: Train Loss: 0.5830079615116119, Validation Loss: 0.7028083801269531\n",
      "Epoch 91: Train Loss: 0.5665218532085419, Validation Loss: 0.7048844695091248\n",
      "Epoch 92: Train Loss: 0.5660891979932785, Validation Loss: 0.707545280456543\n",
      "Epoch 93: Train Loss: 0.56363745033741, Validation Loss: 0.7058653831481934\n",
      "Epoch 94: Train Loss: 0.5549814105033875, Validation Loss: 0.7020727396011353\n",
      "Epoch 95: Train Loss: 0.5349610894918442, Validation Loss: 0.7007400393486023\n",
      "Epoch 96: Train Loss: 0.539066419005394, Validation Loss: 0.7012789249420166\n",
      "Epoch 97: Train Loss: 0.5699187517166138, Validation Loss: 0.7017731070518494\n",
      "Epoch 98: Train Loss: 0.5321731716394424, Validation Loss: 0.7022115588188171\n",
      "Epoch 99: Train Loss: 0.566463440656662, Validation Loss: 0.7023454308509827\n",
      "Epoch 100: Train Loss: 0.5599887669086456, Validation Loss: 0.7066787481307983\n",
      "Epoch 101: Train Loss: 0.5019320398569107, Validation Loss: 0.7082730531692505\n",
      "Epoch 102: Train Loss: 0.5291931852698326, Validation Loss: 0.7066555023193359\n",
      "Epoch 103: Train Loss: 0.529014065861702, Validation Loss: 0.7019339203834534\n",
      "Epoch 104: Train Loss: 0.5074099078774452, Validation Loss: 0.6995190978050232\n",
      "Epoch 105: Train Loss: 0.5065675601363182, Validation Loss: 0.7009720802307129\n",
      "Epoch 106: Train Loss: 0.5507726222276688, Validation Loss: 0.7037314176559448\n",
      "Epoch 107: Train Loss: 0.4999643936753273, Validation Loss: 0.7047902345657349\n",
      "Epoch 108: Train Loss: 0.479867659509182, Validation Loss: 0.7048497796058655\n",
      "Epoch 109: Train Loss: 0.5138595104217529, Validation Loss: 0.7047693133354187\n",
      "Epoch 110: Train Loss: 0.5051606893539429, Validation Loss: 0.7021665573120117\n",
      "Epoch 111: Train Loss: 0.5437687262892723, Validation Loss: 0.7013932466506958\n",
      "Epoch 112: Train Loss: 0.535494863986969, Validation Loss: 0.7078326940536499\n",
      "Epoch 113: Train Loss: 0.5193255916237831, Validation Loss: 0.7142525315284729\n",
      "Epoch 114: Train Loss: 0.4620967209339142, Validation Loss: 0.7151303291320801\n",
      "Early stopping at epoch 115\n",
      "Completed fold 11\n",
      "Epoch 0: Train Loss: 0.8076219856739044, Validation Loss: 0.699966311454773\n",
      "Epoch 1: Train Loss: 0.7955718338489532, Validation Loss: 0.6964144706726074\n",
      "Epoch 2: Train Loss: 0.7113967686891556, Validation Loss: 0.6934973001480103\n",
      "Epoch 3: Train Loss: 0.7156453132629395, Validation Loss: 0.693027138710022\n",
      "Epoch 4: Train Loss: 0.7112880945205688, Validation Loss: 0.6935703754425049\n",
      "Epoch 5: Train Loss: 0.6949571073055267, Validation Loss: 0.6942659616470337\n",
      "Epoch 6: Train Loss: 0.6800227463245392, Validation Loss: 0.6942645311355591\n",
      "Epoch 7: Train Loss: 0.6925119608640671, Validation Loss: 0.6938483715057373\n",
      "Epoch 8: Train Loss: 0.7000885903835297, Validation Loss: 0.6934449076652527\n",
      "Epoch 9: Train Loss: 0.7212137132883072, Validation Loss: 0.6931495666503906\n",
      "Epoch 10: Train Loss: 0.6797182112932205, Validation Loss: 0.6883630156517029\n",
      "Epoch 11: Train Loss: 0.6839107275009155, Validation Loss: 0.6862144470214844\n",
      "Epoch 12: Train Loss: 0.7412727922201157, Validation Loss: 0.6843104362487793\n",
      "Epoch 13: Train Loss: 0.6647874116897583, Validation Loss: 0.6836627721786499\n",
      "Epoch 14: Train Loss: 0.6874020248651505, Validation Loss: 0.6847202181816101\n",
      "Epoch 15: Train Loss: 0.6687350422143936, Validation Loss: 0.6853882670402527\n",
      "Epoch 16: Train Loss: 0.6769033223390579, Validation Loss: 0.6856726408004761\n",
      "Epoch 17: Train Loss: 0.7133427262306213, Validation Loss: 0.6860391497612\n",
      "Epoch 18: Train Loss: 0.7042804062366486, Validation Loss: 0.6862572431564331\n",
      "Epoch 19: Train Loss: 0.682335615158081, Validation Loss: 0.6863196492195129\n",
      "Epoch 20: Train Loss: 0.685366615653038, Validation Loss: 0.6871175765991211\n",
      "Epoch 21: Train Loss: 0.7022829949855804, Validation Loss: 0.6886611580848694\n",
      "Epoch 22: Train Loss: 0.6763238608837128, Validation Loss: 0.6879163384437561\n",
      "Epoch 23: Train Loss: 0.6243511438369751, Validation Loss: 0.685645580291748\n",
      "Epoch 24: Train Loss: 0.6986149847507477, Validation Loss: 0.6842414140701294\n",
      "Epoch 25: Train Loss: 0.709629088640213, Validation Loss: 0.6845680475234985\n",
      "Epoch 26: Train Loss: 0.6315693110227585, Validation Loss: 0.684940755367279\n",
      "Epoch 27: Train Loss: 0.6866152733564377, Validation Loss: 0.685045599937439\n",
      "Epoch 28: Train Loss: 0.6962100714445114, Validation Loss: 0.6850606799125671\n",
      "Epoch 29: Train Loss: 0.6747671216726303, Validation Loss: 0.685118556022644\n",
      "Epoch 30: Train Loss: 0.7130434066057205, Validation Loss: 0.6858537793159485\n",
      "Epoch 31: Train Loss: 0.6957346647977829, Validation Loss: 0.6859468817710876\n",
      "Epoch 32: Train Loss: 0.6588464230298996, Validation Loss: 0.6849137544631958\n",
      "Epoch 33: Train Loss: 0.6367854028940201, Validation Loss: 0.6843693852424622\n",
      "Epoch 34: Train Loss: 0.6354997456073761, Validation Loss: 0.6840609312057495\n",
      "Epoch 35: Train Loss: 0.6396193206310272, Validation Loss: 0.683702826499939\n",
      "Epoch 36: Train Loss: 0.6981272995471954, Validation Loss: 0.6839337348937988\n",
      "Epoch 37: Train Loss: 0.6190464496612549, Validation Loss: 0.6843819618225098\n",
      "Epoch 38: Train Loss: 0.6732962876558304, Validation Loss: 0.6848296523094177\n",
      "Epoch 39: Train Loss: 0.7108583301305771, Validation Loss: 0.6850720047950745\n",
      "Epoch 40: Train Loss: 0.6198719292879105, Validation Loss: 0.6907014846801758\n",
      "Epoch 41: Train Loss: 0.6634543836116791, Validation Loss: 0.6939405202865601\n",
      "Epoch 42: Train Loss: 0.6076588332653046, Validation Loss: 0.6948125958442688\n",
      "Epoch 43: Train Loss: 0.6351357996463776, Validation Loss: 0.6939321160316467\n",
      "Epoch 44: Train Loss: 0.6669226884841919, Validation Loss: 0.6939203143119812\n",
      "Epoch 45: Train Loss: 0.6477103233337402, Validation Loss: 0.6948133111000061\n",
      "Epoch 46: Train Loss: 0.5971841067075729, Validation Loss: 0.6947987675666809\n",
      "Epoch 47: Train Loss: 0.5863246321678162, Validation Loss: 0.6949100494384766\n",
      "Epoch 48: Train Loss: 0.6236085742712021, Validation Loss: 0.6950100660324097\n",
      "Epoch 49: Train Loss: 0.6724350452423096, Validation Loss: 0.6950893402099609\n",
      "Epoch 50: Train Loss: 0.6030677706003189, Validation Loss: 0.6942228674888611\n",
      "Epoch 51: Train Loss: 0.6502453237771988, Validation Loss: 0.6958504915237427\n",
      "Epoch 52: Train Loss: 0.6582602262496948, Validation Loss: 0.6957361102104187\n",
      "Epoch 53: Train Loss: 0.6517233401536942, Validation Loss: 0.6955171227455139\n",
      "Epoch 54: Train Loss: 0.6149051338434219, Validation Loss: 0.6951410174369812\n",
      "Epoch 55: Train Loss: 0.6112926006317139, Validation Loss: 0.6952800154685974\n",
      "Epoch 56: Train Loss: 0.6260401457548141, Validation Loss: 0.6944035291671753\n",
      "Epoch 57: Train Loss: 0.5901875495910645, Validation Loss: 0.6943346261978149\n",
      "Epoch 58: Train Loss: 0.6690159887075424, Validation Loss: 0.6945750117301941\n",
      "Epoch 59: Train Loss: 0.6345277577638626, Validation Loss: 0.694455087184906\n",
      "Epoch 60: Train Loss: 0.6035858988761902, Validation Loss: 0.6968830823898315\n",
      "Epoch 61: Train Loss: 0.6564987897872925, Validation Loss: 0.6950849294662476\n",
      "Epoch 62: Train Loss: 0.6075319349765778, Validation Loss: 0.6904832124710083\n",
      "Epoch 63: Train Loss: 0.5964226126670837, Validation Loss: 0.6892449855804443\n",
      "Epoch 64: Train Loss: 0.5481844246387482, Validation Loss: 0.6889860033988953\n",
      "Epoch 65: Train Loss: 0.6009100675582886, Validation Loss: 0.6894556283950806\n",
      "Epoch 66: Train Loss: 0.5741951316595078, Validation Loss: 0.6889370679855347\n",
      "Epoch 67: Train Loss: 0.6200422495603561, Validation Loss: 0.6890739798545837\n",
      "Epoch 68: Train Loss: 0.5719019323587418, Validation Loss: 0.6893464922904968\n",
      "Epoch 69: Train Loss: 0.6454717963933945, Validation Loss: 0.6893512606620789\n",
      "Epoch 70: Train Loss: 0.6142252832651138, Validation Loss: 0.6907533407211304\n",
      "Epoch 71: Train Loss: 0.6073534786701202, Validation Loss: 0.691666305065155\n",
      "Epoch 72: Train Loss: 0.5615492761135101, Validation Loss: 0.6961659789085388\n",
      "Epoch 73: Train Loss: 0.5557985156774521, Validation Loss: 0.69953453540802\n",
      "Epoch 74: Train Loss: 0.5460737943649292, Validation Loss: 0.7002546787261963\n",
      "Epoch 75: Train Loss: 0.5424079298973083, Validation Loss: 0.6996301412582397\n",
      "Epoch 76: Train Loss: 0.5734394937753677, Validation Loss: 0.6998586654663086\n",
      "Epoch 77: Train Loss: 0.5558241903781891, Validation Loss: 0.6979061365127563\n",
      "Epoch 78: Train Loss: 0.5712793171405792, Validation Loss: 0.6963647603988647\n",
      "Epoch 79: Train Loss: 0.5567673891782761, Validation Loss: 0.6961766481399536\n",
      "Epoch 80: Train Loss: 0.5080950856208801, Validation Loss: 0.6805404424667358\n",
      "Epoch 81: Train Loss: 0.5321428328752518, Validation Loss: 0.6712972521781921\n",
      "Epoch 82: Train Loss: 0.5065698400139809, Validation Loss: 0.6734459400177002\n",
      "Epoch 83: Train Loss: 0.5138919800519943, Validation Loss: 0.6770215630531311\n",
      "Epoch 84: Train Loss: 0.5474275499582291, Validation Loss: 0.6824301481246948\n",
      "Epoch 85: Train Loss: 0.4817565977573395, Validation Loss: 0.6842803359031677\n",
      "Epoch 86: Train Loss: 0.5108300521969795, Validation Loss: 0.6826267838478088\n",
      "Epoch 87: Train Loss: 0.5089950934052467, Validation Loss: 0.6807968020439148\n",
      "Epoch 88: Train Loss: 0.5145571157336235, Validation Loss: 0.6792954206466675\n",
      "Epoch 89: Train Loss: 0.47103144228458405, Validation Loss: 0.6780197024345398\n",
      "Epoch 90: Train Loss: 0.5116014704108238, Validation Loss: 0.6758368015289307\n",
      "Epoch 91: Train Loss: 0.4995420128107071, Validation Loss: 0.6744431257247925\n",
      "Epoch 92: Train Loss: 0.50940752774477, Validation Loss: 0.6800618171691895\n",
      "Epoch 93: Train Loss: 0.4654715433716774, Validation Loss: 0.6900008320808411\n",
      "Epoch 94: Train Loss: 0.47631029039621353, Validation Loss: 0.6905678510665894\n",
      "Epoch 95: Train Loss: 0.46073631942272186, Validation Loss: 0.6907238364219666\n",
      "Epoch 96: Train Loss: 0.46754202246665955, Validation Loss: 0.6844100952148438\n",
      "Epoch 97: Train Loss: 0.41566672921180725, Validation Loss: 0.6804245710372925\n",
      "Epoch 98: Train Loss: 0.4160331264138222, Validation Loss: 0.6792136430740356\n",
      "Epoch 99: Train Loss: 0.4559841677546501, Validation Loss: 0.6801144480705261\n",
      "Epoch 100: Train Loss: 0.4270373433828354, Validation Loss: 0.6703494787216187\n",
      "Epoch 101: Train Loss: 0.4209449216723442, Validation Loss: 0.6767016053199768\n",
      "Epoch 102: Train Loss: 0.42287950962781906, Validation Loss: 0.6820780634880066\n",
      "Epoch 103: Train Loss: 0.3992857038974762, Validation Loss: 0.6783642768859863\n",
      "Epoch 104: Train Loss: 0.38519180566072464, Validation Loss: 0.6756650805473328\n",
      "Epoch 105: Train Loss: 0.3824056014418602, Validation Loss: 0.6672118902206421\n",
      "Epoch 106: Train Loss: 0.3928028866648674, Validation Loss: 0.6682884097099304\n",
      "Epoch 107: Train Loss: 0.40450943261384964, Validation Loss: 0.6677399277687073\n",
      "Epoch 108: Train Loss: 0.3953622132539749, Validation Loss: 0.6673614978790283\n",
      "Epoch 109: Train Loss: 0.344317302107811, Validation Loss: 0.6678283214569092\n",
      "Epoch 110: Train Loss: 0.4004267230629921, Validation Loss: 0.6639750003814697\n",
      "Epoch 111: Train Loss: 0.3917454853653908, Validation Loss: 0.6678957939147949\n",
      "Epoch 112: Train Loss: 0.3720605596899986, Validation Loss: 0.6577848196029663\n",
      "Epoch 113: Train Loss: 0.3456113189458847, Validation Loss: 0.644555389881134\n",
      "Epoch 114: Train Loss: 0.3130558729171753, Validation Loss: 0.6498406529426575\n",
      "Epoch 115: Train Loss: 0.33223187178373337, Validation Loss: 0.6531187891960144\n",
      "Epoch 116: Train Loss: 0.3485654518008232, Validation Loss: 0.653701663017273\n",
      "Epoch 117: Train Loss: 0.33079785108566284, Validation Loss: 0.6524596810340881\n",
      "Epoch 118: Train Loss: 0.30147989839315414, Validation Loss: 0.6497841477394104\n",
      "Epoch 119: Train Loss: 0.32017693668603897, Validation Loss: 0.6491391062736511\n",
      "Epoch 120: Train Loss: 0.3187446743249893, Validation Loss: 0.6428408026695251\n",
      "Epoch 121: Train Loss: 0.3278350457549095, Validation Loss: 0.6526082158088684\n",
      "Epoch 122: Train Loss: 0.29323167353868484, Validation Loss: 0.6450712084770203\n",
      "Epoch 123: Train Loss: 0.28841182589530945, Validation Loss: 0.6166709661483765\n",
      "Epoch 124: Train Loss: 0.28854861855506897, Validation Loss: 0.6192722320556641\n",
      "Epoch 125: Train Loss: 0.2734839916229248, Validation Loss: 0.6102952361106873\n",
      "Epoch 126: Train Loss: 0.28745896369218826, Validation Loss: 0.6049671769142151\n",
      "Epoch 127: Train Loss: 0.29673902317881584, Validation Loss: 0.6024613380432129\n",
      "Epoch 128: Train Loss: 0.28926654532551765, Validation Loss: 0.6137201189994812\n",
      "Epoch 129: Train Loss: 0.24697884172201157, Validation Loss: 0.6166343688964844\n",
      "Epoch 130: Train Loss: 0.273484256118536, Validation Loss: 0.6321504712104797\n",
      "Epoch 131: Train Loss: 0.26257509738206863, Validation Loss: 0.6211642622947693\n",
      "Epoch 132: Train Loss: 0.26127275824546814, Validation Loss: 0.5949636101722717\n",
      "Epoch 133: Train Loss: 0.24092378467321396, Validation Loss: 0.5829533338546753\n",
      "Epoch 134: Train Loss: 0.2893588952720165, Validation Loss: 0.5854724645614624\n",
      "Epoch 135: Train Loss: 0.2628120370209217, Validation Loss: 0.5808301568031311\n",
      "Epoch 136: Train Loss: 0.22869637608528137, Validation Loss: 0.5630638003349304\n",
      "Epoch 137: Train Loss: 0.24302029237151146, Validation Loss: 0.5532780289649963\n",
      "Epoch 138: Train Loss: 0.22609208896756172, Validation Loss: 0.5478688478469849\n",
      "Epoch 139: Train Loss: 0.23144487664103508, Validation Loss: 0.5498510003089905\n",
      "Epoch 140: Train Loss: 0.25877050310373306, Validation Loss: 0.5462133288383484\n",
      "Epoch 141: Train Loss: 0.30287715420126915, Validation Loss: 0.5400899648666382\n",
      "Epoch 142: Train Loss: 0.2195463851094246, Validation Loss: 0.5345678329467773\n",
      "Epoch 143: Train Loss: 0.19132284820079803, Validation Loss: 0.5331618785858154\n",
      "Epoch 144: Train Loss: 0.1997339054942131, Validation Loss: 0.5409373641014099\n",
      "Epoch 145: Train Loss: 0.2114645503461361, Validation Loss: 0.5285044312477112\n",
      "Epoch 146: Train Loss: 0.1969483159482479, Validation Loss: 0.5132017135620117\n",
      "Epoch 147: Train Loss: 0.21263081580400467, Validation Loss: 0.5047016143798828\n",
      "Epoch 148: Train Loss: 0.18606673926115036, Validation Loss: 0.504972517490387\n",
      "Epoch 149: Train Loss: 0.2007674127817154, Validation Loss: 0.5044704675674438\n",
      "Epoch 150: Train Loss: 0.19326834380626678, Validation Loss: 0.4970380961894989\n",
      "Epoch 151: Train Loss: 0.16872534155845642, Validation Loss: 0.481746107339859\n",
      "Epoch 152: Train Loss: 0.20339907333254814, Validation Loss: 0.482206255197525\n",
      "Epoch 153: Train Loss: 0.18533315882086754, Validation Loss: 0.4879510700702667\n",
      "Epoch 154: Train Loss: 0.1708652526140213, Validation Loss: 0.4778563678264618\n",
      "Epoch 155: Train Loss: 0.18151113018393517, Validation Loss: 0.45405128598213196\n",
      "Epoch 156: Train Loss: 0.20251548290252686, Validation Loss: 0.44164684414863586\n",
      "Epoch 157: Train Loss: 0.17273887619376183, Validation Loss: 0.43390947580337524\n",
      "Epoch 158: Train Loss: 0.16361957974731922, Validation Loss: 0.43139201402664185\n",
      "Epoch 159: Train Loss: 0.16512712091207504, Validation Loss: 0.43096625804901123\n",
      "Epoch 160: Train Loss: 0.1841747872531414, Validation Loss: 0.44799917936325073\n",
      "Epoch 161: Train Loss: 0.16501900181174278, Validation Loss: 0.4580093324184418\n",
      "Epoch 162: Train Loss: 0.16203391551971436, Validation Loss: 0.43334901332855225\n",
      "Epoch 163: Train Loss: 0.12076287344098091, Validation Loss: 0.4086734652519226\n",
      "Epoch 164: Train Loss: 0.14895451068878174, Validation Loss: 0.3952135145664215\n",
      "Epoch 165: Train Loss: 0.18091382458806038, Validation Loss: 0.3887224793434143\n",
      "Epoch 166: Train Loss: 0.15941409766674042, Validation Loss: 0.39670342206954956\n",
      "Epoch 167: Train Loss: 0.14950914680957794, Validation Loss: 0.4005928635597229\n",
      "Epoch 168: Train Loss: 0.16078706458210945, Validation Loss: 0.39794397354125977\n",
      "Epoch 169: Train Loss: 0.17441870644688606, Validation Loss: 0.39710280299186707\n",
      "Epoch 170: Train Loss: 0.15220387652516365, Validation Loss: 0.39013373851776123\n",
      "Epoch 171: Train Loss: 0.13479327131062746, Validation Loss: 0.37966763973236084\n",
      "Epoch 172: Train Loss: 0.14707091636955738, Validation Loss: 0.3702102303504944\n",
      "Epoch 173: Train Loss: 0.1438841838389635, Validation Loss: 0.37201136350631714\n",
      "Epoch 174: Train Loss: 0.12793352454900742, Validation Loss: 0.3685140311717987\n",
      "Epoch 175: Train Loss: 0.13663440942764282, Validation Loss: 0.3667973577976227\n",
      "Epoch 176: Train Loss: 0.12836227752268314, Validation Loss: 0.36886849999427795\n",
      "Epoch 177: Train Loss: 0.15698372572660446, Validation Loss: 0.3733714818954468\n",
      "Epoch 178: Train Loss: 0.14957738853991032, Validation Loss: 0.37190377712249756\n",
      "Epoch 179: Train Loss: 0.14377324655652046, Validation Loss: 0.3652629554271698\n",
      "Epoch 180: Train Loss: 0.1574534848332405, Validation Loss: 0.341732919216156\n",
      "Epoch 181: Train Loss: 0.13852132111787796, Validation Loss: 0.34840473532676697\n",
      "Epoch 182: Train Loss: 0.1486659087240696, Validation Loss: 0.35742759704589844\n",
      "Epoch 183: Train Loss: 0.1643400639295578, Validation Loss: 0.31880590319633484\n",
      "Epoch 184: Train Loss: 0.12766328733414412, Validation Loss: 0.2967173457145691\n",
      "Epoch 185: Train Loss: 0.1265092883259058, Validation Loss: 0.29809436202049255\n",
      "Epoch 186: Train Loss: 0.12586872652173042, Validation Loss: 0.3087441921234131\n",
      "Epoch 187: Train Loss: 0.10523861274123192, Validation Loss: 0.3151046633720398\n",
      "Epoch 188: Train Loss: 0.10939237475395203, Validation Loss: 0.32022860646247864\n",
      "Epoch 189: Train Loss: 0.13934053294360638, Validation Loss: 0.3219200372695923\n",
      "Epoch 190: Train Loss: 0.11602707579731941, Validation Loss: 0.3172847032546997\n",
      "Epoch 191: Train Loss: 0.12426658533513546, Validation Loss: 0.3071296513080597\n",
      "Epoch 192: Train Loss: 0.1185560617595911, Validation Loss: 0.29595956206321716\n",
      "Epoch 193: Train Loss: 0.08865598309785128, Validation Loss: 0.28957968950271606\n",
      "Epoch 194: Train Loss: 0.1225605420768261, Validation Loss: 0.27993181347846985\n",
      "Epoch 195: Train Loss: 0.11660769209265709, Validation Loss: 0.2751206159591675\n",
      "Epoch 196: Train Loss: 0.11124810390174389, Validation Loss: 0.27113547921180725\n",
      "Epoch 197: Train Loss: 0.11153810936957598, Validation Loss: 0.27163150906562805\n",
      "Epoch 198: Train Loss: 0.0987215805798769, Validation Loss: 0.27203965187072754\n",
      "Epoch 199: Train Loss: 0.1192008275538683, Validation Loss: 0.27473461627960205\n",
      "Epoch 200: Train Loss: 0.12037985399365425, Validation Loss: 0.2661164104938507\n",
      "Epoch 201: Train Loss: 0.11427561193704605, Validation Loss: 0.2617519199848175\n",
      "Epoch 202: Train Loss: 0.11953559704124928, Validation Loss: 0.26562750339508057\n",
      "Epoch 203: Train Loss: 0.11495077982544899, Validation Loss: 0.2724882662296295\n",
      "Epoch 204: Train Loss: 0.11486996058374643, Validation Loss: 0.27258428931236267\n",
      "Epoch 205: Train Loss: 0.1597144789993763, Validation Loss: 0.2626902759075165\n",
      "Epoch 206: Train Loss: 0.11821913532912731, Validation Loss: 0.26427343487739563\n",
      "Epoch 207: Train Loss: 0.09563428349792957, Validation Loss: 0.27288275957107544\n",
      "Epoch 208: Train Loss: 0.11709573306143284, Validation Loss: 0.2786908447742462\n",
      "Epoch 209: Train Loss: 0.12913137674331665, Validation Loss: 0.2767115533351898\n",
      "Epoch 210: Train Loss: 0.11171432211995125, Validation Loss: 0.2634473145008087\n",
      "Epoch 211: Train Loss: 0.11164929904043674, Validation Loss: 0.2559387981891632\n",
      "Epoch 212: Train Loss: 0.11240098159760237, Validation Loss: 0.26466113328933716\n",
      "Epoch 213: Train Loss: 0.178397367708385, Validation Loss: 0.25976136326789856\n",
      "Epoch 214: Train Loss: 0.19334775768220425, Validation Loss: 0.2820269465446472\n",
      "Epoch 215: Train Loss: 0.12842063419520855, Validation Loss: 0.27952054142951965\n",
      "Epoch 216: Train Loss: 0.11629845388233662, Validation Loss: 0.2806037366390228\n",
      "Epoch 217: Train Loss: 0.11978440545499325, Validation Loss: 0.2818416953086853\n",
      "Epoch 218: Train Loss: 0.10437404550611973, Validation Loss: 0.2851286828517914\n",
      "Epoch 219: Train Loss: 0.12008076347410679, Validation Loss: 0.2853141129016876\n",
      "Epoch 220: Train Loss: 0.10482896119356155, Validation Loss: 0.2818554937839508\n",
      "Epoch 221: Train Loss: 0.11357472278177738, Validation Loss: 0.27462688088417053\n",
      "Epoch 222: Train Loss: 0.09520548395812511, Validation Loss: 0.24912405014038086\n",
      "Epoch 223: Train Loss: 0.12517863139510155, Validation Loss: 0.22762450575828552\n",
      "Epoch 224: Train Loss: 0.13683022372424603, Validation Loss: 0.21356181800365448\n",
      "Epoch 225: Train Loss: 0.1126484889537096, Validation Loss: 0.21626995503902435\n",
      "Epoch 226: Train Loss: 0.10009337402880192, Validation Loss: 0.2218702882528305\n",
      "Epoch 227: Train Loss: 0.0959028322249651, Validation Loss: 0.22285184264183044\n",
      "Epoch 228: Train Loss: 0.07561278715729713, Validation Loss: 0.22088541090488434\n",
      "Epoch 229: Train Loss: 0.11173703335225582, Validation Loss: 0.2196471244096756\n",
      "Epoch 230: Train Loss: 0.0900907926261425, Validation Loss: 0.2259950488805771\n",
      "Epoch 231: Train Loss: 0.12736629880964756, Validation Loss: 0.23565791547298431\n",
      "Epoch 232: Train Loss: 0.10959750041365623, Validation Loss: 0.2370082437992096\n",
      "Epoch 233: Train Loss: 0.0916664395481348, Validation Loss: 0.23465286195278168\n",
      "Epoch 234: Train Loss: 0.08879212569445372, Validation Loss: 0.23316402733325958\n",
      "Epoch 235: Train Loss: 0.09624218940734863, Validation Loss: 0.23020397126674652\n",
      "Epoch 236: Train Loss: 0.11097938567399979, Validation Loss: 0.22475843131542206\n",
      "Epoch 237: Train Loss: 0.09467005543410778, Validation Loss: 0.21897287666797638\n",
      "Epoch 238: Train Loss: 0.08967545442283154, Validation Loss: 0.21442359685897827\n",
      "Epoch 239: Train Loss: 0.0810336759313941, Validation Loss: 0.2125340849161148\n",
      "Epoch 240: Train Loss: 0.07970810495316982, Validation Loss: 0.2042495608329773\n",
      "Epoch 241: Train Loss: 0.1035740040242672, Validation Loss: 0.20863135159015656\n",
      "Epoch 242: Train Loss: 0.09072683285921812, Validation Loss: 0.20069460570812225\n",
      "Epoch 243: Train Loss: 0.09552539605647326, Validation Loss: 0.19581225514411926\n",
      "Epoch 244: Train Loss: 0.08301117178052664, Validation Loss: 0.1935248225927353\n",
      "Epoch 245: Train Loss: 0.08704360015690327, Validation Loss: 0.1932588815689087\n",
      "Epoch 246: Train Loss: 0.0986465010792017, Validation Loss: 0.19374392926692963\n",
      "Epoch 247: Train Loss: 0.0779968025162816, Validation Loss: 0.1959116905927658\n",
      "Epoch 248: Train Loss: 0.11394120752811432, Validation Loss: 0.197021484375\n",
      "Epoch 249: Train Loss: 0.10499158361926675, Validation Loss: 0.19700172543525696\n",
      "Epoch 250: Train Loss: 0.10149694327265024, Validation Loss: 0.1803627908229828\n",
      "Epoch 251: Train Loss: 0.10011523962020874, Validation Loss: 0.16924385726451874\n",
      "Epoch 252: Train Loss: 0.11583421938121319, Validation Loss: 0.17405472695827484\n",
      "Epoch 253: Train Loss: 0.08899382129311562, Validation Loss: 0.17989714443683624\n",
      "Epoch 254: Train Loss: 0.09670007042586803, Validation Loss: 0.18202745914459229\n",
      "Epoch 255: Train Loss: 0.09254665020853281, Validation Loss: 0.17521357536315918\n",
      "Epoch 256: Train Loss: 0.1352297067642212, Validation Loss: 0.16905581951141357\n",
      "Epoch 257: Train Loss: 0.0778180779889226, Validation Loss: 0.16402457654476166\n",
      "Epoch 258: Train Loss: 0.09615575522184372, Validation Loss: 0.16449598968029022\n",
      "Epoch 259: Train Loss: 0.09623981267213821, Validation Loss: 0.16364647448062897\n",
      "Epoch 260: Train Loss: 0.09092869516462088, Validation Loss: 0.18128950893878937\n",
      "Epoch 261: Train Loss: 0.1195718226954341, Validation Loss: 0.1962805539369583\n",
      "Epoch 262: Train Loss: 0.079282539896667, Validation Loss: 0.20815077424049377\n",
      "Epoch 263: Train Loss: 0.12245448492467403, Validation Loss: 0.20230591297149658\n",
      "Epoch 264: Train Loss: 0.09063239395618439, Validation Loss: 0.18427567183971405\n",
      "Epoch 265: Train Loss: 0.09737084154039621, Validation Loss: 0.16808903217315674\n",
      "Epoch 266: Train Loss: 0.09502199850976467, Validation Loss: 0.1662769317626953\n",
      "Epoch 267: Train Loss: 0.06967080663889647, Validation Loss: 0.16727985441684723\n",
      "Epoch 268: Train Loss: 0.06715791765600443, Validation Loss: 0.1682746857404709\n",
      "Epoch 269: Train Loss: 0.0833702995441854, Validation Loss: 0.16848306357860565\n",
      "Epoch 270: Train Loss: 0.09323153272271156, Validation Loss: 0.17064248025417328\n",
      "Epoch 271: Train Loss: 0.09050689823925495, Validation Loss: 0.17486761510372162\n",
      "Epoch 272: Train Loss: 0.08037947444245219, Validation Loss: 0.17456400394439697\n",
      "Epoch 273: Train Loss: 0.08477063849568367, Validation Loss: 0.16698788106441498\n",
      "Epoch 274: Train Loss: 0.12466136924922466, Validation Loss: 0.16581487655639648\n",
      "Epoch 275: Train Loss: 0.09185953065752983, Validation Loss: 0.16800694167613983\n",
      "Epoch 276: Train Loss: 0.09768055658787489, Validation Loss: 0.1669939011335373\n",
      "Epoch 277: Train Loss: 0.07769434433430433, Validation Loss: 0.1678837388753891\n",
      "Epoch 278: Train Loss: 0.08216229686513543, Validation Loss: 0.16826674342155457\n",
      "Epoch 279: Train Loss: 0.086770533118397, Validation Loss: 0.1707436740398407\n",
      "Epoch 280: Train Loss: 0.09017031081020832, Validation Loss: 0.15872222185134888\n",
      "Epoch 281: Train Loss: 0.08198499865829945, Validation Loss: 0.14421863853931427\n",
      "Epoch 282: Train Loss: 0.08052876405417919, Validation Loss: 0.1409374177455902\n",
      "Epoch 283: Train Loss: 0.08133437391370535, Validation Loss: 0.13766726851463318\n",
      "Epoch 284: Train Loss: 0.09639671351760626, Validation Loss: 0.14355234801769257\n",
      "Epoch 285: Train Loss: 0.1155593041330576, Validation Loss: 0.15879598259925842\n",
      "Epoch 286: Train Loss: 0.08278472535312176, Validation Loss: 0.16581620275974274\n",
      "Epoch 287: Train Loss: 0.0922977477312088, Validation Loss: 0.166176438331604\n",
      "Epoch 288: Train Loss: 0.10524944495409727, Validation Loss: 0.16713452339172363\n",
      "Epoch 289: Train Loss: 0.12579483492299914, Validation Loss: 0.16783539950847626\n",
      "Epoch 290: Train Loss: 0.10846321284770966, Validation Loss: 0.1556955724954605\n",
      "Epoch 291: Train Loss: 0.10226860549300909, Validation Loss: 0.14157794415950775\n",
      "Epoch 292: Train Loss: 0.10226600989699364, Validation Loss: 0.13688617944717407\n",
      "Epoch 293: Train Loss: 0.08611003402620554, Validation Loss: 0.15173891186714172\n",
      "Epoch 294: Train Loss: 0.11442132107913494, Validation Loss: 0.15117622911930084\n",
      "Epoch 295: Train Loss: 0.09934280719608068, Validation Loss: 0.1498252898454666\n",
      "Epoch 296: Train Loss: 0.08620900567620993, Validation Loss: 0.1520642638206482\n",
      "Epoch 297: Train Loss: 0.11127209197729826, Validation Loss: 0.15183418989181519\n",
      "Epoch 298: Train Loss: 0.09506868710741401, Validation Loss: 0.1500597596168518\n",
      "Epoch 299: Train Loss: 0.07420512475073338, Validation Loss: 0.14975087344646454\n",
      "Epoch 300: Train Loss: 0.08621385041624308, Validation Loss: 0.13801196217536926\n",
      "Epoch 301: Train Loss: 0.09492494259029627, Validation Loss: 0.13509303331375122\n",
      "Epoch 302: Train Loss: 0.10151132615283132, Validation Loss: 0.14225605130195618\n",
      "Epoch 303: Train Loss: 0.09328415617346764, Validation Loss: 0.14885570108890533\n",
      "Epoch 304: Train Loss: 0.07162688300013542, Validation Loss: 0.15086926519870758\n",
      "Epoch 305: Train Loss: 0.08848512638360262, Validation Loss: 0.14479118585586548\n",
      "Epoch 306: Train Loss: 0.12885939166881144, Validation Loss: 0.141798734664917\n",
      "Epoch 307: Train Loss: 0.10507766949012876, Validation Loss: 0.13766783475875854\n",
      "Epoch 308: Train Loss: 0.07830928359180689, Validation Loss: 0.13831061124801636\n",
      "Epoch 309: Train Loss: 0.07887422991916537, Validation Loss: 0.14018279314041138\n",
      "Epoch 310: Train Loss: 0.0981308575719595, Validation Loss: 0.15031950175762177\n",
      "Epoch 311: Train Loss: 0.0994312483817339, Validation Loss: 0.15083488821983337\n",
      "Epoch 312: Train Loss: 0.07456233631819487, Validation Loss: 0.14559298753738403\n",
      "Epoch 313: Train Loss: 0.09174948930740356, Validation Loss: 0.13703636825084686\n",
      "Epoch 314: Train Loss: 0.0815770011395216, Validation Loss: 0.13154415786266327\n",
      "Epoch 315: Train Loss: 0.06078871060162783, Validation Loss: 0.13365080952644348\n",
      "Epoch 316: Train Loss: 0.07958857389166951, Validation Loss: 0.12928490340709686\n",
      "Epoch 317: Train Loss: 0.07376672606915236, Validation Loss: 0.12841840088367462\n",
      "Epoch 318: Train Loss: 0.07654005195945501, Validation Loss: 0.12495002150535583\n",
      "Epoch 319: Train Loss: 0.08809066750109196, Validation Loss: 0.12187355011701584\n",
      "Epoch 320: Train Loss: 0.11959824897348881, Validation Loss: 0.11918865889310837\n",
      "Epoch 321: Train Loss: 0.07801766600459814, Validation Loss: 0.12330019474029541\n",
      "Epoch 322: Train Loss: 0.0524407709017396, Validation Loss: 0.11846756190061569\n",
      "Epoch 323: Train Loss: 0.10725926421582699, Validation Loss: 0.11850288510322571\n",
      "Epoch 324: Train Loss: 0.08758183103054762, Validation Loss: 0.11238495260477066\n",
      "Epoch 325: Train Loss: 0.08542269421741366, Validation Loss: 0.11166144907474518\n",
      "Epoch 326: Train Loss: 0.08863160293549299, Validation Loss: 0.11235035955905914\n",
      "Epoch 327: Train Loss: 0.09501136653125286, Validation Loss: 0.11169814318418503\n",
      "Epoch 328: Train Loss: 0.07319179363548756, Validation Loss: 0.11450836062431335\n",
      "Epoch 329: Train Loss: 0.11316687613725662, Validation Loss: 0.11411997675895691\n",
      "Epoch 330: Train Loss: 0.07920177560299635, Validation Loss: 0.12447089701890945\n",
      "Epoch 331: Train Loss: 0.08801531698554754, Validation Loss: 0.12440907210111618\n",
      "Epoch 332: Train Loss: 0.08953697700053453, Validation Loss: 0.12551747262477875\n",
      "Epoch 333: Train Loss: 0.09288032911717892, Validation Loss: 0.11726577579975128\n",
      "Epoch 334: Train Loss: 0.08348886761814356, Validation Loss: 0.11795282363891602\n",
      "Epoch 335: Train Loss: 0.08543794881552458, Validation Loss: 0.1169692724943161\n",
      "Epoch 336: Train Loss: 0.0891398498788476, Validation Loss: 0.11046986281871796\n",
      "Epoch 337: Train Loss: 0.09875488094985485, Validation Loss: 0.10956408828496933\n",
      "Epoch 338: Train Loss: 0.08535406738519669, Validation Loss: 0.10746712237596512\n",
      "Epoch 339: Train Loss: 0.08651435235515237, Validation Loss: 0.10480008274316788\n",
      "Epoch 340: Train Loss: 0.09710984025150537, Validation Loss: 0.12010469287633896\n",
      "Epoch 341: Train Loss: 0.07217823015525937, Validation Loss: 0.1291215568780899\n",
      "Epoch 342: Train Loss: 0.09366016089916229, Validation Loss: 0.13086339831352234\n",
      "Epoch 343: Train Loss: 0.09096227586269379, Validation Loss: 0.13124404847621918\n",
      "Epoch 344: Train Loss: 0.07909495569765568, Validation Loss: 0.13419577479362488\n",
      "Epoch 345: Train Loss: 0.07009516283869743, Validation Loss: 0.1339358240365982\n",
      "Epoch 346: Train Loss: 0.08914798311889172, Validation Loss: 0.13546928763389587\n",
      "Epoch 347: Train Loss: 0.07782305590808392, Validation Loss: 0.13109076023101807\n",
      "Epoch 348: Train Loss: 0.07178435940295458, Validation Loss: 0.131389781832695\n",
      "Epoch 349: Train Loss: 0.07572200708091259, Validation Loss: 0.12967832386493683\n",
      "Epoch 350: Train Loss: 0.06967532448470592, Validation Loss: 0.11399140954017639\n",
      "Epoch 351: Train Loss: 0.10597831010818481, Validation Loss: 0.10379204899072647\n",
      "Epoch 352: Train Loss: 0.08716864697635174, Validation Loss: 0.10612576454877853\n",
      "Epoch 353: Train Loss: 0.08764700125902891, Validation Loss: 0.11114861071109772\n",
      "Epoch 354: Train Loss: 0.1327370647341013, Validation Loss: 0.1054399237036705\n",
      "Epoch 355: Train Loss: 0.09466387145221233, Validation Loss: 0.10619832575321198\n",
      "Epoch 356: Train Loss: 0.10279454290866852, Validation Loss: 0.10715155303478241\n",
      "Epoch 357: Train Loss: 0.0834095785394311, Validation Loss: 0.10630133002996445\n",
      "Epoch 358: Train Loss: 0.07636905135586858, Validation Loss: 0.10818631947040558\n",
      "Epoch 359: Train Loss: 0.058354346081614494, Validation Loss: 0.10821520537137985\n",
      "Epoch 360: Train Loss: 0.07876336667686701, Validation Loss: 0.11969515681266785\n",
      "Epoch 361: Train Loss: 0.11487200390547514, Validation Loss: 0.13225214183330536\n",
      "Epoch 362: Train Loss: 0.06598837347701192, Validation Loss: 0.13664624094963074\n",
      "Epoch 363: Train Loss: 0.0970187857747078, Validation Loss: 0.1404394507408142\n",
      "Epoch 364: Train Loss: 0.06779554998502135, Validation Loss: 0.1320965439081192\n",
      "Epoch 365: Train Loss: 0.08214269764721394, Validation Loss: 0.12237101048231125\n",
      "Epoch 366: Train Loss: 0.07547084148973227, Validation Loss: 0.11788766086101532\n",
      "Epoch 367: Train Loss: 0.08287137746810913, Validation Loss: 0.11501666903495789\n",
      "Epoch 368: Train Loss: 0.08311790600419044, Validation Loss: 0.11229941993951797\n",
      "Epoch 369: Train Loss: 0.09734068904072046, Validation Loss: 0.11389279365539551\n",
      "Epoch 370: Train Loss: 0.09439077600836754, Validation Loss: 0.10276781767606735\n",
      "Epoch 371: Train Loss: 0.11208733730018139, Validation Loss: 0.09702054411172867\n",
      "Epoch 372: Train Loss: 0.0774214444682002, Validation Loss: 0.09986670315265656\n",
      "Epoch 373: Train Loss: 0.10906465724110603, Validation Loss: 0.09600435942411423\n",
      "Epoch 374: Train Loss: 0.09914953587576747, Validation Loss: 0.10184469074010849\n",
      "Epoch 375: Train Loss: 0.08360095508396626, Validation Loss: 0.1106543019413948\n",
      "Epoch 376: Train Loss: 0.08170243818312883, Validation Loss: 0.1132863238453865\n",
      "Epoch 377: Train Loss: 0.07491430081427097, Validation Loss: 0.11291108280420303\n",
      "Epoch 378: Train Loss: 0.06870207749307156, Validation Loss: 0.11123926937580109\n",
      "Epoch 379: Train Loss: 0.10186238586902618, Validation Loss: 0.10844235122203827\n",
      "Epoch 380: Train Loss: 0.060484230518341064, Validation Loss: 0.11795444786548615\n",
      "Epoch 381: Train Loss: 0.07951435260474682, Validation Loss: 0.12637172639369965\n",
      "Epoch 382: Train Loss: 0.09785277443006635, Validation Loss: 0.12256906181573868\n",
      "Epoch 383: Train Loss: 0.050620857160538435, Validation Loss: 0.12025492638349533\n",
      "Epoch 384: Train Loss: 0.08810227736830711, Validation Loss: 0.12157197296619415\n",
      "Epoch 385: Train Loss: 0.07885390054434538, Validation Loss: 0.12470109015703201\n",
      "Epoch 386: Train Loss: 0.08240808034315705, Validation Loss: 0.12481024116277695\n",
      "Epoch 387: Train Loss: 0.07103452470619231, Validation Loss: 0.12324874103069305\n",
      "Epoch 388: Train Loss: 0.08243438694626093, Validation Loss: 0.1197262778878212\n",
      "Epoch 389: Train Loss: 0.06500031426548958, Validation Loss: 0.11506295204162598\n",
      "Epoch 390: Train Loss: 0.09612113982439041, Validation Loss: 0.10989904403686523\n",
      "Epoch 391: Train Loss: 0.09164407290518284, Validation Loss: 0.10269524902105331\n",
      "Epoch 392: Train Loss: 0.11270129820331931, Validation Loss: 0.09591693431138992\n",
      "Epoch 393: Train Loss: 0.10230520740151405, Validation Loss: 0.09323272854089737\n",
      "Epoch 394: Train Loss: 0.12185335252434015, Validation Loss: 0.09216117113828659\n",
      "Epoch 395: Train Loss: 0.09389178943820298, Validation Loss: 0.09745416045188904\n",
      "Epoch 396: Train Loss: 0.07132584601640701, Validation Loss: 0.09683622419834137\n",
      "Epoch 397: Train Loss: 0.06944152433425188, Validation Loss: 0.09674607962369919\n",
      "Epoch 398: Train Loss: 0.0725022261030972, Validation Loss: 0.09799685329198837\n",
      "Epoch 399: Train Loss: 0.059052945114672184, Validation Loss: 0.09690097719430923\n",
      "Epoch 400: Train Loss: 0.08362611941993237, Validation Loss: 0.08675677329301834\n",
      "Epoch 401: Train Loss: 0.08614056184887886, Validation Loss: 0.09872600436210632\n",
      "Epoch 402: Train Loss: 0.09647339768707752, Validation Loss: 0.11138303577899933\n",
      "Epoch 403: Train Loss: 0.1056905654259026, Validation Loss: 0.11943409591913223\n",
      "Epoch 404: Train Loss: 0.09834790043532848, Validation Loss: 0.11409435421228409\n",
      "Epoch 405: Train Loss: 0.08724551019258797, Validation Loss: 0.1141781359910965\n",
      "Epoch 406: Train Loss: 0.0704532852396369, Validation Loss: 0.11675918102264404\n",
      "Epoch 407: Train Loss: 0.06959617813117802, Validation Loss: 0.11879036575555801\n",
      "Epoch 408: Train Loss: 0.08047156129032373, Validation Loss: 0.12048981338739395\n",
      "Epoch 409: Train Loss: 0.08058280777186155, Validation Loss: 0.11816863715648651\n",
      "Epoch 410: Train Loss: 0.09326634556055069, Validation Loss: 0.11532953381538391\n",
      "Epoch 411: Train Loss: 0.0741576892323792, Validation Loss: 0.11574454605579376\n",
      "Epoch 412: Train Loss: 0.07342193275690079, Validation Loss: 0.11298935860395432\n",
      "Epoch 413: Train Loss: 0.09364749863743782, Validation Loss: 0.09938321262598038\n",
      "Epoch 414: Train Loss: 0.07405703887343407, Validation Loss: 0.09138285368680954\n",
      "Epoch 415: Train Loss: 0.06639451906085014, Validation Loss: 0.09122838824987411\n",
      "Epoch 416: Train Loss: 0.09166620345786214, Validation Loss: 0.09460074454545975\n",
      "Epoch 417: Train Loss: 0.070334960706532, Validation Loss: 0.09967586398124695\n",
      "Epoch 418: Train Loss: 0.06462021544575691, Validation Loss: 0.10306105762720108\n",
      "Epoch 419: Train Loss: 0.05368427373468876, Validation Loss: 0.10182887315750122\n",
      "Epoch 420: Train Loss: 0.08781976997852325, Validation Loss: 0.0969814732670784\n",
      "Epoch 421: Train Loss: 0.06462735030800104, Validation Loss: 0.09393420070409775\n",
      "Epoch 422: Train Loss: 0.09114340227097273, Validation Loss: 0.0976928249001503\n",
      "Epoch 423: Train Loss: 0.0692079272121191, Validation Loss: 0.1158500611782074\n",
      "Epoch 424: Train Loss: 0.10442798491567373, Validation Loss: 0.12398136407136917\n",
      "Epoch 425: Train Loss: 0.08346863882616162, Validation Loss: 0.11926917731761932\n",
      "Epoch 426: Train Loss: 0.07684900052845478, Validation Loss: 0.11641149967908859\n",
      "Epoch 427: Train Loss: 0.09955783002078533, Validation Loss: 0.11346819996833801\n",
      "Epoch 428: Train Loss: 0.05148504674434662, Validation Loss: 0.11337599158287048\n",
      "Epoch 429: Train Loss: 0.06336124474182725, Validation Loss: 0.11000094562768936\n",
      "Epoch 430: Train Loss: 0.06635687756352127, Validation Loss: 0.10757435858249664\n",
      "Epoch 431: Train Loss: 0.08455415582284331, Validation Loss: 0.09681916236877441\n",
      "Epoch 432: Train Loss: 0.08421317534521222, Validation Loss: 0.08721707761287689\n",
      "Epoch 433: Train Loss: 0.07499458105303347, Validation Loss: 0.08681493997573853\n",
      "Epoch 434: Train Loss: 0.07215531449764967, Validation Loss: 0.08917789161205292\n",
      "Epoch 435: Train Loss: 0.11047963378950953, Validation Loss: 0.09733710438013077\n",
      "Epoch 436: Train Loss: 0.08008256880566478, Validation Loss: 0.10277513414621353\n",
      "Epoch 437: Train Loss: 0.13866394478827715, Validation Loss: 0.10408007353544235\n",
      "Epoch 438: Train Loss: 0.09798422921448946, Validation Loss: 0.10631906986236572\n",
      "Epoch 439: Train Loss: 0.08188226167112589, Validation Loss: 0.10581688582897186\n",
      "Epoch 440: Train Loss: 0.0630780253559351, Validation Loss: 0.10106629133224487\n",
      "Epoch 441: Train Loss: 0.07601495273411274, Validation Loss: 0.10026101768016815\n",
      "Epoch 442: Train Loss: 0.0798838259652257, Validation Loss: 0.0984581857919693\n",
      "Epoch 443: Train Loss: 0.062111853156238794, Validation Loss: 0.10060347616672516\n",
      "Epoch 444: Train Loss: 0.08131013298407197, Validation Loss: 0.09375731647014618\n",
      "Epoch 445: Train Loss: 0.07134720450267196, Validation Loss: 0.08657071739435196\n",
      "Epoch 446: Train Loss: 0.07401963043957949, Validation Loss: 0.08368980884552002\n",
      "Epoch 447: Train Loss: 0.0780771307181567, Validation Loss: 0.0857616737484932\n",
      "Epoch 448: Train Loss: 0.06273187696933746, Validation Loss: 0.08344531059265137\n",
      "Epoch 449: Train Loss: 0.0758371907286346, Validation Loss: 0.08232797682285309\n",
      "Epoch 450: Train Loss: 0.07316734548658133, Validation Loss: 0.08671965450048447\n",
      "Epoch 451: Train Loss: 0.08734660316258669, Validation Loss: 0.07725214958190918\n",
      "Epoch 452: Train Loss: 0.062263866886496544, Validation Loss: 0.07214652001857758\n",
      "Epoch 453: Train Loss: 0.06377284694463015, Validation Loss: 0.07133642584085464\n",
      "Epoch 454: Train Loss: 0.07648356072604656, Validation Loss: 0.07558886706829071\n",
      "Epoch 455: Train Loss: 0.08979715779423714, Validation Loss: 0.07789693772792816\n",
      "Epoch 456: Train Loss: 0.04797720583155751, Validation Loss: 0.07787521183490753\n",
      "Epoch 457: Train Loss: 0.06995240692049265, Validation Loss: 0.07639654725790024\n",
      "Epoch 458: Train Loss: 0.05837978608906269, Validation Loss: 0.0757993757724762\n",
      "Epoch 459: Train Loss: 0.0779607817530632, Validation Loss: 0.07515415549278259\n",
      "Epoch 460: Train Loss: 0.06764066498726606, Validation Loss: 0.07050073891878128\n",
      "Epoch 461: Train Loss: 0.07367182429879904, Validation Loss: 0.07177785038948059\n",
      "Epoch 462: Train Loss: 0.08129970449954271, Validation Loss: 0.07044143229722977\n",
      "Epoch 463: Train Loss: 0.06065668445080519, Validation Loss: 0.07020766288042068\n",
      "Epoch 464: Train Loss: 0.06915545091032982, Validation Loss: 0.07820571213960648\n",
      "Epoch 465: Train Loss: 0.07270842138677835, Validation Loss: 0.0745135173201561\n",
      "Epoch 466: Train Loss: 0.06120416847988963, Validation Loss: 0.07037298381328583\n",
      "Epoch 467: Train Loss: 0.08265489339828491, Validation Loss: 0.07117371261119843\n",
      "Epoch 468: Train Loss: 0.059674459509551525, Validation Loss: 0.06950318813323975\n",
      "Epoch 469: Train Loss: 0.07314572110772133, Validation Loss: 0.06962995231151581\n",
      "Epoch 470: Train Loss: 0.07179054617881775, Validation Loss: 0.0764189288020134\n",
      "Epoch 471: Train Loss: 0.11179700866341591, Validation Loss: 0.08095181733369827\n",
      "Epoch 472: Train Loss: 0.06448646821081638, Validation Loss: 0.08288919925689697\n",
      "Epoch 473: Train Loss: 0.08170560095459223, Validation Loss: 0.0846390351653099\n",
      "Epoch 474: Train Loss: 0.061212376691401005, Validation Loss: 0.08437148481607437\n",
      "Epoch 475: Train Loss: 0.07066130824387074, Validation Loss: 0.08245772123336792\n",
      "Epoch 476: Train Loss: 0.07218643929809332, Validation Loss: 0.08020951598882675\n",
      "Epoch 477: Train Loss: 0.09806947538163513, Validation Loss: 0.07820523530244827\n",
      "Epoch 478: Train Loss: 0.07607145165093243, Validation Loss: 0.07510965317487717\n",
      "Epoch 479: Train Loss: 0.08693115273490548, Validation Loss: 0.07500479370355606\n",
      "Epoch 480: Train Loss: 0.05021256837062538, Validation Loss: 0.07146614044904709\n",
      "Epoch 481: Train Loss: 0.08369785081595182, Validation Loss: 0.06807649880647659\n",
      "Epoch 482: Train Loss: 0.07363880565389991, Validation Loss: 0.06913258880376816\n",
      "Epoch 483: Train Loss: 0.07939661713317037, Validation Loss: 0.06154751405119896\n",
      "Epoch 484: Train Loss: 0.07586589688435197, Validation Loss: 0.056586600840091705\n",
      "Epoch 485: Train Loss: 0.06762036588042974, Validation Loss: 0.051928240805864334\n",
      "Epoch 486: Train Loss: 0.08965323213487864, Validation Loss: 0.04849826171994209\n",
      "Epoch 487: Train Loss: 0.06623576115816832, Validation Loss: 0.04828934371471405\n",
      "Epoch 488: Train Loss: 0.09297887980937958, Validation Loss: 0.048491429537534714\n",
      "Epoch 489: Train Loss: 0.06667876383289695, Validation Loss: 0.04855084419250488\n",
      "Epoch 490: Train Loss: 0.0847049062140286, Validation Loss: 0.05196931213140488\n",
      "Epoch 491: Train Loss: 0.07938686991110444, Validation Loss: 0.0536348782479763\n",
      "Epoch 492: Train Loss: 0.05493475683033466, Validation Loss: 0.05426521971821785\n",
      "Epoch 493: Train Loss: 0.11746396869421005, Validation Loss: 0.05582128465175629\n",
      "Epoch 494: Train Loss: 0.06805678620003164, Validation Loss: 0.06074977293610573\n",
      "Epoch 495: Train Loss: 0.06681665685027838, Validation Loss: 0.06276462227106094\n",
      "Epoch 496: Train Loss: 0.07712358608841896, Validation Loss: 0.06613241136074066\n",
      "Epoch 497: Train Loss: 0.07309628184884787, Validation Loss: 0.06490189582109451\n",
      "Epoch 498: Train Loss: 0.07526240893639624, Validation Loss: 0.06484261155128479\n",
      "Epoch 499: Train Loss: 0.06962504074908793, Validation Loss: 0.06809578835964203\n",
      "Completed fold 12\n",
      "Epoch 0: Train Loss: 0.7062068432569504, Validation Loss: 0.698301374912262\n",
      "Epoch 1: Train Loss: 0.7680511772632599, Validation Loss: 0.6958860754966736\n",
      "Epoch 2: Train Loss: 0.7078532427549362, Validation Loss: 0.6933941841125488\n",
      "Epoch 3: Train Loss: 0.6794155836105347, Validation Loss: 0.6911373138427734\n",
      "Epoch 4: Train Loss: 0.7406742870807648, Validation Loss: 0.6891362071037292\n",
      "Epoch 5: Train Loss: 0.7432913482189178, Validation Loss: 0.6868508458137512\n",
      "Epoch 6: Train Loss: 0.7016296684741974, Validation Loss: 0.684770405292511\n",
      "Epoch 7: Train Loss: 0.6825564056634903, Validation Loss: 0.6839208602905273\n",
      "Epoch 8: Train Loss: 0.6767082810401917, Validation Loss: 0.6831297278404236\n",
      "Epoch 9: Train Loss: 0.6789550334215164, Validation Loss: 0.6828100085258484\n",
      "Epoch 10: Train Loss: 0.7435005009174347, Validation Loss: 0.6840556859970093\n",
      "Epoch 11: Train Loss: 0.6878836452960968, Validation Loss: 0.6839767694473267\n",
      "Epoch 12: Train Loss: 0.6920590549707413, Validation Loss: 0.6856435537338257\n",
      "Epoch 13: Train Loss: 0.6469806581735611, Validation Loss: 0.6870125532150269\n",
      "Epoch 14: Train Loss: 0.6886723041534424, Validation Loss: 0.686363697052002\n",
      "Epoch 15: Train Loss: 0.6645920872688293, Validation Loss: 0.685041606426239\n",
      "Epoch 16: Train Loss: 0.650220587849617, Validation Loss: 0.6834582686424255\n",
      "Epoch 17: Train Loss: 0.6658408939838409, Validation Loss: 0.6827812790870667\n",
      "Epoch 18: Train Loss: 0.6300850212574005, Validation Loss: 0.6824113726615906\n",
      "Epoch 19: Train Loss: 0.6873806118965149, Validation Loss: 0.6824284791946411\n",
      "Epoch 20: Train Loss: 0.6733463704586029, Validation Loss: 0.6815449595451355\n",
      "Epoch 21: Train Loss: 0.7156094759702682, Validation Loss: 0.6821004152297974\n",
      "Epoch 22: Train Loss: 0.7021922618150711, Validation Loss: 0.6820054054260254\n",
      "Epoch 23: Train Loss: 0.6564882099628448, Validation Loss: 0.68344646692276\n",
      "Epoch 24: Train Loss: 0.6377177238464355, Validation Loss: 0.6848148703575134\n",
      "Epoch 25: Train Loss: 0.6783215701580048, Validation Loss: 0.6864906549453735\n",
      "Epoch 26: Train Loss: 0.7035709470510483, Validation Loss: 0.6873083114624023\n",
      "Epoch 27: Train Loss: 0.6584731787443161, Validation Loss: 0.6874589920043945\n",
      "Epoch 28: Train Loss: 0.7033075094223022, Validation Loss: 0.6875852942466736\n",
      "Epoch 29: Train Loss: 0.6729412227869034, Validation Loss: 0.6876077055931091\n",
      "Epoch 30: Train Loss: 0.6842599660158157, Validation Loss: 0.6890466213226318\n",
      "Epoch 31: Train Loss: 0.6640079021453857, Validation Loss: 0.6929947733879089\n",
      "Epoch 32: Train Loss: 0.6894489526748657, Validation Loss: 0.6944177150726318\n",
      "Epoch 33: Train Loss: 0.6757399141788483, Validation Loss: 0.694591224193573\n",
      "Epoch 34: Train Loss: 0.6188541948795319, Validation Loss: 0.6926219463348389\n",
      "Epoch 35: Train Loss: 0.6292255520820618, Validation Loss: 0.6918848156929016\n",
      "Epoch 36: Train Loss: 0.6532544046640396, Validation Loss: 0.6909404993057251\n",
      "Epoch 37: Train Loss: 0.6229947507381439, Validation Loss: 0.6903163194656372\n",
      "Epoch 38: Train Loss: 0.649805948138237, Validation Loss: 0.6902493238449097\n",
      "Epoch 39: Train Loss: 0.6186278760433197, Validation Loss: 0.690201461315155\n",
      "Epoch 40: Train Loss: 0.6705597639083862, Validation Loss: 0.6904948949813843\n",
      "Epoch 41: Train Loss: 0.6452521085739136, Validation Loss: 0.6912876963615417\n",
      "Epoch 42: Train Loss: 0.6666327565908432, Validation Loss: 0.6912336349487305\n",
      "Epoch 43: Train Loss: 0.664432942867279, Validation Loss: 0.687548041343689\n",
      "Epoch 44: Train Loss: 0.6256541907787323, Validation Loss: 0.6854351758956909\n",
      "Epoch 45: Train Loss: 0.6254339069128036, Validation Loss: 0.6855224370956421\n",
      "Epoch 46: Train Loss: 0.616313561797142, Validation Loss: 0.6863128542900085\n",
      "Epoch 47: Train Loss: 0.6464281380176544, Validation Loss: 0.6865001916885376\n",
      "Epoch 48: Train Loss: 0.6117754429578781, Validation Loss: 0.6865515112876892\n",
      "Epoch 49: Train Loss: 0.6193108558654785, Validation Loss: 0.6865402460098267\n",
      "Epoch 50: Train Loss: 0.622914120554924, Validation Loss: 0.6844333410263062\n",
      "Epoch 51: Train Loss: 0.5902882069349289, Validation Loss: 0.6865634322166443\n",
      "Epoch 52: Train Loss: 0.6250647157430649, Validation Loss: 0.6882184743881226\n",
      "Epoch 53: Train Loss: 0.6422172337770462, Validation Loss: 0.6874749660491943\n",
      "Epoch 54: Train Loss: 0.5946753323078156, Validation Loss: 0.6835459470748901\n",
      "Epoch 55: Train Loss: 0.5881998538970947, Validation Loss: 0.6822630763053894\n",
      "Epoch 56: Train Loss: 0.600251168012619, Validation Loss: 0.681329607963562\n",
      "Epoch 57: Train Loss: 0.5986242592334747, Validation Loss: 0.6808617115020752\n",
      "Epoch 58: Train Loss: 0.5874845534563065, Validation Loss: 0.6808218955993652\n",
      "Epoch 59: Train Loss: 0.5997421592473984, Validation Loss: 0.6807615160942078\n",
      "Epoch 60: Train Loss: 0.6419506669044495, Validation Loss: 0.6699225306510925\n",
      "Epoch 61: Train Loss: 0.5910695940256119, Validation Loss: 0.6594355702400208\n",
      "Epoch 62: Train Loss: 0.6172005832195282, Validation Loss: 0.6514424085617065\n",
      "Epoch 63: Train Loss: 0.5851740315556526, Validation Loss: 0.6437684893608093\n",
      "Epoch 64: Train Loss: 0.5894807577133179, Validation Loss: 0.6358638405799866\n",
      "Epoch 65: Train Loss: 0.6476946026086807, Validation Loss: 0.6307414174079895\n",
      "Epoch 66: Train Loss: 0.6134444326162338, Validation Loss: 0.627662718296051\n",
      "Epoch 67: Train Loss: 0.5543171316385269, Validation Loss: 0.6265828013420105\n",
      "Epoch 68: Train Loss: 0.5769412368535995, Validation Loss: 0.6264872550964355\n",
      "Epoch 69: Train Loss: 0.5536226332187653, Validation Loss: 0.6265920400619507\n",
      "Epoch 70: Train Loss: 0.5941787958145142, Validation Loss: 0.6389949321746826\n",
      "Epoch 71: Train Loss: 0.5772670581936836, Validation Loss: 0.6524438858032227\n",
      "Epoch 72: Train Loss: 0.5183447822928429, Validation Loss: 0.6568690538406372\n",
      "Epoch 73: Train Loss: 0.5424342602491379, Validation Loss: 0.6561062335968018\n",
      "Epoch 74: Train Loss: 0.5812491625547409, Validation Loss: 0.6483244299888611\n",
      "Epoch 75: Train Loss: 0.5961861461400986, Validation Loss: 0.6391525864601135\n",
      "Epoch 76: Train Loss: 0.5863872319459915, Validation Loss: 0.6384145021438599\n",
      "Epoch 77: Train Loss: 0.5466185882687569, Validation Loss: 0.6385384798049927\n",
      "Epoch 78: Train Loss: 0.5648884996771812, Validation Loss: 0.6391070485115051\n",
      "Epoch 79: Train Loss: 0.5217747315764427, Validation Loss: 0.6393744349479675\n",
      "Epoch 80: Train Loss: 0.5649538636207581, Validation Loss: 0.6452277898788452\n",
      "Epoch 81: Train Loss: 0.516950398683548, Validation Loss: 0.6446529626846313\n",
      "Epoch 82: Train Loss: 0.5350142940878868, Validation Loss: 0.6386383771896362\n",
      "Epoch 83: Train Loss: 0.5180114433169365, Validation Loss: 0.632882833480835\n",
      "Epoch 84: Train Loss: 0.5382577925920486, Validation Loss: 0.6303717494010925\n",
      "Epoch 85: Train Loss: 0.5265276655554771, Validation Loss: 0.6304206848144531\n",
      "Epoch 86: Train Loss: 0.5239241495728493, Validation Loss: 0.6278230547904968\n",
      "Epoch 87: Train Loss: 0.5202957391738892, Validation Loss: 0.6271171569824219\n",
      "Epoch 88: Train Loss: 0.5281095057725906, Validation Loss: 0.6267331838607788\n",
      "Epoch 89: Train Loss: 0.5601811632514, Validation Loss: 0.6263785362243652\n",
      "Epoch 90: Train Loss: 0.5327089056372643, Validation Loss: 0.6234102845191956\n",
      "Epoch 91: Train Loss: 0.5038088485598564, Validation Loss: 0.6109441518783569\n",
      "Epoch 92: Train Loss: 0.5180283486843109, Validation Loss: 0.6085401177406311\n",
      "Epoch 93: Train Loss: 0.5067195519804955, Validation Loss: 0.6108220219612122\n",
      "Epoch 94: Train Loss: 0.4587930589914322, Validation Loss: 0.6082260012626648\n",
      "Epoch 95: Train Loss: 0.5099570751190186, Validation Loss: 0.6074639558792114\n",
      "Epoch 96: Train Loss: 0.4842889979481697, Validation Loss: 0.6097682118415833\n",
      "Epoch 97: Train Loss: 0.4809439927339554, Validation Loss: 0.6112028956413269\n",
      "Epoch 98: Train Loss: 0.48052501678466797, Validation Loss: 0.6107478737831116\n",
      "Epoch 99: Train Loss: 0.4963454008102417, Validation Loss: 0.6103066802024841\n",
      "Epoch 100: Train Loss: 0.48553916066884995, Validation Loss: 0.601527750492096\n",
      "Epoch 101: Train Loss: 0.4613170474767685, Validation Loss: 0.6098818778991699\n",
      "Epoch 102: Train Loss: 0.4730900302529335, Validation Loss: 0.6047179102897644\n",
      "Epoch 103: Train Loss: 0.47313936054706573, Validation Loss: 0.5901947021484375\n",
      "Epoch 104: Train Loss: 0.5174121484160423, Validation Loss: 0.5873091220855713\n",
      "Epoch 105: Train Loss: 0.4595373868942261, Validation Loss: 0.5903713703155518\n",
      "Epoch 106: Train Loss: 0.4802607074379921, Validation Loss: 0.5966281890869141\n",
      "Epoch 107: Train Loss: 0.4517809897661209, Validation Loss: 0.5983184576034546\n",
      "Epoch 108: Train Loss: 0.44862447679042816, Validation Loss: 0.5975764393806458\n",
      "Epoch 109: Train Loss: 0.47458046674728394, Validation Loss: 0.5968526601791382\n",
      "Epoch 110: Train Loss: 0.47532030940055847, Validation Loss: 0.5873180031776428\n",
      "Epoch 111: Train Loss: 0.43615178018808365, Validation Loss: 0.57999187707901\n",
      "Epoch 112: Train Loss: 0.44924699515104294, Validation Loss: 0.5738525390625\n",
      "Epoch 113: Train Loss: 0.4437059387564659, Validation Loss: 0.5781391859054565\n",
      "Epoch 114: Train Loss: 0.4635946750640869, Validation Loss: 0.565690279006958\n",
      "Epoch 115: Train Loss: 0.41872796416282654, Validation Loss: 0.5720614194869995\n",
      "Epoch 116: Train Loss: 0.4381812810897827, Validation Loss: 0.5750611424446106\n",
      "Epoch 117: Train Loss: 0.42879030853509903, Validation Loss: 0.5763251781463623\n",
      "Epoch 118: Train Loss: 0.46204596012830734, Validation Loss: 0.5775319337844849\n",
      "Epoch 119: Train Loss: 0.411031574010849, Validation Loss: 0.5781794786453247\n",
      "Epoch 120: Train Loss: 0.4110276848077774, Validation Loss: 0.5751494765281677\n",
      "Epoch 121: Train Loss: 0.4103082939982414, Validation Loss: 0.5941712856292725\n",
      "Epoch 122: Train Loss: 0.419278584420681, Validation Loss: 0.5662750601768494\n",
      "Epoch 123: Train Loss: 0.39455752819776535, Validation Loss: 0.5815720558166504\n",
      "Epoch 124: Train Loss: 0.3851681500673294, Validation Loss: 0.5854700803756714\n",
      "Epoch 125: Train Loss: 0.3831939995288849, Validation Loss: 0.598129391670227\n",
      "Epoch 126: Train Loss: 0.3984064608812332, Validation Loss: 0.6008034348487854\n",
      "Epoch 127: Train Loss: 0.37103886157274246, Validation Loss: 0.6044560074806213\n",
      "Epoch 128: Train Loss: 0.36081676185131073, Validation Loss: 0.6066763997077942\n",
      "Epoch 129: Train Loss: 0.388523705303669, Validation Loss: 0.6082077622413635\n",
      "Epoch 130: Train Loss: 0.4144192114472389, Validation Loss: 0.6375779509544373\n",
      "Epoch 131: Train Loss: 0.3853619247674942, Validation Loss: 0.6560443639755249\n",
      "Epoch 132: Train Loss: 0.37999802827835083, Validation Loss: 0.6799396276473999\n",
      "Epoch 133: Train Loss: 0.3950331062078476, Validation Loss: 0.6753775477409363\n",
      "Epoch 134: Train Loss: 0.31664958223700523, Validation Loss: 0.6683249473571777\n",
      "Epoch 135: Train Loss: 0.36145390570163727, Validation Loss: 0.6792789101600647\n",
      "Epoch 136: Train Loss: 0.3598704859614372, Validation Loss: 0.6860368251800537\n",
      "Epoch 137: Train Loss: 0.3428225517272949, Validation Loss: 0.6870439648628235\n",
      "Epoch 138: Train Loss: 0.3354541137814522, Validation Loss: 0.690444827079773\n",
      "Epoch 139: Train Loss: 0.3215186670422554, Validation Loss: 0.6925491094589233\n",
      "Epoch 140: Train Loss: 0.32500220462679863, Validation Loss: 0.7310674786567688\n",
      "Epoch 141: Train Loss: 0.30513980984687805, Validation Loss: 0.8031235933303833\n",
      "Epoch 142: Train Loss: 0.3061685189604759, Validation Loss: 0.8750448226928711\n",
      "Epoch 143: Train Loss: 0.3341721221804619, Validation Loss: 0.940544068813324\n",
      "Epoch 144: Train Loss: 0.3071162700653076, Validation Loss: 0.9182726144790649\n",
      "Epoch 145: Train Loss: 0.2989092916250229, Validation Loss: 0.905879020690918\n",
      "Epoch 146: Train Loss: 0.30704525113105774, Validation Loss: 0.9030355215072632\n",
      "Epoch 147: Train Loss: 0.31179604679346085, Validation Loss: 0.9190080165863037\n",
      "Epoch 148: Train Loss: 0.26182422041893005, Validation Loss: 0.9386717081069946\n",
      "Epoch 149: Train Loss: 0.2686619833111763, Validation Loss: 0.9538024663925171\n",
      "Epoch 150: Train Loss: 0.2550698183476925, Validation Loss: 1.0076079368591309\n",
      "Epoch 151: Train Loss: 0.2768814191222191, Validation Loss: 1.091293454170227\n",
      "Epoch 152: Train Loss: 0.2848541736602783, Validation Loss: 1.094795823097229\n",
      "Epoch 153: Train Loss: 0.2905210182070732, Validation Loss: 1.130154013633728\n",
      "Epoch 154: Train Loss: 0.25920459255576134, Validation Loss: 1.2356071472167969\n",
      "Epoch 155: Train Loss: 0.2678740322589874, Validation Loss: 1.22187077999115\n",
      "Epoch 156: Train Loss: 0.245725117623806, Validation Loss: 1.1863136291503906\n",
      "Epoch 157: Train Loss: 0.24552977457642555, Validation Loss: 1.1825730800628662\n",
      "Epoch 158: Train Loss: 0.28509001433849335, Validation Loss: 1.1889324188232422\n",
      "Epoch 159: Train Loss: 0.2201857529580593, Validation Loss: 1.2000679969787598\n",
      "Epoch 160: Train Loss: 0.220380000770092, Validation Loss: 1.2470513582229614\n",
      "Epoch 161: Train Loss: 0.23102163150906563, Validation Loss: 1.3072073459625244\n",
      "Epoch 162: Train Loss: 0.2349158301949501, Validation Loss: 1.3598119020462036\n",
      "Epoch 163: Train Loss: 0.2357432134449482, Validation Loss: 1.5127127170562744\n",
      "Epoch 164: Train Loss: 0.23143500089645386, Validation Loss: 1.6474344730377197\n",
      "Epoch 165: Train Loss: 0.2424798384308815, Validation Loss: 1.5324064493179321\n",
      "Epoch 166: Train Loss: 0.21959540620446205, Validation Loss: 1.474631428718567\n",
      "Epoch 167: Train Loss: 0.21919818595051765, Validation Loss: 1.4764471054077148\n",
      "Epoch 168: Train Loss: 0.23362648487091064, Validation Loss: 1.4995155334472656\n",
      "Epoch 169: Train Loss: 0.23016050085425377, Validation Loss: 1.5149286985397339\n",
      "Epoch 170: Train Loss: 0.22237912192940712, Validation Loss: 1.5649431943893433\n",
      "Epoch 171: Train Loss: 0.2183888666331768, Validation Loss: 1.5818290710449219\n",
      "Epoch 172: Train Loss: 0.20747731998562813, Validation Loss: 1.6052496433258057\n",
      "Epoch 173: Train Loss: 0.23863422125577927, Validation Loss: 1.6686961650848389\n",
      "Epoch 174: Train Loss: 0.19425703212618828, Validation Loss: 1.6630274057388306\n",
      "Epoch 175: Train Loss: 0.18723617494106293, Validation Loss: 1.670363187789917\n",
      "Epoch 176: Train Loss: 0.20639963820576668, Validation Loss: 1.7328852415084839\n",
      "Epoch 177: Train Loss: 0.19431564956903458, Validation Loss: 1.8041388988494873\n",
      "Epoch 178: Train Loss: 0.17666730657219887, Validation Loss: 1.837624430656433\n",
      "Epoch 179: Train Loss: 0.18362445756793022, Validation Loss: 1.8448314666748047\n",
      "Epoch 180: Train Loss: 0.2073281705379486, Validation Loss: 1.9889293909072876\n",
      "Epoch 181: Train Loss: 0.19682492688298225, Validation Loss: 1.9113320112228394\n",
      "Epoch 182: Train Loss: 0.19567695632576942, Validation Loss: 1.8888400793075562\n",
      "Epoch 183: Train Loss: 0.17954183742403984, Validation Loss: 1.8801711797714233\n",
      "Epoch 184: Train Loss: 0.1984865702688694, Validation Loss: 1.937398910522461\n",
      "Epoch 185: Train Loss: 0.1811153907328844, Validation Loss: 2.0023550987243652\n",
      "Epoch 186: Train Loss: 0.16244010999798775, Validation Loss: 2.052966356277466\n",
      "Epoch 187: Train Loss: 0.19537178799510002, Validation Loss: 2.0533640384674072\n",
      "Epoch 188: Train Loss: 0.16284402459859848, Validation Loss: 2.054089307785034\n",
      "Epoch 189: Train Loss: 0.17611138336360455, Validation Loss: 2.0429656505584717\n",
      "Epoch 190: Train Loss: 0.1608801856637001, Validation Loss: 2.0707509517669678\n",
      "Epoch 191: Train Loss: 0.1782715916633606, Validation Loss: 2.160041570663452\n",
      "Epoch 192: Train Loss: 0.20952990651130676, Validation Loss: 2.2575037479400635\n",
      "Epoch 193: Train Loss: 0.19537093490362167, Validation Loss: 2.0732810497283936\n",
      "Epoch 194: Train Loss: 0.16355730593204498, Validation Loss: 2.0468361377716064\n",
      "Epoch 195: Train Loss: 0.16078868694603443, Validation Loss: 2.1652586460113525\n",
      "Epoch 196: Train Loss: 0.14558912813663483, Validation Loss: 2.249967575073242\n",
      "Epoch 197: Train Loss: 0.17904136329889297, Validation Loss: 2.2132608890533447\n",
      "Epoch 198: Train Loss: 0.15510845370590687, Validation Loss: 2.2261369228363037\n",
      "Epoch 199: Train Loss: 0.14098056964576244, Validation Loss: 2.2388250827789307\n",
      "Epoch 200: Train Loss: 0.1440676748752594, Validation Loss: 2.359849214553833\n",
      "Epoch 201: Train Loss: 0.17752942815423012, Validation Loss: 2.4608843326568604\n",
      "Epoch 202: Train Loss: 0.15076982788741589, Validation Loss: 2.559142589569092\n",
      "Epoch 203: Train Loss: 0.14029543288052082, Validation Loss: 2.3993401527404785\n",
      "Epoch 204: Train Loss: 0.14480244740843773, Validation Loss: 2.3521158695220947\n",
      "Epoch 205: Train Loss: 0.13872136920690536, Validation Loss: 2.464538097381592\n",
      "Epoch 206: Train Loss: 0.14707311615347862, Validation Loss: 2.5148749351501465\n",
      "Epoch 207: Train Loss: 0.14959534257650375, Validation Loss: 2.492777109146118\n",
      "Epoch 208: Train Loss: 0.14369846135377884, Validation Loss: 2.4955124855041504\n",
      "Epoch 209: Train Loss: 0.14897355064749718, Validation Loss: 2.5070273876190186\n",
      "Epoch 210: Train Loss: 0.15742063149809837, Validation Loss: 2.48634672164917\n",
      "Epoch 211: Train Loss: 0.13770882785320282, Validation Loss: 2.693284034729004\n",
      "Epoch 212: Train Loss: 0.155985489487648, Validation Loss: 2.6659014225006104\n",
      "Epoch 213: Train Loss: 0.16399001702666283, Validation Loss: 2.5511019229888916\n",
      "Early stopping at epoch 214\n",
      "Completed fold 13\n",
      "Accuracy: 0.5034965034965035,Precision: 0.559322033898305, Recall: 0.4230769230769231, F1-score: 0.48175182481751827, AUC: 0.5115384615384615\n",
      "Confusion Matrix:\n",
      "[[39 26]\n",
      " [45 33]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "model_save_dir = r'C:\\Users\\User\\Documents\\Lie detect data\\Model'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Define a function to load and pad data\n",
    "def load_data(data_dir, max_length):\n",
    "    subject_data = {'lie': {}, 'truth': {}}\n",
    "    \n",
    "    file_list = os.listdir(data_dir)\n",
    "    \n",
    "    # Parse the files into respective subjects\n",
    "    for file in file_list:\n",
    "        with open(os.path.join(data_dir, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        label_type = 'truth' if 'truth' in file else 'lie'\n",
    "        subj_id = int(file.split('_')[-1].split('.')[0])\n",
    "        \n",
    "        if label_type == 'lie':\n",
    "            subject_key = (subj_id - 1) // 5 + 1  # Mapping 1-5 to subj1, 6-10 to subj2, etc.\n",
    "        else:\n",
    "            subject_key = (subj_id - 1) // 6 + 1  # Mapping 1-6 to subj1, 7-12 to subj2, etc.\n",
    "        \n",
    "        if subject_key not in subject_data[label_type]:\n",
    "            subject_data[label_type][subject_key] = []\n",
    "        \n",
    "        # Pad or truncate the data to match max_length\n",
    "        if data.shape[1] > max_length:\n",
    "            processed_data = data[:, :max_length]  # Truncate if it exceeds max_length\n",
    "        else:\n",
    "            processed_data = np.zeros((data.shape[0], max_length))\n",
    "            processed_data[:, :data.shape[1]] = data  # Pad if it is shorter than max_length\n",
    "        \n",
    "        subject_data[label_type][subject_key].append(processed_data)\n",
    "    \n",
    "    return subject_data\n",
    "\n",
    "# Load dataset and pad/truncate the data\n",
    "data_dir = \"C:\\\\Users\\\\User\\\\Documents\\\\Lie detect data\\\\7M_EEGData\"\n",
    "max_length = 3750  # Define maximum length for padding\n",
    "subject_data = load_data(data_dir, max_length)\n",
    "\n",
    "# Define dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure the data is reshaped to [1, Chans, Samples]\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# Define EEGNet model\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int, kernel_size: tuple, padding: tuple = 0):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.depthwise_conv = nn.Conv2d(self.c_in, self.c_in, kernel_size=self.kernel_size,\n",
    "                                        padding=self.padding, groups=self.c_in)\n",
    "        self.conv2d_1x1 = nn.Conv2d(self.c_in, self.c_out, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y = self.depthwise_conv(x)\n",
    "        y = self.conv2d_1x1(y)\n",
    "        return y\n",
    "\n",
    "class SeparableConv1d(nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int, kernel_size: tuple, padding: tuple = 0):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.depthwise_conv = nn.Conv1d(self.c_in, self.c_in, kernel_size=self.kernel_size,\n",
    "                                        padding=self.padding, groups=self.c_in)\n",
    "        self.conv1d_1x1 = nn.Conv1d(self.c_in, self.c_out, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y = self.depthwise_conv(x)\n",
    "        y = self.conv1d_1x1(y)\n",
    "        return y\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, nb_classes: int = 2, Chans: int = 65, Samples: int = 3750,\n",
    "                 dropoutRate: float = 0.7, kernLength: int = 63,\n",
    "                 F1:int = 8, D:int = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        F2 = F1 * D\n",
    "\n",
    "        # Make kernel size and odd number\n",
    "        try:\n",
    "            assert kernLength % 2 != 0\n",
    "        except AssertionError:\n",
    "            raise ValueError(\"ERROR: kernLength must be odd number\")\n",
    "\n",
    "        # In: (B, Chans, Samples, 1)\n",
    "        # Out: (B, F1, Samples, 1)\n",
    "        self.conv1 = nn.Conv1d(Chans, F1, kernLength, padding=(kernLength // 2))\n",
    "        self.bn1 = nn.BatchNorm1d(F1) # (B, F1, Samples, 1)\n",
    "        # In: (B, F1, Samples, 1)\n",
    "        # Out: (B, F2, Samples - Chans + 1, 1)\n",
    "        self.conv2 = nn.Conv1d(F1, F2, Chans, groups=F1)\n",
    "        self.bn2 = nn.BatchNorm1d(F2) # (B, F2, Samples - Chans + 1, 1)\n",
    "        # In: (B, F2, Samples - Chans + 1, 1)\n",
    "        # Out: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        self.avg_pool = nn.AvgPool1d(4)\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "\n",
    "        # In: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        # Out: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        self.conv3 = SeparableConv1d(F2, F2, kernel_size=31, padding=15)\n",
    "        self.bn3 = nn.BatchNorm1d(F2)\n",
    "        # In: (B, F2, (Samples - Chans + 1) / 4, 1)\n",
    "        # Out: (B, F2, (Samples - Chans + 1) / 32, 1)\n",
    "        self.avg_pool2 = nn.AvgPool1d(8)\n",
    "        # In: (B, F2 *  (Samples - Chans + 1) / 32)\n",
    "        self.fc = nn.Linear(F2 * ((Samples - Chans + 1) // 32), nb_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Block 1\n",
    "        y1 = self.conv1(x)\n",
    "        #print(\"conv1: \", y1.shape)\n",
    "        y1 = self.bn1(y1)\n",
    "        #print(\"bn1: \", y1.shape)\n",
    "        y1 = self.conv2(y1)\n",
    "        #print(\"conv2\", y1.shape)\n",
    "        y1 = F.relu(self.bn2(y1))\n",
    "        #print(\"bn2\", y1.shape)\n",
    "        y1 = self.avg_pool(y1)\n",
    "        #print(\"avg_pool\", y1.shape)\n",
    "        y1 = self.dropout(y1)\n",
    "        #print(\"dropout\", y1.shape)\n",
    "\n",
    "        # Block 2\n",
    "        y2 = self.conv3(y1)\n",
    "        #print(\"conv3\", y2.shape)\n",
    "        y2 = F.relu(self.bn3(y2))\n",
    "        #print(\"bn3\", y2.shape)\n",
    "        y2 = self.avg_pool2(y2)\n",
    "        #print(\"avg_pool2\", y2.shape)\n",
    "        y2 = self.dropout(y2)\n",
    "        #print(\"dropout\", y2.shape)\n",
    "        y2 = torch.flatten(y2, 1)\n",
    "        #print(\"flatten\", y2.shape)\n",
    "        y2 = self.fc(y2)\n",
    "        #print(\"fc\", y2.shape)\n",
    "\n",
    "        return y2\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_and_evaluate(train_loader, val_loader, y_train):\n",
    "    model = EEGNet().to(device)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-2)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "    num_epochs = 500\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 100\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "            fold_model_path = os.path.join(model_save_dir, f'revise_model_fold_{fold_idx}.pth')\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': best_val_loss,\n",
    "            }, fold_model_path)\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Initialize the cross-validation\n",
    "subject_ids = list(range(1, 14))  # Since there are 13 subjects for lie and truth\n",
    "random.shuffle(subject_ids)  # Shuffle to ensure random selection\n",
    "\n",
    "# Initialize arrays to store all labels and predictions\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx in range(13):\n",
    "    # Split subjects into test and training sets\n",
    "    test_lie_subject = subject_ids[fold_idx]\n",
    "    test_truth_subject = subject_ids[(fold_idx + 1) % 13]  # Cycling to get truth subject\n",
    "\n",
    "    # Prepare training and test data\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for subject_id in subject_ids:\n",
    "        if subject_id == test_lie_subject:\n",
    "            X_test.extend(subject_data['lie'][subject_id])\n",
    "            y_test.extend([0] * len(subject_data['lie'][subject_id]))\n",
    "        elif subject_id == test_truth_subject:\n",
    "            X_test.extend(subject_data['truth'][subject_id])\n",
    "            y_test.extend([1] * len(subject_data['truth'][subject_id]))\n",
    "        else:\n",
    "            X_train.extend(subject_data['lie'][subject_id])\n",
    "            y_train.extend([0] * len(subject_data['lie'][subject_id]))\n",
    "            X_train.extend(subject_data['truth'][subject_id])\n",
    "            y_train.extend([1] * len(subject_data['truth'][subject_id]))\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    # Normalize data using scaler fitted on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_test = scaler.transform(X_test.reshape(X_test.shape[0], -1))\n",
    "    X_train = X_train.reshape(-1, 65, max_length)\n",
    "    X_test = X_test.reshape(-1, 65, max_length)\n",
    "\n",
    "    # Save the scaler to a file\n",
    "    with open(r'C:\\Users\\User\\Documents\\Lie detect data\\Model\\RevisedEEGNet_scaler_{}.pkl'.format(fold_idx), 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = EEGDataset(X_train, y_train)\n",
    "    test_dataset = EEGDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    model = train_and_evaluate(train_loader, test_loader, y_train)\n",
    "\n",
    "    # Visualize the filters from the first convolutional layer\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    print(f'Completed fold {fold_idx + 1}')\n",
    "\n",
    "\n",
    "# Calculate additional metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "auc = roc_auc_score(all_labels, all_predictions)\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy},Precision: {precision}, Recall: {recall}, F1-score: {f1}, AUC: {auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae639325-1800-4763-ad37-b5931e4391c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abbb309-50a0-43fa-bcc0-05e6d62e6cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
